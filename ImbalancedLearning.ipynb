{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier, RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from tffm import TFFMClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from sklearn import preprocessing\n",
    "from random import random\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "data = pd.read_csv(r'...\\data_net.csv', sep = ';')\n",
    "data.drop('Unnamed: 0', axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Browsname</th>\n",
       "      <th>Searcher</th>\n",
       "      <th>TypeCon</th>\n",
       "      <th>Country</th>\n",
       "      <th>deviceType</th>\n",
       "      <th>Model</th>\n",
       "      <th>ModelCompany</th>\n",
       "      <th>NewID</th>\n",
       "      <th>Reversed</th>\n",
       "      <th>Type</th>\n",
       "      <th>System</th>\n",
       "      <th>FirstSearcher</th>\n",
       "      <th>Version</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>TimeSpent</th>\n",
       "      <th>InternalCode</th>\n",
       "      <th>id_transf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chrome</td>\n",
       "      <td>Google</td>\n",
       "      <td>wi-fi</td>\n",
       "      <td>ita</td>\n",
       "      <td>SmartPhone</td>\n",
       "      <td>Nokia 2240</td>\n",
       "      <td>Nokia</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>mobile</td>\n",
       "      <td>Android</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>5.0</td>\n",
       "      <td>male</td>\n",
       "      <td>21</td>\n",
       "      <td>1.020</td>\n",
       "      <td>fergie</td>\n",
       "      <td>25465885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Firefox</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>cable</td>\n",
       "      <td>fra</td>\n",
       "      <td>SmartPhone</td>\n",
       "      <td>Meizu 4 pro</td>\n",
       "      <td>Meizu</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>TV</td>\n",
       "      <td>Android</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>4.2</td>\n",
       "      <td>female</td>\n",
       "      <td>34</td>\n",
       "      <td>0.123</td>\n",
       "      <td>krag</td>\n",
       "      <td>65458971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chrome</td>\n",
       "      <td>Yandex</td>\n",
       "      <td>wi-fi</td>\n",
       "      <td>rus</td>\n",
       "      <td>SmartPhone</td>\n",
       "      <td>Iphone 6</td>\n",
       "      <td>Iphone</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>mobile</td>\n",
       "      <td>Ios</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>9.0</td>\n",
       "      <td>male</td>\n",
       "      <td>53</td>\n",
       "      <td>0.140</td>\n",
       "      <td>leslie</td>\n",
       "      <td>21547895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IE</td>\n",
       "      <td>Google</td>\n",
       "      <td>cable</td>\n",
       "      <td>usa</td>\n",
       "      <td>SmartPhone</td>\n",
       "      <td>Iphone X</td>\n",
       "      <td>Iphone</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>web</td>\n",
       "      <td>Ios</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>9.0</td>\n",
       "      <td>male</td>\n",
       "      <td>30</td>\n",
       "      <td>0.010</td>\n",
       "      <td>fergie</td>\n",
       "      <td>32541568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IE</td>\n",
       "      <td>Google</td>\n",
       "      <td>wi-fi</td>\n",
       "      <td>usa</td>\n",
       "      <td>SmartPhone</td>\n",
       "      <td>Galaxy J4</td>\n",
       "      <td>SAMSUNG</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>TV</td>\n",
       "      <td>Android</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>5.5.1</td>\n",
       "      <td>female</td>\n",
       "      <td>15</td>\n",
       "      <td>1.100</td>\n",
       "      <td>frent</td>\n",
       "      <td>12023515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Browsname Searcher TypeCon Country  deviceType        Model ModelCompany  \\\n",
       "0    Chrome   Google   wi-fi     ita  SmartPhone   Nokia 2240        Nokia   \n",
       "1   Firefox    Yahoo   cable     fra  SmartPhone  Meizu 4 pro        Meizu   \n",
       "2    Chrome   Yandex   wi-fi     rus  SmartPhone     Iphone 6       Iphone   \n",
       "3        IE   Google   cable     usa  SmartPhone     Iphone X       Iphone   \n",
       "4        IE   Google   wi-fi     usa  SmartPhone    Galaxy J4      SAMSUNG   \n",
       "\n",
       "  NewID Reversed    Type   System FirstSearcher Version     Sex  Age  \\\n",
       "0   yes       no  mobile  Android         Yahoo     5.0    male   21   \n",
       "1   yes       no      TV  Android         Yahoo     4.2  female   34   \n",
       "2   yes      yes  mobile      Ios         Yahoo     9.0    male   53   \n",
       "3    no       no     web      Ios         Yahoo     9.0    male   30   \n",
       "4    no       no      TV  Android         Yahoo   5.5.1  female   15   \n",
       "\n",
       "   TimeSpent InternalCode  id_transf  \n",
       "0      1.020       fergie   25465885  \n",
       "1      0.123         krag   65458971  \n",
       "2      0.140       leslie   21547895  \n",
       "3      0.010       fergie   32541568  \n",
       "4      1.100        frent   12023515  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features & labels\n",
    "labels = data['Click']\n",
    "features = data.drop('Click', axis = 1)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dictionary of unique attributes' lists\n",
    "unique_attr = {}\n",
    "for attr in features.columns:\n",
    "    unique_attr[attr] = features[attr].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHot encoder\n",
    "encoder = preprocessing.OneHotEncoder(categories=[unique_attr[i] for i in unique_attr], sparse = False, handle_unknown='ignore')\n",
    "features['CONCAT'] = features.values.tolist()\n",
    "features['CONCAT'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time on encoding: 50.699 s\n"
     ]
    }
   ],
   "source": [
    "# encoding into the list\n",
    "t0 = time()\n",
    "features['ENCODED'] = [encoder.fit_transform([i]).flatten() for i in features['CONCAT']]\n",
    "print (\"time on encoding:\", round(time()-t0, 3), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the array with feature vectors\n",
    "features_list = [list(i) for i in features['ENCODED']]\n",
    "features_list_array = np.array(features_list)\n",
    "# create the array with label vector \n",
    "labels_list_array = np.array(labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_list_array, labels_list_array, random_state=35, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.98      0.96      9255\n",
      "         1.0       0.31      0.11      0.16       745\n",
      "\n",
      "   micro avg       0.92      0.92      0.92     10000\n",
      "   macro avg       0.62      0.54      0.56     10000\n",
      "weighted avg       0.89      0.92      0.90     10000\n",
      "\n",
      "[[9075  180]\n",
      " [ 665   80]]\n",
      "Accuracy is  91.55\n",
      "Time on model's work: 7.456 s\n",
      "====================================================================================================\n",
      "GradientBoostingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      1.00      0.96      9255\n",
      "         1.0       0.98      0.06      0.12       745\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     10000\n",
      "   macro avg       0.95      0.53      0.54     10000\n",
      "weighted avg       0.93      0.93      0.90     10000\n",
      "\n",
      "[[9254    1]\n",
      " [ 699   46]]\n",
      "Accuracy is  93.0\n",
      "Time on model's work: 476.008 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.98      0.96      9255\n",
      "         1.0       0.30      0.11      0.16       745\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     10000\n",
      "   macro avg       0.62      0.55      0.56     10000\n",
      "weighted avg       0.89      0.91      0.90     10000\n",
      "\n",
      "[[9065  190]\n",
      " [ 662   83]]\n",
      "Accuracy is  91.47999999999999\n",
      "Time on model's work: 14.262 s\n",
      "====================================================================================================\n",
      "AdaBoostClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      1.00      0.96      9255\n",
      "         1.0       0.98      0.06      0.11       745\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     10000\n",
      "   macro avg       0.95      0.53      0.54     10000\n",
      "weighted avg       0.93      0.93      0.90     10000\n",
      "\n",
      "[[9254    1]\n",
      " [ 700   45]]\n",
      "Accuracy is  92.99\n",
      "Time on model's work: 105.722 s\n",
      "====================================================================================================\n",
      "BaggingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.97      0.95      9255\n",
      "         1.0       0.30      0.13      0.19       745\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     10000\n",
      "   macro avg       0.62      0.55      0.57     10000\n",
      "weighted avg       0.89      0.91      0.90     10000\n",
      "\n",
      "[[9020  235]\n",
      " [ 645  100]]\n",
      "Accuracy is  91.2\n",
      "Time on model's work: 99.55 s\n",
      "====================================================================================================\n",
      "DecisionTreeClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.97      0.95      9255\n",
      "         1.0       0.28      0.17      0.21       745\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     10000\n",
      "   macro avg       0.61      0.57      0.58     10000\n",
      "weighted avg       0.89      0.91      0.90     10000\n",
      "\n",
      "[[8940  315]\n",
      " [ 620  125]]\n",
      "Accuracy is  90.64999999999999\n",
      "Time on model's work: 28.546 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.97      0.95      9255\n",
      "         1.0       0.27      0.14      0.19       745\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     10000\n",
      "   macro avg       0.60      0.56      0.57     10000\n",
      "weighted avg       0.88      0.91      0.89     10000\n",
      "\n",
      "[[8961  294]\n",
      " [ 638  107]]\n",
      "Accuracy is  90.68\n",
      "Time on model's work: 1155.684 s\n",
      "====================================================================================================\n",
      "[16:32:39] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:32:42] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:32:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:32:48] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:32:51] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:32:54] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:32:56] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:32:59] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:33:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:33:05] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:33:08] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:33:11] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:33:14] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:33:16] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:33:19] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:33:22] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:33:25] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:33:27] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:33:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:33:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:33:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:33:38] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:33:41] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:33:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:33:46] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:33:49] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:33:52] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:33:55] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:33:57] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:34:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:34:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:34:06] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:34:08] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:34:11] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:34:14] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:34:16] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:34:19] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:34:22] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:34:24] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:34:27] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:34:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:34:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:34:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:34:38] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:34:41] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:34:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:34:46] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:34:49] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:34:52] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:34:54] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:34:57] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:35:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:35:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:35:06] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:35:09] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:35:11] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:35:14] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:35:17] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:35:19] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:35:22] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:35:25] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:35:27] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:35:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:35:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:35:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:35:38] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:35:41] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:35:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:35:46] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:35:49] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:35:52] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:35:54] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:35:58] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:36:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:36:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:36:06] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:36:09] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:36:11] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:36:14] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:36:17] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:36:19] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:36:22] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:36:25] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:36:28] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:36:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:36:34] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:36:37] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:36:40] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:36:42] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:36:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:36:48] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:36:51] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:36:53] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:36:56] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:36:59] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:37:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:37:04] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:37:07] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:37:10] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[16:37:12] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "XGBClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      1.00      0.96      9255\n",
      "         1.0       1.00      0.06      0.11       745\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     10000\n",
      "   macro avg       0.96      0.53      0.54     10000\n",
      "weighted avg       0.93      0.93      0.90     10000\n",
      "\n",
      "[[9255    0]\n",
      " [ 700   45]]\n",
      "Accuracy is  93.0\n",
      "Time on model's work: 281.461 s\n",
      "====================================================================================================\n",
      "0:\tlearn: 0.6308511\ttotal: 433ms\tremaining: 7m 12s\n",
      "1:\tlearn: 0.5859325\ttotal: 717ms\tremaining: 5m 57s\n",
      "2:\tlearn: 0.5333418\ttotal: 1.09s\tremaining: 6m 2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3:\tlearn: 0.5014626\ttotal: 1.37s\tremaining: 5m 40s\n",
      "4:\tlearn: 0.4716411\ttotal: 1.64s\tremaining: 5m 26s\n",
      "5:\tlearn: 0.4325288\ttotal: 2s\tremaining: 5m 30s\n",
      "6:\tlearn: 0.4116941\ttotal: 2.25s\tremaining: 5m 20s\n",
      "7:\tlearn: 0.3921239\ttotal: 2.53s\tremaining: 5m 13s\n",
      "8:\tlearn: 0.3694596\ttotal: 2.92s\tremaining: 5m 21s\n",
      "9:\tlearn: 0.3555392\ttotal: 3.19s\tremaining: 5m 15s\n",
      "10:\tlearn: 0.3431922\ttotal: 3.46s\tremaining: 5m 11s\n",
      "11:\tlearn: 0.3237243\ttotal: 3.82s\tremaining: 5m 14s\n",
      "12:\tlearn: 0.3138143\ttotal: 4.09s\tremaining: 5m 10s\n",
      "13:\tlearn: 0.3056673\ttotal: 4.35s\tremaining: 5m 6s\n",
      "14:\tlearn: 0.2918893\ttotal: 4.73s\tremaining: 5m 10s\n",
      "15:\tlearn: 0.2853170\ttotal: 5.01s\tremaining: 5m 8s\n",
      "16:\tlearn: 0.2795128\ttotal: 5.28s\tremaining: 5m 5s\n",
      "17:\tlearn: 0.2695613\ttotal: 5.63s\tremaining: 5m 7s\n",
      "18:\tlearn: 0.2644742\ttotal: 5.91s\tremaining: 5m 4s\n",
      "19:\tlearn: 0.2608495\ttotal: 6.17s\tremaining: 5m 2s\n",
      "20:\tlearn: 0.2549968\ttotal: 6.52s\tremaining: 5m 3s\n",
      "21:\tlearn: 0.2522301\ttotal: 6.8s\tremaining: 5m 2s\n",
      "22:\tlearn: 0.2489211\ttotal: 7.16s\tremaining: 5m 4s\n",
      "23:\tlearn: 0.2466875\ttotal: 7.43s\tremaining: 5m 2s\n",
      "24:\tlearn: 0.2424273\ttotal: 7.77s\tremaining: 5m 3s\n",
      "25:\tlearn: 0.2379932\ttotal: 8.12s\tremaining: 5m 4s\n",
      "26:\tlearn: 0.2364429\ttotal: 8.4s\tremaining: 5m 2s\n",
      "27:\tlearn: 0.2351550\ttotal: 8.67s\tremaining: 5m\n",
      "28:\tlearn: 0.2337149\ttotal: 8.97s\tremaining: 5m\n",
      "29:\tlearn: 0.2309512\ttotal: 9.32s\tremaining: 5m 1s\n",
      "30:\tlearn: 0.2297114\ttotal: 9.63s\tremaining: 5m 1s\n",
      "31:\tlearn: 0.2286104\ttotal: 9.92s\tremaining: 5m\n",
      "32:\tlearn: 0.2264869\ttotal: 10.3s\tremaining: 5m 1s\n",
      "33:\tlearn: 0.2258348\ttotal: 10.6s\tremaining: 5m\n",
      "34:\tlearn: 0.2236786\ttotal: 10.9s\tremaining: 5m\n",
      "35:\tlearn: 0.2222905\ttotal: 11.3s\tremaining: 5m 1s\n",
      "36:\tlearn: 0.2208971\ttotal: 11.6s\tremaining: 5m 2s\n",
      "37:\tlearn: 0.2193843\ttotal: 12s\tremaining: 5m 2s\n",
      "38:\tlearn: 0.2187070\ttotal: 12.3s\tremaining: 5m 3s\n",
      "39:\tlearn: 0.2182897\ttotal: 12.6s\tremaining: 5m 2s\n",
      "40:\tlearn: 0.2179750\ttotal: 12.9s\tremaining: 5m 1s\n",
      "41:\tlearn: 0.2169453\ttotal: 13.2s\tremaining: 5m 1s\n",
      "42:\tlearn: 0.2165879\ttotal: 13.5s\tremaining: 5m 1s\n",
      "43:\tlearn: 0.2160702\ttotal: 13.8s\tremaining: 5m\n",
      "44:\tlearn: 0.2158448\ttotal: 14.1s\tremaining: 4m 59s\n",
      "45:\tlearn: 0.2148718\ttotal: 14.5s\tremaining: 5m\n",
      "46:\tlearn: 0.2146306\ttotal: 14.8s\tremaining: 5m\n",
      "47:\tlearn: 0.2143627\ttotal: 15.1s\tremaining: 4m 59s\n",
      "48:\tlearn: 0.2136961\ttotal: 15.5s\tremaining: 4m 59s\n",
      "49:\tlearn: 0.2134543\ttotal: 15.8s\tremaining: 5m 1s\n",
      "50:\tlearn: 0.2133467\ttotal: 16.1s\tremaining: 5m\n",
      "51:\tlearn: 0.2127817\ttotal: 16.5s\tremaining: 5m\n",
      "52:\tlearn: 0.2122245\ttotal: 16.8s\tremaining: 5m\n",
      "53:\tlearn: 0.2117841\ttotal: 17.2s\tremaining: 5m 1s\n",
      "54:\tlearn: 0.2116311\ttotal: 17.5s\tremaining: 5m\n",
      "55:\tlearn: 0.2115719\ttotal: 17.8s\tremaining: 4m 59s\n",
      "56:\tlearn: 0.2111458\ttotal: 18.2s\tremaining: 5m\n",
      "57:\tlearn: 0.2108087\ttotal: 18.5s\tremaining: 5m\n",
      "58:\tlearn: 0.2107588\ttotal: 18.8s\tremaining: 4m 59s\n",
      "59:\tlearn: 0.2107049\ttotal: 19.1s\tremaining: 4m 58s\n",
      "60:\tlearn: 0.2103613\ttotal: 19.4s\tremaining: 4m 59s\n",
      "61:\tlearn: 0.2102811\ttotal: 19.7s\tremaining: 4m 58s\n",
      "62:\tlearn: 0.2100128\ttotal: 20.1s\tremaining: 4m 59s\n",
      "63:\tlearn: 0.2097471\ttotal: 20.5s\tremaining: 5m\n",
      "64:\tlearn: 0.2094954\ttotal: 20.9s\tremaining: 5m\n",
      "65:\tlearn: 0.2094377\ttotal: 21.2s\tremaining: 4m 59s\n",
      "66:\tlearn: 0.2092017\ttotal: 21.6s\tremaining: 5m 1s\n",
      "67:\tlearn: 0.2090094\ttotal: 22s\tremaining: 5m 1s\n",
      "68:\tlearn: 0.2089860\ttotal: 22.3s\tremaining: 5m\n",
      "69:\tlearn: 0.2089530\ttotal: 22.6s\tremaining: 5m\n",
      "70:\tlearn: 0.2089399\ttotal: 22.9s\tremaining: 4m 59s\n",
      "71:\tlearn: 0.2089056\ttotal: 23.2s\tremaining: 4m 58s\n",
      "72:\tlearn: 0.2088863\ttotal: 23.5s\tremaining: 4m 58s\n",
      "73:\tlearn: 0.2088490\ttotal: 23.8s\tremaining: 4m 57s\n",
      "74:\tlearn: 0.2088423\ttotal: 24.1s\tremaining: 4m 56s\n",
      "75:\tlearn: 0.2087829\ttotal: 24.4s\tremaining: 4m 56s\n",
      "76:\tlearn: 0.2087166\ttotal: 24.7s\tremaining: 4m 56s\n",
      "77:\tlearn: 0.2085517\ttotal: 25.1s\tremaining: 4m 56s\n",
      "78:\tlearn: 0.2085238\ttotal: 25.4s\tremaining: 4m 55s\n",
      "79:\tlearn: 0.2085075\ttotal: 25.7s\tremaining: 4m 55s\n",
      "80:\tlearn: 0.2084792\ttotal: 25.9s\tremaining: 4m 54s\n",
      "81:\tlearn: 0.2084620\ttotal: 26.3s\tremaining: 4m 54s\n",
      "82:\tlearn: 0.2084536\ttotal: 26.6s\tremaining: 4m 53s\n",
      "83:\tlearn: 0.2084070\ttotal: 26.9s\tremaining: 4m 52s\n",
      "84:\tlearn: 0.2083572\ttotal: 27.2s\tremaining: 4m 52s\n",
      "85:\tlearn: 0.2083510\ttotal: 27.4s\tremaining: 4m 51s\n",
      "86:\tlearn: 0.2082677\ttotal: 27.8s\tremaining: 4m 51s\n",
      "87:\tlearn: 0.2081413\ttotal: 28.1s\tremaining: 4m 51s\n",
      "88:\tlearn: 0.2081317\ttotal: 28.4s\tremaining: 4m 50s\n",
      "89:\tlearn: 0.2081160\ttotal: 28.7s\tremaining: 4m 49s\n",
      "90:\tlearn: 0.2080147\ttotal: 29s\tremaining: 4m 49s\n",
      "91:\tlearn: 0.2079937\ttotal: 29.3s\tremaining: 4m 49s\n",
      "92:\tlearn: 0.2078940\ttotal: 29.7s\tremaining: 4m 49s\n",
      "93:\tlearn: 0.2078009\ttotal: 30.1s\tremaining: 4m 50s\n",
      "94:\tlearn: 0.2077940\ttotal: 30.4s\tremaining: 4m 49s\n",
      "95:\tlearn: 0.2077043\ttotal: 30.8s\tremaining: 4m 50s\n",
      "96:\tlearn: 0.2076908\ttotal: 31.2s\tremaining: 4m 50s\n",
      "97:\tlearn: 0.2076627\ttotal: 31.5s\tremaining: 4m 50s\n",
      "98:\tlearn: 0.2075800\ttotal: 31.9s\tremaining: 4m 50s\n",
      "99:\tlearn: 0.2075582\ttotal: 32.3s\tremaining: 4m 50s\n",
      "100:\tlearn: 0.2075537\ttotal: 32.6s\tremaining: 4m 49s\n",
      "101:\tlearn: 0.2075403\ttotal: 32.8s\tremaining: 4m 49s\n",
      "102:\tlearn: 0.2075094\ttotal: 33.2s\tremaining: 4m 48s\n",
      "103:\tlearn: 0.2074287\ttotal: 33.6s\tremaining: 4m 49s\n",
      "104:\tlearn: 0.2074001\ttotal: 33.9s\tremaining: 4m 48s\n",
      "105:\tlearn: 0.2073310\ttotal: 34.3s\tremaining: 4m 48s\n",
      "106:\tlearn: 0.2073183\ttotal: 34.5s\tremaining: 4m 48s\n",
      "107:\tlearn: 0.2073013\ttotal: 34.8s\tremaining: 4m 47s\n",
      "108:\tlearn: 0.2072839\ttotal: 35.1s\tremaining: 4m 46s\n",
      "109:\tlearn: 0.2072700\ttotal: 35.4s\tremaining: 4m 46s\n",
      "110:\tlearn: 0.2072579\ttotal: 35.7s\tremaining: 4m 45s\n",
      "111:\tlearn: 0.2072492\ttotal: 36s\tremaining: 4m 45s\n",
      "112:\tlearn: 0.2072405\ttotal: 36.2s\tremaining: 4m 44s\n",
      "113:\tlearn: 0.2072230\ttotal: 36.6s\tremaining: 4m 44s\n",
      "114:\tlearn: 0.2072139\ttotal: 36.8s\tremaining: 4m 43s\n",
      "115:\tlearn: 0.2071987\ttotal: 37.1s\tremaining: 4m 42s\n",
      "116:\tlearn: 0.2071251\ttotal: 37.5s\tremaining: 4m 42s\n",
      "117:\tlearn: 0.2071159\ttotal: 37.7s\tremaining: 4m 42s\n",
      "118:\tlearn: 0.2071070\ttotal: 38s\tremaining: 4m 41s\n",
      "119:\tlearn: 0.2070572\ttotal: 38.3s\tremaining: 4m 41s\n",
      "120:\tlearn: 0.2070406\ttotal: 38.6s\tremaining: 4m 40s\n",
      "121:\tlearn: 0.2070313\ttotal: 38.9s\tremaining: 4m 40s\n",
      "122:\tlearn: 0.2070124\ttotal: 39.2s\tremaining: 4m 39s\n",
      "123:\tlearn: 0.2069967\ttotal: 39.5s\tremaining: 4m 38s\n",
      "124:\tlearn: 0.2069891\ttotal: 39.7s\tremaining: 4m 38s\n",
      "125:\tlearn: 0.2069759\ttotal: 40s\tremaining: 4m 37s\n",
      "126:\tlearn: 0.2069726\ttotal: 40.3s\tremaining: 4m 36s\n",
      "127:\tlearn: 0.2069506\ttotal: 40.6s\tremaining: 4m 36s\n",
      "128:\tlearn: 0.2069415\ttotal: 40.9s\tremaining: 4m 35s\n",
      "129:\tlearn: 0.2069028\ttotal: 41.2s\tremaining: 4m 35s\n",
      "130:\tlearn: 0.2068630\ttotal: 41.6s\tremaining: 4m 35s\n",
      "131:\tlearn: 0.2068249\ttotal: 41.9s\tremaining: 4m 35s\n",
      "132:\tlearn: 0.2068205\ttotal: 42.1s\tremaining: 4m 34s\n",
      "133:\tlearn: 0.2068152\ttotal: 42.4s\tremaining: 4m 34s\n",
      "134:\tlearn: 0.2067806\ttotal: 42.7s\tremaining: 4m 33s\n",
      "135:\tlearn: 0.2067652\ttotal: 43s\tremaining: 4m 33s\n",
      "136:\tlearn: 0.2067517\ttotal: 43.2s\tremaining: 4m 32s\n",
      "137:\tlearn: 0.2067368\ttotal: 43.6s\tremaining: 4m 32s\n",
      "138:\tlearn: 0.2067289\ttotal: 43.9s\tremaining: 4m 31s\n",
      "139:\tlearn: 0.2066802\ttotal: 44.2s\tremaining: 4m 31s\n",
      "140:\tlearn: 0.2066738\ttotal: 44.5s\tremaining: 4m 30s\n",
      "141:\tlearn: 0.2066649\ttotal: 44.7s\tremaining: 4m 30s\n",
      "142:\tlearn: 0.2066449\ttotal: 45s\tremaining: 4m 29s\n",
      "143:\tlearn: 0.2066396\ttotal: 45.3s\tremaining: 4m 29s\n",
      "144:\tlearn: 0.2066328\ttotal: 45.6s\tremaining: 4m 28s\n",
      "145:\tlearn: 0.2066279\ttotal: 45.8s\tremaining: 4m 28s\n",
      "146:\tlearn: 0.2066204\ttotal: 46.1s\tremaining: 4m 27s\n",
      "147:\tlearn: 0.2066102\ttotal: 46.3s\tremaining: 4m 26s\n",
      "148:\tlearn: 0.2065927\ttotal: 46.6s\tremaining: 4m 26s\n",
      "149:\tlearn: 0.2065707\ttotal: 47s\tremaining: 4m 26s\n",
      "150:\tlearn: 0.2065669\ttotal: 47.3s\tremaining: 4m 25s\n",
      "151:\tlearn: 0.2065620\ttotal: 47.6s\tremaining: 4m 25s\n",
      "152:\tlearn: 0.2065560\ttotal: 47.8s\tremaining: 4m 24s\n",
      "153:\tlearn: 0.2065513\ttotal: 48.1s\tremaining: 4m 24s\n",
      "154:\tlearn: 0.2065487\ttotal: 48.4s\tremaining: 4m 23s\n",
      "155:\tlearn: 0.2065404\ttotal: 48.6s\tremaining: 4m 23s\n",
      "156:\tlearn: 0.2065314\ttotal: 48.9s\tremaining: 4m 22s\n",
      "157:\tlearn: 0.2065275\ttotal: 49.2s\tremaining: 4m 22s\n",
      "158:\tlearn: 0.2064969\ttotal: 49.5s\tremaining: 4m 22s\n",
      "159:\tlearn: 0.2064511\ttotal: 49.9s\tremaining: 4m 21s\n",
      "160:\tlearn: 0.2064400\ttotal: 50.2s\tremaining: 4m 21s\n",
      "161:\tlearn: 0.2064362\ttotal: 50.4s\tremaining: 4m 20s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162:\tlearn: 0.2064280\ttotal: 50.7s\tremaining: 4m 20s\n",
      "163:\tlearn: 0.2063871\ttotal: 51s\tremaining: 4m 20s\n",
      "164:\tlearn: 0.2063443\ttotal: 51.3s\tremaining: 4m 19s\n",
      "165:\tlearn: 0.2063376\ttotal: 51.6s\tremaining: 4m 19s\n",
      "166:\tlearn: 0.2063248\ttotal: 51.9s\tremaining: 4m 18s\n",
      "167:\tlearn: 0.2063194\ttotal: 52.1s\tremaining: 4m 18s\n",
      "168:\tlearn: 0.2063094\ttotal: 52.4s\tremaining: 4m 17s\n",
      "169:\tlearn: 0.2062956\ttotal: 52.7s\tremaining: 4m 17s\n",
      "170:\tlearn: 0.2062918\ttotal: 53s\tremaining: 4m 16s\n",
      "171:\tlearn: 0.2062742\ttotal: 53.2s\tremaining: 4m 16s\n",
      "172:\tlearn: 0.2062661\ttotal: 53.5s\tremaining: 4m 15s\n",
      "173:\tlearn: 0.2062595\ttotal: 53.8s\tremaining: 4m 15s\n",
      "174:\tlearn: 0.2062530\ttotal: 54.1s\tremaining: 4m 14s\n",
      "175:\tlearn: 0.2062067\ttotal: 54.4s\tremaining: 4m 14s\n",
      "176:\tlearn: 0.2061669\ttotal: 54.8s\tremaining: 4m 14s\n",
      "177:\tlearn: 0.2061613\ttotal: 55s\tremaining: 4m 14s\n",
      "178:\tlearn: 0.2061569\ttotal: 55.3s\tremaining: 4m 13s\n",
      "179:\tlearn: 0.2061521\ttotal: 55.5s\tremaining: 4m 12s\n",
      "180:\tlearn: 0.2061489\ttotal: 55.8s\tremaining: 4m 12s\n",
      "181:\tlearn: 0.2061022\ttotal: 56.1s\tremaining: 4m 12s\n",
      "182:\tlearn: 0.2060814\ttotal: 56.4s\tremaining: 4m 11s\n",
      "183:\tlearn: 0.2060708\ttotal: 56.7s\tremaining: 4m 11s\n",
      "184:\tlearn: 0.2060638\ttotal: 57s\tremaining: 4m 10s\n",
      "185:\tlearn: 0.2060505\ttotal: 57.2s\tremaining: 4m 10s\n",
      "186:\tlearn: 0.2060136\ttotal: 57.6s\tremaining: 4m 10s\n",
      "187:\tlearn: 0.2060013\ttotal: 57.9s\tremaining: 4m 10s\n",
      "188:\tlearn: 0.2059952\ttotal: 58.2s\tremaining: 4m 9s\n",
      "189:\tlearn: 0.2059850\ttotal: 58.4s\tremaining: 4m 9s\n",
      "190:\tlearn: 0.2059779\ttotal: 58.7s\tremaining: 4m 8s\n",
      "191:\tlearn: 0.2059654\ttotal: 59s\tremaining: 4m 8s\n",
      "192:\tlearn: 0.2059523\ttotal: 59.3s\tremaining: 4m 7s\n",
      "193:\tlearn: 0.2059413\ttotal: 59.5s\tremaining: 4m 7s\n",
      "194:\tlearn: 0.2059135\ttotal: 59.9s\tremaining: 4m 7s\n",
      "195:\tlearn: 0.2059039\ttotal: 1m\tremaining: 4m 7s\n",
      "196:\tlearn: 0.2058634\ttotal: 1m\tremaining: 4m 7s\n",
      "197:\tlearn: 0.2058543\ttotal: 1m\tremaining: 4m 6s\n",
      "198:\tlearn: 0.2058506\ttotal: 1m 1s\tremaining: 4m 6s\n",
      "199:\tlearn: 0.2058465\ttotal: 1m 1s\tremaining: 4m 5s\n",
      "200:\tlearn: 0.2058377\ttotal: 1m 1s\tremaining: 4m 5s\n",
      "201:\tlearn: 0.2058240\ttotal: 1m 2s\tremaining: 4m 4s\n",
      "202:\tlearn: 0.2058094\ttotal: 1m 2s\tremaining: 4m 4s\n",
      "203:\tlearn: 0.2057941\ttotal: 1m 2s\tremaining: 4m 4s\n",
      "204:\tlearn: 0.2057867\ttotal: 1m 3s\tremaining: 4m 4s\n",
      "205:\tlearn: 0.2057800\ttotal: 1m 3s\tremaining: 4m 4s\n",
      "206:\tlearn: 0.2057639\ttotal: 1m 3s\tremaining: 4m 3s\n",
      "207:\tlearn: 0.2057508\ttotal: 1m 3s\tremaining: 4m 3s\n",
      "208:\tlearn: 0.2057324\ttotal: 1m 4s\tremaining: 4m 3s\n",
      "209:\tlearn: 0.2057260\ttotal: 1m 4s\tremaining: 4m 2s\n",
      "210:\tlearn: 0.2057221\ttotal: 1m 4s\tremaining: 4m 2s\n",
      "211:\tlearn: 0.2057130\ttotal: 1m 5s\tremaining: 4m 2s\n",
      "212:\tlearn: 0.2057055\ttotal: 1m 5s\tremaining: 4m 1s\n",
      "213:\tlearn: 0.2056752\ttotal: 1m 5s\tremaining: 4m 1s\n",
      "214:\tlearn: 0.2056648\ttotal: 1m 6s\tremaining: 4m 1s\n",
      "215:\tlearn: 0.2056478\ttotal: 1m 6s\tremaining: 4m\n",
      "216:\tlearn: 0.2056384\ttotal: 1m 6s\tremaining: 4m\n",
      "217:\tlearn: 0.2056303\ttotal: 1m 6s\tremaining: 4m\n",
      "218:\tlearn: 0.2056233\ttotal: 1m 7s\tremaining: 3m 59s\n",
      "219:\tlearn: 0.2056134\ttotal: 1m 7s\tremaining: 3m 59s\n",
      "220:\tlearn: 0.2055991\ttotal: 1m 7s\tremaining: 3m 59s\n",
      "221:\tlearn: 0.2055649\ttotal: 1m 8s\tremaining: 3m 59s\n",
      "222:\tlearn: 0.2055343\ttotal: 1m 8s\tremaining: 3m 59s\n",
      "223:\tlearn: 0.2054564\ttotal: 1m 8s\tremaining: 3m 58s\n",
      "224:\tlearn: 0.2054401\ttotal: 1m 9s\tremaining: 3m 58s\n",
      "225:\tlearn: 0.2054366\ttotal: 1m 9s\tremaining: 3m 58s\n",
      "226:\tlearn: 0.2053936\ttotal: 1m 9s\tremaining: 3m 58s\n",
      "227:\tlearn: 0.2053364\ttotal: 1m 10s\tremaining: 3m 57s\n",
      "228:\tlearn: 0.2053328\ttotal: 1m 10s\tremaining: 3m 57s\n",
      "229:\tlearn: 0.2053219\ttotal: 1m 10s\tremaining: 3m 57s\n",
      "230:\tlearn: 0.2053075\ttotal: 1m 11s\tremaining: 3m 56s\n",
      "231:\tlearn: 0.2052654\ttotal: 1m 11s\tremaining: 3m 56s\n",
      "232:\tlearn: 0.2052159\ttotal: 1m 11s\tremaining: 3m 56s\n",
      "233:\tlearn: 0.2052003\ttotal: 1m 11s\tremaining: 3m 55s\n",
      "234:\tlearn: 0.2051904\ttotal: 1m 12s\tremaining: 3m 55s\n",
      "235:\tlearn: 0.2051760\ttotal: 1m 12s\tremaining: 3m 54s\n",
      "236:\tlearn: 0.2051633\ttotal: 1m 12s\tremaining: 3m 54s\n",
      "237:\tlearn: 0.2051160\ttotal: 1m 13s\tremaining: 3m 54s\n",
      "238:\tlearn: 0.2050928\ttotal: 1m 13s\tremaining: 3m 53s\n",
      "239:\tlearn: 0.2050770\ttotal: 1m 13s\tremaining: 3m 53s\n",
      "240:\tlearn: 0.2050462\ttotal: 1m 14s\tremaining: 3m 53s\n",
      "241:\tlearn: 0.2050369\ttotal: 1m 14s\tremaining: 3m 52s\n",
      "242:\tlearn: 0.2050267\ttotal: 1m 14s\tremaining: 3m 52s\n",
      "243:\tlearn: 0.2050220\ttotal: 1m 14s\tremaining: 3m 52s\n",
      "244:\tlearn: 0.2049909\ttotal: 1m 15s\tremaining: 3m 51s\n",
      "245:\tlearn: 0.2049795\ttotal: 1m 15s\tremaining: 3m 51s\n",
      "246:\tlearn: 0.2049605\ttotal: 1m 15s\tremaining: 3m 50s\n",
      "247:\tlearn: 0.2049412\ttotal: 1m 16s\tremaining: 3m 50s\n",
      "248:\tlearn: 0.2049302\ttotal: 1m 16s\tremaining: 3m 50s\n",
      "249:\tlearn: 0.2049194\ttotal: 1m 16s\tremaining: 3m 49s\n",
      "250:\tlearn: 0.2049075\ttotal: 1m 16s\tremaining: 3m 49s\n",
      "251:\tlearn: 0.2048739\ttotal: 1m 17s\tremaining: 3m 49s\n",
      "252:\tlearn: 0.2048659\ttotal: 1m 17s\tremaining: 3m 48s\n",
      "253:\tlearn: 0.2048313\ttotal: 1m 17s\tremaining: 3m 48s\n",
      "254:\tlearn: 0.2047994\ttotal: 1m 18s\tremaining: 3m 48s\n",
      "255:\tlearn: 0.2047867\ttotal: 1m 18s\tremaining: 3m 48s\n",
      "256:\tlearn: 0.2047708\ttotal: 1m 18s\tremaining: 3m 47s\n",
      "257:\tlearn: 0.2047372\ttotal: 1m 19s\tremaining: 3m 47s\n",
      "258:\tlearn: 0.2047266\ttotal: 1m 19s\tremaining: 3m 47s\n",
      "259:\tlearn: 0.2046814\ttotal: 1m 19s\tremaining: 3m 47s\n",
      "260:\tlearn: 0.2046669\ttotal: 1m 20s\tremaining: 3m 46s\n",
      "261:\tlearn: 0.2046324\ttotal: 1m 20s\tremaining: 3m 46s\n",
      "262:\tlearn: 0.2046035\ttotal: 1m 20s\tremaining: 3m 46s\n",
      "263:\tlearn: 0.2045938\ttotal: 1m 20s\tremaining: 3m 45s\n",
      "264:\tlearn: 0.2045835\ttotal: 1m 21s\tremaining: 3m 45s\n",
      "265:\tlearn: 0.2045736\ttotal: 1m 21s\tremaining: 3m 45s\n",
      "266:\tlearn: 0.2045629\ttotal: 1m 21s\tremaining: 3m 44s\n",
      "267:\tlearn: 0.2045302\ttotal: 1m 22s\tremaining: 3m 45s\n",
      "268:\tlearn: 0.2045166\ttotal: 1m 22s\tremaining: 3m 44s\n",
      "269:\tlearn: 0.2045085\ttotal: 1m 23s\tremaining: 3m 44s\n",
      "270:\tlearn: 0.2044927\ttotal: 1m 23s\tremaining: 3m 44s\n",
      "271:\tlearn: 0.2044829\ttotal: 1m 23s\tremaining: 3m 44s\n",
      "272:\tlearn: 0.2044699\ttotal: 1m 24s\tremaining: 3m 43s\n",
      "273:\tlearn: 0.2044568\ttotal: 1m 24s\tremaining: 3m 43s\n",
      "274:\tlearn: 0.2044440\ttotal: 1m 24s\tremaining: 3m 43s\n",
      "275:\tlearn: 0.2044377\ttotal: 1m 24s\tremaining: 3m 42s\n",
      "276:\tlearn: 0.2044280\ttotal: 1m 25s\tremaining: 3m 42s\n",
      "277:\tlearn: 0.2044084\ttotal: 1m 25s\tremaining: 3m 42s\n",
      "278:\tlearn: 0.2043672\ttotal: 1m 25s\tremaining: 3m 42s\n",
      "279:\tlearn: 0.2043500\ttotal: 1m 26s\tremaining: 3m 41s\n",
      "280:\tlearn: 0.2043390\ttotal: 1m 26s\tremaining: 3m 41s\n",
      "281:\tlearn: 0.2043304\ttotal: 1m 26s\tremaining: 3m 40s\n",
      "282:\tlearn: 0.2043040\ttotal: 1m 27s\tremaining: 3m 40s\n",
      "283:\tlearn: 0.2042804\ttotal: 1m 27s\tremaining: 3m 40s\n",
      "284:\tlearn: 0.2042562\ttotal: 1m 27s\tremaining: 3m 39s\n",
      "285:\tlearn: 0.2042336\ttotal: 1m 27s\tremaining: 3m 39s\n",
      "286:\tlearn: 0.2042278\ttotal: 1m 28s\tremaining: 3m 39s\n",
      "287:\tlearn: 0.2042166\ttotal: 1m 28s\tremaining: 3m 38s\n",
      "288:\tlearn: 0.2042033\ttotal: 1m 28s\tremaining: 3m 38s\n",
      "289:\tlearn: 0.2041731\ttotal: 1m 29s\tremaining: 3m 38s\n",
      "290:\tlearn: 0.2041087\ttotal: 1m 29s\tremaining: 3m 38s\n",
      "291:\tlearn: 0.2040773\ttotal: 1m 29s\tremaining: 3m 37s\n",
      "292:\tlearn: 0.2040641\ttotal: 1m 30s\tremaining: 3m 37s\n",
      "293:\tlearn: 0.2040512\ttotal: 1m 30s\tremaining: 3m 37s\n",
      "294:\tlearn: 0.2040260\ttotal: 1m 30s\tremaining: 3m 36s\n",
      "295:\tlearn: 0.2040042\ttotal: 1m 30s\tremaining: 3m 36s\n",
      "296:\tlearn: 0.2039954\ttotal: 1m 31s\tremaining: 3m 35s\n",
      "297:\tlearn: 0.2039815\ttotal: 1m 31s\tremaining: 3m 35s\n",
      "298:\tlearn: 0.2039704\ttotal: 1m 31s\tremaining: 3m 35s\n",
      "299:\tlearn: 0.2039621\ttotal: 1m 32s\tremaining: 3m 34s\n",
      "300:\tlearn: 0.2039495\ttotal: 1m 32s\tremaining: 3m 34s\n",
      "301:\tlearn: 0.2039397\ttotal: 1m 32s\tremaining: 3m 33s\n",
      "302:\tlearn: 0.2039182\ttotal: 1m 32s\tremaining: 3m 33s\n",
      "303:\tlearn: 0.2039049\ttotal: 1m 33s\tremaining: 3m 33s\n",
      "304:\tlearn: 0.2038975\ttotal: 1m 33s\tremaining: 3m 32s\n",
      "305:\tlearn: 0.2038896\ttotal: 1m 33s\tremaining: 3m 32s\n",
      "306:\tlearn: 0.2038797\ttotal: 1m 33s\tremaining: 3m 32s\n",
      "307:\tlearn: 0.2038724\ttotal: 1m 34s\tremaining: 3m 31s\n",
      "308:\tlearn: 0.2038645\ttotal: 1m 34s\tremaining: 3m 31s\n",
      "309:\tlearn: 0.2038336\ttotal: 1m 34s\tremaining: 3m 31s\n",
      "310:\tlearn: 0.2038181\ttotal: 1m 35s\tremaining: 3m 30s\n",
      "311:\tlearn: 0.2038037\ttotal: 1m 35s\tremaining: 3m 30s\n",
      "312:\tlearn: 0.2037820\ttotal: 1m 35s\tremaining: 3m 29s\n",
      "313:\tlearn: 0.2037656\ttotal: 1m 35s\tremaining: 3m 29s\n",
      "314:\tlearn: 0.2037580\ttotal: 1m 36s\tremaining: 3m 29s\n",
      "315:\tlearn: 0.2037502\ttotal: 1m 36s\tremaining: 3m 28s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316:\tlearn: 0.2037123\ttotal: 1m 36s\tremaining: 3m 28s\n",
      "317:\tlearn: 0.2036810\ttotal: 1m 37s\tremaining: 3m 28s\n",
      "318:\tlearn: 0.2036632\ttotal: 1m 37s\tremaining: 3m 27s\n",
      "319:\tlearn: 0.2036434\ttotal: 1m 37s\tremaining: 3m 27s\n",
      "320:\tlearn: 0.2036029\ttotal: 1m 38s\tremaining: 3m 27s\n",
      "321:\tlearn: 0.2035976\ttotal: 1m 38s\tremaining: 3m 26s\n",
      "322:\tlearn: 0.2035665\ttotal: 1m 38s\tremaining: 3m 26s\n",
      "323:\tlearn: 0.2035197\ttotal: 1m 39s\tremaining: 3m 26s\n",
      "324:\tlearn: 0.2035131\ttotal: 1m 39s\tremaining: 3m 26s\n",
      "325:\tlearn: 0.2034826\ttotal: 1m 39s\tremaining: 3m 26s\n",
      "326:\tlearn: 0.2034737\ttotal: 1m 39s\tremaining: 3m 25s\n",
      "327:\tlearn: 0.2034579\ttotal: 1m 40s\tremaining: 3m 25s\n",
      "328:\tlearn: 0.2034033\ttotal: 1m 40s\tremaining: 3m 25s\n",
      "329:\tlearn: 0.2033838\ttotal: 1m 40s\tremaining: 3m 25s\n",
      "330:\tlearn: 0.2033762\ttotal: 1m 41s\tremaining: 3m 24s\n",
      "331:\tlearn: 0.2033637\ttotal: 1m 41s\tremaining: 3m 24s\n",
      "332:\tlearn: 0.2033382\ttotal: 1m 41s\tremaining: 3m 23s\n",
      "333:\tlearn: 0.2033315\ttotal: 1m 42s\tremaining: 3m 23s\n",
      "334:\tlearn: 0.2033133\ttotal: 1m 42s\tremaining: 3m 23s\n",
      "335:\tlearn: 0.2033038\ttotal: 1m 42s\tremaining: 3m 22s\n",
      "336:\tlearn: 0.2032849\ttotal: 1m 42s\tremaining: 3m 22s\n",
      "337:\tlearn: 0.2032779\ttotal: 1m 43s\tremaining: 3m 22s\n",
      "338:\tlearn: 0.2032687\ttotal: 1m 43s\tremaining: 3m 21s\n",
      "339:\tlearn: 0.2032632\ttotal: 1m 43s\tremaining: 3m 21s\n",
      "340:\tlearn: 0.2032549\ttotal: 1m 43s\tremaining: 3m 20s\n",
      "341:\tlearn: 0.2032296\ttotal: 1m 44s\tremaining: 3m 20s\n",
      "342:\tlearn: 0.2032199\ttotal: 1m 44s\tremaining: 3m 20s\n",
      "343:\tlearn: 0.2032132\ttotal: 1m 44s\tremaining: 3m 19s\n",
      "344:\tlearn: 0.2032054\ttotal: 1m 45s\tremaining: 3m 19s\n",
      "345:\tlearn: 0.2031984\ttotal: 1m 45s\tremaining: 3m 19s\n",
      "346:\tlearn: 0.2031930\ttotal: 1m 45s\tremaining: 3m 18s\n",
      "347:\tlearn: 0.2031713\ttotal: 1m 45s\tremaining: 3m 18s\n",
      "348:\tlearn: 0.2031660\ttotal: 1m 46s\tremaining: 3m 18s\n",
      "349:\tlearn: 0.2031473\ttotal: 1m 46s\tremaining: 3m 17s\n",
      "350:\tlearn: 0.2031259\ttotal: 1m 46s\tremaining: 3m 17s\n",
      "351:\tlearn: 0.2031191\ttotal: 1m 47s\tremaining: 3m 17s\n",
      "352:\tlearn: 0.2031033\ttotal: 1m 47s\tremaining: 3m 16s\n",
      "353:\tlearn: 0.2030825\ttotal: 1m 47s\tremaining: 3m 16s\n",
      "354:\tlearn: 0.2030773\ttotal: 1m 47s\tremaining: 3m 16s\n",
      "355:\tlearn: 0.2030412\ttotal: 1m 48s\tremaining: 3m 15s\n",
      "356:\tlearn: 0.2030355\ttotal: 1m 48s\tremaining: 3m 15s\n",
      "357:\tlearn: 0.2030191\ttotal: 1m 48s\tremaining: 3m 15s\n",
      "358:\tlearn: 0.2030051\ttotal: 1m 49s\tremaining: 3m 14s\n",
      "359:\tlearn: 0.2029905\ttotal: 1m 49s\tremaining: 3m 14s\n",
      "360:\tlearn: 0.2029658\ttotal: 1m 49s\tremaining: 3m 14s\n",
      "361:\tlearn: 0.2029581\ttotal: 1m 49s\tremaining: 3m 13s\n",
      "362:\tlearn: 0.2029385\ttotal: 1m 50s\tremaining: 3m 13s\n",
      "363:\tlearn: 0.2029221\ttotal: 1m 50s\tremaining: 3m 13s\n",
      "364:\tlearn: 0.2029167\ttotal: 1m 50s\tremaining: 3m 12s\n",
      "365:\tlearn: 0.2029110\ttotal: 1m 51s\tremaining: 3m 12s\n",
      "366:\tlearn: 0.2029036\ttotal: 1m 51s\tremaining: 3m 12s\n",
      "367:\tlearn: 0.2028886\ttotal: 1m 51s\tremaining: 3m 11s\n",
      "368:\tlearn: 0.2028801\ttotal: 1m 52s\tremaining: 3m 11s\n",
      "369:\tlearn: 0.2028740\ttotal: 1m 52s\tremaining: 3m 11s\n",
      "370:\tlearn: 0.2028509\ttotal: 1m 52s\tremaining: 3m 10s\n",
      "371:\tlearn: 0.2028462\ttotal: 1m 52s\tremaining: 3m 10s\n",
      "372:\tlearn: 0.2028279\ttotal: 1m 53s\tremaining: 3m 10s\n",
      "373:\tlearn: 0.2028218\ttotal: 1m 53s\tremaining: 3m 9s\n",
      "374:\tlearn: 0.2027909\ttotal: 1m 53s\tremaining: 3m 9s\n",
      "375:\tlearn: 0.2027824\ttotal: 1m 53s\tremaining: 3m 9s\n",
      "376:\tlearn: 0.2027732\ttotal: 1m 54s\tremaining: 3m 8s\n",
      "377:\tlearn: 0.2027646\ttotal: 1m 54s\tremaining: 3m 8s\n",
      "378:\tlearn: 0.2027568\ttotal: 1m 54s\tremaining: 3m 8s\n",
      "379:\tlearn: 0.2027449\ttotal: 1m 55s\tremaining: 3m 7s\n",
      "380:\tlearn: 0.2027403\ttotal: 1m 55s\tremaining: 3m 7s\n",
      "381:\tlearn: 0.2027347\ttotal: 1m 55s\tremaining: 3m 6s\n",
      "382:\tlearn: 0.2027287\ttotal: 1m 55s\tremaining: 3m 6s\n",
      "383:\tlearn: 0.2027083\ttotal: 1m 56s\tremaining: 3m 6s\n",
      "384:\tlearn: 0.2027046\ttotal: 1m 56s\tremaining: 3m 5s\n",
      "385:\tlearn: 0.2026997\ttotal: 1m 56s\tremaining: 3m 5s\n",
      "386:\tlearn: 0.2026911\ttotal: 1m 56s\tremaining: 3m 5s\n",
      "387:\tlearn: 0.2026833\ttotal: 1m 57s\tremaining: 3m 4s\n",
      "388:\tlearn: 0.2026605\ttotal: 1m 57s\tremaining: 3m 4s\n",
      "389:\tlearn: 0.2025921\ttotal: 1m 57s\tremaining: 3m 4s\n",
      "390:\tlearn: 0.2025702\ttotal: 1m 58s\tremaining: 3m 4s\n",
      "391:\tlearn: 0.2025622\ttotal: 1m 58s\tremaining: 3m 3s\n",
      "392:\tlearn: 0.2025385\ttotal: 1m 58s\tremaining: 3m 3s\n",
      "393:\tlearn: 0.2025279\ttotal: 1m 59s\tremaining: 3m 3s\n",
      "394:\tlearn: 0.2025224\ttotal: 1m 59s\tremaining: 3m 2s\n",
      "395:\tlearn: 0.2025013\ttotal: 1m 59s\tremaining: 3m 2s\n",
      "396:\tlearn: 0.2024787\ttotal: 2m\tremaining: 3m 2s\n",
      "397:\tlearn: 0.2024521\ttotal: 2m\tremaining: 3m 1s\n",
      "398:\tlearn: 0.2024482\ttotal: 2m\tremaining: 3m 1s\n",
      "399:\tlearn: 0.2024343\ttotal: 2m\tremaining: 3m 1s\n",
      "400:\tlearn: 0.2024313\ttotal: 2m 1s\tremaining: 3m\n",
      "401:\tlearn: 0.2024235\ttotal: 2m 1s\tremaining: 3m\n",
      "402:\tlearn: 0.2023979\ttotal: 2m 1s\tremaining: 3m\n",
      "403:\tlearn: 0.2023912\ttotal: 2m 1s\tremaining: 2m 59s\n",
      "404:\tlearn: 0.2023720\ttotal: 2m 2s\tremaining: 2m 59s\n",
      "405:\tlearn: 0.2023526\ttotal: 2m 2s\tremaining: 2m 59s\n",
      "406:\tlearn: 0.2023429\ttotal: 2m 2s\tremaining: 2m 59s\n",
      "407:\tlearn: 0.2023365\ttotal: 2m 3s\tremaining: 2m 58s\n",
      "408:\tlearn: 0.2023010\ttotal: 2m 3s\tremaining: 2m 58s\n",
      "409:\tlearn: 0.2022956\ttotal: 2m 3s\tremaining: 2m 58s\n",
      "410:\tlearn: 0.2022903\ttotal: 2m 4s\tremaining: 2m 57s\n",
      "411:\tlearn: 0.2022838\ttotal: 2m 4s\tremaining: 2m 57s\n",
      "412:\tlearn: 0.2022739\ttotal: 2m 4s\tremaining: 2m 57s\n",
      "413:\tlearn: 0.2022567\ttotal: 2m 4s\tremaining: 2m 56s\n",
      "414:\tlearn: 0.2022473\ttotal: 2m 5s\tremaining: 2m 56s\n",
      "415:\tlearn: 0.2022429\ttotal: 2m 5s\tremaining: 2m 56s\n",
      "416:\tlearn: 0.2022333\ttotal: 2m 5s\tremaining: 2m 55s\n",
      "417:\tlearn: 0.2021969\ttotal: 2m 5s\tremaining: 2m 55s\n",
      "418:\tlearn: 0.2021906\ttotal: 2m 6s\tremaining: 2m 55s\n",
      "419:\tlearn: 0.2021819\ttotal: 2m 6s\tremaining: 2m 54s\n",
      "420:\tlearn: 0.2021770\ttotal: 2m 6s\tremaining: 2m 54s\n",
      "421:\tlearn: 0.2021442\ttotal: 2m 7s\tremaining: 2m 54s\n",
      "422:\tlearn: 0.2021073\ttotal: 2m 7s\tremaining: 2m 53s\n",
      "423:\tlearn: 0.2020961\ttotal: 2m 7s\tremaining: 2m 53s\n",
      "424:\tlearn: 0.2020884\ttotal: 2m 7s\tremaining: 2m 53s\n",
      "425:\tlearn: 0.2020847\ttotal: 2m 8s\tremaining: 2m 52s\n",
      "426:\tlearn: 0.2020629\ttotal: 2m 8s\tremaining: 2m 52s\n",
      "427:\tlearn: 0.2020191\ttotal: 2m 9s\tremaining: 2m 52s\n",
      "428:\tlearn: 0.2020110\ttotal: 2m 9s\tremaining: 2m 52s\n",
      "429:\tlearn: 0.2019672\ttotal: 2m 9s\tremaining: 2m 51s\n",
      "430:\tlearn: 0.2019125\ttotal: 2m 10s\tremaining: 2m 51s\n",
      "431:\tlearn: 0.2018947\ttotal: 2m 10s\tremaining: 2m 51s\n",
      "432:\tlearn: 0.2018916\ttotal: 2m 10s\tremaining: 2m 51s\n",
      "433:\tlearn: 0.2018741\ttotal: 2m 10s\tremaining: 2m 50s\n",
      "434:\tlearn: 0.2018652\ttotal: 2m 11s\tremaining: 2m 50s\n",
      "435:\tlearn: 0.2018576\ttotal: 2m 11s\tremaining: 2m 50s\n",
      "436:\tlearn: 0.2018471\ttotal: 2m 11s\tremaining: 2m 49s\n",
      "437:\tlearn: 0.2018334\ttotal: 2m 12s\tremaining: 2m 49s\n",
      "438:\tlearn: 0.2018273\ttotal: 2m 12s\tremaining: 2m 49s\n",
      "439:\tlearn: 0.2018203\ttotal: 2m 12s\tremaining: 2m 48s\n",
      "440:\tlearn: 0.2018113\ttotal: 2m 12s\tremaining: 2m 48s\n",
      "441:\tlearn: 0.2017875\ttotal: 2m 13s\tremaining: 2m 48s\n",
      "442:\tlearn: 0.2017840\ttotal: 2m 13s\tremaining: 2m 47s\n",
      "443:\tlearn: 0.2017690\ttotal: 2m 13s\tremaining: 2m 47s\n",
      "444:\tlearn: 0.2017618\ttotal: 2m 14s\tremaining: 2m 47s\n",
      "445:\tlearn: 0.2017471\ttotal: 2m 14s\tremaining: 2m 46s\n",
      "446:\tlearn: 0.2017429\ttotal: 2m 14s\tremaining: 2m 46s\n",
      "447:\tlearn: 0.2016772\ttotal: 2m 15s\tremaining: 2m 46s\n",
      "448:\tlearn: 0.2016435\ttotal: 2m 15s\tremaining: 2m 46s\n",
      "449:\tlearn: 0.2016403\ttotal: 2m 15s\tremaining: 2m 45s\n",
      "450:\tlearn: 0.2016291\ttotal: 2m 16s\tremaining: 2m 45s\n",
      "451:\tlearn: 0.2016146\ttotal: 2m 16s\tremaining: 2m 45s\n",
      "452:\tlearn: 0.2016028\ttotal: 2m 16s\tremaining: 2m 44s\n",
      "453:\tlearn: 0.2015812\ttotal: 2m 16s\tremaining: 2m 44s\n",
      "454:\tlearn: 0.2015777\ttotal: 2m 17s\tremaining: 2m 44s\n",
      "455:\tlearn: 0.2015685\ttotal: 2m 17s\tremaining: 2m 43s\n",
      "456:\tlearn: 0.2015612\ttotal: 2m 17s\tremaining: 2m 43s\n",
      "457:\tlearn: 0.2015549\ttotal: 2m 18s\tremaining: 2m 43s\n",
      "458:\tlearn: 0.2015520\ttotal: 2m 18s\tremaining: 2m 42s\n",
      "459:\tlearn: 0.2015274\ttotal: 2m 18s\tremaining: 2m 42s\n",
      "460:\tlearn: 0.2015246\ttotal: 2m 18s\tremaining: 2m 42s\n",
      "461:\tlearn: 0.2015111\ttotal: 2m 19s\tremaining: 2m 41s\n",
      "462:\tlearn: 0.2014868\ttotal: 2m 19s\tremaining: 2m 41s\n",
      "463:\tlearn: 0.2014642\ttotal: 2m 19s\tremaining: 2m 41s\n",
      "464:\tlearn: 0.2014628\ttotal: 2m 20s\tremaining: 2m 41s\n",
      "465:\tlearn: 0.2014574\ttotal: 2m 20s\tremaining: 2m 40s\n",
      "466:\tlearn: 0.2014423\ttotal: 2m 20s\tremaining: 2m 40s\n",
      "467:\tlearn: 0.2014298\ttotal: 2m 20s\tremaining: 2m 40s\n",
      "468:\tlearn: 0.2014262\ttotal: 2m 21s\tremaining: 2m 39s\n",
      "469:\tlearn: 0.2014051\ttotal: 2m 21s\tremaining: 2m 39s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470:\tlearn: 0.2013875\ttotal: 2m 21s\tremaining: 2m 39s\n",
      "471:\tlearn: 0.2013781\ttotal: 2m 22s\tremaining: 2m 38s\n",
      "472:\tlearn: 0.2013725\ttotal: 2m 22s\tremaining: 2m 38s\n",
      "473:\tlearn: 0.2013698\ttotal: 2m 22s\tremaining: 2m 38s\n",
      "474:\tlearn: 0.2013571\ttotal: 2m 23s\tremaining: 2m 38s\n",
      "475:\tlearn: 0.2013528\ttotal: 2m 23s\tremaining: 2m 37s\n",
      "476:\tlearn: 0.2013420\ttotal: 2m 23s\tremaining: 2m 37s\n",
      "477:\tlearn: 0.2013381\ttotal: 2m 23s\tremaining: 2m 37s\n",
      "478:\tlearn: 0.2013225\ttotal: 2m 24s\tremaining: 2m 36s\n",
      "479:\tlearn: 0.2012984\ttotal: 2m 24s\tremaining: 2m 36s\n",
      "480:\tlearn: 0.2012886\ttotal: 2m 25s\tremaining: 2m 36s\n",
      "481:\tlearn: 0.2012564\ttotal: 2m 25s\tremaining: 2m 36s\n",
      "482:\tlearn: 0.2012530\ttotal: 2m 25s\tremaining: 2m 35s\n",
      "483:\tlearn: 0.2012406\ttotal: 2m 25s\tremaining: 2m 35s\n",
      "484:\tlearn: 0.2012341\ttotal: 2m 26s\tremaining: 2m 35s\n",
      "485:\tlearn: 0.2012282\ttotal: 2m 26s\tremaining: 2m 34s\n",
      "486:\tlearn: 0.2011997\ttotal: 2m 26s\tremaining: 2m 34s\n",
      "487:\tlearn: 0.2011916\ttotal: 2m 27s\tremaining: 2m 34s\n",
      "488:\tlearn: 0.2011627\ttotal: 2m 27s\tremaining: 2m 34s\n",
      "489:\tlearn: 0.2011532\ttotal: 2m 27s\tremaining: 2m 33s\n",
      "490:\tlearn: 0.2011503\ttotal: 2m 27s\tremaining: 2m 33s\n",
      "491:\tlearn: 0.2011349\ttotal: 2m 28s\tremaining: 2m 33s\n",
      "492:\tlearn: 0.2011185\ttotal: 2m 28s\tremaining: 2m 32s\n",
      "493:\tlearn: 0.2011120\ttotal: 2m 28s\tremaining: 2m 32s\n",
      "494:\tlearn: 0.2010965\ttotal: 2m 29s\tremaining: 2m 32s\n",
      "495:\tlearn: 0.2010664\ttotal: 2m 29s\tremaining: 2m 31s\n",
      "496:\tlearn: 0.2010525\ttotal: 2m 29s\tremaining: 2m 31s\n",
      "497:\tlearn: 0.2010268\ttotal: 2m 30s\tremaining: 2m 31s\n",
      "498:\tlearn: 0.2010145\ttotal: 2m 30s\tremaining: 2m 31s\n",
      "499:\tlearn: 0.2009908\ttotal: 2m 30s\tremaining: 2m 30s\n",
      "500:\tlearn: 0.2009710\ttotal: 2m 31s\tremaining: 2m 30s\n",
      "501:\tlearn: 0.2009661\ttotal: 2m 31s\tremaining: 2m 30s\n",
      "502:\tlearn: 0.2009615\ttotal: 2m 31s\tremaining: 2m 29s\n",
      "503:\tlearn: 0.2009423\ttotal: 2m 31s\tremaining: 2m 29s\n",
      "504:\tlearn: 0.2009260\ttotal: 2m 32s\tremaining: 2m 29s\n",
      "505:\tlearn: 0.2009077\ttotal: 2m 32s\tremaining: 2m 28s\n",
      "506:\tlearn: 0.2008998\ttotal: 2m 32s\tremaining: 2m 28s\n",
      "507:\tlearn: 0.2008824\ttotal: 2m 33s\tremaining: 2m 28s\n",
      "508:\tlearn: 0.2008634\ttotal: 2m 33s\tremaining: 2m 28s\n",
      "509:\tlearn: 0.2008568\ttotal: 2m 33s\tremaining: 2m 27s\n",
      "510:\tlearn: 0.2008477\ttotal: 2m 34s\tremaining: 2m 27s\n",
      "511:\tlearn: 0.2008341\ttotal: 2m 34s\tremaining: 2m 27s\n",
      "512:\tlearn: 0.2008077\ttotal: 2m 34s\tremaining: 2m 26s\n",
      "513:\tlearn: 0.2008032\ttotal: 2m 35s\tremaining: 2m 26s\n",
      "514:\tlearn: 0.2007858\ttotal: 2m 35s\tremaining: 2m 26s\n",
      "515:\tlearn: 0.2007821\ttotal: 2m 35s\tremaining: 2m 25s\n",
      "516:\tlearn: 0.2007800\ttotal: 2m 35s\tremaining: 2m 25s\n",
      "517:\tlearn: 0.2007722\ttotal: 2m 36s\tremaining: 2m 25s\n",
      "518:\tlearn: 0.2007647\ttotal: 2m 36s\tremaining: 2m 24s\n",
      "519:\tlearn: 0.2007590\ttotal: 2m 36s\tremaining: 2m 24s\n",
      "520:\tlearn: 0.2007524\ttotal: 2m 36s\tremaining: 2m 24s\n",
      "521:\tlearn: 0.2007447\ttotal: 2m 37s\tremaining: 2m 23s\n",
      "522:\tlearn: 0.2007346\ttotal: 2m 37s\tremaining: 2m 23s\n",
      "523:\tlearn: 0.2007189\ttotal: 2m 37s\tremaining: 2m 23s\n",
      "524:\tlearn: 0.2007048\ttotal: 2m 38s\tremaining: 2m 23s\n",
      "525:\tlearn: 0.2006988\ttotal: 2m 38s\tremaining: 2m 22s\n",
      "526:\tlearn: 0.2006799\ttotal: 2m 38s\tremaining: 2m 22s\n",
      "527:\tlearn: 0.2006521\ttotal: 2m 39s\tremaining: 2m 22s\n",
      "528:\tlearn: 0.2006128\ttotal: 2m 39s\tremaining: 2m 21s\n",
      "529:\tlearn: 0.2006054\ttotal: 2m 39s\tremaining: 2m 21s\n",
      "530:\tlearn: 0.2005988\ttotal: 2m 39s\tremaining: 2m 21s\n",
      "531:\tlearn: 0.2005947\ttotal: 2m 40s\tremaining: 2m 20s\n",
      "532:\tlearn: 0.2005718\ttotal: 2m 40s\tremaining: 2m 20s\n",
      "533:\tlearn: 0.2005461\ttotal: 2m 40s\tremaining: 2m 20s\n",
      "534:\tlearn: 0.2005349\ttotal: 2m 41s\tremaining: 2m 19s\n",
      "535:\tlearn: 0.2005252\ttotal: 2m 41s\tremaining: 2m 19s\n",
      "536:\tlearn: 0.2005101\ttotal: 2m 41s\tremaining: 2m 19s\n",
      "537:\tlearn: 0.2005026\ttotal: 2m 41s\tremaining: 2m 19s\n",
      "538:\tlearn: 0.2004998\ttotal: 2m 42s\tremaining: 2m 18s\n",
      "539:\tlearn: 0.2004648\ttotal: 2m 42s\tremaining: 2m 18s\n",
      "540:\tlearn: 0.2004427\ttotal: 2m 42s\tremaining: 2m 18s\n",
      "541:\tlearn: 0.2004154\ttotal: 2m 43s\tremaining: 2m 17s\n",
      "542:\tlearn: 0.2003980\ttotal: 2m 43s\tremaining: 2m 17s\n",
      "543:\tlearn: 0.2003955\ttotal: 2m 43s\tremaining: 2m 17s\n",
      "544:\tlearn: 0.2003860\ttotal: 2m 43s\tremaining: 2m 16s\n",
      "545:\tlearn: 0.2003792\ttotal: 2m 44s\tremaining: 2m 16s\n",
      "546:\tlearn: 0.2003576\ttotal: 2m 44s\tremaining: 2m 16s\n",
      "547:\tlearn: 0.2003508\ttotal: 2m 44s\tremaining: 2m 15s\n",
      "548:\tlearn: 0.2003353\ttotal: 2m 45s\tremaining: 2m 15s\n",
      "549:\tlearn: 0.2003223\ttotal: 2m 45s\tremaining: 2m 15s\n",
      "550:\tlearn: 0.2003191\ttotal: 2m 45s\tremaining: 2m 14s\n",
      "551:\tlearn: 0.2003114\ttotal: 2m 45s\tremaining: 2m 14s\n",
      "552:\tlearn: 0.2003012\ttotal: 2m 46s\tremaining: 2m 14s\n",
      "553:\tlearn: 0.2002870\ttotal: 2m 46s\tremaining: 2m 13s\n",
      "554:\tlearn: 0.2002847\ttotal: 2m 46s\tremaining: 2m 13s\n",
      "555:\tlearn: 0.2002610\ttotal: 2m 46s\tremaining: 2m 13s\n",
      "556:\tlearn: 0.2002502\ttotal: 2m 47s\tremaining: 2m 13s\n",
      "557:\tlearn: 0.2002413\ttotal: 2m 47s\tremaining: 2m 12s\n",
      "558:\tlearn: 0.2002367\ttotal: 2m 47s\tremaining: 2m 12s\n",
      "559:\tlearn: 0.2002294\ttotal: 2m 48s\tremaining: 2m 12s\n",
      "560:\tlearn: 0.2001976\ttotal: 2m 49s\tremaining: 2m 12s\n",
      "561:\tlearn: 0.2001904\ttotal: 2m 49s\tremaining: 2m 12s\n",
      "562:\tlearn: 0.2001881\ttotal: 2m 50s\tremaining: 2m 11s\n",
      "563:\tlearn: 0.2001835\ttotal: 2m 50s\tremaining: 2m 11s\n",
      "564:\tlearn: 0.2001768\ttotal: 2m 50s\tremaining: 2m 11s\n",
      "565:\tlearn: 0.2001574\ttotal: 2m 51s\tremaining: 2m 11s\n",
      "566:\tlearn: 0.2001507\ttotal: 2m 51s\tremaining: 2m 10s\n",
      "567:\tlearn: 0.2001484\ttotal: 2m 51s\tremaining: 2m 10s\n",
      "568:\tlearn: 0.2001340\ttotal: 2m 52s\tremaining: 2m 10s\n",
      "569:\tlearn: 0.2001291\ttotal: 2m 52s\tremaining: 2m 10s\n",
      "570:\tlearn: 0.2001229\ttotal: 2m 52s\tremaining: 2m 9s\n",
      "571:\tlearn: 0.2001160\ttotal: 2m 53s\tremaining: 2m 9s\n",
      "572:\tlearn: 0.2001088\ttotal: 2m 53s\tremaining: 2m 9s\n",
      "573:\tlearn: 0.2000991\ttotal: 2m 54s\tremaining: 2m 9s\n",
      "574:\tlearn: 0.2000915\ttotal: 2m 54s\tremaining: 2m 8s\n",
      "575:\tlearn: 0.2000785\ttotal: 2m 54s\tremaining: 2m 8s\n",
      "576:\tlearn: 0.2000708\ttotal: 2m 55s\tremaining: 2m 8s\n",
      "577:\tlearn: 0.2000421\ttotal: 2m 55s\tremaining: 2m 8s\n",
      "578:\tlearn: 0.2000385\ttotal: 2m 55s\tremaining: 2m 7s\n",
      "579:\tlearn: 0.2000248\ttotal: 2m 56s\tremaining: 2m 7s\n",
      "580:\tlearn: 0.2000213\ttotal: 2m 56s\tremaining: 2m 7s\n",
      "581:\tlearn: 0.1999999\ttotal: 2m 56s\tremaining: 2m 6s\n",
      "582:\tlearn: 0.1999934\ttotal: 2m 56s\tremaining: 2m 6s\n",
      "583:\tlearn: 0.1999905\ttotal: 2m 57s\tremaining: 2m 6s\n",
      "584:\tlearn: 0.1999841\ttotal: 2m 57s\tremaining: 2m 5s\n",
      "585:\tlearn: 0.1999811\ttotal: 2m 57s\tremaining: 2m 5s\n",
      "586:\tlearn: 0.1999734\ttotal: 2m 58s\tremaining: 2m 5s\n",
      "587:\tlearn: 0.1999685\ttotal: 2m 58s\tremaining: 2m 5s\n",
      "588:\tlearn: 0.1999613\ttotal: 2m 58s\tremaining: 2m 4s\n",
      "589:\tlearn: 0.1999425\ttotal: 2m 59s\tremaining: 2m 4s\n",
      "590:\tlearn: 0.1999354\ttotal: 2m 59s\tremaining: 2m 4s\n",
      "591:\tlearn: 0.1999201\ttotal: 2m 59s\tremaining: 2m 3s\n",
      "592:\tlearn: 0.1999071\ttotal: 3m\tremaining: 2m 3s\n",
      "593:\tlearn: 0.1998664\ttotal: 3m\tremaining: 2m 3s\n",
      "594:\tlearn: 0.1998604\ttotal: 3m\tremaining: 2m 3s\n",
      "595:\tlearn: 0.1998518\ttotal: 3m 1s\tremaining: 2m 2s\n",
      "596:\tlearn: 0.1998248\ttotal: 3m 1s\tremaining: 2m 2s\n",
      "597:\tlearn: 0.1998205\ttotal: 3m 1s\tremaining: 2m 2s\n",
      "598:\tlearn: 0.1997984\ttotal: 3m 2s\tremaining: 2m 1s\n",
      "599:\tlearn: 0.1997830\ttotal: 3m 2s\tremaining: 2m 1s\n",
      "600:\tlearn: 0.1997739\ttotal: 3m 2s\tremaining: 2m 1s\n",
      "601:\tlearn: 0.1997562\ttotal: 3m 3s\tremaining: 2m 1s\n",
      "602:\tlearn: 0.1997506\ttotal: 3m 3s\tremaining: 2m\n",
      "603:\tlearn: 0.1997410\ttotal: 3m 3s\tremaining: 2m\n",
      "604:\tlearn: 0.1997349\ttotal: 3m 4s\tremaining: 2m\n",
      "605:\tlearn: 0.1997333\ttotal: 3m 4s\tremaining: 1m 59s\n",
      "606:\tlearn: 0.1997084\ttotal: 3m 4s\tremaining: 1m 59s\n",
      "607:\tlearn: 0.1997016\ttotal: 3m 5s\tremaining: 1m 59s\n",
      "608:\tlearn: 0.1996904\ttotal: 3m 5s\tremaining: 1m 59s\n",
      "609:\tlearn: 0.1996873\ttotal: 3m 5s\tremaining: 1m 58s\n",
      "610:\tlearn: 0.1996833\ttotal: 3m 5s\tremaining: 1m 58s\n",
      "611:\tlearn: 0.1996693\ttotal: 3m 6s\tremaining: 1m 58s\n",
      "612:\tlearn: 0.1996653\ttotal: 3m 6s\tremaining: 1m 57s\n",
      "613:\tlearn: 0.1996484\ttotal: 3m 6s\tremaining: 1m 57s\n",
      "614:\tlearn: 0.1996362\ttotal: 3m 7s\tremaining: 1m 57s\n",
      "615:\tlearn: 0.1996199\ttotal: 3m 7s\tremaining: 1m 57s\n",
      "616:\tlearn: 0.1996176\ttotal: 3m 8s\tremaining: 1m 56s\n",
      "617:\tlearn: 0.1996015\ttotal: 3m 8s\tremaining: 1m 56s\n",
      "618:\tlearn: 0.1995863\ttotal: 3m 8s\tremaining: 1m 56s\n",
      "619:\tlearn: 0.1995808\ttotal: 3m 9s\tremaining: 1m 55s\n",
      "620:\tlearn: 0.1995789\ttotal: 3m 9s\tremaining: 1m 55s\n",
      "621:\tlearn: 0.1995763\ttotal: 3m 9s\tremaining: 1m 55s\n",
      "622:\tlearn: 0.1995645\ttotal: 3m 9s\tremaining: 1m 54s\n",
      "623:\tlearn: 0.1995494\ttotal: 3m 10s\tremaining: 1m 54s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624:\tlearn: 0.1995361\ttotal: 3m 10s\tremaining: 1m 54s\n",
      "625:\tlearn: 0.1995181\ttotal: 3m 10s\tremaining: 1m 54s\n",
      "626:\tlearn: 0.1995079\ttotal: 3m 11s\tremaining: 1m 53s\n",
      "627:\tlearn: 0.1995046\ttotal: 3m 11s\tremaining: 1m 53s\n",
      "628:\tlearn: 0.1994973\ttotal: 3m 11s\tremaining: 1m 53s\n",
      "629:\tlearn: 0.1994812\ttotal: 3m 11s\tremaining: 1m 52s\n",
      "630:\tlearn: 0.1994704\ttotal: 3m 12s\tremaining: 1m 52s\n",
      "631:\tlearn: 0.1994663\ttotal: 3m 12s\tremaining: 1m 52s\n",
      "632:\tlearn: 0.1994613\ttotal: 3m 12s\tremaining: 1m 51s\n",
      "633:\tlearn: 0.1994547\ttotal: 3m 13s\tremaining: 1m 51s\n",
      "634:\tlearn: 0.1994340\ttotal: 3m 13s\tremaining: 1m 51s\n",
      "635:\tlearn: 0.1994234\ttotal: 3m 13s\tremaining: 1m 50s\n",
      "636:\tlearn: 0.1994140\ttotal: 3m 13s\tremaining: 1m 50s\n",
      "637:\tlearn: 0.1994077\ttotal: 3m 14s\tremaining: 1m 50s\n",
      "638:\tlearn: 0.1993928\ttotal: 3m 14s\tremaining: 1m 49s\n",
      "639:\tlearn: 0.1993814\ttotal: 3m 14s\tremaining: 1m 49s\n",
      "640:\tlearn: 0.1993475\ttotal: 3m 15s\tremaining: 1m 49s\n",
      "641:\tlearn: 0.1993308\ttotal: 3m 15s\tremaining: 1m 48s\n",
      "642:\tlearn: 0.1993268\ttotal: 3m 15s\tremaining: 1m 48s\n",
      "643:\tlearn: 0.1993163\ttotal: 3m 15s\tremaining: 1m 48s\n",
      "644:\tlearn: 0.1992945\ttotal: 3m 16s\tremaining: 1m 48s\n",
      "645:\tlearn: 0.1992855\ttotal: 3m 16s\tremaining: 1m 47s\n",
      "646:\tlearn: 0.1992721\ttotal: 3m 17s\tremaining: 1m 47s\n",
      "647:\tlearn: 0.1992674\ttotal: 3m 17s\tremaining: 1m 47s\n",
      "648:\tlearn: 0.1992467\ttotal: 3m 17s\tremaining: 1m 46s\n",
      "649:\tlearn: 0.1992353\ttotal: 3m 17s\tremaining: 1m 46s\n",
      "650:\tlearn: 0.1992172\ttotal: 3m 18s\tremaining: 1m 46s\n",
      "651:\tlearn: 0.1992050\ttotal: 3m 18s\tremaining: 1m 45s\n",
      "652:\tlearn: 0.1991994\ttotal: 3m 18s\tremaining: 1m 45s\n",
      "653:\tlearn: 0.1991974\ttotal: 3m 19s\tremaining: 1m 45s\n",
      "654:\tlearn: 0.1991938\ttotal: 3m 19s\tremaining: 1m 44s\n",
      "655:\tlearn: 0.1991706\ttotal: 3m 19s\tremaining: 1m 44s\n",
      "656:\tlearn: 0.1991458\ttotal: 3m 19s\tremaining: 1m 44s\n",
      "657:\tlearn: 0.1991350\ttotal: 3m 20s\tremaining: 1m 44s\n",
      "658:\tlearn: 0.1990963\ttotal: 3m 20s\tremaining: 1m 43s\n",
      "659:\tlearn: 0.1990690\ttotal: 3m 21s\tremaining: 1m 43s\n",
      "660:\tlearn: 0.1990596\ttotal: 3m 21s\tremaining: 1m 43s\n",
      "661:\tlearn: 0.1990440\ttotal: 3m 21s\tremaining: 1m 42s\n",
      "662:\tlearn: 0.1990244\ttotal: 3m 21s\tremaining: 1m 42s\n",
      "663:\tlearn: 0.1990105\ttotal: 3m 22s\tremaining: 1m 42s\n",
      "664:\tlearn: 0.1989807\ttotal: 3m 22s\tremaining: 1m 42s\n",
      "665:\tlearn: 0.1989687\ttotal: 3m 22s\tremaining: 1m 41s\n",
      "666:\tlearn: 0.1989572\ttotal: 3m 23s\tremaining: 1m 41s\n",
      "667:\tlearn: 0.1989499\ttotal: 3m 23s\tremaining: 1m 41s\n",
      "668:\tlearn: 0.1989276\ttotal: 3m 23s\tremaining: 1m 40s\n",
      "669:\tlearn: 0.1989246\ttotal: 3m 24s\tremaining: 1m 40s\n",
      "670:\tlearn: 0.1989101\ttotal: 3m 24s\tremaining: 1m 40s\n",
      "671:\tlearn: 0.1988990\ttotal: 3m 24s\tremaining: 1m 40s\n",
      "672:\tlearn: 0.1988901\ttotal: 3m 25s\tremaining: 1m 39s\n",
      "673:\tlearn: 0.1988843\ttotal: 3m 25s\tremaining: 1m 39s\n",
      "674:\tlearn: 0.1988787\ttotal: 3m 25s\tremaining: 1m 39s\n",
      "675:\tlearn: 0.1988374\ttotal: 3m 26s\tremaining: 1m 38s\n",
      "676:\tlearn: 0.1988160\ttotal: 3m 26s\tremaining: 1m 38s\n",
      "677:\tlearn: 0.1988089\ttotal: 3m 26s\tremaining: 1m 38s\n",
      "678:\tlearn: 0.1988051\ttotal: 3m 27s\tremaining: 1m 37s\n",
      "679:\tlearn: 0.1987863\ttotal: 3m 27s\tremaining: 1m 37s\n",
      "680:\tlearn: 0.1987667\ttotal: 3m 27s\tremaining: 1m 37s\n",
      "681:\tlearn: 0.1987519\ttotal: 3m 28s\tremaining: 1m 37s\n",
      "682:\tlearn: 0.1987439\ttotal: 3m 28s\tremaining: 1m 36s\n",
      "683:\tlearn: 0.1987294\ttotal: 3m 28s\tremaining: 1m 36s\n",
      "684:\tlearn: 0.1987230\ttotal: 3m 29s\tremaining: 1m 36s\n",
      "685:\tlearn: 0.1987132\ttotal: 3m 29s\tremaining: 1m 35s\n",
      "686:\tlearn: 0.1987055\ttotal: 3m 29s\tremaining: 1m 35s\n",
      "687:\tlearn: 0.1987020\ttotal: 3m 29s\tremaining: 1m 35s\n",
      "688:\tlearn: 0.1987005\ttotal: 3m 30s\tremaining: 1m 34s\n",
      "689:\tlearn: 0.1986945\ttotal: 3m 30s\tremaining: 1m 34s\n",
      "690:\tlearn: 0.1986836\ttotal: 3m 30s\tremaining: 1m 34s\n",
      "691:\tlearn: 0.1986683\ttotal: 3m 31s\tremaining: 1m 34s\n",
      "692:\tlearn: 0.1986598\ttotal: 3m 31s\tremaining: 1m 33s\n",
      "693:\tlearn: 0.1986465\ttotal: 3m 31s\tremaining: 1m 33s\n",
      "694:\tlearn: 0.1986413\ttotal: 3m 32s\tremaining: 1m 33s\n",
      "695:\tlearn: 0.1986398\ttotal: 3m 32s\tremaining: 1m 32s\n",
      "696:\tlearn: 0.1986381\ttotal: 3m 32s\tremaining: 1m 32s\n",
      "697:\tlearn: 0.1986178\ttotal: 3m 33s\tremaining: 1m 32s\n",
      "698:\tlearn: 0.1986107\ttotal: 3m 33s\tremaining: 1m 31s\n",
      "699:\tlearn: 0.1986061\ttotal: 3m 33s\tremaining: 1m 31s\n",
      "700:\tlearn: 0.1986008\ttotal: 3m 34s\tremaining: 1m 31s\n",
      "701:\tlearn: 0.1985910\ttotal: 3m 34s\tremaining: 1m 30s\n",
      "702:\tlearn: 0.1985811\ttotal: 3m 34s\tremaining: 1m 30s\n",
      "703:\tlearn: 0.1985695\ttotal: 3m 35s\tremaining: 1m 30s\n",
      "704:\tlearn: 0.1985646\ttotal: 3m 35s\tremaining: 1m 30s\n",
      "705:\tlearn: 0.1985599\ttotal: 3m 35s\tremaining: 1m 29s\n",
      "706:\tlearn: 0.1985479\ttotal: 3m 35s\tremaining: 1m 29s\n",
      "707:\tlearn: 0.1985460\ttotal: 3m 36s\tremaining: 1m 29s\n",
      "708:\tlearn: 0.1985327\ttotal: 3m 36s\tremaining: 1m 28s\n",
      "709:\tlearn: 0.1985162\ttotal: 3m 36s\tremaining: 1m 28s\n",
      "710:\tlearn: 0.1984978\ttotal: 3m 37s\tremaining: 1m 28s\n",
      "711:\tlearn: 0.1984942\ttotal: 3m 37s\tremaining: 1m 28s\n",
      "712:\tlearn: 0.1984866\ttotal: 3m 37s\tremaining: 1m 27s\n",
      "713:\tlearn: 0.1984834\ttotal: 3m 38s\tremaining: 1m 27s\n",
      "714:\tlearn: 0.1984647\ttotal: 3m 38s\tremaining: 1m 27s\n",
      "715:\tlearn: 0.1984623\ttotal: 3m 38s\tremaining: 1m 26s\n",
      "716:\tlearn: 0.1984578\ttotal: 3m 39s\tremaining: 1m 26s\n",
      "717:\tlearn: 0.1984521\ttotal: 3m 39s\tremaining: 1m 26s\n",
      "718:\tlearn: 0.1984251\ttotal: 3m 39s\tremaining: 1m 25s\n",
      "719:\tlearn: 0.1984210\ttotal: 3m 40s\tremaining: 1m 25s\n",
      "720:\tlearn: 0.1984176\ttotal: 3m 40s\tremaining: 1m 25s\n",
      "721:\tlearn: 0.1984144\ttotal: 3m 40s\tremaining: 1m 25s\n",
      "722:\tlearn: 0.1984070\ttotal: 3m 41s\tremaining: 1m 24s\n",
      "723:\tlearn: 0.1983961\ttotal: 3m 41s\tremaining: 1m 24s\n",
      "724:\tlearn: 0.1983819\ttotal: 3m 41s\tremaining: 1m 24s\n",
      "725:\tlearn: 0.1983721\ttotal: 3m 42s\tremaining: 1m 23s\n",
      "726:\tlearn: 0.1983682\ttotal: 3m 42s\tremaining: 1m 23s\n",
      "727:\tlearn: 0.1983637\ttotal: 3m 42s\tremaining: 1m 23s\n",
      "728:\tlearn: 0.1983605\ttotal: 3m 43s\tremaining: 1m 22s\n",
      "729:\tlearn: 0.1983525\ttotal: 3m 43s\tremaining: 1m 22s\n",
      "730:\tlearn: 0.1983357\ttotal: 3m 43s\tremaining: 1m 22s\n",
      "731:\tlearn: 0.1983102\ttotal: 3m 44s\tremaining: 1m 22s\n",
      "732:\tlearn: 0.1983023\ttotal: 3m 44s\tremaining: 1m 21s\n",
      "733:\tlearn: 0.1983001\ttotal: 3m 44s\tremaining: 1m 21s\n",
      "734:\tlearn: 0.1982946\ttotal: 3m 45s\tremaining: 1m 21s\n",
      "735:\tlearn: 0.1982913\ttotal: 3m 45s\tremaining: 1m 20s\n",
      "736:\tlearn: 0.1982779\ttotal: 3m 45s\tremaining: 1m 20s\n",
      "737:\tlearn: 0.1982680\ttotal: 3m 46s\tremaining: 1m 20s\n",
      "738:\tlearn: 0.1982506\ttotal: 3m 46s\tremaining: 1m 19s\n",
      "739:\tlearn: 0.1982340\ttotal: 3m 46s\tremaining: 1m 19s\n",
      "740:\tlearn: 0.1982128\ttotal: 3m 47s\tremaining: 1m 19s\n",
      "741:\tlearn: 0.1982004\ttotal: 3m 47s\tremaining: 1m 19s\n",
      "742:\tlearn: 0.1981830\ttotal: 3m 47s\tremaining: 1m 18s\n",
      "743:\tlearn: 0.1981780\ttotal: 3m 48s\tremaining: 1m 18s\n",
      "744:\tlearn: 0.1981759\ttotal: 3m 48s\tremaining: 1m 18s\n",
      "745:\tlearn: 0.1981726\ttotal: 3m 48s\tremaining: 1m 17s\n",
      "746:\tlearn: 0.1981553\ttotal: 3m 49s\tremaining: 1m 17s\n",
      "747:\tlearn: 0.1981515\ttotal: 3m 49s\tremaining: 1m 17s\n",
      "748:\tlearn: 0.1981424\ttotal: 3m 49s\tremaining: 1m 17s\n",
      "749:\tlearn: 0.1981252\ttotal: 3m 50s\tremaining: 1m 16s\n",
      "750:\tlearn: 0.1980844\ttotal: 3m 50s\tremaining: 1m 16s\n",
      "751:\tlearn: 0.1980808\ttotal: 3m 50s\tremaining: 1m 16s\n",
      "752:\tlearn: 0.1980787\ttotal: 3m 51s\tremaining: 1m 15s\n",
      "753:\tlearn: 0.1980627\ttotal: 3m 51s\tremaining: 1m 15s\n",
      "754:\tlearn: 0.1980616\ttotal: 3m 51s\tremaining: 1m 15s\n",
      "755:\tlearn: 0.1980394\ttotal: 3m 51s\tremaining: 1m 14s\n",
      "756:\tlearn: 0.1980167\ttotal: 3m 52s\tremaining: 1m 14s\n",
      "757:\tlearn: 0.1980139\ttotal: 3m 52s\tremaining: 1m 14s\n",
      "758:\tlearn: 0.1980091\ttotal: 3m 53s\tremaining: 1m 14s\n",
      "759:\tlearn: 0.1980052\ttotal: 3m 53s\tremaining: 1m 13s\n",
      "760:\tlearn: 0.1979836\ttotal: 3m 53s\tremaining: 1m 13s\n",
      "761:\tlearn: 0.1979768\ttotal: 3m 53s\tremaining: 1m 13s\n",
      "762:\tlearn: 0.1979739\ttotal: 3m 54s\tremaining: 1m 12s\n",
      "763:\tlearn: 0.1979561\ttotal: 3m 54s\tremaining: 1m 12s\n",
      "764:\tlearn: 0.1979532\ttotal: 3m 54s\tremaining: 1m 12s\n",
      "765:\tlearn: 0.1979434\ttotal: 3m 55s\tremaining: 1m 11s\n",
      "766:\tlearn: 0.1979291\ttotal: 3m 55s\tremaining: 1m 11s\n",
      "767:\tlearn: 0.1979214\ttotal: 3m 55s\tremaining: 1m 11s\n",
      "768:\tlearn: 0.1979043\ttotal: 3m 56s\tremaining: 1m 10s\n",
      "769:\tlearn: 0.1979006\ttotal: 3m 56s\tremaining: 1m 10s\n",
      "770:\tlearn: 0.1978845\ttotal: 3m 56s\tremaining: 1m 10s\n",
      "771:\tlearn: 0.1978564\ttotal: 3m 57s\tremaining: 1m 10s\n",
      "772:\tlearn: 0.1978498\ttotal: 3m 57s\tremaining: 1m 9s\n",
      "773:\tlearn: 0.1978445\ttotal: 3m 57s\tremaining: 1m 9s\n",
      "774:\tlearn: 0.1978361\ttotal: 3m 58s\tremaining: 1m 9s\n",
      "775:\tlearn: 0.1978263\ttotal: 3m 58s\tremaining: 1m 8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776:\tlearn: 0.1978104\ttotal: 3m 58s\tremaining: 1m 8s\n",
      "777:\tlearn: 0.1978075\ttotal: 3m 59s\tremaining: 1m 8s\n",
      "778:\tlearn: 0.1978034\ttotal: 3m 59s\tremaining: 1m 7s\n",
      "779:\tlearn: 0.1977894\ttotal: 3m 59s\tremaining: 1m 7s\n",
      "780:\tlearn: 0.1977877\ttotal: 4m\tremaining: 1m 7s\n",
      "781:\tlearn: 0.1977653\ttotal: 4m\tremaining: 1m 7s\n",
      "782:\tlearn: 0.1977621\ttotal: 4m\tremaining: 1m 6s\n",
      "783:\tlearn: 0.1977397\ttotal: 4m 1s\tremaining: 1m 6s\n",
      "784:\tlearn: 0.1977381\ttotal: 4m 1s\tremaining: 1m 6s\n",
      "785:\tlearn: 0.1977255\ttotal: 4m 1s\tremaining: 1m 5s\n",
      "786:\tlearn: 0.1977171\ttotal: 4m 2s\tremaining: 1m 5s\n",
      "787:\tlearn: 0.1977031\ttotal: 4m 2s\tremaining: 1m 5s\n",
      "788:\tlearn: 0.1976945\ttotal: 4m 2s\tremaining: 1m 4s\n",
      "789:\tlearn: 0.1976887\ttotal: 4m 3s\tremaining: 1m 4s\n",
      "790:\tlearn: 0.1976763\ttotal: 4m 3s\tremaining: 1m 4s\n",
      "791:\tlearn: 0.1976674\ttotal: 4m 3s\tremaining: 1m 3s\n",
      "792:\tlearn: 0.1976620\ttotal: 4m 3s\tremaining: 1m 3s\n",
      "793:\tlearn: 0.1976421\ttotal: 4m 4s\tremaining: 1m 3s\n",
      "794:\tlearn: 0.1976327\ttotal: 4m 4s\tremaining: 1m 3s\n",
      "795:\tlearn: 0.1976244\ttotal: 4m 4s\tremaining: 1m 2s\n",
      "796:\tlearn: 0.1976228\ttotal: 4m 5s\tremaining: 1m 2s\n",
      "797:\tlearn: 0.1976128\ttotal: 4m 5s\tremaining: 1m 2s\n",
      "798:\tlearn: 0.1975944\ttotal: 4m 5s\tremaining: 1m 1s\n",
      "799:\tlearn: 0.1975886\ttotal: 4m 6s\tremaining: 1m 1s\n",
      "800:\tlearn: 0.1975842\ttotal: 4m 6s\tremaining: 1m 1s\n",
      "801:\tlearn: 0.1975773\ttotal: 4m 6s\tremaining: 1m\n",
      "802:\tlearn: 0.1975685\ttotal: 4m 7s\tremaining: 1m\n",
      "803:\tlearn: 0.1975629\ttotal: 4m 7s\tremaining: 1m\n",
      "804:\tlearn: 0.1975607\ttotal: 4m 7s\tremaining: 1m\n",
      "805:\tlearn: 0.1975473\ttotal: 4m 8s\tremaining: 59.7s\n",
      "806:\tlearn: 0.1975328\ttotal: 4m 8s\tremaining: 59.4s\n",
      "807:\tlearn: 0.1975286\ttotal: 4m 8s\tremaining: 59.1s\n",
      "808:\tlearn: 0.1975188\ttotal: 4m 8s\tremaining: 58.8s\n",
      "809:\tlearn: 0.1975120\ttotal: 4m 9s\tremaining: 58.5s\n",
      "810:\tlearn: 0.1974918\ttotal: 4m 9s\tremaining: 58.2s\n",
      "811:\tlearn: 0.1974865\ttotal: 4m 10s\tremaining: 57.9s\n",
      "812:\tlearn: 0.1974814\ttotal: 4m 10s\tremaining: 57.6s\n",
      "813:\tlearn: 0.1974739\ttotal: 4m 10s\tremaining: 57.3s\n",
      "814:\tlearn: 0.1974721\ttotal: 4m 10s\tremaining: 57s\n",
      "815:\tlearn: 0.1974709\ttotal: 4m 11s\tremaining: 56.6s\n",
      "816:\tlearn: 0.1974563\ttotal: 4m 11s\tremaining: 56.4s\n",
      "817:\tlearn: 0.1974540\ttotal: 4m 12s\tremaining: 56.1s\n",
      "818:\tlearn: 0.1974471\ttotal: 4m 12s\tremaining: 55.8s\n",
      "819:\tlearn: 0.1974447\ttotal: 4m 12s\tremaining: 55.5s\n",
      "820:\tlearn: 0.1974287\ttotal: 4m 13s\tremaining: 55.2s\n",
      "821:\tlearn: 0.1974244\ttotal: 4m 13s\tremaining: 54.9s\n",
      "822:\tlearn: 0.1974223\ttotal: 4m 13s\tremaining: 54.6s\n",
      "823:\tlearn: 0.1974171\ttotal: 4m 14s\tremaining: 54.3s\n",
      "824:\tlearn: 0.1974150\ttotal: 4m 14s\tremaining: 54s\n",
      "825:\tlearn: 0.1973955\ttotal: 4m 14s\tremaining: 53.7s\n",
      "826:\tlearn: 0.1973871\ttotal: 4m 15s\tremaining: 53.4s\n",
      "827:\tlearn: 0.1973811\ttotal: 4m 15s\tremaining: 53.1s\n",
      "828:\tlearn: 0.1973699\ttotal: 4m 15s\tremaining: 52.8s\n",
      "829:\tlearn: 0.1973626\ttotal: 4m 16s\tremaining: 52.5s\n",
      "830:\tlearn: 0.1973237\ttotal: 4m 16s\tremaining: 52.2s\n",
      "831:\tlearn: 0.1973198\ttotal: 4m 17s\tremaining: 51.9s\n",
      "832:\tlearn: 0.1973060\ttotal: 4m 17s\tremaining: 51.6s\n",
      "833:\tlearn: 0.1972958\ttotal: 4m 17s\tremaining: 51.3s\n",
      "834:\tlearn: 0.1972813\ttotal: 4m 18s\tremaining: 51s\n",
      "835:\tlearn: 0.1972657\ttotal: 4m 18s\tremaining: 50.7s\n",
      "836:\tlearn: 0.1972636\ttotal: 4m 18s\tremaining: 50.4s\n",
      "837:\tlearn: 0.1972562\ttotal: 4m 19s\tremaining: 50.1s\n",
      "838:\tlearn: 0.1972531\ttotal: 4m 19s\tremaining: 49.8s\n",
      "839:\tlearn: 0.1972516\ttotal: 4m 19s\tremaining: 49.5s\n",
      "840:\tlearn: 0.1972501\ttotal: 4m 20s\tremaining: 49.2s\n",
      "841:\tlearn: 0.1972471\ttotal: 4m 20s\tremaining: 48.9s\n",
      "842:\tlearn: 0.1972412\ttotal: 4m 20s\tremaining: 48.6s\n",
      "843:\tlearn: 0.1972392\ttotal: 4m 21s\tremaining: 48.3s\n",
      "844:\tlearn: 0.1972379\ttotal: 4m 21s\tremaining: 47.9s\n",
      "845:\tlearn: 0.1972243\ttotal: 4m 21s\tremaining: 47.6s\n",
      "846:\tlearn: 0.1972207\ttotal: 4m 22s\tremaining: 47.3s\n",
      "847:\tlearn: 0.1972120\ttotal: 4m 22s\tremaining: 47s\n",
      "848:\tlearn: 0.1971746\ttotal: 4m 22s\tremaining: 46.7s\n",
      "849:\tlearn: 0.1971705\ttotal: 4m 22s\tremaining: 46.4s\n",
      "850:\tlearn: 0.1971686\ttotal: 4m 23s\tremaining: 46.1s\n",
      "851:\tlearn: 0.1971592\ttotal: 4m 23s\tremaining: 45.8s\n",
      "852:\tlearn: 0.1971406\ttotal: 4m 23s\tremaining: 45.5s\n",
      "853:\tlearn: 0.1971331\ttotal: 4m 24s\tremaining: 45.2s\n",
      "854:\tlearn: 0.1971277\ttotal: 4m 24s\tremaining: 44.9s\n",
      "855:\tlearn: 0.1971126\ttotal: 4m 24s\tremaining: 44.6s\n",
      "856:\tlearn: 0.1971103\ttotal: 4m 25s\tremaining: 44.2s\n",
      "857:\tlearn: 0.1971058\ttotal: 4m 25s\tremaining: 43.9s\n",
      "858:\tlearn: 0.1970885\ttotal: 4m 25s\tremaining: 43.6s\n",
      "859:\tlearn: 0.1970861\ttotal: 4m 26s\tremaining: 43.3s\n",
      "860:\tlearn: 0.1970743\ttotal: 4m 26s\tremaining: 43s\n",
      "861:\tlearn: 0.1970660\ttotal: 4m 26s\tremaining: 42.7s\n",
      "862:\tlearn: 0.1970571\ttotal: 4m 27s\tremaining: 42.4s\n",
      "863:\tlearn: 0.1970546\ttotal: 4m 27s\tremaining: 42.1s\n",
      "864:\tlearn: 0.1970531\ttotal: 4m 27s\tremaining: 41.8s\n",
      "865:\tlearn: 0.1970412\ttotal: 4m 28s\tremaining: 41.5s\n",
      "866:\tlearn: 0.1970363\ttotal: 4m 28s\tremaining: 41.2s\n",
      "867:\tlearn: 0.1970311\ttotal: 4m 29s\tremaining: 40.9s\n",
      "868:\tlearn: 0.1970279\ttotal: 4m 29s\tremaining: 40.6s\n",
      "869:\tlearn: 0.1970242\ttotal: 4m 29s\tremaining: 40.3s\n",
      "870:\tlearn: 0.1970186\ttotal: 4m 30s\tremaining: 40s\n",
      "871:\tlearn: 0.1970106\ttotal: 4m 30s\tremaining: 39.7s\n",
      "872:\tlearn: 0.1970059\ttotal: 4m 30s\tremaining: 39.4s\n",
      "873:\tlearn: 0.1969933\ttotal: 4m 31s\tremaining: 39.1s\n",
      "874:\tlearn: 0.1969919\ttotal: 4m 31s\tremaining: 38.8s\n",
      "875:\tlearn: 0.1969778\ttotal: 4m 31s\tremaining: 38.5s\n",
      "876:\tlearn: 0.1969501\ttotal: 4m 32s\tremaining: 38.2s\n",
      "877:\tlearn: 0.1969390\ttotal: 4m 32s\tremaining: 37.9s\n",
      "878:\tlearn: 0.1969350\ttotal: 4m 32s\tremaining: 37.5s\n",
      "879:\tlearn: 0.1969315\ttotal: 4m 33s\tremaining: 37.2s\n",
      "880:\tlearn: 0.1969150\ttotal: 4m 33s\tremaining: 36.9s\n",
      "881:\tlearn: 0.1969124\ttotal: 4m 33s\tremaining: 36.6s\n",
      "882:\tlearn: 0.1968966\ttotal: 4m 34s\tremaining: 36.3s\n",
      "883:\tlearn: 0.1968734\ttotal: 4m 34s\tremaining: 36s\n",
      "884:\tlearn: 0.1968607\ttotal: 4m 34s\tremaining: 35.7s\n",
      "885:\tlearn: 0.1968555\ttotal: 4m 35s\tremaining: 35.4s\n",
      "886:\tlearn: 0.1968473\ttotal: 4m 35s\tremaining: 35.1s\n",
      "887:\tlearn: 0.1968457\ttotal: 4m 35s\tremaining: 34.8s\n",
      "888:\tlearn: 0.1968354\ttotal: 4m 36s\tremaining: 34.5s\n",
      "889:\tlearn: 0.1968322\ttotal: 4m 36s\tremaining: 34.2s\n",
      "890:\tlearn: 0.1968248\ttotal: 4m 36s\tremaining: 33.9s\n",
      "891:\tlearn: 0.1968214\ttotal: 4m 37s\tremaining: 33.6s\n",
      "892:\tlearn: 0.1968069\ttotal: 4m 37s\tremaining: 33.3s\n",
      "893:\tlearn: 0.1967765\ttotal: 4m 38s\tremaining: 33s\n",
      "894:\tlearn: 0.1967729\ttotal: 4m 38s\tremaining: 32.7s\n",
      "895:\tlearn: 0.1967608\ttotal: 4m 38s\tremaining: 32.3s\n",
      "896:\tlearn: 0.1967510\ttotal: 4m 39s\tremaining: 32s\n",
      "897:\tlearn: 0.1967424\ttotal: 4m 39s\tremaining: 31.7s\n",
      "898:\tlearn: 0.1967406\ttotal: 4m 39s\tremaining: 31.4s\n",
      "899:\tlearn: 0.1967230\ttotal: 4m 40s\tremaining: 31.1s\n",
      "900:\tlearn: 0.1967154\ttotal: 4m 40s\tremaining: 30.8s\n",
      "901:\tlearn: 0.1967033\ttotal: 4m 40s\tremaining: 30.5s\n",
      "902:\tlearn: 0.1966889\ttotal: 4m 41s\tremaining: 30.2s\n",
      "903:\tlearn: 0.1966808\ttotal: 4m 41s\tremaining: 29.9s\n",
      "904:\tlearn: 0.1966675\ttotal: 4m 41s\tremaining: 29.6s\n",
      "905:\tlearn: 0.1966560\ttotal: 4m 42s\tremaining: 29.3s\n",
      "906:\tlearn: 0.1966405\ttotal: 4m 42s\tremaining: 29s\n",
      "907:\tlearn: 0.1966219\ttotal: 4m 42s\tremaining: 28.7s\n",
      "908:\tlearn: 0.1966123\ttotal: 4m 43s\tremaining: 28.3s\n",
      "909:\tlearn: 0.1966067\ttotal: 4m 43s\tremaining: 28s\n",
      "910:\tlearn: 0.1965934\ttotal: 4m 43s\tremaining: 27.7s\n",
      "911:\tlearn: 0.1965909\ttotal: 4m 44s\tremaining: 27.4s\n",
      "912:\tlearn: 0.1965761\ttotal: 4m 44s\tremaining: 27.1s\n",
      "913:\tlearn: 0.1965741\ttotal: 4m 44s\tremaining: 26.8s\n",
      "914:\tlearn: 0.1965556\ttotal: 4m 45s\tremaining: 26.5s\n",
      "915:\tlearn: 0.1965409\ttotal: 4m 45s\tremaining: 26.2s\n",
      "916:\tlearn: 0.1965225\ttotal: 4m 45s\tremaining: 25.9s\n",
      "917:\tlearn: 0.1965177\ttotal: 4m 46s\tremaining: 25.6s\n",
      "918:\tlearn: 0.1965063\ttotal: 4m 46s\tremaining: 25.3s\n",
      "919:\tlearn: 0.1965028\ttotal: 4m 46s\tremaining: 24.9s\n",
      "920:\tlearn: 0.1964997\ttotal: 4m 47s\tremaining: 24.6s\n",
      "921:\tlearn: 0.1964943\ttotal: 4m 47s\tremaining: 24.3s\n",
      "922:\tlearn: 0.1964680\ttotal: 4m 48s\tremaining: 24s\n",
      "923:\tlearn: 0.1964619\ttotal: 4m 48s\tremaining: 23.7s\n",
      "924:\tlearn: 0.1964594\ttotal: 4m 48s\tremaining: 23.4s\n",
      "925:\tlearn: 0.1964539\ttotal: 4m 49s\tremaining: 23.1s\n",
      "926:\tlearn: 0.1964520\ttotal: 4m 49s\tremaining: 22.8s\n",
      "927:\tlearn: 0.1964331\ttotal: 4m 49s\tremaining: 22.5s\n",
      "928:\tlearn: 0.1964264\ttotal: 4m 50s\tremaining: 22.2s\n",
      "929:\tlearn: 0.1964120\ttotal: 4m 50s\tremaining: 21.9s\n",
      "930:\tlearn: 0.1964076\ttotal: 4m 51s\tremaining: 21.6s\n",
      "931:\tlearn: 0.1964027\ttotal: 4m 51s\tremaining: 21.3s\n",
      "932:\tlearn: 0.1964002\ttotal: 4m 52s\tremaining: 21s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "933:\tlearn: 0.1963884\ttotal: 4m 52s\tremaining: 20.7s\n",
      "934:\tlearn: 0.1963722\ttotal: 4m 52s\tremaining: 20.4s\n",
      "935:\tlearn: 0.1963565\ttotal: 4m 53s\tremaining: 20s\n",
      "936:\tlearn: 0.1963475\ttotal: 4m 53s\tremaining: 19.7s\n",
      "937:\tlearn: 0.1963354\ttotal: 4m 53s\tremaining: 19.4s\n",
      "938:\tlearn: 0.1963329\ttotal: 4m 54s\tremaining: 19.1s\n",
      "939:\tlearn: 0.1963304\ttotal: 4m 54s\tremaining: 18.8s\n",
      "940:\tlearn: 0.1963191\ttotal: 4m 54s\tremaining: 18.5s\n",
      "941:\tlearn: 0.1963139\ttotal: 4m 55s\tremaining: 18.2s\n",
      "942:\tlearn: 0.1963082\ttotal: 4m 55s\tremaining: 17.9s\n",
      "943:\tlearn: 0.1962885\ttotal: 4m 55s\tremaining: 17.5s\n",
      "944:\tlearn: 0.1962817\ttotal: 4m 56s\tremaining: 17.2s\n",
      "945:\tlearn: 0.1962779\ttotal: 4m 56s\tremaining: 16.9s\n",
      "946:\tlearn: 0.1962754\ttotal: 4m 56s\tremaining: 16.6s\n",
      "947:\tlearn: 0.1962630\ttotal: 4m 57s\tremaining: 16.3s\n",
      "948:\tlearn: 0.1962613\ttotal: 4m 57s\tremaining: 16s\n",
      "949:\tlearn: 0.1962502\ttotal: 4m 57s\tremaining: 15.7s\n",
      "950:\tlearn: 0.1962395\ttotal: 4m 58s\tremaining: 15.4s\n",
      "951:\tlearn: 0.1962278\ttotal: 4m 58s\tremaining: 15s\n",
      "952:\tlearn: 0.1962206\ttotal: 4m 58s\tremaining: 14.7s\n",
      "953:\tlearn: 0.1962126\ttotal: 4m 59s\tremaining: 14.4s\n",
      "954:\tlearn: 0.1962094\ttotal: 4m 59s\tremaining: 14.1s\n",
      "955:\tlearn: 0.1961822\ttotal: 4m 59s\tremaining: 13.8s\n",
      "956:\tlearn: 0.1961790\ttotal: 4m 59s\tremaining: 13.5s\n",
      "957:\tlearn: 0.1961707\ttotal: 5m\tremaining: 13.2s\n",
      "958:\tlearn: 0.1961670\ttotal: 5m\tremaining: 12.9s\n",
      "959:\tlearn: 0.1961552\ttotal: 5m\tremaining: 12.5s\n",
      "960:\tlearn: 0.1961457\ttotal: 5m 1s\tremaining: 12.2s\n",
      "961:\tlearn: 0.1961087\ttotal: 5m 1s\tremaining: 11.9s\n",
      "962:\tlearn: 0.1960977\ttotal: 5m 2s\tremaining: 11.6s\n",
      "963:\tlearn: 0.1960937\ttotal: 5m 2s\tremaining: 11.3s\n",
      "964:\tlearn: 0.1960867\ttotal: 5m 2s\tremaining: 11s\n",
      "965:\tlearn: 0.1960801\ttotal: 5m 3s\tremaining: 10.7s\n",
      "966:\tlearn: 0.1960759\ttotal: 5m 3s\tremaining: 10.4s\n",
      "967:\tlearn: 0.1960683\ttotal: 5m 3s\tremaining: 10s\n",
      "968:\tlearn: 0.1960671\ttotal: 5m 4s\tremaining: 9.73s\n",
      "969:\tlearn: 0.1960499\ttotal: 5m 4s\tremaining: 9.42s\n",
      "970:\tlearn: 0.1960362\ttotal: 5m 4s\tremaining: 9.1s\n",
      "971:\tlearn: 0.1960321\ttotal: 5m 5s\tremaining: 8.79s\n",
      "972:\tlearn: 0.1960295\ttotal: 5m 5s\tremaining: 8.47s\n",
      "973:\tlearn: 0.1960274\ttotal: 5m 5s\tremaining: 8.16s\n",
      "974:\tlearn: 0.1960228\ttotal: 5m 6s\tremaining: 7.85s\n",
      "975:\tlearn: 0.1960160\ttotal: 5m 6s\tremaining: 7.53s\n",
      "976:\tlearn: 0.1960084\ttotal: 5m 6s\tremaining: 7.22s\n",
      "977:\tlearn: 0.1960042\ttotal: 5m 7s\tremaining: 6.91s\n",
      "978:\tlearn: 0.1959987\ttotal: 5m 7s\tremaining: 6.59s\n",
      "979:\tlearn: 0.1959901\ttotal: 5m 7s\tremaining: 6.28s\n",
      "980:\tlearn: 0.1959777\ttotal: 5m 8s\tremaining: 5.97s\n",
      "981:\tlearn: 0.1959666\ttotal: 5m 8s\tremaining: 5.65s\n",
      "982:\tlearn: 0.1959627\ttotal: 5m 8s\tremaining: 5.34s\n",
      "983:\tlearn: 0.1959548\ttotal: 5m 9s\tremaining: 5.03s\n",
      "984:\tlearn: 0.1959201\ttotal: 5m 9s\tremaining: 4.72s\n",
      "985:\tlearn: 0.1959127\ttotal: 5m 10s\tremaining: 4.4s\n",
      "986:\tlearn: 0.1959039\ttotal: 5m 10s\tremaining: 4.09s\n",
      "987:\tlearn: 0.1959017\ttotal: 5m 10s\tremaining: 3.77s\n",
      "988:\tlearn: 0.1958876\ttotal: 5m 11s\tremaining: 3.46s\n",
      "989:\tlearn: 0.1958801\ttotal: 5m 11s\tremaining: 3.15s\n",
      "990:\tlearn: 0.1958699\ttotal: 5m 11s\tremaining: 2.83s\n",
      "991:\tlearn: 0.1958668\ttotal: 5m 12s\tremaining: 2.52s\n",
      "992:\tlearn: 0.1958656\ttotal: 5m 12s\tremaining: 2.2s\n",
      "993:\tlearn: 0.1958594\ttotal: 5m 12s\tremaining: 1.89s\n",
      "994:\tlearn: 0.1958528\ttotal: 5m 13s\tremaining: 1.57s\n",
      "995:\tlearn: 0.1958492\ttotal: 5m 13s\tremaining: 1.26s\n",
      "996:\tlearn: 0.1958286\ttotal: 5m 13s\tremaining: 945ms\n",
      "997:\tlearn: 0.1958131\ttotal: 5m 14s\tremaining: 630ms\n",
      "998:\tlearn: 0.1957979\ttotal: 5m 14s\tremaining: 315ms\n",
      "999:\tlearn: 0.1957941\ttotal: 5m 15s\tremaining: 0us\n",
      "CatBoostClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      1.00      0.96      9255\n",
      "         1.0       0.92      0.07      0.12       745\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     10000\n",
      "   macro avg       0.93      0.53      0.54     10000\n",
      "weighted avg       0.93      0.93      0.90     10000\n",
      "\n",
      "[[9251    4]\n",
      " [ 696   49]]\n",
      "Accuracy is  93.0\n",
      "Time on model's work: 326.548 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      1.00      0.96      9255\n",
      "         1.0       0.98      0.06      0.12       745\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     10000\n",
      "   macro avg       0.95      0.53      0.54     10000\n",
      "weighted avg       0.93      0.93      0.90     10000\n",
      "\n",
      "[[9254    1]\n",
      " [ 699   46]]\n",
      "Accuracy is  93.0\n",
      "Time on model's work: 1.156 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      1.00      0.96      9255\n",
      "         1.0       1.00      0.06      0.11       745\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     10000\n",
      "   macro avg       0.96      0.53      0.54     10000\n",
      "weighted avg       0.93      0.93      0.90     10000\n",
      "\n",
      "[[9255    0]\n",
      " [ 700   45]]\n",
      "Accuracy is  93.0\n",
      "Time on model's work: 0.846 s\n",
      "====================================================================================================\n",
      "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n",
      "TFFMClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      1.00      0.96      9255\n",
      "         1.0       0.65      0.05      0.09       745\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     10000\n",
      "   macro avg       0.79      0.52      0.53     10000\n",
      "weighted avg       0.91      0.93      0.90     10000\n",
      "\n",
      "[[9235   20]\n",
      " [ 708   37]]\n",
      "Accuracy is  92.72\n",
      "Time on model's work: 137.038 s\n",
      "====================================================================================================\n",
      "TOTAL TIME SPENT:  2634.366 s\n"
     ]
    }
   ],
   "source": [
    "# Testing different classifiers\n",
    "clfs = [\n",
    "        ['RandomForestClassifier', RandomForestClassifier()],\n",
    "        ['GradientBoostingClassifier', GradientBoostingClassifier()],\n",
    "        ['ExtraTreesClassifier', ExtraTreesClassifier()],\n",
    "        ['AdaBoostClassifier', AdaBoostClassifier()],\n",
    "        ['BaggingClassifier', BaggingClassifier()],\n",
    "        ['DecisionTreeClassifier', DecisionTreeClassifier()],\n",
    "        ['MLPClassifier', MLPClassifier()],\n",
    "        ['XGBClassifier', XGBClassifier(silent=False)],\n",
    "        ['CatBoostClassifier', CatBoostClassifier()],\n",
    "        ['LogisticRegression', LogisticRegression()],\n",
    "        ['SGDClassifier', SGDClassifier()],\n",
    "        ['TFFMClassifier', TFFMClassifier()]\n",
    "       ]\n",
    "t = time()\n",
    "for name, clf in clfs:\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    prediction = clf.predict(X_test)\n",
    "    print(name)\n",
    "    print(classification_report(y_test, prediction))\n",
    "    print(confusion_matrix(y_test, prediction))\n",
    "    print('Accuracy is ', accuracy_score(y_test, prediction)*100)\n",
    "    print (\"Time on model's work:\", round(time()-t0, 3), \"s\")\n",
    "    print('='*100)\n",
    "print (\"TOTAL TIME SPENT: \", round(time()-t, 3), \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models were not able to predict imbalanced clas '1' correctly. Let's use some methods to deal with imbalanced classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Resampling variant\n",
    "a) delete instances from the over-represented class - under-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2 dataframe with all clicked rows and select the same number of rows as in '1' from '0' data \n",
    "data_1 = data[data['Is_lp_click'] == 1]\n",
    "data_0 = data[data['Is_lp_click'] == 0].sample(35059, random_state = 3)\n",
    "# concat 2 dataframes and shuffle it\n",
    "data_concat = pd.concat([data_1, data_0])\n",
    "data_under = data_concat.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time on encoding: 69.695 s\n"
     ]
    }
   ],
   "source": [
    "# features & labels\n",
    "labels = data_under['Is_lp_click']\n",
    "features = data_under.drop('Is_lp_click', axis = 1)\n",
    "# create the dictionary of unique attributes' lists\n",
    "unique_attr = {}\n",
    "for attr in features.columns:\n",
    "    unique_attr[attr] = features[attr].unique().tolist()\n",
    "unique_attr\n",
    "# features encoding\n",
    "encoder = preprocessing.OneHotEncoder(categories=[unique_attr[i] for i in unique_attr], sparse = False, handle_unknown='ignore')\n",
    "encoder\n",
    "features['CONCAT'] = features.values.tolist()\n",
    "features['CONCAT'].head()\n",
    "t0 = time()\n",
    "features['ENCODED'] = [encoder.fit_transform([i]).flatten() for i in features['CONCAT']]\n",
    "print (\"time on encoding:\", round(time()-t0, 3), \"s\")\n",
    "# create the array with feature vectors\n",
    "features_list = [list(i) for i in features['ENCODED']]\n",
    "features_list_array = np.array(features_list)\n",
    "# create the array with label vector \n",
    "labels_list_array = np.array(labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features_list_array, labels_list_array, random_state=35, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.69      0.72      6986\n",
      "         1.0       0.71      0.77      0.74      7038\n",
      "\n",
      "   micro avg       0.73      0.73      0.73     14024\n",
      "   macro avg       0.73      0.73      0.73     14024\n",
      "weighted avg       0.73      0.73      0.73     14024\n",
      "\n",
      "[[4825 2161]\n",
      " [1624 5414]]\n",
      "Accuracy is  73.01055333713634\n",
      "Time on model's work: 20.163 s\n",
      "====================================================================================================\n",
      "GradientBoostingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.62      0.73      6986\n",
      "         1.0       0.71      0.92      0.80      7038\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     14024\n",
      "   macro avg       0.80      0.77      0.76     14024\n",
      "weighted avg       0.80      0.77      0.76     14024\n",
      "\n",
      "[[4304 2682]\n",
      " [ 537 6501]]\n",
      "Accuracy is  77.04649172846548\n",
      "Time on model's work: 736.97 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.71      0.72      6986\n",
      "         1.0       0.72      0.73      0.72      7038\n",
      "\n",
      "   micro avg       0.72      0.72      0.72     14024\n",
      "   macro avg       0.72      0.72      0.72     14024\n",
      "weighted avg       0.72      0.72      0.72     14024\n",
      "\n",
      "[[4948 2038]\n",
      " [1895 5143]]\n",
      "Accuracy is  71.95521962350256\n",
      "Time on model's work: 23.708 s\n",
      "====================================================================================================\n",
      "AdaBoostClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.62      0.73      6986\n",
      "         1.0       0.71      0.91      0.80      7038\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     14024\n",
      "   macro avg       0.79      0.77      0.76     14024\n",
      "weighted avg       0.79      0.77      0.76     14024\n",
      "\n",
      "[[4354 2632]\n",
      " [ 633 6405]]\n",
      "Accuracy is  76.71848260125499\n",
      "Time on model's work: 139.741 s\n",
      "====================================================================================================\n",
      "BaggingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.68      0.72      6986\n",
      "         1.0       0.71      0.78      0.74      7038\n",
      "\n",
      "   micro avg       0.73      0.73      0.73     14024\n",
      "   macro avg       0.73      0.73      0.73     14024\n",
      "weighted avg       0.73      0.73      0.73     14024\n",
      "\n",
      "[[4775 2211]\n",
      " [1561 5477]]\n",
      "Accuracy is  73.1032515687393\n",
      "Time on model's work: 116.26 s\n",
      "====================================================================================================\n",
      "DecisionTreeClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.71      0.71      6986\n",
      "         1.0       0.71      0.71      0.71      7038\n",
      "\n",
      "   micro avg       0.71      0.71      0.71     14024\n",
      "   macro avg       0.71      0.71      0.71     14024\n",
      "weighted avg       0.71      0.71      0.71     14024\n",
      "\n",
      "[[4988 1998]\n",
      " [2047 4991]]\n",
      "Accuracy is  71.15658870507701\n",
      "Time on model's work: 35.701 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.68      0.71      6986\n",
      "         1.0       0.71      0.77      0.74      7038\n",
      "\n",
      "   micro avg       0.73      0.73      0.73     14024\n",
      "   macro avg       0.73      0.73      0.72     14024\n",
      "weighted avg       0.73      0.73      0.72     14024\n",
      "\n",
      "[[4743 2243]\n",
      " [1611 5427]]\n",
      "Accuracy is  72.51853964632059\n",
      "Time on model's work: 1768.842 s\n",
      "====================================================================================================\n",
      "[18:10:47] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:10:52] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:10:57] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:11:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:11:06] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:11:11] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:11:16] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:11:20] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:11:25] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:11:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:11:36] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:11:41] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:11:46] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:11:50] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:11:55] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:12:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:12:04] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:12:09] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:12:14] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:12:18] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:12:23] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:12:28] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:12:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:12:37] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:12:42] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:12:47] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:12:51] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:12:56] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:13:01] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:13:06] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:13:11] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:13:15] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:13:20] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:13:25] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:13:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:13:34] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:13:39] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:13:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:13:48] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:13:53] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:13:58] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:14:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:14:07] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:14:12] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:14:16] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:14:21] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:14:26] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:14:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:14:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:14:40] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:14:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:14:50] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:14:54] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:14:59] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:15:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:15:08] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:15:13] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:15:17] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:15:22] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:15:26] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:15:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:15:36] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:15:41] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:15:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:15:50] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:15:55] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:05] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:10] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:15] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:20] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:25] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:34] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:39] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:48] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:52] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:16:57] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:01] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:06] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:10] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:15] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:19] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:24] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:38] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:42] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:47] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:52] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:17:56] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:18:01] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:18:06] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:18:11] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:18:16] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:18:20] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:18:25] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:18:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[18:18:34] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "XGBClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.61      0.73      6986\n",
      "         1.0       0.71      0.93      0.80      7038\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     14024\n",
      "   macro avg       0.80      0.77      0.76     14024\n",
      "weighted avg       0.80      0.77      0.76     14024\n",
      "\n",
      "[[4288 2698]\n",
      " [ 524 6514]]\n",
      "Accuracy is  77.02509982886481\n",
      "Time on model's work: 480.744 s\n",
      "====================================================================================================\n",
      "0:\tlearn: 0.6663111\ttotal: 508ms\tremaining: 8m 27s\n",
      "1:\tlearn: 0.6433391\ttotal: 965ms\tremaining: 8m 1s\n",
      "2:\tlearn: 0.6239241\ttotal: 1.41s\tremaining: 7m 49s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3:\tlearn: 0.6073020\ttotal: 1.86s\tremaining: 7m 44s\n",
      "4:\tlearn: 0.5929036\ttotal: 2.31s\tremaining: 7m 39s\n",
      "5:\tlearn: 0.5807676\ttotal: 2.71s\tremaining: 7m 29s\n",
      "6:\tlearn: 0.5704816\ttotal: 3.12s\tremaining: 7m 22s\n",
      "7:\tlearn: 0.5614763\ttotal: 3.51s\tremaining: 7m 15s\n",
      "8:\tlearn: 0.5534719\ttotal: 3.9s\tremaining: 7m 9s\n",
      "9:\tlearn: 0.5465642\ttotal: 4.29s\tremaining: 7m 4s\n",
      "10:\tlearn: 0.5403775\ttotal: 4.64s\tremaining: 6m 57s\n",
      "11:\tlearn: 0.5347500\ttotal: 5.07s\tremaining: 6m 57s\n",
      "12:\tlearn: 0.5300839\ttotal: 5.47s\tremaining: 6m 55s\n",
      "13:\tlearn: 0.5261025\ttotal: 5.81s\tremaining: 6m 49s\n",
      "14:\tlearn: 0.5225917\ttotal: 6.22s\tremaining: 6m 48s\n",
      "15:\tlearn: 0.5193816\ttotal: 6.59s\tremaining: 6m 45s\n",
      "16:\tlearn: 0.5164804\ttotal: 7s\tremaining: 6m 44s\n",
      "17:\tlearn: 0.5139630\ttotal: 7.35s\tremaining: 6m 41s\n",
      "18:\tlearn: 0.5117275\ttotal: 7.69s\tremaining: 6m 37s\n",
      "19:\tlearn: 0.5096184\ttotal: 8.08s\tremaining: 6m 35s\n",
      "20:\tlearn: 0.5078931\ttotal: 8.43s\tremaining: 6m 33s\n",
      "21:\tlearn: 0.5062018\ttotal: 8.82s\tremaining: 6m 32s\n",
      "22:\tlearn: 0.5046138\ttotal: 9.18s\tremaining: 6m 29s\n",
      "23:\tlearn: 0.5031299\ttotal: 9.58s\tremaining: 6m 29s\n",
      "24:\tlearn: 0.5017661\ttotal: 10s\tremaining: 6m 29s\n",
      "25:\tlearn: 0.5005262\ttotal: 10.4s\tremaining: 6m 31s\n",
      "26:\tlearn: 0.4995134\ttotal: 10.8s\tremaining: 6m 30s\n",
      "27:\tlearn: 0.4985962\ttotal: 11.3s\tremaining: 6m 31s\n",
      "28:\tlearn: 0.4977437\ttotal: 11.7s\tremaining: 6m 30s\n",
      "29:\tlearn: 0.4969295\ttotal: 12.1s\tremaining: 6m 32s\n",
      "30:\tlearn: 0.4961976\ttotal: 12.5s\tremaining: 6m 31s\n",
      "31:\tlearn: 0.4956614\ttotal: 12.9s\tremaining: 6m 30s\n",
      "32:\tlearn: 0.4951819\ttotal: 13.3s\tremaining: 6m 28s\n",
      "33:\tlearn: 0.4947237\ttotal: 13.7s\tremaining: 6m 27s\n",
      "34:\tlearn: 0.4942152\ttotal: 14.1s\tremaining: 6m 27s\n",
      "35:\tlearn: 0.4938496\ttotal: 14.4s\tremaining: 6m 26s\n",
      "36:\tlearn: 0.4935234\ttotal: 14.8s\tremaining: 6m 25s\n",
      "37:\tlearn: 0.4931643\ttotal: 15.2s\tremaining: 6m 24s\n",
      "38:\tlearn: 0.4928642\ttotal: 15.5s\tremaining: 6m 23s\n",
      "39:\tlearn: 0.4925014\ttotal: 15.9s\tremaining: 6m 22s\n",
      "40:\tlearn: 0.4922597\ttotal: 16.3s\tremaining: 6m 20s\n",
      "41:\tlearn: 0.4919966\ttotal: 16.7s\tremaining: 6m 20s\n",
      "42:\tlearn: 0.4917741\ttotal: 17s\tremaining: 6m 17s\n",
      "43:\tlearn: 0.4915429\ttotal: 17.4s\tremaining: 6m 17s\n",
      "44:\tlearn: 0.4913445\ttotal: 17.8s\tremaining: 6m 16s\n",
      "45:\tlearn: 0.4911472\ttotal: 18.1s\tremaining: 6m 16s\n",
      "46:\tlearn: 0.4909076\ttotal: 18.5s\tremaining: 6m 15s\n",
      "47:\tlearn: 0.4907406\ttotal: 18.9s\tremaining: 6m 15s\n",
      "48:\tlearn: 0.4905812\ttotal: 19.2s\tremaining: 6m 12s\n",
      "49:\tlearn: 0.4903567\ttotal: 19.5s\tremaining: 6m 10s\n",
      "50:\tlearn: 0.4902117\ttotal: 19.8s\tremaining: 6m 9s\n",
      "51:\tlearn: 0.4900278\ttotal: 20.2s\tremaining: 6m 7s\n",
      "52:\tlearn: 0.4898907\ttotal: 20.5s\tremaining: 6m 6s\n",
      "53:\tlearn: 0.4897361\ttotal: 20.9s\tremaining: 6m 5s\n",
      "54:\tlearn: 0.4896023\ttotal: 21.2s\tremaining: 6m 4s\n",
      "55:\tlearn: 0.4894471\ttotal: 21.5s\tremaining: 6m 2s\n",
      "56:\tlearn: 0.4893439\ttotal: 21.8s\tremaining: 6m\n",
      "57:\tlearn: 0.4892234\ttotal: 22.1s\tremaining: 5m 58s\n",
      "58:\tlearn: 0.4891525\ttotal: 22.4s\tremaining: 5m 57s\n",
      "59:\tlearn: 0.4890391\ttotal: 22.8s\tremaining: 5m 56s\n",
      "60:\tlearn: 0.4889279\ttotal: 23.1s\tremaining: 5m 56s\n",
      "61:\tlearn: 0.4888173\ttotal: 23.5s\tremaining: 5m 55s\n",
      "62:\tlearn: 0.4887489\ttotal: 23.7s\tremaining: 5m 52s\n",
      "63:\tlearn: 0.4886903\ttotal: 24s\tremaining: 5m 51s\n",
      "64:\tlearn: 0.4886042\ttotal: 24.3s\tremaining: 5m 50s\n",
      "65:\tlearn: 0.4885000\ttotal: 24.7s\tremaining: 5m 49s\n",
      "66:\tlearn: 0.4883732\ttotal: 25.1s\tremaining: 5m 49s\n",
      "67:\tlearn: 0.4882901\ttotal: 25.3s\tremaining: 5m 47s\n",
      "68:\tlearn: 0.4882121\ttotal: 25.7s\tremaining: 5m 46s\n",
      "69:\tlearn: 0.4881402\ttotal: 26s\tremaining: 5m 45s\n",
      "70:\tlearn: 0.4880135\ttotal: 26.3s\tremaining: 5m 44s\n",
      "71:\tlearn: 0.4879626\ttotal: 26.6s\tremaining: 5m 43s\n",
      "72:\tlearn: 0.4878306\ttotal: 27s\tremaining: 5m 42s\n",
      "73:\tlearn: 0.4877766\ttotal: 27.3s\tremaining: 5m 41s\n",
      "74:\tlearn: 0.4877307\ttotal: 27.6s\tremaining: 5m 40s\n",
      "75:\tlearn: 0.4876973\ttotal: 27.8s\tremaining: 5m 38s\n",
      "76:\tlearn: 0.4875978\ttotal: 28.1s\tremaining: 5m 36s\n",
      "77:\tlearn: 0.4875645\ttotal: 28.3s\tremaining: 5m 34s\n",
      "78:\tlearn: 0.4874662\ttotal: 28.7s\tremaining: 5m 34s\n",
      "79:\tlearn: 0.4874093\ttotal: 29s\tremaining: 5m 33s\n",
      "80:\tlearn: 0.4873736\ttotal: 29.3s\tremaining: 5m 32s\n",
      "81:\tlearn: 0.4873164\ttotal: 29.5s\tremaining: 5m 30s\n",
      "82:\tlearn: 0.4872728\ttotal: 29.8s\tremaining: 5m 29s\n",
      "83:\tlearn: 0.4872285\ttotal: 30.1s\tremaining: 5m 27s\n",
      "84:\tlearn: 0.4871945\ttotal: 30.3s\tremaining: 5m 26s\n",
      "85:\tlearn: 0.4871590\ttotal: 30.6s\tremaining: 5m 25s\n",
      "86:\tlearn: 0.4871284\ttotal: 30.8s\tremaining: 5m 23s\n",
      "87:\tlearn: 0.4870755\ttotal: 31.1s\tremaining: 5m 22s\n",
      "88:\tlearn: 0.4870288\ttotal: 31.4s\tremaining: 5m 21s\n",
      "89:\tlearn: 0.4869832\ttotal: 31.7s\tremaining: 5m 20s\n",
      "90:\tlearn: 0.4869364\ttotal: 31.9s\tremaining: 5m 18s\n",
      "91:\tlearn: 0.4869147\ttotal: 32.2s\tremaining: 5m 17s\n",
      "92:\tlearn: 0.4868762\ttotal: 32.4s\tremaining: 5m 16s\n",
      "93:\tlearn: 0.4868396\ttotal: 32.6s\tremaining: 5m 14s\n",
      "94:\tlearn: 0.4868073\ttotal: 32.9s\tremaining: 5m 13s\n",
      "95:\tlearn: 0.4867837\ttotal: 33.1s\tremaining: 5m 11s\n",
      "96:\tlearn: 0.4867485\ttotal: 33.4s\tremaining: 5m 10s\n",
      "97:\tlearn: 0.4867170\ttotal: 33.6s\tremaining: 5m 9s\n",
      "98:\tlearn: 0.4866705\ttotal: 33.9s\tremaining: 5m 8s\n",
      "99:\tlearn: 0.4866410\ttotal: 34.2s\tremaining: 5m 7s\n",
      "100:\tlearn: 0.4866097\ttotal: 34.4s\tremaining: 5m 6s\n",
      "101:\tlearn: 0.4865371\ttotal: 34.8s\tremaining: 5m 6s\n",
      "102:\tlearn: 0.4865091\ttotal: 35s\tremaining: 5m 5s\n",
      "103:\tlearn: 0.4864840\ttotal: 35.3s\tremaining: 5m 3s\n",
      "104:\tlearn: 0.4864632\ttotal: 35.5s\tremaining: 5m 2s\n",
      "105:\tlearn: 0.4864231\ttotal: 35.8s\tremaining: 5m 1s\n",
      "106:\tlearn: 0.4864005\ttotal: 36s\tremaining: 5m\n",
      "107:\tlearn: 0.4863817\ttotal: 36.2s\tremaining: 4m 59s\n",
      "108:\tlearn: 0.4863500\ttotal: 36.5s\tremaining: 4m 58s\n",
      "109:\tlearn: 0.4863243\ttotal: 36.7s\tremaining: 4m 57s\n",
      "110:\tlearn: 0.4862832\ttotal: 37s\tremaining: 4m 56s\n",
      "111:\tlearn: 0.4862605\ttotal: 37.2s\tremaining: 4m 55s\n",
      "112:\tlearn: 0.4862190\ttotal: 37.5s\tremaining: 4m 54s\n",
      "113:\tlearn: 0.4862028\ttotal: 37.8s\tremaining: 4m 53s\n",
      "114:\tlearn: 0.4861634\ttotal: 38.1s\tremaining: 4m 52s\n",
      "115:\tlearn: 0.4861423\ttotal: 38.3s\tremaining: 4m 51s\n",
      "116:\tlearn: 0.4861224\ttotal: 38.5s\tremaining: 4m 50s\n",
      "117:\tlearn: 0.4860926\ttotal: 38.8s\tremaining: 4m 49s\n",
      "118:\tlearn: 0.4860622\ttotal: 39s\tremaining: 4m 48s\n",
      "119:\tlearn: 0.4860269\ttotal: 39.2s\tremaining: 4m 47s\n",
      "120:\tlearn: 0.4860055\ttotal: 39.5s\tremaining: 4m 46s\n",
      "121:\tlearn: 0.4859802\ttotal: 39.7s\tremaining: 4m 45s\n",
      "122:\tlearn: 0.4859653\ttotal: 40s\tremaining: 4m 44s\n",
      "123:\tlearn: 0.4859430\ttotal: 40.2s\tremaining: 4m 44s\n",
      "124:\tlearn: 0.4859211\ttotal: 40.4s\tremaining: 4m 43s\n",
      "125:\tlearn: 0.4858925\ttotal: 40.7s\tremaining: 4m 42s\n",
      "126:\tlearn: 0.4858358\ttotal: 41s\tremaining: 4m 41s\n",
      "127:\tlearn: 0.4858014\ttotal: 41.2s\tremaining: 4m 40s\n",
      "128:\tlearn: 0.4857878\ttotal: 41.4s\tremaining: 4m 39s\n",
      "129:\tlearn: 0.4857506\ttotal: 41.7s\tremaining: 4m 39s\n",
      "130:\tlearn: 0.4857270\ttotal: 41.9s\tremaining: 4m 38s\n",
      "131:\tlearn: 0.4857093\ttotal: 42.2s\tremaining: 4m 37s\n",
      "132:\tlearn: 0.4856773\ttotal: 42.4s\tremaining: 4m 36s\n",
      "133:\tlearn: 0.4856257\ttotal: 42.8s\tremaining: 4m 36s\n",
      "134:\tlearn: 0.4855691\ttotal: 43.2s\tremaining: 4m 36s\n",
      "135:\tlearn: 0.4855430\ttotal: 43.4s\tremaining: 4m 35s\n",
      "136:\tlearn: 0.4855202\ttotal: 43.7s\tremaining: 4m 35s\n",
      "137:\tlearn: 0.4855040\ttotal: 44s\tremaining: 4m 34s\n",
      "138:\tlearn: 0.4854909\ttotal: 44.2s\tremaining: 4m 33s\n",
      "139:\tlearn: 0.4854584\ttotal: 44.5s\tremaining: 4m 33s\n",
      "140:\tlearn: 0.4854096\ttotal: 44.9s\tremaining: 4m 33s\n",
      "141:\tlearn: 0.4853976\ttotal: 45.2s\tremaining: 4m 33s\n",
      "142:\tlearn: 0.4853736\ttotal: 45.5s\tremaining: 4m 32s\n",
      "143:\tlearn: 0.4853313\ttotal: 45.7s\tremaining: 4m 31s\n",
      "144:\tlearn: 0.4853170\ttotal: 46s\tremaining: 4m 31s\n",
      "145:\tlearn: 0.4852944\ttotal: 46.2s\tremaining: 4m 30s\n",
      "146:\tlearn: 0.4852652\ttotal: 46.5s\tremaining: 4m 30s\n",
      "147:\tlearn: 0.4852459\ttotal: 46.9s\tremaining: 4m 29s\n",
      "148:\tlearn: 0.4852222\ttotal: 47.2s\tremaining: 4m 29s\n",
      "149:\tlearn: 0.4851979\ttotal: 47.4s\tremaining: 4m 28s\n",
      "150:\tlearn: 0.4851481\ttotal: 47.8s\tremaining: 4m 28s\n",
      "151:\tlearn: 0.4850873\ttotal: 48.1s\tremaining: 4m 28s\n",
      "152:\tlearn: 0.4850640\ttotal: 48.4s\tremaining: 4m 27s\n",
      "153:\tlearn: 0.4850545\ttotal: 48.6s\tremaining: 4m 27s\n",
      "154:\tlearn: 0.4850319\ttotal: 48.9s\tremaining: 4m 26s\n",
      "155:\tlearn: 0.4850066\ttotal: 49.2s\tremaining: 4m 25s\n",
      "156:\tlearn: 0.4849815\ttotal: 49.5s\tremaining: 4m 25s\n",
      "157:\tlearn: 0.4849627\ttotal: 49.7s\tremaining: 4m 25s\n",
      "158:\tlearn: 0.4848814\ttotal: 50s\tremaining: 4m 24s\n",
      "159:\tlearn: 0.4848575\ttotal: 50.3s\tremaining: 4m 24s\n",
      "160:\tlearn: 0.4848248\ttotal: 50.6s\tremaining: 4m 23s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161:\tlearn: 0.4848040\ttotal: 51s\tremaining: 4m 23s\n",
      "162:\tlearn: 0.4847738\ttotal: 51.3s\tremaining: 4m 23s\n",
      "163:\tlearn: 0.4847216\ttotal: 51.6s\tremaining: 4m 23s\n",
      "164:\tlearn: 0.4846974\ttotal: 51.9s\tremaining: 4m 22s\n",
      "165:\tlearn: 0.4846378\ttotal: 52.2s\tremaining: 4m 22s\n",
      "166:\tlearn: 0.4846175\ttotal: 52.5s\tremaining: 4m 21s\n",
      "167:\tlearn: 0.4845995\ttotal: 52.7s\tremaining: 4m 21s\n",
      "168:\tlearn: 0.4845735\ttotal: 53.1s\tremaining: 4m 20s\n",
      "169:\tlearn: 0.4845582\ttotal: 53.3s\tremaining: 4m 20s\n",
      "170:\tlearn: 0.4845465\ttotal: 53.5s\tremaining: 4m 19s\n",
      "171:\tlearn: 0.4845393\ttotal: 53.7s\tremaining: 4m 18s\n",
      "172:\tlearn: 0.4845261\ttotal: 54s\tremaining: 4m 17s\n",
      "173:\tlearn: 0.4845077\ttotal: 54.2s\tremaining: 4m 17s\n",
      "174:\tlearn: 0.4844643\ttotal: 54.5s\tremaining: 4m 17s\n",
      "175:\tlearn: 0.4844475\ttotal: 54.8s\tremaining: 4m 16s\n",
      "176:\tlearn: 0.4844131\ttotal: 55.1s\tremaining: 4m 16s\n",
      "177:\tlearn: 0.4843956\ttotal: 55.3s\tremaining: 4m 15s\n",
      "178:\tlearn: 0.4843829\ttotal: 55.5s\tremaining: 4m 14s\n",
      "179:\tlearn: 0.4843682\ttotal: 55.7s\tremaining: 4m 13s\n",
      "180:\tlearn: 0.4843543\ttotal: 56s\tremaining: 4m 13s\n",
      "181:\tlearn: 0.4843453\ttotal: 56.2s\tremaining: 4m 12s\n",
      "182:\tlearn: 0.4843252\ttotal: 56.4s\tremaining: 4m 11s\n",
      "183:\tlearn: 0.4843094\ttotal: 56.7s\tremaining: 4m 11s\n",
      "184:\tlearn: 0.4842795\ttotal: 56.9s\tremaining: 4m 10s\n",
      "185:\tlearn: 0.4842456\ttotal: 57.2s\tremaining: 4m 10s\n",
      "186:\tlearn: 0.4842181\ttotal: 57.5s\tremaining: 4m 9s\n",
      "187:\tlearn: 0.4841994\ttotal: 57.7s\tremaining: 4m 9s\n",
      "188:\tlearn: 0.4841699\ttotal: 58s\tremaining: 4m 9s\n",
      "189:\tlearn: 0.4841468\ttotal: 58.4s\tremaining: 4m 8s\n",
      "190:\tlearn: 0.4841271\ttotal: 58.6s\tremaining: 4m 8s\n",
      "191:\tlearn: 0.4841124\ttotal: 58.9s\tremaining: 4m 7s\n",
      "192:\tlearn: 0.4840980\ttotal: 59.1s\tremaining: 4m 7s\n",
      "193:\tlearn: 0.4840776\ttotal: 59.4s\tremaining: 4m 6s\n",
      "194:\tlearn: 0.4840200\ttotal: 59.7s\tremaining: 4m 6s\n",
      "195:\tlearn: 0.4839862\ttotal: 60s\tremaining: 4m 5s\n",
      "196:\tlearn: 0.4839581\ttotal: 1m\tremaining: 4m 5s\n",
      "197:\tlearn: 0.4839384\ttotal: 1m\tremaining: 4m 5s\n",
      "198:\tlearn: 0.4839241\ttotal: 1m\tremaining: 4m 4s\n",
      "199:\tlearn: 0.4839143\ttotal: 1m\tremaining: 4m 3s\n",
      "200:\tlearn: 0.4838989\ttotal: 1m 1s\tremaining: 4m 3s\n",
      "201:\tlearn: 0.4838856\ttotal: 1m 1s\tremaining: 4m 2s\n",
      "202:\tlearn: 0.4838664\ttotal: 1m 1s\tremaining: 4m 2s\n",
      "203:\tlearn: 0.4838089\ttotal: 1m 1s\tremaining: 4m 1s\n",
      "204:\tlearn: 0.4837903\ttotal: 1m 2s\tremaining: 4m 1s\n",
      "205:\tlearn: 0.4837823\ttotal: 1m 2s\tremaining: 4m\n",
      "206:\tlearn: 0.4837595\ttotal: 1m 2s\tremaining: 4m\n",
      "207:\tlearn: 0.4837482\ttotal: 1m 2s\tremaining: 3m 59s\n",
      "208:\tlearn: 0.4837103\ttotal: 1m 3s\tremaining: 3m 58s\n",
      "209:\tlearn: 0.4836776\ttotal: 1m 3s\tremaining: 3m 58s\n",
      "210:\tlearn: 0.4836463\ttotal: 1m 3s\tremaining: 3m 58s\n",
      "211:\tlearn: 0.4836312\ttotal: 1m 3s\tremaining: 3m 57s\n",
      "212:\tlearn: 0.4836067\ttotal: 1m 4s\tremaining: 3m 57s\n",
      "213:\tlearn: 0.4835658\ttotal: 1m 4s\tremaining: 3m 56s\n",
      "214:\tlearn: 0.4835473\ttotal: 1m 4s\tremaining: 3m 56s\n",
      "215:\tlearn: 0.4834986\ttotal: 1m 5s\tremaining: 3m 56s\n",
      "216:\tlearn: 0.4834814\ttotal: 1m 5s\tremaining: 3m 55s\n",
      "217:\tlearn: 0.4834472\ttotal: 1m 5s\tremaining: 3m 55s\n",
      "218:\tlearn: 0.4834266\ttotal: 1m 5s\tremaining: 3m 54s\n",
      "219:\tlearn: 0.4833961\ttotal: 1m 6s\tremaining: 3m 54s\n",
      "220:\tlearn: 0.4833768\ttotal: 1m 6s\tremaining: 3m 53s\n",
      "221:\tlearn: 0.4833630\ttotal: 1m 6s\tremaining: 3m 53s\n",
      "222:\tlearn: 0.4833537\ttotal: 1m 6s\tremaining: 3m 52s\n",
      "223:\tlearn: 0.4833420\ttotal: 1m 7s\tremaining: 3m 52s\n",
      "224:\tlearn: 0.4833069\ttotal: 1m 7s\tremaining: 3m 52s\n",
      "225:\tlearn: 0.4832772\ttotal: 1m 7s\tremaining: 3m 52s\n",
      "226:\tlearn: 0.4832650\ttotal: 1m 8s\tremaining: 3m 52s\n",
      "227:\tlearn: 0.4832492\ttotal: 1m 8s\tremaining: 3m 51s\n",
      "228:\tlearn: 0.4832449\ttotal: 1m 8s\tremaining: 3m 51s\n",
      "229:\tlearn: 0.4832205\ttotal: 1m 8s\tremaining: 3m 50s\n",
      "230:\tlearn: 0.4832028\ttotal: 1m 9s\tremaining: 3m 50s\n",
      "231:\tlearn: 0.4831758\ttotal: 1m 9s\tremaining: 3m 50s\n",
      "232:\tlearn: 0.4831642\ttotal: 1m 9s\tremaining: 3m 49s\n",
      "233:\tlearn: 0.4831483\ttotal: 1m 10s\tremaining: 3m 49s\n",
      "234:\tlearn: 0.4831272\ttotal: 1m 10s\tremaining: 3m 49s\n",
      "235:\tlearn: 0.4830995\ttotal: 1m 10s\tremaining: 3m 48s\n",
      "236:\tlearn: 0.4830819\ttotal: 1m 10s\tremaining: 3m 48s\n",
      "237:\tlearn: 0.4830657\ttotal: 1m 11s\tremaining: 3m 47s\n",
      "238:\tlearn: 0.4830471\ttotal: 1m 11s\tremaining: 3m 47s\n",
      "239:\tlearn: 0.4830062\ttotal: 1m 11s\tremaining: 3m 47s\n",
      "240:\tlearn: 0.4829941\ttotal: 1m 11s\tremaining: 3m 46s\n",
      "241:\tlearn: 0.4829585\ttotal: 1m 12s\tremaining: 3m 46s\n",
      "242:\tlearn: 0.4829458\ttotal: 1m 12s\tremaining: 3m 45s\n",
      "243:\tlearn: 0.4829238\ttotal: 1m 12s\tremaining: 3m 45s\n",
      "244:\tlearn: 0.4829085\ttotal: 1m 12s\tremaining: 3m 44s\n",
      "245:\tlearn: 0.4828874\ttotal: 1m 13s\tremaining: 3m 44s\n",
      "246:\tlearn: 0.4828731\ttotal: 1m 13s\tremaining: 3m 43s\n",
      "247:\tlearn: 0.4828281\ttotal: 1m 13s\tremaining: 3m 43s\n",
      "248:\tlearn: 0.4828195\ttotal: 1m 14s\tremaining: 3m 43s\n",
      "249:\tlearn: 0.4828045\ttotal: 1m 14s\tremaining: 3m 42s\n",
      "250:\tlearn: 0.4827824\ttotal: 1m 14s\tremaining: 3m 42s\n",
      "251:\tlearn: 0.4827487\ttotal: 1m 14s\tremaining: 3m 42s\n",
      "252:\tlearn: 0.4827401\ttotal: 1m 15s\tremaining: 3m 42s\n",
      "253:\tlearn: 0.4827270\ttotal: 1m 15s\tremaining: 3m 41s\n",
      "254:\tlearn: 0.4827010\ttotal: 1m 15s\tremaining: 3m 41s\n",
      "255:\tlearn: 0.4826699\ttotal: 1m 15s\tremaining: 3m 40s\n",
      "256:\tlearn: 0.4826560\ttotal: 1m 16s\tremaining: 3m 40s\n",
      "257:\tlearn: 0.4826422\ttotal: 1m 16s\tremaining: 3m 39s\n",
      "258:\tlearn: 0.4826311\ttotal: 1m 16s\tremaining: 3m 39s\n",
      "259:\tlearn: 0.4826134\ttotal: 1m 16s\tremaining: 3m 38s\n",
      "260:\tlearn: 0.4825959\ttotal: 1m 17s\tremaining: 3m 38s\n",
      "261:\tlearn: 0.4825804\ttotal: 1m 17s\tremaining: 3m 38s\n",
      "262:\tlearn: 0.4825702\ttotal: 1m 17s\tremaining: 3m 37s\n",
      "263:\tlearn: 0.4825469\ttotal: 1m 17s\tremaining: 3m 37s\n",
      "264:\tlearn: 0.4825367\ttotal: 1m 18s\tremaining: 3m 36s\n",
      "265:\tlearn: 0.4825060\ttotal: 1m 18s\tremaining: 3m 36s\n",
      "266:\tlearn: 0.4824960\ttotal: 1m 18s\tremaining: 3m 36s\n",
      "267:\tlearn: 0.4824850\ttotal: 1m 18s\tremaining: 3m 35s\n",
      "268:\tlearn: 0.4824687\ttotal: 1m 19s\tremaining: 3m 35s\n",
      "269:\tlearn: 0.4824546\ttotal: 1m 19s\tremaining: 3m 35s\n",
      "270:\tlearn: 0.4824469\ttotal: 1m 19s\tremaining: 3m 34s\n",
      "271:\tlearn: 0.4824127\ttotal: 1m 20s\tremaining: 3m 34s\n",
      "272:\tlearn: 0.4823997\ttotal: 1m 20s\tremaining: 3m 34s\n",
      "273:\tlearn: 0.4823809\ttotal: 1m 20s\tremaining: 3m 33s\n",
      "274:\tlearn: 0.4823392\ttotal: 1m 20s\tremaining: 3m 33s\n",
      "275:\tlearn: 0.4823155\ttotal: 1m 21s\tremaining: 3m 33s\n",
      "276:\tlearn: 0.4822989\ttotal: 1m 21s\tremaining: 3m 32s\n",
      "277:\tlearn: 0.4822800\ttotal: 1m 21s\tremaining: 3m 32s\n",
      "278:\tlearn: 0.4822443\ttotal: 1m 22s\tremaining: 3m 31s\n",
      "279:\tlearn: 0.4822060\ttotal: 1m 22s\tremaining: 3m 31s\n",
      "280:\tlearn: 0.4821928\ttotal: 1m 22s\tremaining: 3m 31s\n",
      "281:\tlearn: 0.4821813\ttotal: 1m 22s\tremaining: 3m 30s\n",
      "282:\tlearn: 0.4821718\ttotal: 1m 23s\tremaining: 3m 30s\n",
      "283:\tlearn: 0.4821590\ttotal: 1m 23s\tremaining: 3m 29s\n",
      "284:\tlearn: 0.4821492\ttotal: 1m 23s\tremaining: 3m 29s\n",
      "285:\tlearn: 0.4821320\ttotal: 1m 23s\tremaining: 3m 29s\n",
      "286:\tlearn: 0.4821059\ttotal: 1m 24s\tremaining: 3m 29s\n",
      "287:\tlearn: 0.4820820\ttotal: 1m 24s\tremaining: 3m 28s\n",
      "288:\tlearn: 0.4820343\ttotal: 1m 24s\tremaining: 3m 28s\n",
      "289:\tlearn: 0.4819991\ttotal: 1m 25s\tremaining: 3m 28s\n",
      "290:\tlearn: 0.4819749\ttotal: 1m 25s\tremaining: 3m 28s\n",
      "291:\tlearn: 0.4819534\ttotal: 1m 25s\tremaining: 3m 28s\n",
      "292:\tlearn: 0.4819240\ttotal: 1m 26s\tremaining: 3m 28s\n",
      "293:\tlearn: 0.4818751\ttotal: 1m 26s\tremaining: 3m 27s\n",
      "294:\tlearn: 0.4818584\ttotal: 1m 26s\tremaining: 3m 27s\n",
      "295:\tlearn: 0.4818210\ttotal: 1m 27s\tremaining: 3m 27s\n",
      "296:\tlearn: 0.4818043\ttotal: 1m 27s\tremaining: 3m 26s\n",
      "297:\tlearn: 0.4817951\ttotal: 1m 27s\tremaining: 3m 26s\n",
      "298:\tlearn: 0.4817845\ttotal: 1m 27s\tremaining: 3m 26s\n",
      "299:\tlearn: 0.4817633\ttotal: 1m 28s\tremaining: 3m 26s\n",
      "300:\tlearn: 0.4817521\ttotal: 1m 28s\tremaining: 3m 25s\n",
      "301:\tlearn: 0.4817427\ttotal: 1m 28s\tremaining: 3m 25s\n",
      "302:\tlearn: 0.4817088\ttotal: 1m 29s\tremaining: 3m 25s\n",
      "303:\tlearn: 0.4816949\ttotal: 1m 29s\tremaining: 3m 24s\n",
      "304:\tlearn: 0.4816398\ttotal: 1m 29s\tremaining: 3m 24s\n",
      "305:\tlearn: 0.4815925\ttotal: 1m 30s\tremaining: 3m 24s\n",
      "306:\tlearn: 0.4815790\ttotal: 1m 30s\tremaining: 3m 23s\n",
      "307:\tlearn: 0.4815607\ttotal: 1m 30s\tremaining: 3m 23s\n",
      "308:\tlearn: 0.4815403\ttotal: 1m 31s\tremaining: 3m 23s\n",
      "309:\tlearn: 0.4815267\ttotal: 1m 31s\tremaining: 3m 23s\n",
      "310:\tlearn: 0.4815184\ttotal: 1m 31s\tremaining: 3m 23s\n",
      "311:\tlearn: 0.4815041\ttotal: 1m 31s\tremaining: 3m 22s\n",
      "312:\tlearn: 0.4814846\ttotal: 1m 32s\tremaining: 3m 22s\n",
      "313:\tlearn: 0.4814706\ttotal: 1m 32s\tremaining: 3m 22s\n",
      "314:\tlearn: 0.4814413\ttotal: 1m 32s\tremaining: 3m 21s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315:\tlearn: 0.4814318\ttotal: 1m 33s\tremaining: 3m 21s\n",
      "316:\tlearn: 0.4814203\ttotal: 1m 33s\tremaining: 3m 21s\n",
      "317:\tlearn: 0.4813978\ttotal: 1m 33s\tremaining: 3m 20s\n",
      "318:\tlearn: 0.4813840\ttotal: 1m 33s\tremaining: 3m 20s\n",
      "319:\tlearn: 0.4813726\ttotal: 1m 34s\tremaining: 3m 19s\n",
      "320:\tlearn: 0.4813555\ttotal: 1m 34s\tremaining: 3m 19s\n",
      "321:\tlearn: 0.4813428\ttotal: 1m 34s\tremaining: 3m 19s\n",
      "322:\tlearn: 0.4813327\ttotal: 1m 34s\tremaining: 3m 18s\n",
      "323:\tlearn: 0.4813144\ttotal: 1m 35s\tremaining: 3m 18s\n",
      "324:\tlearn: 0.4813016\ttotal: 1m 35s\tremaining: 3m 17s\n",
      "325:\tlearn: 0.4812762\ttotal: 1m 35s\tremaining: 3m 17s\n",
      "326:\tlearn: 0.4812591\ttotal: 1m 35s\tremaining: 3m 17s\n",
      "327:\tlearn: 0.4812166\ttotal: 1m 36s\tremaining: 3m 16s\n",
      "328:\tlearn: 0.4812051\ttotal: 1m 36s\tremaining: 3m 16s\n",
      "329:\tlearn: 0.4811978\ttotal: 1m 36s\tremaining: 3m 16s\n",
      "330:\tlearn: 0.4811807\ttotal: 1m 36s\tremaining: 3m 15s\n",
      "331:\tlearn: 0.4811578\ttotal: 1m 37s\tremaining: 3m 15s\n",
      "332:\tlearn: 0.4811309\ttotal: 1m 37s\tremaining: 3m 15s\n",
      "333:\tlearn: 0.4810925\ttotal: 1m 37s\tremaining: 3m 14s\n",
      "334:\tlearn: 0.4810844\ttotal: 1m 38s\tremaining: 3m 14s\n",
      "335:\tlearn: 0.4810597\ttotal: 1m 38s\tremaining: 3m 14s\n",
      "336:\tlearn: 0.4810491\ttotal: 1m 38s\tremaining: 3m 13s\n",
      "337:\tlearn: 0.4810205\ttotal: 1m 38s\tremaining: 3m 13s\n",
      "338:\tlearn: 0.4810126\ttotal: 1m 39s\tremaining: 3m 13s\n",
      "339:\tlearn: 0.4809914\ttotal: 1m 39s\tremaining: 3m 13s\n",
      "340:\tlearn: 0.4809833\ttotal: 1m 39s\tremaining: 3m 12s\n",
      "341:\tlearn: 0.4809656\ttotal: 1m 40s\tremaining: 3m 12s\n",
      "342:\tlearn: 0.4809534\ttotal: 1m 40s\tremaining: 3m 12s\n",
      "343:\tlearn: 0.4809444\ttotal: 1m 40s\tremaining: 3m 12s\n",
      "344:\tlearn: 0.4809349\ttotal: 1m 41s\tremaining: 3m 11s\n",
      "345:\tlearn: 0.4809148\ttotal: 1m 41s\tremaining: 3m 11s\n",
      "346:\tlearn: 0.4809014\ttotal: 1m 41s\tremaining: 3m 11s\n",
      "347:\tlearn: 0.4808904\ttotal: 1m 41s\tremaining: 3m 10s\n",
      "348:\tlearn: 0.4808658\ttotal: 1m 42s\tremaining: 3m 10s\n",
      "349:\tlearn: 0.4808567\ttotal: 1m 42s\tremaining: 3m 10s\n",
      "350:\tlearn: 0.4808425\ttotal: 1m 42s\tremaining: 3m 10s\n",
      "351:\tlearn: 0.4808303\ttotal: 1m 43s\tremaining: 3m 9s\n",
      "352:\tlearn: 0.4807995\ttotal: 1m 43s\tremaining: 3m 9s\n",
      "353:\tlearn: 0.4807893\ttotal: 1m 43s\tremaining: 3m 9s\n",
      "354:\tlearn: 0.4807789\ttotal: 1m 44s\tremaining: 3m 9s\n",
      "355:\tlearn: 0.4807680\ttotal: 1m 44s\tremaining: 3m 8s\n",
      "356:\tlearn: 0.4807526\ttotal: 1m 44s\tremaining: 3m 8s\n",
      "357:\tlearn: 0.4807344\ttotal: 1m 44s\tremaining: 3m 7s\n",
      "358:\tlearn: 0.4807094\ttotal: 1m 45s\tremaining: 3m 7s\n",
      "359:\tlearn: 0.4806874\ttotal: 1m 45s\tremaining: 3m 7s\n",
      "360:\tlearn: 0.4806776\ttotal: 1m 45s\tremaining: 3m 6s\n",
      "361:\tlearn: 0.4806672\ttotal: 1m 45s\tremaining: 3m 6s\n",
      "362:\tlearn: 0.4806580\ttotal: 1m 46s\tremaining: 3m 6s\n",
      "363:\tlearn: 0.4806470\ttotal: 1m 46s\tremaining: 3m 5s\n",
      "364:\tlearn: 0.4806380\ttotal: 1m 46s\tremaining: 3m 5s\n",
      "365:\tlearn: 0.4806240\ttotal: 1m 46s\tremaining: 3m 4s\n",
      "366:\tlearn: 0.4806157\ttotal: 1m 47s\tremaining: 3m 4s\n",
      "367:\tlearn: 0.4806043\ttotal: 1m 47s\tremaining: 3m 4s\n",
      "368:\tlearn: 0.4805909\ttotal: 1m 47s\tremaining: 3m 3s\n",
      "369:\tlearn: 0.4805619\ttotal: 1m 47s\tremaining: 3m 3s\n",
      "370:\tlearn: 0.4805485\ttotal: 1m 48s\tremaining: 3m 3s\n",
      "371:\tlearn: 0.4805387\ttotal: 1m 48s\tremaining: 3m 2s\n",
      "372:\tlearn: 0.4805246\ttotal: 1m 48s\tremaining: 3m 2s\n",
      "373:\tlearn: 0.4805105\ttotal: 1m 48s\tremaining: 3m 2s\n",
      "374:\tlearn: 0.4805049\ttotal: 1m 49s\tremaining: 3m 1s\n",
      "375:\tlearn: 0.4804823\ttotal: 1m 49s\tremaining: 3m 1s\n",
      "376:\tlearn: 0.4804763\ttotal: 1m 49s\tremaining: 3m 1s\n",
      "377:\tlearn: 0.4804579\ttotal: 1m 49s\tremaining: 3m\n",
      "378:\tlearn: 0.4804460\ttotal: 1m 50s\tremaining: 3m\n",
      "379:\tlearn: 0.4804292\ttotal: 1m 50s\tremaining: 2m 59s\n",
      "380:\tlearn: 0.4804021\ttotal: 1m 50s\tremaining: 2m 59s\n",
      "381:\tlearn: 0.4803891\ttotal: 1m 50s\tremaining: 2m 59s\n",
      "382:\tlearn: 0.4803469\ttotal: 1m 51s\tremaining: 2m 59s\n",
      "383:\tlearn: 0.4803384\ttotal: 1m 51s\tremaining: 2m 58s\n",
      "384:\tlearn: 0.4803225\ttotal: 1m 51s\tremaining: 2m 58s\n",
      "385:\tlearn: 0.4803144\ttotal: 1m 51s\tremaining: 2m 57s\n",
      "386:\tlearn: 0.4803037\ttotal: 1m 52s\tremaining: 2m 57s\n",
      "387:\tlearn: 0.4802712\ttotal: 1m 52s\tremaining: 2m 57s\n",
      "388:\tlearn: 0.4802560\ttotal: 1m 52s\tremaining: 2m 56s\n",
      "389:\tlearn: 0.4802384\ttotal: 1m 52s\tremaining: 2m 56s\n",
      "390:\tlearn: 0.4802247\ttotal: 1m 53s\tremaining: 2m 56s\n",
      "391:\tlearn: 0.4802152\ttotal: 1m 53s\tremaining: 2m 55s\n",
      "392:\tlearn: 0.4802047\ttotal: 1m 53s\tremaining: 2m 55s\n",
      "393:\tlearn: 0.4801960\ttotal: 1m 53s\tremaining: 2m 55s\n",
      "394:\tlearn: 0.4801874\ttotal: 1m 54s\tremaining: 2m 54s\n",
      "395:\tlearn: 0.4801716\ttotal: 1m 54s\tremaining: 2m 54s\n",
      "396:\tlearn: 0.4801632\ttotal: 1m 54s\tremaining: 2m 53s\n",
      "397:\tlearn: 0.4801467\ttotal: 1m 54s\tremaining: 2m 53s\n",
      "398:\tlearn: 0.4801372\ttotal: 1m 55s\tremaining: 2m 53s\n",
      "399:\tlearn: 0.4801210\ttotal: 1m 55s\tremaining: 2m 52s\n",
      "400:\tlearn: 0.4801052\ttotal: 1m 55s\tremaining: 2m 52s\n",
      "401:\tlearn: 0.4800961\ttotal: 1m 55s\tremaining: 2m 52s\n",
      "402:\tlearn: 0.4800819\ttotal: 1m 56s\tremaining: 2m 52s\n",
      "403:\tlearn: 0.4800750\ttotal: 1m 56s\tremaining: 2m 51s\n",
      "404:\tlearn: 0.4800689\ttotal: 1m 56s\tremaining: 2m 51s\n",
      "405:\tlearn: 0.4800602\ttotal: 1m 56s\tremaining: 2m 50s\n",
      "406:\tlearn: 0.4800517\ttotal: 1m 57s\tremaining: 2m 50s\n",
      "407:\tlearn: 0.4800384\ttotal: 1m 57s\tremaining: 2m 50s\n",
      "408:\tlearn: 0.4800300\ttotal: 1m 57s\tremaining: 2m 49s\n",
      "409:\tlearn: 0.4800232\ttotal: 1m 57s\tremaining: 2m 49s\n",
      "410:\tlearn: 0.4800118\ttotal: 1m 58s\tremaining: 2m 49s\n",
      "411:\tlearn: 0.4800013\ttotal: 1m 58s\tremaining: 2m 48s\n",
      "412:\tlearn: 0.4799782\ttotal: 1m 58s\tremaining: 2m 48s\n",
      "413:\tlearn: 0.4799648\ttotal: 1m 58s\tremaining: 2m 48s\n",
      "414:\tlearn: 0.4799458\ttotal: 1m 59s\tremaining: 2m 47s\n",
      "415:\tlearn: 0.4799342\ttotal: 1m 59s\tremaining: 2m 47s\n",
      "416:\tlearn: 0.4799090\ttotal: 1m 59s\tremaining: 2m 47s\n",
      "417:\tlearn: 0.4798882\ttotal: 2m\tremaining: 2m 47s\n",
      "418:\tlearn: 0.4798761\ttotal: 2m\tremaining: 2m 46s\n",
      "419:\tlearn: 0.4798584\ttotal: 2m\tremaining: 2m 46s\n",
      "420:\tlearn: 0.4798429\ttotal: 2m\tremaining: 2m 46s\n",
      "421:\tlearn: 0.4798291\ttotal: 2m 1s\tremaining: 2m 45s\n",
      "422:\tlearn: 0.4797976\ttotal: 2m 1s\tremaining: 2m 45s\n",
      "423:\tlearn: 0.4797852\ttotal: 2m 1s\tremaining: 2m 45s\n",
      "424:\tlearn: 0.4797711\ttotal: 2m 1s\tremaining: 2m 44s\n",
      "425:\tlearn: 0.4797528\ttotal: 2m 2s\tremaining: 2m 44s\n",
      "426:\tlearn: 0.4797352\ttotal: 2m 2s\tremaining: 2m 44s\n",
      "427:\tlearn: 0.4797243\ttotal: 2m 2s\tremaining: 2m 43s\n",
      "428:\tlearn: 0.4797118\ttotal: 2m 2s\tremaining: 2m 43s\n",
      "429:\tlearn: 0.4797009\ttotal: 2m 3s\tremaining: 2m 43s\n",
      "430:\tlearn: 0.4796857\ttotal: 2m 3s\tremaining: 2m 42s\n",
      "431:\tlearn: 0.4796677\ttotal: 2m 3s\tremaining: 2m 42s\n",
      "432:\tlearn: 0.4796581\ttotal: 2m 3s\tremaining: 2m 42s\n",
      "433:\tlearn: 0.4796456\ttotal: 2m 4s\tremaining: 2m 42s\n",
      "434:\tlearn: 0.4796394\ttotal: 2m 4s\tremaining: 2m 41s\n",
      "435:\tlearn: 0.4796353\ttotal: 2m 4s\tremaining: 2m 41s\n",
      "436:\tlearn: 0.4796257\ttotal: 2m 5s\tremaining: 2m 41s\n",
      "437:\tlearn: 0.4796040\ttotal: 2m 5s\tremaining: 2m 40s\n",
      "438:\tlearn: 0.4795967\ttotal: 2m 5s\tremaining: 2m 40s\n",
      "439:\tlearn: 0.4795841\ttotal: 2m 5s\tremaining: 2m 40s\n",
      "440:\tlearn: 0.4795706\ttotal: 2m 6s\tremaining: 2m 39s\n",
      "441:\tlearn: 0.4795619\ttotal: 2m 6s\tremaining: 2m 39s\n",
      "442:\tlearn: 0.4795392\ttotal: 2m 6s\tremaining: 2m 39s\n",
      "443:\tlearn: 0.4795280\ttotal: 2m 6s\tremaining: 2m 38s\n",
      "444:\tlearn: 0.4795072\ttotal: 2m 7s\tremaining: 2m 38s\n",
      "445:\tlearn: 0.4794948\ttotal: 2m 7s\tremaining: 2m 38s\n",
      "446:\tlearn: 0.4794834\ttotal: 2m 7s\tremaining: 2m 37s\n",
      "447:\tlearn: 0.4794657\ttotal: 2m 7s\tremaining: 2m 37s\n",
      "448:\tlearn: 0.4794569\ttotal: 2m 8s\tremaining: 2m 37s\n",
      "449:\tlearn: 0.4794471\ttotal: 2m 8s\tremaining: 2m 36s\n",
      "450:\tlearn: 0.4794353\ttotal: 2m 8s\tremaining: 2m 36s\n",
      "451:\tlearn: 0.4794264\ttotal: 2m 8s\tremaining: 2m 36s\n",
      "452:\tlearn: 0.4794205\ttotal: 2m 9s\tremaining: 2m 35s\n",
      "453:\tlearn: 0.4794098\ttotal: 2m 9s\tremaining: 2m 35s\n",
      "454:\tlearn: 0.4793870\ttotal: 2m 9s\tremaining: 2m 35s\n",
      "455:\tlearn: 0.4793709\ttotal: 2m 9s\tremaining: 2m 34s\n",
      "456:\tlearn: 0.4793614\ttotal: 2m 10s\tremaining: 2m 34s\n",
      "457:\tlearn: 0.4793520\ttotal: 2m 10s\tremaining: 2m 34s\n",
      "458:\tlearn: 0.4793419\ttotal: 2m 10s\tremaining: 2m 33s\n",
      "459:\tlearn: 0.4793339\ttotal: 2m 10s\tremaining: 2m 33s\n",
      "460:\tlearn: 0.4793116\ttotal: 2m 11s\tremaining: 2m 33s\n",
      "461:\tlearn: 0.4792925\ttotal: 2m 11s\tremaining: 2m 33s\n",
      "462:\tlearn: 0.4792766\ttotal: 2m 11s\tremaining: 2m 32s\n",
      "463:\tlearn: 0.4792512\ttotal: 2m 11s\tremaining: 2m 32s\n",
      "464:\tlearn: 0.4792430\ttotal: 2m 12s\tremaining: 2m 32s\n",
      "465:\tlearn: 0.4792119\ttotal: 2m 12s\tremaining: 2m 31s\n",
      "466:\tlearn: 0.4792069\ttotal: 2m 12s\tremaining: 2m 31s\n",
      "467:\tlearn: 0.4791992\ttotal: 2m 12s\tremaining: 2m 31s\n",
      "468:\tlearn: 0.4791871\ttotal: 2m 13s\tremaining: 2m 30s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469:\tlearn: 0.4791794\ttotal: 2m 13s\tremaining: 2m 30s\n",
      "470:\tlearn: 0.4791645\ttotal: 2m 13s\tremaining: 2m 30s\n",
      "471:\tlearn: 0.4790972\ttotal: 2m 14s\tremaining: 2m 29s\n",
      "472:\tlearn: 0.4790878\ttotal: 2m 14s\tremaining: 2m 29s\n",
      "473:\tlearn: 0.4790814\ttotal: 2m 14s\tremaining: 2m 29s\n",
      "474:\tlearn: 0.4790575\ttotal: 2m 14s\tremaining: 2m 28s\n",
      "475:\tlearn: 0.4790503\ttotal: 2m 15s\tremaining: 2m 28s\n",
      "476:\tlearn: 0.4790439\ttotal: 2m 15s\tremaining: 2m 28s\n",
      "477:\tlearn: 0.4790347\ttotal: 2m 15s\tremaining: 2m 28s\n",
      "478:\tlearn: 0.4790221\ttotal: 2m 15s\tremaining: 2m 27s\n",
      "479:\tlearn: 0.4790137\ttotal: 2m 16s\tremaining: 2m 27s\n",
      "480:\tlearn: 0.4790035\ttotal: 2m 16s\tremaining: 2m 27s\n",
      "481:\tlearn: 0.4789832\ttotal: 2m 16s\tremaining: 2m 26s\n",
      "482:\tlearn: 0.4789730\ttotal: 2m 16s\tremaining: 2m 26s\n",
      "483:\tlearn: 0.4789654\ttotal: 2m 17s\tremaining: 2m 26s\n",
      "484:\tlearn: 0.4789568\ttotal: 2m 17s\tremaining: 2m 26s\n",
      "485:\tlearn: 0.4789402\ttotal: 2m 17s\tremaining: 2m 25s\n",
      "486:\tlearn: 0.4789215\ttotal: 2m 18s\tremaining: 2m 25s\n",
      "487:\tlearn: 0.4789135\ttotal: 2m 18s\tremaining: 2m 25s\n",
      "488:\tlearn: 0.4789026\ttotal: 2m 18s\tremaining: 2m 24s\n",
      "489:\tlearn: 0.4788741\ttotal: 2m 18s\tremaining: 2m 24s\n",
      "490:\tlearn: 0.4788584\ttotal: 2m 19s\tremaining: 2m 24s\n",
      "491:\tlearn: 0.4788391\ttotal: 2m 19s\tremaining: 2m 23s\n",
      "492:\tlearn: 0.4788327\ttotal: 2m 19s\tremaining: 2m 23s\n",
      "493:\tlearn: 0.4788239\ttotal: 2m 19s\tremaining: 2m 23s\n",
      "494:\tlearn: 0.4787972\ttotal: 2m 20s\tremaining: 2m 23s\n",
      "495:\tlearn: 0.4787900\ttotal: 2m 20s\tremaining: 2m 22s\n",
      "496:\tlearn: 0.4787825\ttotal: 2m 20s\tremaining: 2m 22s\n",
      "497:\tlearn: 0.4787709\ttotal: 2m 21s\tremaining: 2m 22s\n",
      "498:\tlearn: 0.4787561\ttotal: 2m 21s\tremaining: 2m 22s\n",
      "499:\tlearn: 0.4787455\ttotal: 2m 21s\tremaining: 2m 21s\n",
      "500:\tlearn: 0.4787340\ttotal: 2m 22s\tremaining: 2m 21s\n",
      "501:\tlearn: 0.4787230\ttotal: 2m 22s\tremaining: 2m 21s\n",
      "502:\tlearn: 0.4786961\ttotal: 2m 22s\tremaining: 2m 21s\n",
      "503:\tlearn: 0.4786849\ttotal: 2m 23s\tremaining: 2m 20s\n",
      "504:\tlearn: 0.4786780\ttotal: 2m 23s\tremaining: 2m 20s\n",
      "505:\tlearn: 0.4786720\ttotal: 2m 23s\tremaining: 2m 20s\n",
      "506:\tlearn: 0.4786594\ttotal: 2m 23s\tremaining: 2m 19s\n",
      "507:\tlearn: 0.4786481\ttotal: 2m 24s\tremaining: 2m 19s\n",
      "508:\tlearn: 0.4786257\ttotal: 2m 24s\tremaining: 2m 19s\n",
      "509:\tlearn: 0.4786122\ttotal: 2m 24s\tremaining: 2m 19s\n",
      "510:\tlearn: 0.4786049\ttotal: 2m 25s\tremaining: 2m 18s\n",
      "511:\tlearn: 0.4785969\ttotal: 2m 25s\tremaining: 2m 18s\n",
      "512:\tlearn: 0.4785698\ttotal: 2m 25s\tremaining: 2m 18s\n",
      "513:\tlearn: 0.4785654\ttotal: 2m 25s\tremaining: 2m 18s\n",
      "514:\tlearn: 0.4785293\ttotal: 2m 26s\tremaining: 2m 17s\n",
      "515:\tlearn: 0.4785217\ttotal: 2m 26s\tremaining: 2m 17s\n",
      "516:\tlearn: 0.4784733\ttotal: 2m 27s\tremaining: 2m 17s\n",
      "517:\tlearn: 0.4784656\ttotal: 2m 27s\tremaining: 2m 17s\n",
      "518:\tlearn: 0.4784607\ttotal: 2m 27s\tremaining: 2m 16s\n",
      "519:\tlearn: 0.4784466\ttotal: 2m 28s\tremaining: 2m 16s\n",
      "520:\tlearn: 0.4784286\ttotal: 2m 28s\tremaining: 2m 16s\n",
      "521:\tlearn: 0.4784188\ttotal: 2m 28s\tremaining: 2m 16s\n",
      "522:\tlearn: 0.4784056\ttotal: 2m 29s\tremaining: 2m 15s\n",
      "523:\tlearn: 0.4783839\ttotal: 2m 29s\tremaining: 2m 15s\n",
      "524:\tlearn: 0.4783720\ttotal: 2m 29s\tremaining: 2m 15s\n",
      "525:\tlearn: 0.4783673\ttotal: 2m 29s\tremaining: 2m 15s\n",
      "526:\tlearn: 0.4783487\ttotal: 2m 30s\tremaining: 2m 14s\n",
      "527:\tlearn: 0.4783213\ttotal: 2m 30s\tremaining: 2m 14s\n",
      "528:\tlearn: 0.4783108\ttotal: 2m 30s\tremaining: 2m 14s\n",
      "529:\tlearn: 0.4782983\ttotal: 2m 31s\tremaining: 2m 14s\n",
      "530:\tlearn: 0.4782933\ttotal: 2m 31s\tremaining: 2m 13s\n",
      "531:\tlearn: 0.4782772\ttotal: 2m 31s\tremaining: 2m 13s\n",
      "532:\tlearn: 0.4782413\ttotal: 2m 32s\tremaining: 2m 13s\n",
      "533:\tlearn: 0.4782271\ttotal: 2m 32s\tremaining: 2m 13s\n",
      "534:\tlearn: 0.4782199\ttotal: 2m 32s\tremaining: 2m 12s\n",
      "535:\tlearn: 0.4782072\ttotal: 2m 33s\tremaining: 2m 12s\n",
      "536:\tlearn: 0.4781972\ttotal: 2m 33s\tremaining: 2m 12s\n",
      "537:\tlearn: 0.4781885\ttotal: 2m 33s\tremaining: 2m 12s\n",
      "538:\tlearn: 0.4781805\ttotal: 2m 33s\tremaining: 2m 11s\n",
      "539:\tlearn: 0.4781709\ttotal: 2m 34s\tremaining: 2m 11s\n",
      "540:\tlearn: 0.4781343\ttotal: 2m 34s\tremaining: 2m 11s\n",
      "541:\tlearn: 0.4781259\ttotal: 2m 34s\tremaining: 2m 10s\n",
      "542:\tlearn: 0.4781161\ttotal: 2m 35s\tremaining: 2m 10s\n",
      "543:\tlearn: 0.4780957\ttotal: 2m 35s\tremaining: 2m 10s\n",
      "544:\tlearn: 0.4780654\ttotal: 2m 35s\tremaining: 2m 9s\n",
      "545:\tlearn: 0.4780526\ttotal: 2m 35s\tremaining: 2m 9s\n",
      "546:\tlearn: 0.4780483\ttotal: 2m 36s\tremaining: 2m 9s\n",
      "547:\tlearn: 0.4780339\ttotal: 2m 36s\tremaining: 2m 9s\n",
      "548:\tlearn: 0.4780241\ttotal: 2m 36s\tremaining: 2m 8s\n",
      "549:\tlearn: 0.4780054\ttotal: 2m 37s\tremaining: 2m 8s\n",
      "550:\tlearn: 0.4779978\ttotal: 2m 37s\tremaining: 2m 8s\n",
      "551:\tlearn: 0.4779900\ttotal: 2m 37s\tremaining: 2m 7s\n",
      "552:\tlearn: 0.4779798\ttotal: 2m 37s\tremaining: 2m 7s\n",
      "553:\tlearn: 0.4779638\ttotal: 2m 38s\tremaining: 2m 7s\n",
      "554:\tlearn: 0.4779460\ttotal: 2m 38s\tremaining: 2m 6s\n",
      "555:\tlearn: 0.4779334\ttotal: 2m 38s\tremaining: 2m 6s\n",
      "556:\tlearn: 0.4779122\ttotal: 2m 39s\tremaining: 2m 6s\n",
      "557:\tlearn: 0.4778994\ttotal: 2m 39s\tremaining: 2m 6s\n",
      "558:\tlearn: 0.4778924\ttotal: 2m 39s\tremaining: 2m 5s\n",
      "559:\tlearn: 0.4778742\ttotal: 2m 39s\tremaining: 2m 5s\n",
      "560:\tlearn: 0.4778595\ttotal: 2m 40s\tremaining: 2m 5s\n",
      "561:\tlearn: 0.4778494\ttotal: 2m 40s\tremaining: 2m 4s\n",
      "562:\tlearn: 0.4778384\ttotal: 2m 40s\tremaining: 2m 4s\n",
      "563:\tlearn: 0.4778304\ttotal: 2m 40s\tremaining: 2m 4s\n",
      "564:\tlearn: 0.4778160\ttotal: 2m 41s\tremaining: 2m 4s\n",
      "565:\tlearn: 0.4777976\ttotal: 2m 41s\tremaining: 2m 3s\n",
      "566:\tlearn: 0.4777902\ttotal: 2m 41s\tremaining: 2m 3s\n",
      "567:\tlearn: 0.4777694\ttotal: 2m 41s\tremaining: 2m 3s\n",
      "568:\tlearn: 0.4777580\ttotal: 2m 42s\tremaining: 2m 2s\n",
      "569:\tlearn: 0.4777475\ttotal: 2m 42s\tremaining: 2m 2s\n",
      "570:\tlearn: 0.4777415\ttotal: 2m 42s\tremaining: 2m 2s\n",
      "571:\tlearn: 0.4777263\ttotal: 2m 43s\tremaining: 2m 1s\n",
      "572:\tlearn: 0.4777185\ttotal: 2m 43s\tremaining: 2m 1s\n",
      "573:\tlearn: 0.4777010\ttotal: 2m 43s\tremaining: 2m 1s\n",
      "574:\tlearn: 0.4776936\ttotal: 2m 43s\tremaining: 2m 1s\n",
      "575:\tlearn: 0.4776871\ttotal: 2m 44s\tremaining: 2m\n",
      "576:\tlearn: 0.4776663\ttotal: 2m 44s\tremaining: 2m\n",
      "577:\tlearn: 0.4776492\ttotal: 2m 44s\tremaining: 2m\n",
      "578:\tlearn: 0.4776421\ttotal: 2m 45s\tremaining: 2m\n",
      "579:\tlearn: 0.4776241\ttotal: 2m 45s\tremaining: 1m 59s\n",
      "580:\tlearn: 0.4776108\ttotal: 2m 45s\tremaining: 1m 59s\n",
      "581:\tlearn: 0.4775696\ttotal: 2m 46s\tremaining: 1m 59s\n",
      "582:\tlearn: 0.4775609\ttotal: 2m 46s\tremaining: 1m 59s\n",
      "583:\tlearn: 0.4775496\ttotal: 2m 46s\tremaining: 1m 58s\n",
      "584:\tlearn: 0.4775369\ttotal: 2m 46s\tremaining: 1m 58s\n",
      "585:\tlearn: 0.4775272\ttotal: 2m 47s\tremaining: 1m 58s\n",
      "586:\tlearn: 0.4775181\ttotal: 2m 47s\tremaining: 1m 57s\n",
      "587:\tlearn: 0.4775023\ttotal: 2m 47s\tremaining: 1m 57s\n",
      "588:\tlearn: 0.4774897\ttotal: 2m 48s\tremaining: 1m 57s\n",
      "589:\tlearn: 0.4774780\ttotal: 2m 48s\tremaining: 1m 57s\n",
      "590:\tlearn: 0.4774631\ttotal: 2m 48s\tremaining: 1m 56s\n",
      "591:\tlearn: 0.4774515\ttotal: 2m 48s\tremaining: 1m 56s\n",
      "592:\tlearn: 0.4774449\ttotal: 2m 49s\tremaining: 1m 56s\n",
      "593:\tlearn: 0.4774395\ttotal: 2m 49s\tremaining: 1m 55s\n",
      "594:\tlearn: 0.4774333\ttotal: 2m 49s\tremaining: 1m 55s\n",
      "595:\tlearn: 0.4774266\ttotal: 2m 50s\tremaining: 1m 55s\n",
      "596:\tlearn: 0.4774087\ttotal: 2m 50s\tremaining: 1m 55s\n",
      "597:\tlearn: 0.4774010\ttotal: 2m 50s\tremaining: 1m 54s\n",
      "598:\tlearn: 0.4773903\ttotal: 2m 51s\tremaining: 1m 54s\n",
      "599:\tlearn: 0.4773840\ttotal: 2m 51s\tremaining: 1m 54s\n",
      "600:\tlearn: 0.4773704\ttotal: 2m 51s\tremaining: 1m 53s\n",
      "601:\tlearn: 0.4773615\ttotal: 2m 51s\tremaining: 1m 53s\n",
      "602:\tlearn: 0.4773463\ttotal: 2m 52s\tremaining: 1m 53s\n",
      "603:\tlearn: 0.4773353\ttotal: 2m 52s\tremaining: 1m 53s\n",
      "604:\tlearn: 0.4773325\ttotal: 2m 52s\tremaining: 1m 52s\n",
      "605:\tlearn: 0.4773185\ttotal: 2m 52s\tremaining: 1m 52s\n",
      "606:\tlearn: 0.4773106\ttotal: 2m 53s\tremaining: 1m 52s\n",
      "607:\tlearn: 0.4772867\ttotal: 2m 53s\tremaining: 1m 51s\n",
      "608:\tlearn: 0.4772646\ttotal: 2m 53s\tremaining: 1m 51s\n",
      "609:\tlearn: 0.4772596\ttotal: 2m 54s\tremaining: 1m 51s\n",
      "610:\tlearn: 0.4772407\ttotal: 2m 54s\tremaining: 1m 51s\n",
      "611:\tlearn: 0.4772343\ttotal: 2m 54s\tremaining: 1m 50s\n",
      "612:\tlearn: 0.4772056\ttotal: 2m 55s\tremaining: 1m 50s\n",
      "613:\tlearn: 0.4771982\ttotal: 2m 55s\tremaining: 1m 50s\n",
      "614:\tlearn: 0.4771827\ttotal: 2m 55s\tremaining: 1m 49s\n",
      "615:\tlearn: 0.4771606\ttotal: 2m 55s\tremaining: 1m 49s\n",
      "616:\tlearn: 0.4771445\ttotal: 2m 56s\tremaining: 1m 49s\n",
      "617:\tlearn: 0.4771297\ttotal: 2m 56s\tremaining: 1m 49s\n",
      "618:\tlearn: 0.4771159\ttotal: 2m 56s\tremaining: 1m 48s\n",
      "619:\tlearn: 0.4770999\ttotal: 2m 57s\tremaining: 1m 48s\n",
      "620:\tlearn: 0.4770925\ttotal: 2m 57s\tremaining: 1m 48s\n",
      "621:\tlearn: 0.4770846\ttotal: 2m 58s\tremaining: 1m 48s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "622:\tlearn: 0.4770739\ttotal: 2m 58s\tremaining: 1m 47s\n",
      "623:\tlearn: 0.4770671\ttotal: 2m 58s\tremaining: 1m 47s\n",
      "624:\tlearn: 0.4770502\ttotal: 2m 59s\tremaining: 1m 47s\n",
      "625:\tlearn: 0.4770359\ttotal: 2m 59s\tremaining: 1m 47s\n",
      "626:\tlearn: 0.4770248\ttotal: 2m 59s\tremaining: 1m 46s\n",
      "627:\tlearn: 0.4770198\ttotal: 3m\tremaining: 1m 46s\n",
      "628:\tlearn: 0.4770151\ttotal: 3m\tremaining: 1m 46s\n",
      "629:\tlearn: 0.4770035\ttotal: 3m\tremaining: 1m 46s\n",
      "630:\tlearn: 0.4769957\ttotal: 3m 1s\tremaining: 1m 46s\n",
      "631:\tlearn: 0.4769461\ttotal: 3m 1s\tremaining: 1m 45s\n",
      "632:\tlearn: 0.4769377\ttotal: 3m 1s\tremaining: 1m 45s\n",
      "633:\tlearn: 0.4769295\ttotal: 3m 2s\tremaining: 1m 45s\n",
      "634:\tlearn: 0.4769128\ttotal: 3m 2s\tremaining: 1m 44s\n",
      "635:\tlearn: 0.4769072\ttotal: 3m 2s\tremaining: 1m 44s\n",
      "636:\tlearn: 0.4768923\ttotal: 3m 3s\tremaining: 1m 44s\n",
      "637:\tlearn: 0.4768834\ttotal: 3m 3s\tremaining: 1m 44s\n",
      "638:\tlearn: 0.4768712\ttotal: 3m 3s\tremaining: 1m 43s\n",
      "639:\tlearn: 0.4768628\ttotal: 3m 3s\tremaining: 1m 43s\n",
      "640:\tlearn: 0.4768387\ttotal: 3m 4s\tremaining: 1m 43s\n",
      "641:\tlearn: 0.4768290\ttotal: 3m 4s\tremaining: 1m 42s\n",
      "642:\tlearn: 0.4768229\ttotal: 3m 4s\tremaining: 1m 42s\n",
      "643:\tlearn: 0.4768134\ttotal: 3m 4s\tremaining: 1m 42s\n",
      "644:\tlearn: 0.4768068\ttotal: 3m 5s\tremaining: 1m 41s\n",
      "645:\tlearn: 0.4767917\ttotal: 3m 5s\tremaining: 1m 41s\n",
      "646:\tlearn: 0.4767683\ttotal: 3m 5s\tremaining: 1m 41s\n",
      "647:\tlearn: 0.4767604\ttotal: 3m 6s\tremaining: 1m 41s\n",
      "648:\tlearn: 0.4767441\ttotal: 3m 6s\tremaining: 1m 40s\n",
      "649:\tlearn: 0.4767357\ttotal: 3m 6s\tremaining: 1m 40s\n",
      "650:\tlearn: 0.4767205\ttotal: 3m 6s\tremaining: 1m 40s\n",
      "651:\tlearn: 0.4766994\ttotal: 3m 7s\tremaining: 1m 39s\n",
      "652:\tlearn: 0.4766947\ttotal: 3m 7s\tremaining: 1m 39s\n",
      "653:\tlearn: 0.4766911\ttotal: 3m 7s\tremaining: 1m 39s\n",
      "654:\tlearn: 0.4766818\ttotal: 3m 7s\tremaining: 1m 39s\n",
      "655:\tlearn: 0.4766754\ttotal: 3m 8s\tremaining: 1m 38s\n",
      "656:\tlearn: 0.4766615\ttotal: 3m 8s\tremaining: 1m 38s\n",
      "657:\tlearn: 0.4766348\ttotal: 3m 8s\tremaining: 1m 38s\n",
      "658:\tlearn: 0.4766241\ttotal: 3m 9s\tremaining: 1m 37s\n",
      "659:\tlearn: 0.4765961\ttotal: 3m 9s\tremaining: 1m 37s\n",
      "660:\tlearn: 0.4765859\ttotal: 3m 9s\tremaining: 1m 37s\n",
      "661:\tlearn: 0.4765732\ttotal: 3m 9s\tremaining: 1m 36s\n",
      "662:\tlearn: 0.4765666\ttotal: 3m 10s\tremaining: 1m 36s\n",
      "663:\tlearn: 0.4765489\ttotal: 3m 10s\tremaining: 1m 36s\n",
      "664:\tlearn: 0.4765445\ttotal: 3m 10s\tremaining: 1m 36s\n",
      "665:\tlearn: 0.4765374\ttotal: 3m 10s\tremaining: 1m 35s\n",
      "666:\tlearn: 0.4765041\ttotal: 3m 11s\tremaining: 1m 35s\n",
      "667:\tlearn: 0.4764838\ttotal: 3m 11s\tremaining: 1m 35s\n",
      "668:\tlearn: 0.4764754\ttotal: 3m 11s\tremaining: 1m 34s\n",
      "669:\tlearn: 0.4764541\ttotal: 3m 12s\tremaining: 1m 34s\n",
      "670:\tlearn: 0.4764175\ttotal: 3m 12s\tremaining: 1m 34s\n",
      "671:\tlearn: 0.4764099\ttotal: 3m 12s\tremaining: 1m 34s\n",
      "672:\tlearn: 0.4764030\ttotal: 3m 12s\tremaining: 1m 33s\n",
      "673:\tlearn: 0.4763990\ttotal: 3m 13s\tremaining: 1m 33s\n",
      "674:\tlearn: 0.4763860\ttotal: 3m 13s\tremaining: 1m 33s\n",
      "675:\tlearn: 0.4763782\ttotal: 3m 13s\tremaining: 1m 32s\n",
      "676:\tlearn: 0.4763495\ttotal: 3m 14s\tremaining: 1m 32s\n",
      "677:\tlearn: 0.4763266\ttotal: 3m 14s\tremaining: 1m 32s\n",
      "678:\tlearn: 0.4763174\ttotal: 3m 14s\tremaining: 1m 32s\n",
      "679:\tlearn: 0.4763048\ttotal: 3m 15s\tremaining: 1m 31s\n",
      "680:\tlearn: 0.4762793\ttotal: 3m 15s\tremaining: 1m 31s\n",
      "681:\tlearn: 0.4762597\ttotal: 3m 16s\tremaining: 1m 31s\n",
      "682:\tlearn: 0.4762550\ttotal: 3m 16s\tremaining: 1m 31s\n",
      "683:\tlearn: 0.4762488\ttotal: 3m 16s\tremaining: 1m 30s\n",
      "684:\tlearn: 0.4762425\ttotal: 3m 16s\tremaining: 1m 30s\n",
      "685:\tlearn: 0.4762263\ttotal: 3m 17s\tremaining: 1m 30s\n",
      "686:\tlearn: 0.4762204\ttotal: 3m 17s\tremaining: 1m 30s\n",
      "687:\tlearn: 0.4762037\ttotal: 3m 17s\tremaining: 1m 29s\n",
      "688:\tlearn: 0.4761941\ttotal: 3m 18s\tremaining: 1m 29s\n",
      "689:\tlearn: 0.4761874\ttotal: 3m 18s\tremaining: 1m 29s\n",
      "690:\tlearn: 0.4761707\ttotal: 3m 18s\tremaining: 1m 28s\n",
      "691:\tlearn: 0.4761586\ttotal: 3m 19s\tremaining: 1m 28s\n",
      "692:\tlearn: 0.4761481\ttotal: 3m 19s\tremaining: 1m 28s\n",
      "693:\tlearn: 0.4761381\ttotal: 3m 19s\tremaining: 1m 28s\n",
      "694:\tlearn: 0.4761295\ttotal: 3m 20s\tremaining: 1m 27s\n",
      "695:\tlearn: 0.4761255\ttotal: 3m 20s\tremaining: 1m 27s\n",
      "696:\tlearn: 0.4761157\ttotal: 3m 20s\tremaining: 1m 27s\n",
      "697:\tlearn: 0.4761074\ttotal: 3m 20s\tremaining: 1m 26s\n",
      "698:\tlearn: 0.4760980\ttotal: 3m 21s\tremaining: 1m 26s\n",
      "699:\tlearn: 0.4760678\ttotal: 3m 21s\tremaining: 1m 26s\n",
      "700:\tlearn: 0.4760406\ttotal: 3m 21s\tremaining: 1m 26s\n",
      "701:\tlearn: 0.4760336\ttotal: 3m 22s\tremaining: 1m 25s\n",
      "702:\tlearn: 0.4760148\ttotal: 3m 22s\tremaining: 1m 25s\n",
      "703:\tlearn: 0.4759920\ttotal: 3m 22s\tremaining: 1m 25s\n",
      "704:\tlearn: 0.4759756\ttotal: 3m 23s\tremaining: 1m 25s\n",
      "705:\tlearn: 0.4759694\ttotal: 3m 23s\tremaining: 1m 24s\n",
      "706:\tlearn: 0.4759628\ttotal: 3m 23s\tremaining: 1m 24s\n",
      "707:\tlearn: 0.4759566\ttotal: 3m 24s\tremaining: 1m 24s\n",
      "708:\tlearn: 0.4759520\ttotal: 3m 24s\tremaining: 1m 23s\n",
      "709:\tlearn: 0.4759357\ttotal: 3m 24s\tremaining: 1m 23s\n",
      "710:\tlearn: 0.4759230\ttotal: 3m 25s\tremaining: 1m 23s\n",
      "711:\tlearn: 0.4759098\ttotal: 3m 25s\tremaining: 1m 23s\n",
      "712:\tlearn: 0.4758976\ttotal: 3m 25s\tremaining: 1m 22s\n",
      "713:\tlearn: 0.4758929\ttotal: 3m 26s\tremaining: 1m 22s\n",
      "714:\tlearn: 0.4758676\ttotal: 3m 26s\tremaining: 1m 22s\n",
      "715:\tlearn: 0.4758496\ttotal: 3m 26s\tremaining: 1m 22s\n",
      "716:\tlearn: 0.4758277\ttotal: 3m 27s\tremaining: 1m 21s\n",
      "717:\tlearn: 0.4758143\ttotal: 3m 27s\tremaining: 1m 21s\n",
      "718:\tlearn: 0.4758078\ttotal: 3m 27s\tremaining: 1m 21s\n",
      "719:\tlearn: 0.4758047\ttotal: 3m 28s\tremaining: 1m 20s\n",
      "720:\tlearn: 0.4757944\ttotal: 3m 28s\tremaining: 1m 20s\n",
      "721:\tlearn: 0.4757859\ttotal: 3m 28s\tremaining: 1m 20s\n",
      "722:\tlearn: 0.4757702\ttotal: 3m 28s\tremaining: 1m 20s\n",
      "723:\tlearn: 0.4757636\ttotal: 3m 29s\tremaining: 1m 19s\n",
      "724:\tlearn: 0.4757565\ttotal: 3m 29s\tremaining: 1m 19s\n",
      "725:\tlearn: 0.4757498\ttotal: 3m 29s\tremaining: 1m 19s\n",
      "726:\tlearn: 0.4757402\ttotal: 3m 29s\tremaining: 1m 18s\n",
      "727:\tlearn: 0.4757343\ttotal: 3m 30s\tremaining: 1m 18s\n",
      "728:\tlearn: 0.4757221\ttotal: 3m 30s\tremaining: 1m 18s\n",
      "729:\tlearn: 0.4757147\ttotal: 3m 30s\tremaining: 1m 17s\n",
      "730:\tlearn: 0.4757097\ttotal: 3m 30s\tremaining: 1m 17s\n",
      "731:\tlearn: 0.4756859\ttotal: 3m 31s\tremaining: 1m 17s\n",
      "732:\tlearn: 0.4756816\ttotal: 3m 31s\tremaining: 1m 17s\n",
      "733:\tlearn: 0.4756751\ttotal: 3m 31s\tremaining: 1m 16s\n",
      "734:\tlearn: 0.4756622\ttotal: 3m 32s\tremaining: 1m 16s\n",
      "735:\tlearn: 0.4756492\ttotal: 3m 32s\tremaining: 1m 16s\n",
      "736:\tlearn: 0.4756160\ttotal: 3m 32s\tremaining: 1m 15s\n",
      "737:\tlearn: 0.4756000\ttotal: 3m 33s\tremaining: 1m 15s\n",
      "738:\tlearn: 0.4755947\ttotal: 3m 33s\tremaining: 1m 15s\n",
      "739:\tlearn: 0.4755885\ttotal: 3m 33s\tremaining: 1m 15s\n",
      "740:\tlearn: 0.4755835\ttotal: 3m 33s\tremaining: 1m 14s\n",
      "741:\tlearn: 0.4755793\ttotal: 3m 34s\tremaining: 1m 14s\n",
      "742:\tlearn: 0.4755659\ttotal: 3m 34s\tremaining: 1m 14s\n",
      "743:\tlearn: 0.4755630\ttotal: 3m 34s\tremaining: 1m 13s\n",
      "744:\tlearn: 0.4755473\ttotal: 3m 35s\tremaining: 1m 13s\n",
      "745:\tlearn: 0.4755285\ttotal: 3m 35s\tremaining: 1m 13s\n",
      "746:\tlearn: 0.4755245\ttotal: 3m 35s\tremaining: 1m 13s\n",
      "747:\tlearn: 0.4755182\ttotal: 3m 35s\tremaining: 1m 12s\n",
      "748:\tlearn: 0.4755098\ttotal: 3m 36s\tremaining: 1m 12s\n",
      "749:\tlearn: 0.4755005\ttotal: 3m 36s\tremaining: 1m 12s\n",
      "750:\tlearn: 0.4754888\ttotal: 3m 36s\tremaining: 1m 11s\n",
      "751:\tlearn: 0.4754818\ttotal: 3m 36s\tremaining: 1m 11s\n",
      "752:\tlearn: 0.4754632\ttotal: 3m 37s\tremaining: 1m 11s\n",
      "753:\tlearn: 0.4754514\ttotal: 3m 37s\tremaining: 1m 10s\n",
      "754:\tlearn: 0.4754428\ttotal: 3m 37s\tremaining: 1m 10s\n",
      "755:\tlearn: 0.4754371\ttotal: 3m 38s\tremaining: 1m 10s\n",
      "756:\tlearn: 0.4754129\ttotal: 3m 38s\tremaining: 1m 10s\n",
      "757:\tlearn: 0.4753994\ttotal: 3m 38s\tremaining: 1m 9s\n",
      "758:\tlearn: 0.4753676\ttotal: 3m 38s\tremaining: 1m 9s\n",
      "759:\tlearn: 0.4753617\ttotal: 3m 39s\tremaining: 1m 9s\n",
      "760:\tlearn: 0.4753437\ttotal: 3m 39s\tremaining: 1m 8s\n",
      "761:\tlearn: 0.4753267\ttotal: 3m 39s\tremaining: 1m 8s\n",
      "762:\tlearn: 0.4753214\ttotal: 3m 39s\tremaining: 1m 8s\n",
      "763:\tlearn: 0.4753089\ttotal: 3m 40s\tremaining: 1m 8s\n",
      "764:\tlearn: 0.4753036\ttotal: 3m 40s\tremaining: 1m 7s\n",
      "765:\tlearn: 0.4752984\ttotal: 3m 40s\tremaining: 1m 7s\n",
      "766:\tlearn: 0.4752931\ttotal: 3m 40s\tremaining: 1m 7s\n",
      "767:\tlearn: 0.4752780\ttotal: 3m 41s\tremaining: 1m 6s\n",
      "768:\tlearn: 0.4752698\ttotal: 3m 41s\tremaining: 1m 6s\n",
      "769:\tlearn: 0.4752662\ttotal: 3m 41s\tremaining: 1m 6s\n",
      "770:\tlearn: 0.4752598\ttotal: 3m 41s\tremaining: 1m 5s\n",
      "771:\tlearn: 0.4752351\ttotal: 3m 42s\tremaining: 1m 5s\n",
      "772:\tlearn: 0.4752219\ttotal: 3m 42s\tremaining: 1m 5s\n",
      "773:\tlearn: 0.4752177\ttotal: 3m 42s\tremaining: 1m 5s\n",
      "774:\tlearn: 0.4752136\ttotal: 3m 42s\tremaining: 1m 4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "775:\tlearn: 0.4752065\ttotal: 3m 43s\tremaining: 1m 4s\n",
      "776:\tlearn: 0.4751937\ttotal: 3m 43s\tremaining: 1m 4s\n",
      "777:\tlearn: 0.4751860\ttotal: 3m 43s\tremaining: 1m 3s\n",
      "778:\tlearn: 0.4751765\ttotal: 3m 43s\tremaining: 1m 3s\n",
      "779:\tlearn: 0.4751537\ttotal: 3m 44s\tremaining: 1m 3s\n",
      "780:\tlearn: 0.4751386\ttotal: 3m 44s\tremaining: 1m 2s\n",
      "781:\tlearn: 0.4751298\ttotal: 3m 44s\tremaining: 1m 2s\n",
      "782:\tlearn: 0.4751209\ttotal: 3m 45s\tremaining: 1m 2s\n",
      "783:\tlearn: 0.4751136\ttotal: 3m 45s\tremaining: 1m 2s\n",
      "784:\tlearn: 0.4751037\ttotal: 3m 45s\tremaining: 1m 1s\n",
      "785:\tlearn: 0.4750920\ttotal: 3m 45s\tremaining: 1m 1s\n",
      "786:\tlearn: 0.4750770\ttotal: 3m 46s\tremaining: 1m 1s\n",
      "787:\tlearn: 0.4750690\ttotal: 3m 46s\tremaining: 1m\n",
      "788:\tlearn: 0.4750594\ttotal: 3m 46s\tremaining: 1m\n",
      "789:\tlearn: 0.4750525\ttotal: 3m 46s\tremaining: 1m\n",
      "790:\tlearn: 0.4750404\ttotal: 3m 47s\tremaining: 1m\n",
      "791:\tlearn: 0.4750298\ttotal: 3m 47s\tremaining: 59.7s\n",
      "792:\tlearn: 0.4750060\ttotal: 3m 47s\tremaining: 59.5s\n",
      "793:\tlearn: 0.4749946\ttotal: 3m 48s\tremaining: 59.2s\n",
      "794:\tlearn: 0.4749919\ttotal: 3m 48s\tremaining: 58.9s\n",
      "795:\tlearn: 0.4749860\ttotal: 3m 48s\tremaining: 58.6s\n",
      "796:\tlearn: 0.4749686\ttotal: 3m 48s\tremaining: 58.3s\n",
      "797:\tlearn: 0.4749590\ttotal: 3m 48s\tremaining: 58s\n",
      "798:\tlearn: 0.4749510\ttotal: 3m 49s\tremaining: 57.7s\n",
      "799:\tlearn: 0.4749430\ttotal: 3m 49s\tremaining: 57.4s\n",
      "800:\tlearn: 0.4749279\ttotal: 3m 49s\tremaining: 57.1s\n",
      "801:\tlearn: 0.4749245\ttotal: 3m 50s\tremaining: 56.8s\n",
      "802:\tlearn: 0.4749202\ttotal: 3m 50s\tremaining: 56.5s\n",
      "803:\tlearn: 0.4748806\ttotal: 3m 50s\tremaining: 56.2s\n",
      "804:\tlearn: 0.4748702\ttotal: 3m 50s\tremaining: 55.9s\n",
      "805:\tlearn: 0.4748657\ttotal: 3m 50s\tremaining: 55.6s\n",
      "806:\tlearn: 0.4748576\ttotal: 3m 51s\tremaining: 55.3s\n",
      "807:\tlearn: 0.4748309\ttotal: 3m 51s\tremaining: 55s\n",
      "808:\tlearn: 0.4748137\ttotal: 3m 51s\tremaining: 54.7s\n",
      "809:\tlearn: 0.4748026\ttotal: 3m 52s\tremaining: 54.5s\n",
      "810:\tlearn: 0.4747994\ttotal: 3m 52s\tremaining: 54.2s\n",
      "811:\tlearn: 0.4747878\ttotal: 3m 52s\tremaining: 53.9s\n",
      "812:\tlearn: 0.4747791\ttotal: 3m 53s\tremaining: 53.6s\n",
      "813:\tlearn: 0.4747746\ttotal: 3m 53s\tremaining: 53.3s\n",
      "814:\tlearn: 0.4747665\ttotal: 3m 53s\tremaining: 53s\n",
      "815:\tlearn: 0.4747621\ttotal: 3m 53s\tremaining: 52.7s\n",
      "816:\tlearn: 0.4747541\ttotal: 3m 54s\tremaining: 52.4s\n",
      "817:\tlearn: 0.4747360\ttotal: 3m 54s\tremaining: 52.1s\n",
      "818:\tlearn: 0.4747274\ttotal: 3m 54s\tremaining: 51.8s\n",
      "819:\tlearn: 0.4747229\ttotal: 3m 54s\tremaining: 51.5s\n",
      "820:\tlearn: 0.4747085\ttotal: 3m 55s\tremaining: 51.3s\n",
      "821:\tlearn: 0.4746995\ttotal: 3m 55s\tremaining: 51s\n",
      "822:\tlearn: 0.4746883\ttotal: 3m 55s\tremaining: 50.7s\n",
      "823:\tlearn: 0.4746760\ttotal: 3m 55s\tremaining: 50.4s\n",
      "824:\tlearn: 0.4746687\ttotal: 3m 56s\tremaining: 50.1s\n",
      "825:\tlearn: 0.4746605\ttotal: 3m 56s\tremaining: 49.8s\n",
      "826:\tlearn: 0.4746556\ttotal: 3m 56s\tremaining: 49.5s\n",
      "827:\tlearn: 0.4746233\ttotal: 3m 57s\tremaining: 49.3s\n",
      "828:\tlearn: 0.4746083\ttotal: 3m 57s\tremaining: 49s\n",
      "829:\tlearn: 0.4746011\ttotal: 3m 57s\tremaining: 48.7s\n",
      "830:\tlearn: 0.4745903\ttotal: 3m 58s\tremaining: 48.4s\n",
      "831:\tlearn: 0.4745757\ttotal: 3m 58s\tremaining: 48.2s\n",
      "832:\tlearn: 0.4745674\ttotal: 3m 58s\tremaining: 47.9s\n",
      "833:\tlearn: 0.4745471\ttotal: 3m 59s\tremaining: 47.6s\n",
      "834:\tlearn: 0.4745423\ttotal: 3m 59s\tremaining: 47.3s\n",
      "835:\tlearn: 0.4745362\ttotal: 3m 59s\tremaining: 47s\n",
      "836:\tlearn: 0.4745206\ttotal: 4m\tremaining: 46.8s\n",
      "837:\tlearn: 0.4745134\ttotal: 4m\tremaining: 46.5s\n",
      "838:\tlearn: 0.4745062\ttotal: 4m\tremaining: 46.2s\n",
      "839:\tlearn: 0.4744978\ttotal: 4m\tremaining: 45.9s\n",
      "840:\tlearn: 0.4744892\ttotal: 4m 1s\tremaining: 45.6s\n",
      "841:\tlearn: 0.4744817\ttotal: 4m 1s\tremaining: 45.3s\n",
      "842:\tlearn: 0.4744765\ttotal: 4m 1s\tremaining: 45s\n",
      "843:\tlearn: 0.4744502\ttotal: 4m 2s\tremaining: 44.7s\n",
      "844:\tlearn: 0.4744470\ttotal: 4m 2s\tremaining: 44.5s\n",
      "845:\tlearn: 0.4744414\ttotal: 4m 2s\tremaining: 44.2s\n",
      "846:\tlearn: 0.4744302\ttotal: 4m 2s\tremaining: 43.9s\n",
      "847:\tlearn: 0.4744273\ttotal: 4m 3s\tremaining: 43.6s\n",
      "848:\tlearn: 0.4744169\ttotal: 4m 3s\tremaining: 43.3s\n",
      "849:\tlearn: 0.4744124\ttotal: 4m 3s\tremaining: 43s\n",
      "850:\tlearn: 0.4744026\ttotal: 4m 3s\tremaining: 42.7s\n",
      "851:\tlearn: 0.4743865\ttotal: 4m 4s\tremaining: 42.4s\n",
      "852:\tlearn: 0.4743823\ttotal: 4m 4s\tremaining: 42.1s\n",
      "853:\tlearn: 0.4743693\ttotal: 4m 4s\tremaining: 41.8s\n",
      "854:\tlearn: 0.4743648\ttotal: 4m 5s\tremaining: 41.6s\n",
      "855:\tlearn: 0.4743596\ttotal: 4m 5s\tremaining: 41.3s\n",
      "856:\tlearn: 0.4743490\ttotal: 4m 5s\tremaining: 41s\n",
      "857:\tlearn: 0.4743248\ttotal: 4m 5s\tremaining: 40.7s\n",
      "858:\tlearn: 0.4743167\ttotal: 4m 6s\tremaining: 40.4s\n",
      "859:\tlearn: 0.4743006\ttotal: 4m 6s\tremaining: 40.1s\n",
      "860:\tlearn: 0.4742934\ttotal: 4m 6s\tremaining: 39.8s\n",
      "861:\tlearn: 0.4742889\ttotal: 4m 7s\tremaining: 39.5s\n",
      "862:\tlearn: 0.4742695\ttotal: 4m 7s\tremaining: 39.3s\n",
      "863:\tlearn: 0.4742604\ttotal: 4m 7s\tremaining: 39s\n",
      "864:\tlearn: 0.4742547\ttotal: 4m 7s\tremaining: 38.7s\n",
      "865:\tlearn: 0.4742495\ttotal: 4m 8s\tremaining: 38.4s\n",
      "866:\tlearn: 0.4742420\ttotal: 4m 8s\tremaining: 38.1s\n",
      "867:\tlearn: 0.4742226\ttotal: 4m 8s\tremaining: 37.8s\n",
      "868:\tlearn: 0.4742192\ttotal: 4m 8s\tremaining: 37.5s\n",
      "869:\tlearn: 0.4742007\ttotal: 4m 9s\tremaining: 37.2s\n",
      "870:\tlearn: 0.4741953\ttotal: 4m 9s\tremaining: 36.9s\n",
      "871:\tlearn: 0.4741734\ttotal: 4m 9s\tremaining: 36.7s\n",
      "872:\tlearn: 0.4741656\ttotal: 4m 9s\tremaining: 36.4s\n",
      "873:\tlearn: 0.4741368\ttotal: 4m 10s\tremaining: 36.1s\n",
      "874:\tlearn: 0.4741255\ttotal: 4m 10s\tremaining: 35.8s\n",
      "875:\tlearn: 0.4741160\ttotal: 4m 10s\tremaining: 35.5s\n",
      "876:\tlearn: 0.4741102\ttotal: 4m 11s\tremaining: 35.2s\n",
      "877:\tlearn: 0.4741043\ttotal: 4m 11s\tremaining: 34.9s\n",
      "878:\tlearn: 0.4740955\ttotal: 4m 11s\tremaining: 34.6s\n",
      "879:\tlearn: 0.4740784\ttotal: 4m 11s\tremaining: 34.3s\n",
      "880:\tlearn: 0.4740461\ttotal: 4m 12s\tremaining: 34.1s\n",
      "881:\tlearn: 0.4740419\ttotal: 4m 12s\tremaining: 33.8s\n",
      "882:\tlearn: 0.4740313\ttotal: 4m 12s\tremaining: 33.5s\n",
      "883:\tlearn: 0.4740241\ttotal: 4m 13s\tremaining: 33.2s\n",
      "884:\tlearn: 0.4740078\ttotal: 4m 13s\tremaining: 32.9s\n",
      "885:\tlearn: 0.4739908\ttotal: 4m 13s\tremaining: 32.6s\n",
      "886:\tlearn: 0.4739640\ttotal: 4m 13s\tremaining: 32.3s\n",
      "887:\tlearn: 0.4739604\ttotal: 4m 14s\tremaining: 32.1s\n",
      "888:\tlearn: 0.4739558\ttotal: 4m 14s\tremaining: 31.8s\n",
      "889:\tlearn: 0.4739463\ttotal: 4m 14s\tremaining: 31.5s\n",
      "890:\tlearn: 0.4739415\ttotal: 4m 14s\tremaining: 31.2s\n",
      "891:\tlearn: 0.4739339\ttotal: 4m 15s\tremaining: 30.9s\n",
      "892:\tlearn: 0.4739156\ttotal: 4m 15s\tremaining: 30.6s\n",
      "893:\tlearn: 0.4739081\ttotal: 4m 15s\tremaining: 30.3s\n",
      "894:\tlearn: 0.4738870\ttotal: 4m 16s\tremaining: 30.1s\n",
      "895:\tlearn: 0.4738543\ttotal: 4m 16s\tremaining: 29.8s\n",
      "896:\tlearn: 0.4738350\ttotal: 4m 16s\tremaining: 29.5s\n",
      "897:\tlearn: 0.4738115\ttotal: 4m 17s\tremaining: 29.2s\n",
      "898:\tlearn: 0.4738054\ttotal: 4m 17s\tremaining: 28.9s\n",
      "899:\tlearn: 0.4738003\ttotal: 4m 17s\tremaining: 28.6s\n",
      "900:\tlearn: 0.4737960\ttotal: 4m 18s\tremaining: 28.4s\n",
      "901:\tlearn: 0.4737919\ttotal: 4m 18s\tremaining: 28.1s\n",
      "902:\tlearn: 0.4737813\ttotal: 4m 18s\tremaining: 27.8s\n",
      "903:\tlearn: 0.4737686\ttotal: 4m 18s\tremaining: 27.5s\n",
      "904:\tlearn: 0.4737510\ttotal: 4m 19s\tremaining: 27.2s\n",
      "905:\tlearn: 0.4737386\ttotal: 4m 19s\tremaining: 26.9s\n",
      "906:\tlearn: 0.4737308\ttotal: 4m 19s\tremaining: 26.6s\n",
      "907:\tlearn: 0.4737171\ttotal: 4m 19s\tremaining: 26.3s\n",
      "908:\tlearn: 0.4737088\ttotal: 4m 20s\tremaining: 26s\n",
      "909:\tlearn: 0.4736907\ttotal: 4m 20s\tremaining: 25.8s\n",
      "910:\tlearn: 0.4736787\ttotal: 4m 20s\tremaining: 25.5s\n",
      "911:\tlearn: 0.4736663\ttotal: 4m 21s\tremaining: 25.2s\n",
      "912:\tlearn: 0.4736544\ttotal: 4m 21s\tremaining: 24.9s\n",
      "913:\tlearn: 0.4736459\ttotal: 4m 21s\tremaining: 24.6s\n",
      "914:\tlearn: 0.4736248\ttotal: 4m 21s\tremaining: 24.3s\n",
      "915:\tlearn: 0.4736201\ttotal: 4m 22s\tremaining: 24s\n",
      "916:\tlearn: 0.4736093\ttotal: 4m 22s\tremaining: 23.7s\n",
      "917:\tlearn: 0.4735912\ttotal: 4m 22s\tremaining: 23.5s\n",
      "918:\tlearn: 0.4735791\ttotal: 4m 23s\tremaining: 23.2s\n",
      "919:\tlearn: 0.4735679\ttotal: 4m 23s\tremaining: 22.9s\n",
      "920:\tlearn: 0.4735608\ttotal: 4m 23s\tremaining: 22.6s\n",
      "921:\tlearn: 0.4735499\ttotal: 4m 23s\tremaining: 22.3s\n",
      "922:\tlearn: 0.4735399\ttotal: 4m 24s\tremaining: 22s\n",
      "923:\tlearn: 0.4735356\ttotal: 4m 24s\tremaining: 21.7s\n",
      "924:\tlearn: 0.4735223\ttotal: 4m 24s\tremaining: 21.5s\n",
      "925:\tlearn: 0.4735154\ttotal: 4m 24s\tremaining: 21.2s\n",
      "926:\tlearn: 0.4735068\ttotal: 4m 25s\tremaining: 20.9s\n",
      "927:\tlearn: 0.4735010\ttotal: 4m 25s\tremaining: 20.6s\n",
      "928:\tlearn: 0.4734930\ttotal: 4m 25s\tremaining: 20.3s\n",
      "929:\tlearn: 0.4734652\ttotal: 4m 25s\tremaining: 20s\n",
      "930:\tlearn: 0.4734428\ttotal: 4m 26s\tremaining: 19.7s\n",
      "931:\tlearn: 0.4734339\ttotal: 4m 26s\tremaining: 19.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "932:\tlearn: 0.4734175\ttotal: 4m 26s\tremaining: 19.2s\n",
      "933:\tlearn: 0.4733987\ttotal: 4m 27s\tremaining: 18.9s\n",
      "934:\tlearn: 0.4733912\ttotal: 4m 27s\tremaining: 18.6s\n",
      "935:\tlearn: 0.4733775\ttotal: 4m 27s\tremaining: 18.3s\n",
      "936:\tlearn: 0.4733671\ttotal: 4m 28s\tremaining: 18s\n",
      "937:\tlearn: 0.4733570\ttotal: 4m 28s\tremaining: 17.7s\n",
      "938:\tlearn: 0.4733511\ttotal: 4m 28s\tremaining: 17.5s\n",
      "939:\tlearn: 0.4733459\ttotal: 4m 29s\tremaining: 17.2s\n",
      "940:\tlearn: 0.4733149\ttotal: 4m 29s\tremaining: 16.9s\n",
      "941:\tlearn: 0.4733088\ttotal: 4m 29s\tremaining: 16.6s\n",
      "942:\tlearn: 0.4732921\ttotal: 4m 30s\tremaining: 16.3s\n",
      "943:\tlearn: 0.4732793\ttotal: 4m 30s\tremaining: 16s\n",
      "944:\tlearn: 0.4732761\ttotal: 4m 30s\tremaining: 15.8s\n",
      "945:\tlearn: 0.4732722\ttotal: 4m 30s\tremaining: 15.5s\n",
      "946:\tlearn: 0.4732610\ttotal: 4m 31s\tremaining: 15.2s\n",
      "947:\tlearn: 0.4732488\ttotal: 4m 31s\tremaining: 14.9s\n",
      "948:\tlearn: 0.4732428\ttotal: 4m 31s\tremaining: 14.6s\n",
      "949:\tlearn: 0.4732351\ttotal: 4m 32s\tremaining: 14.3s\n",
      "950:\tlearn: 0.4732288\ttotal: 4m 32s\tremaining: 14s\n",
      "951:\tlearn: 0.4732232\ttotal: 4m 32s\tremaining: 13.7s\n",
      "952:\tlearn: 0.4732107\ttotal: 4m 32s\tremaining: 13.5s\n",
      "953:\tlearn: 0.4732041\ttotal: 4m 33s\tremaining: 13.2s\n",
      "954:\tlearn: 0.4731979\ttotal: 4m 33s\tremaining: 12.9s\n",
      "955:\tlearn: 0.4731900\ttotal: 4m 33s\tremaining: 12.6s\n",
      "956:\tlearn: 0.4731581\ttotal: 4m 33s\tremaining: 12.3s\n",
      "957:\tlearn: 0.4731343\ttotal: 4m 34s\tremaining: 12s\n",
      "958:\tlearn: 0.4731296\ttotal: 4m 34s\tremaining: 11.7s\n",
      "959:\tlearn: 0.4731091\ttotal: 4m 34s\tremaining: 11.5s\n",
      "960:\tlearn: 0.4730860\ttotal: 4m 35s\tremaining: 11.2s\n",
      "961:\tlearn: 0.4730799\ttotal: 4m 35s\tremaining: 10.9s\n",
      "962:\tlearn: 0.4730728\ttotal: 4m 35s\tremaining: 10.6s\n",
      "963:\tlearn: 0.4730527\ttotal: 4m 36s\tremaining: 10.3s\n",
      "964:\tlearn: 0.4730473\ttotal: 4m 36s\tremaining: 10s\n",
      "965:\tlearn: 0.4730201\ttotal: 4m 37s\tremaining: 9.75s\n",
      "966:\tlearn: 0.4730129\ttotal: 4m 37s\tremaining: 9.46s\n",
      "967:\tlearn: 0.4730014\ttotal: 4m 37s\tremaining: 9.18s\n",
      "968:\tlearn: 0.4729829\ttotal: 4m 38s\tremaining: 8.9s\n",
      "969:\tlearn: 0.4729643\ttotal: 4m 38s\tremaining: 8.61s\n",
      "970:\tlearn: 0.4729578\ttotal: 4m 38s\tremaining: 8.32s\n",
      "971:\tlearn: 0.4729527\ttotal: 4m 39s\tremaining: 8.04s\n",
      "972:\tlearn: 0.4729477\ttotal: 4m 39s\tremaining: 7.75s\n",
      "973:\tlearn: 0.4729363\ttotal: 4m 39s\tremaining: 7.46s\n",
      "974:\tlearn: 0.4729175\ttotal: 4m 39s\tremaining: 7.18s\n",
      "975:\tlearn: 0.4729122\ttotal: 4m 40s\tremaining: 6.89s\n",
      "976:\tlearn: 0.4729052\ttotal: 4m 40s\tremaining: 6.6s\n",
      "977:\tlearn: 0.4728993\ttotal: 4m 40s\tremaining: 6.31s\n",
      "978:\tlearn: 0.4728922\ttotal: 4m 40s\tremaining: 6.02s\n",
      "979:\tlearn: 0.4728882\ttotal: 4m 41s\tremaining: 5.74s\n",
      "980:\tlearn: 0.4728795\ttotal: 4m 41s\tremaining: 5.45s\n",
      "981:\tlearn: 0.4728711\ttotal: 4m 41s\tremaining: 5.16s\n",
      "982:\tlearn: 0.4728632\ttotal: 4m 41s\tremaining: 4.88s\n",
      "983:\tlearn: 0.4728579\ttotal: 4m 42s\tremaining: 4.59s\n",
      "984:\tlearn: 0.4728531\ttotal: 4m 42s\tremaining: 4.3s\n",
      "985:\tlearn: 0.4728482\ttotal: 4m 42s\tremaining: 4.01s\n",
      "986:\tlearn: 0.4728318\ttotal: 4m 43s\tremaining: 3.73s\n",
      "987:\tlearn: 0.4728243\ttotal: 4m 43s\tremaining: 3.44s\n",
      "988:\tlearn: 0.4728146\ttotal: 4m 43s\tremaining: 3.15s\n",
      "989:\tlearn: 0.4728065\ttotal: 4m 44s\tremaining: 2.87s\n",
      "990:\tlearn: 0.4727964\ttotal: 4m 44s\tremaining: 2.58s\n",
      "991:\tlearn: 0.4727915\ttotal: 4m 44s\tremaining: 2.29s\n",
      "992:\tlearn: 0.4727670\ttotal: 4m 44s\tremaining: 2.01s\n",
      "993:\tlearn: 0.4727584\ttotal: 4m 45s\tremaining: 1.72s\n",
      "994:\tlearn: 0.4727450\ttotal: 4m 45s\tremaining: 1.43s\n",
      "995:\tlearn: 0.4727337\ttotal: 4m 45s\tremaining: 1.15s\n",
      "996:\tlearn: 0.4727293\ttotal: 4m 46s\tremaining: 861ms\n",
      "997:\tlearn: 0.4727216\ttotal: 4m 46s\tremaining: 574ms\n",
      "998:\tlearn: 0.4727180\ttotal: 4m 46s\tremaining: 287ms\n",
      "999:\tlearn: 0.4727116\ttotal: 4m 46s\tremaining: 0us\n",
      "CatBoostClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.62      0.73      6986\n",
      "         1.0       0.71      0.92      0.80      7038\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     14024\n",
      "   macro avg       0.79      0.77      0.76     14024\n",
      "weighted avg       0.79      0.77      0.76     14024\n",
      "\n",
      "[[4356 2630]\n",
      " [ 598 6440]]\n",
      "Accuracy is  76.98231602966344\n",
      "Time on model's work: 306.913 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.63      0.73      6986\n",
      "         1.0       0.71      0.90      0.80      7038\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     14024\n",
      "   macro avg       0.79      0.77      0.76     14024\n",
      "weighted avg       0.79      0.77      0.76     14024\n",
      "\n",
      "[[4427 2559]\n",
      " [ 693 6345]]\n",
      "Accuracy is  76.81118083285796\n",
      "Time on model's work: 3.187 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.92      0.70      6986\n",
      "         1.0       0.78      0.28      0.41      7038\n",
      "\n",
      "   micro avg       0.60      0.60      0.60     14024\n",
      "   macro avg       0.67      0.60      0.55     14024\n",
      "weighted avg       0.67      0.60      0.55     14024\n",
      "\n",
      "[[6432  554]\n",
      " [5066 1972]]\n",
      "Accuracy is  59.92584141471763\n",
      "Time on model's work: 1.925 s\n",
      "====================================================================================================\n",
      "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n",
      "TFFMClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.63      0.73      6986\n",
      "         1.0       0.71      0.89      0.79      7038\n",
      "\n",
      "   micro avg       0.76      0.76      0.76     14024\n",
      "   macro avg       0.78      0.76      0.76     14024\n",
      "weighted avg       0.78      0.76      0.76     14024\n",
      "\n",
      "[[4434 2552]\n",
      " [ 749 6289]]\n",
      "Accuracy is  76.46177980604678\n",
      "Time on model's work: 229.59 s\n",
      "====================================================================================================\n",
      "TOTAL TIME SPENT:  3863.884 s\n"
     ]
    }
   ],
   "source": [
    "clfs = [\n",
    "        ['RandomForestClassifier', RandomForestClassifier()],\n",
    "        ['GradientBoostingClassifier', GradientBoostingClassifier()],\n",
    "        ['ExtraTreesClassifier', ExtraTreesClassifier()],\n",
    "        ['AdaBoostClassifier', AdaBoostClassifier()],\n",
    "        ['BaggingClassifier', BaggingClassifier()],\n",
    "        ['DecisionTreeClassifier', DecisionTreeClassifier()],\n",
    "        ['MLPClassifier', MLPClassifier()],\n",
    "        ['XGBClassifier', XGBClassifier()],\n",
    "        ['CatBoostClassifier', CatBoostClassifier()],\n",
    "        ['LogisticRegression', LogisticRegression()],\n",
    "        ['SGDClassifier', SGDClassifier()],\n",
    "        ['TFFMClassifier', TFFMClassifier()]\n",
    "       ]\n",
    "t = time()\n",
    "for name, clf in clfs:\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    prediction = clf.predict(X_test)\n",
    "    print(name)\n",
    "    print(classification_report(y_test, prediction))\n",
    "    print(confusion_matrix(y_test, prediction))\n",
    "    print('Accuracy is ', accuracy_score(y_test, prediction)*100)\n",
    "    print (\"Time on model's work:\", round(time()-t0, 3), \"s\")\n",
    "    print('='*100)\n",
    "print (\"TOTAL TIME SPENT: \", round(time()-t, 3), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFFM sparse\n",
    "# only CSR format supported\n",
    "X_train_sparse = sp.csr_matrix(X_train)\n",
    "X_test_sparse = sp.csr_matrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:26<00:00,  1.93epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[order=2] accuracy: 0.7620507701083856\n",
      "[[4428 2613]\n",
      " [ 724 6259]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.63      0.73      7041\n",
      "         1.0       0.71      0.90      0.79      6983\n",
      "\n",
      "   micro avg       0.76      0.76      0.76     14024\n",
      "   macro avg       0.78      0.76      0.76     14024\n",
      "weighted avg       0.78      0.76      0.76     14024\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:49<00:00,  1.02epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[order=3] accuracy: 0.761908157444381\n",
      "[[4448 2593]\n",
      " [ 746 6237]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.63      0.73      7041\n",
      "         1.0       0.71      0.89      0.79      6983\n",
      "\n",
      "   micro avg       0.76      0.76      0.76     14024\n",
      "   macro avg       0.78      0.76      0.76     14024\n",
      "weighted avg       0.78      0.76      0.76     14024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# input_type='sparse' /// rank == 10\n",
    "for order in [2, 3]:\n",
    "    model = TFFMClassifier(\n",
    "        order=order, \n",
    "        rank=10, \n",
    "        optimizer=tf.train.AdamOptimizer(learning_rate=0.001), \n",
    "        n_epochs=50, \n",
    "        batch_size=512,\n",
    "        init_std=0.001,\n",
    "        reg=0.01,\n",
    "        input_type='sparse',\n",
    "        #log_dir='./tmp/logs',\n",
    "        #verbose=1,\n",
    "        seed=42\n",
    "    )\n",
    "    model.fit(X_train_sparse, y_train, show_progress=True)\n",
    "    predictions = model.predict(X_test_sparse)\n",
    "    print('[order={}] accuracy: {}'.format(order, accuracy_score(y_test, predictions)))\n",
    "    # this will close tf.Session and free resources\n",
    "    print(confusion_matrix(y_test,predictions)) \n",
    "    print(classification_report(y_test, predictions))\n",
    "    model.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/50 [00:00<?, ?epoch/s]\n",
      "  2%|         | 1/50 [00:00<00:30,  1.63epoch/s]\n",
      "  4%|         | 2/50 [00:01<00:27,  1.75epoch/s]\n",
      "  6%|         | 3/50 [00:01<00:25,  1.84epoch/s]\n",
      "  8%|         | 4/50 [00:02<00:24,  1.91epoch/s]\n",
      " 10%|         | 5/50 [00:02<00:22,  1.96epoch/s]\n",
      " 12%|        | 6/50 [00:02<00:21,  2.00epoch/s]\n",
      " 14%|        | 7/50 [00:03<00:21,  2.02epoch/s]\n",
      " 16%|        | 8/50 [00:03<00:20,  2.03epoch/s]\n",
      " 18%|        | 9/50 [00:04<00:20,  2.03epoch/s]\n",
      " 20%|        | 10/50 [00:04<00:19,  2.04epoch/s]\n",
      " 22%|       | 11/50 [00:05<00:18,  2.06epoch/s]\n",
      " 24%|       | 12/50 [00:05<00:18,  2.07epoch/s]\n",
      " 26%|       | 13/50 [00:06<00:17,  2.08epoch/s]\n",
      " 28%|       | 14/50 [00:06<00:17,  2.08epoch/s]\n",
      " 30%|       | 15/50 [00:07<00:16,  2.08epoch/s]\n",
      " 32%|      | 16/50 [00:07<00:16,  2.09epoch/s]\n",
      " 34%|      | 17/50 [00:08<00:15,  2.09epoch/s]\n",
      " 36%|      | 18/50 [00:08<00:15,  2.10epoch/s]\n",
      " 38%|      | 19/50 [00:09<00:15,  2.06epoch/s]\n",
      " 40%|      | 20/50 [00:09<00:14,  2.04epoch/s]\n",
      " 42%|     | 21/50 [00:10<00:14,  2.05epoch/s]\n",
      " 44%|     | 22/50 [00:10<00:13,  2.06epoch/s]\n",
      " 46%|     | 23/50 [00:11<00:13,  2.06epoch/s]\n",
      " 48%|     | 24/50 [00:11<00:12,  2.06epoch/s]\n",
      " 50%|     | 25/50 [00:12<00:12,  2.06epoch/s]\n",
      " 52%|    | 26/50 [00:12<00:11,  2.07epoch/s]\n",
      " 54%|    | 27/50 [00:13<00:11,  2.06epoch/s]\n",
      " 56%|    | 28/50 [00:13<00:10,  2.07epoch/s]\n",
      " 58%|    | 29/50 [00:14<00:10,  2.08epoch/s]\n",
      " 60%|    | 30/50 [00:14<00:09,  2.07epoch/s]\n",
      " 62%|   | 31/50 [00:15<00:09,  2.05epoch/s]\n",
      " 64%|   | 32/50 [00:15<00:08,  2.05epoch/s]\n",
      " 66%|   | 33/50 [00:16<00:08,  2.05epoch/s]\n",
      " 68%|   | 34/50 [00:16<00:07,  2.06epoch/s]\n",
      " 70%|   | 35/50 [00:17<00:07,  2.06epoch/s]\n",
      " 72%|  | 36/50 [00:17<00:06,  2.07epoch/s]\n",
      " 74%|  | 37/50 [00:18<00:06,  2.05epoch/s]\n",
      " 76%|  | 38/50 [00:18<00:05,  2.04epoch/s]\n",
      " 78%|  | 39/50 [00:19<00:05,  2.03epoch/s]\n",
      " 80%|  | 40/50 [00:19<00:04,  2.04epoch/s]\n",
      " 82%| | 41/50 [00:19<00:04,  2.05epoch/s]\n",
      " 84%| | 42/50 [00:20<00:03,  2.06epoch/s]\n",
      " 86%| | 43/50 [00:20<00:03,  2.04epoch/s]\n",
      " 88%| | 44/50 [00:21<00:02,  2.05epoch/s]\n",
      " 90%| | 45/50 [00:21<00:02,  2.05epoch/s]\n",
      " 92%|| 46/50 [00:22<00:01,  2.06epoch/s]\n",
      " 94%|| 47/50 [00:22<00:01,  2.06epoch/s]\n",
      " 96%|| 48/50 [00:23<00:00,  2.07epoch/s]\n",
      " 98%|| 49/50 [00:23<00:00,  2.07epoch/s]\n",
      "100%|| 50/50 [00:24<00:00,  2.08epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[order=3] accuracy: 0.7507130633200229\n",
      "[[4336 2705]\n",
      " [ 791 6192]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.62      0.71      7041\n",
      "         1.0       0.70      0.89      0.78      6983\n",
      "\n",
      "   micro avg       0.75      0.75      0.75     14024\n",
      "   macro avg       0.77      0.75      0.75     14024\n",
      "weighted avg       0.77      0.75      0.75     14024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FtrlOptimizer\n",
    "# input_type='sparse' /// rank == 10\n",
    "model = TFFMClassifier(\n",
    "    order=2, \n",
    "    rank=10, \n",
    "    optimizer=tf.train.FtrlOptimizer(0.01, l2_regularization_strength=0.001, l2_shrinkage_regularization_strength=0.001), \n",
    "    n_epochs=50, \n",
    "    batch_size=1024,\n",
    "    init_std=0.001,\n",
    "    reg=0.01,\n",
    "    input_type='sparse',\n",
    "    #log_dir='./tmp/logs',\n",
    "    #verbose=1,\n",
    "    seed=42\n",
    ")\n",
    "model.fit(X_train_sparse, y_train, show_progress=True)\n",
    "predictions = model.predict(X_test_sparse)\n",
    "print('[order={}] accuracy: {}'.format(order, accuracy_score(y_test, predictions)))\n",
    "# this will close tf.Session and free resources\n",
    "print(confusion_matrix(y_test,predictions)) \n",
    "print(classification_report(y_test, predictions))\n",
    "model.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/50 [00:00<?, ?epoch/s]\n",
      "  2%|         | 1/50 [00:00<00:29,  1.68epoch/s]\n",
      "  4%|         | 2/50 [00:01<00:26,  1.79epoch/s]\n",
      "  6%|         | 3/50 [00:01<00:24,  1.88epoch/s]\n",
      "  8%|         | 4/50 [00:02<00:23,  1.95epoch/s]\n",
      " 10%|         | 5/50 [00:02<00:22,  2.00epoch/s]\n",
      " 12%|        | 6/50 [00:02<00:21,  2.04epoch/s]\n",
      " 14%|        | 7/50 [00:03<00:20,  2.07epoch/s]\n",
      " 16%|        | 8/50 [00:03<00:20,  2.08epoch/s]\n",
      " 18%|        | 9/50 [00:04<00:19,  2.10epoch/s]\n",
      " 20%|        | 10/50 [00:04<00:19,  2.10epoch/s]\n",
      " 22%|       | 11/50 [00:05<00:18,  2.11epoch/s]\n",
      " 24%|       | 12/50 [00:05<00:17,  2.12epoch/s]\n",
      " 26%|       | 13/50 [00:06<00:17,  2.11epoch/s]\n",
      " 28%|       | 14/50 [00:06<00:17,  2.07epoch/s]\n",
      " 30%|       | 15/50 [00:07<00:17,  2.05epoch/s]\n",
      " 32%|      | 16/50 [00:07<00:16,  2.05epoch/s]\n",
      " 34%|      | 17/50 [00:08<00:16,  2.05epoch/s]\n",
      " 36%|      | 18/50 [00:08<00:15,  2.02epoch/s]\n",
      " 38%|      | 19/50 [00:09<00:15,  2.04epoch/s]\n",
      " 40%|      | 20/50 [00:09<00:14,  2.04epoch/s]\n",
      " 42%|     | 21/50 [00:10<00:14,  2.07epoch/s]\n",
      " 44%|     | 22/50 [00:10<00:13,  2.08epoch/s]\n",
      " 46%|     | 23/50 [00:11<00:12,  2.09epoch/s]\n",
      " 48%|     | 24/50 [00:11<00:12,  2.06epoch/s]\n",
      " 50%|     | 25/50 [00:12<00:12,  2.04epoch/s]\n",
      " 52%|    | 26/50 [00:12<00:11,  2.04epoch/s]\n",
      " 54%|    | 27/50 [00:13<00:11,  2.05epoch/s]\n",
      " 56%|    | 28/50 [00:13<00:10,  2.07epoch/s]\n",
      " 58%|    | 29/50 [00:14<00:10,  2.08epoch/s]\n",
      " 60%|    | 30/50 [00:14<00:09,  2.10epoch/s]\n",
      " 62%|   | 31/50 [00:14<00:09,  2.08epoch/s]\n",
      " 64%|   | 32/50 [00:15<00:08,  2.08epoch/s]\n",
      " 66%|   | 33/50 [00:15<00:08,  2.08epoch/s]\n",
      " 68%|   | 34/50 [00:16<00:07,  2.06epoch/s]\n",
      " 70%|   | 35/50 [00:16<00:07,  2.08epoch/s]\n",
      " 72%|  | 36/50 [00:17<00:06,  2.06epoch/s]\n",
      " 74%|  | 37/50 [00:17<00:06,  2.06epoch/s]\n",
      " 76%|  | 38/50 [00:18<00:05,  2.02epoch/s]\n",
      " 78%|  | 39/50 [00:18<00:05,  2.02epoch/s]\n",
      " 80%|  | 40/50 [00:19<00:04,  2.01epoch/s]\n",
      " 82%| | 41/50 [00:19<00:04,  2.01epoch/s]\n",
      " 84%| | 42/50 [00:20<00:03,  2.01epoch/s]\n",
      " 86%| | 43/50 [00:20<00:03,  2.00epoch/s]\n",
      " 88%| | 44/50 [00:21<00:02,  2.02epoch/s]\n",
      " 90%| | 45/50 [00:21<00:02,  1.99epoch/s]\n",
      " 92%|| 46/50 [00:22<00:02,  2.00epoch/s]\n",
      " 94%|| 47/50 [00:22<00:01,  2.03epoch/s]\n",
      " 96%|| 48/50 [00:23<00:00,  2.05epoch/s]\n",
      " 98%|| 49/50 [00:23<00:00,  2.07epoch/s]\n",
      "100%|| 50/50 [00:24<00:00,  2.07epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[order=3] accuracy: 0.7623359954363947\n",
      "[[4427 2614]\n",
      " [ 719 6264]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.63      0.73      7041\n",
      "         1.0       0.71      0.90      0.79      6983\n",
      "\n",
      "   micro avg       0.76      0.76      0.76     14024\n",
      "   macro avg       0.78      0.76      0.76     14024\n",
      "weighted avg       0.78      0.76      0.76     14024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sample_weight='balanced'\n",
    "model = TFFMClassifier(\n",
    "    order=2,\n",
    "    sample_weight='balanced',\n",
    "    rank=10, \n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate=0.001), \n",
    "    n_epochs=50, \n",
    "    batch_size=1024,\n",
    "    init_std=0.001,\n",
    "    reg=0.01,\n",
    "    input_type='sparse',\n",
    "    #log_dir='./tmp/logs',\n",
    "    #verbose=1,\n",
    "    seed=42\n",
    ")\n",
    "model.fit(X_train_sparse, y_train, show_progress=True)\n",
    "predictions = model.predict(X_test_sparse)\n",
    "print('[order={}] accuracy: {}'.format(order, accuracy_score(y_test, predictions)))\n",
    "# this will close tf.Session and free resources\n",
    "print(confusion_matrix(y_test,predictions)) \n",
    "print(classification_report(y_test, predictions))\n",
    "model.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/20 [00:00<?, ?epoch/s]\n",
      "  5%|         | 1/20 [00:00<00:11,  1.66epoch/s]\n",
      " 10%|         | 2/20 [00:01<00:10,  1.78epoch/s]\n",
      " 15%|        | 3/20 [00:01<00:09,  1.87epoch/s]\n",
      " 20%|        | 4/20 [00:02<00:08,  1.94epoch/s]\n",
      " 25%|       | 5/20 [00:02<00:07,  1.99epoch/s]\n",
      " 30%|       | 6/20 [00:02<00:06,  2.03epoch/s]\n",
      " 35%|      | 7/20 [00:03<00:06,  2.05epoch/s]\n",
      " 40%|      | 8/20 [00:03<00:05,  2.07epoch/s]\n",
      " 45%|     | 9/20 [00:04<00:05,  2.08epoch/s]\n",
      " 50%|     | 10/20 [00:04<00:04,  2.10epoch/s]\n",
      " 55%|    | 11/20 [00:05<00:04,  2.10epoch/s]\n",
      " 60%|    | 12/20 [00:05<00:03,  2.07epoch/s]\n",
      " 65%|   | 13/20 [00:06<00:03,  2.07epoch/s]\n",
      " 70%|   | 14/20 [00:06<00:02,  2.08epoch/s]\n",
      " 75%|  | 15/20 [00:07<00:02,  2.09epoch/s]\n",
      " 80%|  | 16/20 [00:07<00:01,  2.10epoch/s]\n",
      " 85%| | 17/20 [00:08<00:01,  2.10epoch/s]\n",
      " 90%| | 18/20 [00:08<00:00,  2.11epoch/s]\n",
      " 95%|| 19/20 [00:09<00:00,  2.11epoch/s]\n",
      "100%|| 20/20 [00:09<00:00,  2.11epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[order=3] accuracy: 0.7638334284084427\n",
      "[[4509 2532]\n",
      " [ 780 6203]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.64      0.73      7041\n",
      "         1.0       0.71      0.89      0.79      6983\n",
      "\n",
      "   micro avg       0.76      0.76      0.76     14024\n",
      "   macro avg       0.78      0.76      0.76     14024\n",
      "weighted avg       0.78      0.76      0.76     14024\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/20 [00:00<?, ?epoch/s]\n",
      "  5%|         | 1/20 [00:00<00:11,  1.67epoch/s]\n",
      " 10%|         | 2/20 [00:01<00:10,  1.77epoch/s]\n",
      " 15%|        | 3/20 [00:01<00:09,  1.87epoch/s]\n",
      " 20%|        | 4/20 [00:02<00:08,  1.94epoch/s]\n",
      " 25%|       | 5/20 [00:02<00:07,  1.99epoch/s]\n",
      " 30%|       | 6/20 [00:02<00:06,  2.03epoch/s]\n",
      " 35%|      | 7/20 [00:03<00:06,  2.06epoch/s]\n",
      " 40%|      | 8/20 [00:03<00:05,  2.08epoch/s]\n",
      " 45%|     | 9/20 [00:04<00:05,  2.09epoch/s]\n",
      " 50%|     | 10/20 [00:04<00:04,  2.07epoch/s]\n",
      " 55%|    | 11/20 [00:05<00:04,  2.09epoch/s]\n",
      " 60%|    | 12/20 [00:05<00:03,  2.10epoch/s]\n",
      " 65%|   | 13/20 [00:06<00:03,  2.11epoch/s]\n",
      " 70%|   | 14/20 [00:06<00:02,  2.11epoch/s]\n",
      " 75%|  | 15/20 [00:07<00:02,  2.09epoch/s]\n",
      " 80%|  | 16/20 [00:07<00:01,  2.09epoch/s]\n",
      " 85%| | 17/20 [00:08<00:01,  2.07epoch/s]\n",
      " 90%| | 18/20 [00:08<00:01,  1.93epoch/s]\n",
      " 95%|| 19/20 [00:09<00:00,  1.97epoch/s]\n",
      "100%|| 20/20 [00:09<00:00,  2.02epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[order=3] accuracy: 0.7651169423844837\n",
      "[[4292 2749]\n",
      " [ 545 6438]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.61      0.72      7041\n",
      "         1.0       0.70      0.92      0.80      6983\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     14024\n",
      "   macro avg       0.79      0.77      0.76     14024\n",
      "weighted avg       0.79      0.77      0.76     14024\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/20 [00:00<?, ?epoch/s]\n",
      "  5%|         | 1/20 [00:00<00:11,  1.69epoch/s]\n",
      " 10%|         | 2/20 [00:01<00:09,  1.81epoch/s]\n",
      " 15%|        | 3/20 [00:01<00:09,  1.86epoch/s]\n",
      " 20%|        | 4/20 [00:02<00:08,  1.84epoch/s]\n",
      " 25%|       | 5/20 [00:02<00:07,  1.88epoch/s]\n",
      " 30%|       | 6/20 [00:03<00:07,  1.89epoch/s]\n",
      " 35%|      | 7/20 [00:03<00:06,  1.93epoch/s]\n",
      " 40%|      | 8/20 [00:04<00:06,  1.98epoch/s]\n",
      " 45%|     | 9/20 [00:04<00:05,  1.99epoch/s]\n",
      " 50%|     | 10/20 [00:05<00:05,  1.98epoch/s]\n",
      " 55%|    | 11/20 [00:05<00:04,  1.99epoch/s]\n",
      " 60%|    | 12/20 [00:06<00:04,  1.86epoch/s]\n",
      " 65%|   | 13/20 [00:06<00:03,  1.93epoch/s]\n",
      " 70%|   | 14/20 [00:07<00:03,  1.97epoch/s]\n",
      " 75%|  | 15/20 [00:07<00:02,  2.02epoch/s]\n",
      " 80%|  | 16/20 [00:08<00:01,  2.05epoch/s]\n",
      " 85%| | 17/20 [00:08<00:01,  2.08epoch/s]\n",
      " 90%| | 18/20 [00:09<00:00,  2.10epoch/s]\n",
      " 95%|| 19/20 [00:09<00:00,  2.11epoch/s]\n",
      "100%|| 20/20 [00:09<00:00,  2.12epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[order=3] accuracy: 0.7600541928123218\n",
      "[[4155 2886]\n",
      " [ 479 6504]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.59      0.71      7041\n",
      "         1.0       0.69      0.93      0.79      6983\n",
      "\n",
      "   micro avg       0.76      0.76      0.76     14024\n",
      "   macro avg       0.79      0.76      0.75     14024\n",
      "weighted avg       0.80      0.76      0.75     14024\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/20 [00:00<?, ?epoch/s]\n",
      "  5%|         | 1/20 [00:00<00:13,  1.42epoch/s]\n",
      " 10%|         | 2/20 [00:01<00:11,  1.56epoch/s]\n",
      " 15%|        | 3/20 [00:01<00:10,  1.66epoch/s]\n",
      " 20%|        | 4/20 [00:02<00:09,  1.76epoch/s]\n",
      " 25%|       | 5/20 [00:02<00:08,  1.84epoch/s]\n",
      " 30%|       | 6/20 [00:03<00:07,  1.91epoch/s]\n",
      " 35%|      | 7/20 [00:03<00:06,  1.94epoch/s]\n",
      " 40%|      | 8/20 [00:04<00:06,  1.96epoch/s]\n",
      " 45%|     | 9/20 [00:04<00:05,  2.00epoch/s]\n",
      " 50%|     | 10/20 [00:05<00:04,  2.03epoch/s]\n",
      " 55%|    | 11/20 [00:05<00:04,  2.04epoch/s]\n",
      " 60%|    | 12/20 [00:06<00:03,  2.04epoch/s]\n",
      " 65%|   | 13/20 [00:06<00:03,  2.02epoch/s]\n",
      " 70%|   | 14/20 [00:07<00:02,  2.03epoch/s]\n",
      " 75%|  | 15/20 [00:07<00:02,  2.05epoch/s]\n",
      " 80%|  | 16/20 [00:08<00:01,  2.07epoch/s]\n",
      " 85%| | 17/20 [00:08<00:01,  2.04epoch/s]\n",
      " 90%| | 18/20 [00:09<00:00,  2.06epoch/s]\n",
      " 95%|| 19/20 [00:09<00:00,  2.07epoch/s]\n",
      "100%|| 20/20 [00:09<00:00,  2.06epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[order=3] accuracy: 0.7501426126640046\n",
      "[[3960 3081]\n",
      " [ 423 6560]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.56      0.69      7041\n",
      "         1.0       0.68      0.94      0.79      6983\n",
      "\n",
      "   micro avg       0.75      0.75      0.75     14024\n",
      "   macro avg       0.79      0.75      0.74     14024\n",
      "weighted avg       0.79      0.75      0.74     14024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# weight - optional\n",
    "pos_class_weight = list(map(float,range(1, 5)))\n",
    "for weight in pos_class_weight:\n",
    "    model = TFFMClassifier(\n",
    "        order=2,\n",
    "        pos_class_weight=weight,\n",
    "        rank=10, \n",
    "        optimizer=tf.train.AdamOptimizer(learning_rate=0.001), \n",
    "        n_epochs=20, \n",
    "        batch_size=1024,\n",
    "        init_std=0.001,\n",
    "        reg=0.01,\n",
    "        input_type='sparse',\n",
    "        #log_dir='./tmp/logs',\n",
    "        #verbose=1,\n",
    "        seed=42\n",
    "    )\n",
    "    model.fit(X_train_sparse, y_train, show_progress=True)\n",
    "    predictions = model.predict(X_test_sparse)\n",
    "    print('[order={}] accuracy: {}'.format(order, accuracy_score(y_test, predictions)))\n",
    "    # this will close tf.Session and free resources\n",
    "    print(confusion_matrix(y_test,predictions)) \n",
    "    print(classification_report(y_test, predictions))\n",
    "    model.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100 [00:00<?, ?epoch/s]\n",
      "  1%|          | 1/100 [00:00<01:00,  1.63epoch/s]\n",
      "  2%|         | 2/100 [00:01<00:56,  1.74epoch/s]\n",
      "  3%|         | 3/100 [00:01<00:52,  1.84epoch/s]\n",
      "  4%|         | 4/100 [00:02<00:50,  1.91epoch/s]\n",
      "  5%|         | 5/100 [00:02<00:48,  1.97epoch/s]\n",
      "  6%|         | 6/100 [00:03<00:47,  1.99epoch/s]\n",
      "  7%|         | 7/100 [00:03<00:46,  2.01epoch/s]\n",
      "  8%|         | 8/100 [00:03<00:45,  2.03epoch/s]\n",
      "  9%|         | 9/100 [00:04<00:44,  2.05epoch/s]\n",
      " 10%|         | 10/100 [00:04<00:43,  2.07epoch/s]\n",
      " 11%|         | 11/100 [00:05<00:43,  2.07epoch/s]\n",
      " 12%|        | 12/100 [00:05<00:42,  2.07epoch/s]\n",
      " 13%|        | 13/100 [00:06<00:42,  2.06epoch/s]\n",
      " 14%|        | 14/100 [00:06<00:41,  2.07epoch/s]\n",
      " 15%|        | 15/100 [00:07<00:40,  2.09epoch/s]\n",
      " 16%|        | 16/100 [00:07<00:40,  2.08epoch/s]\n",
      " 17%|        | 17/100 [00:08<00:39,  2.09epoch/s]\n",
      " 18%|        | 18/100 [00:08<00:39,  2.09epoch/s]\n",
      " 19%|        | 19/100 [00:09<00:38,  2.10epoch/s]\n",
      " 20%|        | 20/100 [00:09<00:38,  2.10epoch/s]\n",
      " 21%|        | 21/100 [00:10<00:38,  2.07epoch/s]\n",
      " 22%|       | 22/100 [00:10<00:41,  1.89epoch/s]\n",
      " 23%|       | 23/100 [00:11<00:39,  1.93epoch/s]\n",
      " 24%|       | 24/100 [00:11<00:39,  1.93epoch/s]\n",
      " 25%|       | 25/100 [00:12<00:37,  1.98epoch/s]\n",
      " 26%|       | 26/100 [00:12<00:36,  2.02epoch/s]\n",
      " 27%|       | 27/100 [00:13<00:35,  2.06epoch/s]\n",
      " 28%|       | 28/100 [00:13<00:34,  2.08epoch/s]\n",
      " 29%|       | 29/100 [00:14<00:34,  2.07epoch/s]\n",
      " 30%|       | 30/100 [00:14<00:33,  2.09epoch/s]\n",
      " 31%|       | 31/100 [00:15<00:32,  2.11epoch/s]\n",
      " 32%|      | 32/100 [00:15<00:32,  2.12epoch/s]\n",
      " 33%|      | 33/100 [00:16<00:31,  2.13epoch/s]\n",
      " 34%|      | 34/100 [00:16<00:30,  2.14epoch/s]\n",
      " 35%|      | 35/100 [00:17<00:30,  2.14epoch/s]\n",
      " 36%|      | 36/100 [00:17<00:29,  2.14epoch/s]\n",
      " 37%|      | 37/100 [00:17<00:29,  2.14epoch/s]\n",
      " 38%|      | 38/100 [00:18<00:28,  2.15epoch/s]\n",
      " 39%|      | 39/100 [00:18<00:28,  2.15epoch/s]\n",
      " 40%|      | 40/100 [00:19<00:27,  2.15epoch/s]\n",
      " 41%|      | 41/100 [00:19<00:27,  2.15epoch/s]\n",
      " 42%|     | 42/100 [00:20<00:26,  2.15epoch/s]\n",
      " 43%|     | 43/100 [00:20<00:26,  2.15epoch/s]\n",
      " 44%|     | 44/100 [00:21<00:26,  2.15epoch/s]\n",
      " 45%|     | 45/100 [00:21<00:25,  2.15epoch/s]\n",
      " 46%|     | 46/100 [00:22<00:25,  2.15epoch/s]\n",
      " 47%|     | 47/100 [00:22<00:24,  2.15epoch/s]\n",
      " 48%|     | 48/100 [00:23<00:24,  2.14epoch/s]\n",
      " 49%|     | 49/100 [00:23<00:23,  2.15epoch/s]\n",
      " 50%|     | 50/100 [00:23<00:23,  2.15epoch/s]\n",
      " 51%|     | 51/100 [00:24<00:22,  2.15epoch/s]\n",
      " 52%|    | 52/100 [00:24<00:22,  2.15epoch/s]\n",
      " 53%|    | 53/100 [00:25<00:21,  2.15epoch/s]\n",
      " 54%|    | 54/100 [00:25<00:21,  2.11epoch/s]\n",
      " 55%|    | 55/100 [00:26<00:21,  2.07epoch/s]\n",
      " 56%|    | 56/100 [00:26<00:21,  2.09epoch/s]\n",
      " 57%|    | 57/100 [00:27<00:20,  2.11epoch/s]\n",
      " 58%|    | 58/100 [00:27<00:19,  2.12epoch/s]\n",
      " 59%|    | 59/100 [00:28<00:19,  2.13epoch/s]\n",
      " 60%|    | 60/100 [00:28<00:18,  2.14epoch/s]\n",
      " 61%|    | 61/100 [00:29<00:18,  2.13epoch/s]\n",
      " 62%|   | 62/100 [00:29<00:18,  2.10epoch/s]\n",
      " 63%|   | 63/100 [00:30<00:17,  2.12epoch/s]\n",
      " 64%|   | 64/100 [00:30<00:16,  2.13epoch/s]\n",
      " 65%|   | 65/100 [00:31<00:16,  2.13epoch/s]\n",
      " 66%|   | 66/100 [00:31<00:15,  2.14epoch/s]\n",
      " 67%|   | 67/100 [00:31<00:15,  2.14epoch/s]\n",
      " 68%|   | 68/100 [00:32<00:14,  2.14epoch/s]\n",
      " 69%|   | 69/100 [00:32<00:14,  2.15epoch/s]\n",
      " 70%|   | 70/100 [00:33<00:13,  2.15epoch/s]\n",
      " 71%|   | 71/100 [00:33<00:13,  2.15epoch/s]\n",
      " 72%|  | 72/100 [00:34<00:13,  2.15epoch/s]\n",
      " 73%|  | 73/100 [00:34<00:12,  2.15epoch/s]\n",
      " 74%|  | 74/100 [00:35<00:12,  2.15epoch/s]\n",
      " 75%|  | 75/100 [00:35<00:11,  2.15epoch/s]\n",
      " 76%|  | 76/100 [00:36<00:11,  2.15epoch/s]\n",
      " 77%|  | 77/100 [00:36<00:10,  2.11epoch/s]\n",
      " 78%|  | 78/100 [00:37<00:11,  1.98epoch/s]\n",
      " 79%|  | 79/100 [00:37<00:10,  1.98epoch/s]\n",
      " 80%|  | 80/100 [00:38<00:10,  1.89epoch/s]\n",
      " 81%|  | 81/100 [00:38<00:10,  1.90epoch/s]\n",
      " 82%| | 82/100 [00:39<00:09,  1.93epoch/s]\n",
      " 83%| | 83/100 [00:39<00:08,  1.97epoch/s]\n",
      " 84%| | 84/100 [00:40<00:08,  1.92epoch/s]\n",
      " 85%| | 85/100 [00:40<00:07,  1.89epoch/s]\n",
      " 86%| | 86/100 [00:41<00:07,  1.95epoch/s]\n",
      " 87%| | 87/100 [00:41<00:06,  1.99epoch/s]\n",
      " 88%| | 88/100 [00:42<00:06,  1.99epoch/s]\n",
      " 89%| | 89/100 [00:42<00:05,  1.98epoch/s]\n",
      " 90%| | 90/100 [00:43<00:05,  1.96epoch/s]\n",
      " 91%| | 91/100 [00:43<00:04,  2.00epoch/s]\n",
      " 92%|| 92/100 [00:44<00:03,  2.02epoch/s]\n",
      " 93%|| 93/100 [00:44<00:03,  2.05epoch/s]\n",
      " 94%|| 94/100 [00:45<00:02,  2.07epoch/s]\n",
      " 95%|| 95/100 [00:45<00:02,  2.07epoch/s]\n",
      " 96%|| 96/100 [00:46<00:01,  2.08epoch/s]\n",
      " 97%|| 97/100 [00:46<00:01,  2.09epoch/s]\n",
      " 98%|| 98/100 [00:47<00:00,  2.09epoch/s]\n",
      " 99%|| 99/100 [00:47<00:00,  2.10epoch/s]\n",
      "100%|| 100/100 [00:48<00:00,  2.10epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[order=3] accuracy: 0.7580576155162578\n",
      "[[4204 2837]\n",
      " [ 556 6427]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.60      0.71      7041\n",
      "         1.0       0.69      0.92      0.79      6983\n",
      "\n",
      "   micro avg       0.76      0.76      0.76     14024\n",
      "   macro avg       0.79      0.76      0.75     14024\n",
      "weighted avg       0.79      0.76      0.75     14024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# weight - 2.0 best\n",
    "model = TFFMClassifier(\n",
    "    order=2,\n",
    "    pos_class_weight=2.0,\n",
    "    rank=10, \n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate=0.001), \n",
    "    n_epochs=100, \n",
    "    batch_size=1024,\n",
    "    init_std=0.001,\n",
    "    reg=0.01,\n",
    "    input_type='sparse',\n",
    "    #log_dir='./tmp/logs',\n",
    "    #verbose=1,\n",
    "    seed=42\n",
    ")\n",
    "model.fit(X_train_sparse, y_train, show_progress=True)\n",
    "predictions = model.predict(X_test_sparse)\n",
    "print('[order={}] accuracy: {}'.format(order, accuracy_score(y_test, predictions)))\n",
    "# this will close tf.Session and free resources\n",
    "print(confusion_matrix(y_test,predictions)) \n",
    "print(classification_report(y_test, predictions))\n",
    "model.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100 [00:00<?, ?epoch/s]\n",
      "  1%|          | 1/100 [00:00<01:02,  1.58epoch/s]\n",
      "  2%|         | 2/100 [00:01<01:01,  1.60epoch/s]\n",
      "  3%|         | 3/100 [00:01<00:57,  1.69epoch/s]\n",
      "  4%|         | 4/100 [00:02<00:53,  1.79epoch/s]\n",
      "  5%|         | 5/100 [00:02<00:50,  1.87epoch/s]\n",
      "  6%|         | 6/100 [00:03<00:48,  1.93epoch/s]\n",
      "  7%|         | 7/100 [00:03<00:47,  1.97epoch/s]\n",
      "  8%|         | 8/100 [00:04<00:46,  2.00epoch/s]\n",
      "  9%|         | 9/100 [00:04<00:45,  2.01epoch/s]\n",
      " 10%|         | 10/100 [00:05<00:44,  2.02epoch/s]\n",
      " 11%|         | 11/100 [00:05<00:43,  2.03epoch/s]\n",
      " 12%|        | 12/100 [00:06<00:43,  2.03epoch/s]\n",
      " 13%|        | 13/100 [00:06<00:42,  2.03epoch/s]\n",
      " 14%|        | 14/100 [00:07<00:42,  2.03epoch/s]\n",
      " 15%|        | 15/100 [00:07<00:41,  2.05epoch/s]\n",
      " 16%|        | 16/100 [00:08<00:41,  2.01epoch/s]\n",
      " 17%|        | 17/100 [00:08<00:41,  1.99epoch/s]\n",
      " 18%|        | 18/100 [00:09<00:40,  2.00epoch/s]\n",
      " 19%|        | 19/100 [00:09<00:40,  2.02epoch/s]\n",
      " 20%|        | 20/100 [00:10<00:39,  2.03epoch/s]\n",
      " 21%|        | 21/100 [00:10<00:38,  2.05epoch/s]\n",
      " 22%|       | 22/100 [00:11<00:37,  2.07epoch/s]\n",
      " 23%|       | 23/100 [00:11<00:36,  2.08epoch/s]\n",
      " 24%|       | 24/100 [00:11<00:36,  2.09epoch/s]\n",
      " 25%|       | 25/100 [00:12<00:35,  2.09epoch/s]\n",
      " 26%|       | 26/100 [00:12<00:35,  2.08epoch/s]\n",
      " 27%|       | 27/100 [00:13<00:35,  2.07epoch/s]\n",
      " 28%|       | 28/100 [00:13<00:35,  2.05epoch/s]\n",
      " 29%|       | 29/100 [00:14<00:34,  2.04epoch/s]\n",
      " 30%|       | 30/100 [00:14<00:35,  1.99epoch/s]\n",
      " 31%|       | 31/100 [00:15<00:34,  2.02epoch/s]\n",
      " 32%|      | 32/100 [00:15<00:33,  2.02epoch/s]\n",
      " 33%|      | 33/100 [00:16<00:32,  2.04epoch/s]\n",
      " 34%|      | 34/100 [00:16<00:32,  2.06epoch/s]\n",
      " 35%|      | 35/100 [00:17<00:31,  2.07epoch/s]\n",
      " 36%|      | 36/100 [00:17<00:30,  2.09epoch/s]\n",
      " 37%|      | 37/100 [00:18<00:30,  2.09epoch/s]\n",
      " 38%|      | 38/100 [00:18<00:29,  2.09epoch/s]\n",
      " 39%|      | 39/100 [00:19<00:29,  2.09epoch/s]\n",
      " 40%|      | 40/100 [00:19<00:28,  2.10epoch/s]\n",
      " 41%|      | 41/100 [00:20<00:28,  2.10epoch/s]\n",
      " 42%|     | 42/100 [00:20<00:27,  2.10epoch/s]\n",
      " 43%|     | 43/100 [00:21<00:27,  2.10epoch/s]\n",
      " 44%|     | 44/100 [00:21<00:26,  2.09epoch/s]\n",
      " 45%|     | 45/100 [00:22<00:26,  2.09epoch/s]\n",
      " 46%|     | 46/100 [00:22<00:25,  2.09epoch/s]\n",
      " 47%|     | 47/100 [00:23<00:25,  2.09epoch/s]\n",
      " 48%|     | 48/100 [00:23<00:24,  2.09epoch/s]\n",
      " 49%|     | 49/100 [00:24<00:24,  2.09epoch/s]\n",
      " 50%|     | 50/100 [00:24<00:24,  2.08epoch/s]\n",
      " 51%|     | 51/100 [00:25<00:23,  2.07epoch/s]\n",
      " 52%|    | 52/100 [00:25<00:23,  2.05epoch/s]\n",
      " 53%|    | 53/100 [00:26<00:23,  2.03epoch/s]\n",
      " 54%|    | 54/100 [00:26<00:22,  2.04epoch/s]\n",
      " 55%|    | 55/100 [00:26<00:22,  2.03epoch/s]\n",
      " 56%|    | 56/100 [00:27<00:21,  2.03epoch/s]\n",
      " 57%|    | 57/100 [00:27<00:21,  2.02epoch/s]\n",
      " 58%|    | 58/100 [00:28<00:20,  2.03epoch/s]\n",
      " 59%|    | 59/100 [00:28<00:20,  2.04epoch/s]\n",
      " 60%|    | 60/100 [00:29<00:19,  2.05epoch/s]\n",
      " 61%|    | 61/100 [00:29<00:18,  2.06epoch/s]\n",
      " 62%|   | 62/100 [00:30<00:18,  2.07epoch/s]\n",
      " 63%|   | 63/100 [00:30<00:17,  2.08epoch/s]\n",
      " 64%|   | 64/100 [00:31<00:17,  2.03epoch/s]\n",
      " 65%|   | 65/100 [00:31<00:17,  2.01epoch/s]\n",
      " 66%|   | 66/100 [00:32<00:16,  2.01epoch/s]\n",
      " 67%|   | 67/100 [00:32<00:16,  2.01epoch/s]\n",
      " 68%|   | 68/100 [00:33<00:15,  2.02epoch/s]\n",
      " 69%|   | 69/100 [00:33<00:15,  2.02epoch/s]\n",
      " 70%|   | 70/100 [00:34<00:14,  2.01epoch/s]\n",
      " 71%|   | 71/100 [00:34<00:15,  1.91epoch/s]\n",
      " 72%|  | 72/100 [00:35<00:14,  1.89epoch/s]\n",
      " 73%|  | 73/100 [00:36<00:14,  1.92epoch/s]\n",
      " 74%|  | 74/100 [00:36<00:13,  1.95epoch/s]\n",
      " 75%|  | 75/100 [00:36<00:12,  1.98epoch/s]\n",
      " 76%|  | 76/100 [00:37<00:12,  1.99epoch/s]\n",
      " 77%|  | 77/100 [00:37<00:11,  2.00epoch/s]\n",
      " 78%|  | 78/100 [00:38<00:10,  2.01epoch/s]\n",
      " 79%|  | 79/100 [00:38<00:10,  2.01epoch/s]\n",
      " 80%|  | 80/100 [00:39<00:09,  2.03epoch/s]\n",
      " 81%|  | 81/100 [00:39<00:09,  2.04epoch/s]\n",
      " 82%| | 82/100 [00:40<00:08,  2.05epoch/s]\n",
      " 83%| | 83/100 [00:40<00:08,  2.01epoch/s]\n",
      " 84%| | 84/100 [00:41<00:08,  1.96epoch/s]\n",
      " 85%| | 85/100 [00:42<00:07,  1.93epoch/s]\n",
      " 86%| | 86/100 [00:42<00:07,  1.93epoch/s]\n",
      " 87%| | 87/100 [00:43<00:06,  1.95epoch/s]\n",
      " 88%| | 88/100 [00:43<00:06,  1.97epoch/s]\n",
      " 89%| | 89/100 [00:44<00:05,  1.99epoch/s]\n",
      " 90%| | 90/100 [00:44<00:04,  2.00epoch/s]\n",
      " 91%| | 91/100 [00:45<00:04,  1.99epoch/s]\n",
      " 92%|| 92/100 [00:45<00:04,  1.93epoch/s]\n",
      " 93%|| 93/100 [00:46<00:03,  1.83epoch/s]\n",
      " 94%|| 94/100 [00:46<00:03,  1.88epoch/s]\n",
      " 95%|| 95/100 [00:47<00:02,  1.92epoch/s]\n",
      " 96%|| 96/100 [00:47<00:02,  1.96epoch/s]\n",
      " 97%|| 97/100 [00:48<00:01,  1.99epoch/s]\n",
      " 98%|| 98/100 [00:48<00:01,  1.98epoch/s]\n",
      " 99%|| 99/100 [00:49<00:00,  1.98epoch/s]\n",
      "100%|| 100/100 [00:49<00:00,  1.98epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[order=3] accuracy: 0.7678265830005705\n",
      "[[4326 2715]\n",
      " [ 541 6442]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.61      0.73      7041\n",
      "         1.0       0.70      0.92      0.80      6983\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     14024\n",
      "   macro avg       0.80      0.77      0.76     14024\n",
      "weighted avg       0.80      0.77      0.76     14024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BEST OPTIMIZED TFFM\n",
    "model = TFFMClassifier(\n",
    "    order=2, \n",
    "    rank=10,\n",
    "    pos_class_weight=2.0,\n",
    "    optimizer=tf.train.FtrlOptimizer(0.01, l2_regularization_strength=0.001, l2_shrinkage_regularization_strength=0.001), \n",
    "    n_epochs=100, \n",
    "    batch_size=1024,\n",
    "    init_std=0.001,\n",
    "    reg=0.01,\n",
    "    input_type='sparse',\n",
    "    #log_dir='./tmp/logs',\n",
    "    #verbose=1,\n",
    "    seed=42\n",
    ")\n",
    "model.fit(X_train_sparse, y_train, show_progress=True)\n",
    "predictions = model.predict(X_test_sparse)\n",
    "print('[order={}] accuracy: {}'.format(order, accuracy_score(y_test, predictions)))\n",
    "# this will close tf.Session and free resources\n",
    "print(confusion_matrix(y_test,predictions)) \n",
    "print(classification_report(y_test, predictions))\n",
    "model.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "56094/56094 [==============================] - ETA: 1:04 - loss: 0.7754 - acc: 0.511 - ETA: 19s - loss: 0.7481 - acc: 0.526 - ETA: 12s - loss: 0.7456 - acc: 0.51 - ETA: 10s - loss: 0.7361 - acc: 0.52 - ETA: 8s - loss: 0.7298 - acc: 0.5240 - ETA: 7s - loss: 0.7252 - acc: 0.532 - ETA: 6s - loss: 0.7197 - acc: 0.536 - ETA: 6s - loss: 0.7169 - acc: 0.535 - ETA: 5s - loss: 0.7160 - acc: 0.534 - ETA: 5s - loss: 0.7142 - acc: 0.536 - ETA: 5s - loss: 0.7124 - acc: 0.537 - ETA: 5s - loss: 0.7096 - acc: 0.541 - ETA: 5s - loss: 0.7055 - acc: 0.548 - ETA: 5s - loss: 0.7023 - acc: 0.551 - ETA: 4s - loss: 0.6990 - acc: 0.557 - ETA: 4s - loss: 0.6956 - acc: 0.562 - ETA: 4s - loss: 0.6932 - acc: 0.566 - ETA: 4s - loss: 0.6918 - acc: 0.568 - ETA: 4s - loss: 0.6885 - acc: 0.572 - ETA: 4s - loss: 0.6859 - acc: 0.577 - ETA: 4s - loss: 0.6835 - acc: 0.581 - ETA: 3s - loss: 0.6809 - acc: 0.585 - ETA: 3s - loss: 0.6771 - acc: 0.591 - ETA: 3s - loss: 0.6730 - acc: 0.596 - ETA: 3s - loss: 0.6710 - acc: 0.599 - ETA: 3s - loss: 0.6682 - acc: 0.603 - ETA: 3s - loss: 0.6655 - acc: 0.606 - ETA: 3s - loss: 0.6633 - acc: 0.609 - ETA: 3s - loss: 0.6607 - acc: 0.613 - ETA: 3s - loss: 0.6585 - acc: 0.615 - ETA: 3s - loss: 0.6576 - acc: 0.617 - ETA: 3s - loss: 0.6553 - acc: 0.621 - ETA: 2s - loss: 0.6524 - acc: 0.625 - ETA: 2s - loss: 0.6512 - acc: 0.626 - ETA: 2s - loss: 0.6497 - acc: 0.628 - ETA: 2s - loss: 0.6481 - acc: 0.630 - ETA: 2s - loss: 0.6461 - acc: 0.632 - ETA: 2s - loss: 0.6435 - acc: 0.636 - ETA: 2s - loss: 0.6422 - acc: 0.638 - ETA: 2s - loss: 0.6402 - acc: 0.640 - ETA: 2s - loss: 0.6391 - acc: 0.641 - ETA: 2s - loss: 0.6374 - acc: 0.643 - ETA: 2s - loss: 0.6354 - acc: 0.646 - ETA: 2s - loss: 0.6341 - acc: 0.648 - ETA: 2s - loss: 0.6319 - acc: 0.650 - ETA: 1s - loss: 0.6305 - acc: 0.651 - ETA: 1s - loss: 0.6292 - acc: 0.653 - ETA: 1s - loss: 0.6289 - acc: 0.654 - ETA: 1s - loss: 0.6270 - acc: 0.656 - ETA: 1s - loss: 0.6258 - acc: 0.657 - ETA: 1s - loss: 0.6247 - acc: 0.659 - ETA: 1s - loss: 0.6239 - acc: 0.660 - ETA: 1s - loss: 0.6229 - acc: 0.661 - ETA: 1s - loss: 0.6221 - acc: 0.663 - ETA: 1s - loss: 0.6212 - acc: 0.664 - ETA: 1s - loss: 0.6198 - acc: 0.665 - ETA: 1s - loss: 0.6186 - acc: 0.667 - ETA: 1s - loss: 0.6174 - acc: 0.668 - ETA: 1s - loss: 0.6162 - acc: 0.670 - ETA: 1s - loss: 0.6153 - acc: 0.671 - ETA: 0s - loss: 0.6142 - acc: 0.672 - ETA: 0s - loss: 0.6135 - acc: 0.673 - ETA: 0s - loss: 0.6122 - acc: 0.674 - ETA: 0s - loss: 0.6110 - acc: 0.676 - ETA: 0s - loss: 0.6106 - acc: 0.676 - ETA: 0s - loss: 0.6091 - acc: 0.678 - ETA: 0s - loss: 0.6084 - acc: 0.678 - ETA: 0s - loss: 0.6074 - acc: 0.680 - ETA: 0s - loss: 0.6073 - acc: 0.680 - ETA: 0s - loss: 0.6064 - acc: 0.681 - ETA: 0s - loss: 0.6059 - acc: 0.681 - ETA: 0s - loss: 0.6048 - acc: 0.682 - ETA: 0s - loss: 0.6047 - acc: 0.683 - ETA: 0s - loss: 0.6042 - acc: 0.683 - ETA: 0s - loss: 0.6035 - acc: 0.684 - 5s 82us/step - loss: 0.6031 - acc: 0.6853\n",
      "Epoch 2/20\n",
      "56094/56094 [==============================] - ETA: 4s - loss: 0.6085 - acc: 0.691 - ETA: 3s - loss: 0.5535 - acc: 0.741 - ETA: 3s - loss: 0.5465 - acc: 0.745 - ETA: 3s - loss: 0.5532 - acc: 0.743 - ETA: 3s - loss: 0.5500 - acc: 0.743 - ETA: 3s - loss: 0.5463 - acc: 0.748 - ETA: 3s - loss: 0.5426 - acc: 0.750 - ETA: 3s - loss: 0.5427 - acc: 0.751 - ETA: 3s - loss: 0.5461 - acc: 0.750 - ETA: 3s - loss: 0.5466 - acc: 0.750 - ETA: 3s - loss: 0.5493 - acc: 0.748 - ETA: 3s - loss: 0.5500 - acc: 0.747 - ETA: 3s - loss: 0.5505 - acc: 0.747 - ETA: 3s - loss: 0.5507 - acc: 0.746 - ETA: 3s - loss: 0.5499 - acc: 0.746 - ETA: 3s - loss: 0.5493 - acc: 0.747 - ETA: 2s - loss: 0.5499 - acc: 0.746 - ETA: 2s - loss: 0.5519 - acc: 0.744 - ETA: 2s - loss: 0.5521 - acc: 0.744 - ETA: 2s - loss: 0.5525 - acc: 0.744 - ETA: 2s - loss: 0.5519 - acc: 0.744 - ETA: 2s - loss: 0.5520 - acc: 0.744 - ETA: 2s - loss: 0.5519 - acc: 0.743 - ETA: 2s - loss: 0.5521 - acc: 0.743 - ETA: 2s - loss: 0.5521 - acc: 0.742 - ETA: 2s - loss: 0.5522 - acc: 0.742 - ETA: 2s - loss: 0.5516 - acc: 0.742 - ETA: 2s - loss: 0.5517 - acc: 0.742 - ETA: 2s - loss: 0.5525 - acc: 0.742 - ETA: 2s - loss: 0.5526 - acc: 0.742 - ETA: 2s - loss: 0.5525 - acc: 0.742 - ETA: 2s - loss: 0.5522 - acc: 0.742 - ETA: 2s - loss: 0.5515 - acc: 0.742 - ETA: 2s - loss: 0.5516 - acc: 0.741 - ETA: 2s - loss: 0.5511 - acc: 0.742 - ETA: 2s - loss: 0.5514 - acc: 0.741 - ETA: 1s - loss: 0.5513 - acc: 0.742 - ETA: 1s - loss: 0.5504 - acc: 0.743 - ETA: 1s - loss: 0.5503 - acc: 0.743 - ETA: 1s - loss: 0.5506 - acc: 0.743 - ETA: 1s - loss: 0.5505 - acc: 0.743 - ETA: 1s - loss: 0.5498 - acc: 0.743 - ETA: 1s - loss: 0.5492 - acc: 0.744 - ETA: 1s - loss: 0.5494 - acc: 0.743 - ETA: 1s - loss: 0.5497 - acc: 0.743 - ETA: 1s - loss: 0.5492 - acc: 0.743 - ETA: 1s - loss: 0.5493 - acc: 0.743 - ETA: 1s - loss: 0.5488 - acc: 0.743 - ETA: 1s - loss: 0.5482 - acc: 0.744 - ETA: 1s - loss: 0.5475 - acc: 0.744 - ETA: 1s - loss: 0.5479 - acc: 0.744 - ETA: 1s - loss: 0.5474 - acc: 0.744 - ETA: 1s - loss: 0.5473 - acc: 0.744 - ETA: 0s - loss: 0.5467 - acc: 0.745 - ETA: 0s - loss: 0.5460 - acc: 0.745 - ETA: 0s - loss: 0.5462 - acc: 0.745 - ETA: 0s - loss: 0.5453 - acc: 0.746 - ETA: 0s - loss: 0.5452 - acc: 0.746 - ETA: 0s - loss: 0.5450 - acc: 0.746 - ETA: 0s - loss: 0.5453 - acc: 0.746 - ETA: 0s - loss: 0.5455 - acc: 0.745 - ETA: 0s - loss: 0.5450 - acc: 0.746 - ETA: 0s - loss: 0.5446 - acc: 0.746 - ETA: 0s - loss: 0.5443 - acc: 0.746 - ETA: 0s - loss: 0.5443 - acc: 0.746 - ETA: 0s - loss: 0.5442 - acc: 0.746 - ETA: 0s - loss: 0.5443 - acc: 0.746 - ETA: 0s - loss: 0.5450 - acc: 0.745 - ETA: 0s - loss: 0.5450 - acc: 0.745 - 4s 73us/step - loss: 0.5451 - acc: 0.7457\n",
      "Epoch 3/20\n",
      "56094/56094 [==============================] - ETA: 3s - loss: 0.5216 - acc: 0.753 - ETA: 3s - loss: 0.5697 - acc: 0.729 - ETA: 3s - loss: 0.5483 - acc: 0.742 - ETA: 3s - loss: 0.5400 - acc: 0.750 - ETA: 3s - loss: 0.5410 - acc: 0.751 - ETA: 3s - loss: 0.5415 - acc: 0.749 - ETA: 3s - loss: 0.5443 - acc: 0.746 - ETA: 3s - loss: 0.5420 - acc: 0.747 - ETA: 3s - loss: 0.5374 - acc: 0.749 - ETA: 3s - loss: 0.5408 - acc: 0.745 - ETA: 3s - loss: 0.5434 - acc: 0.744 - ETA: 3s - loss: 0.5416 - acc: 0.745 - ETA: 3s - loss: 0.5418 - acc: 0.745 - ETA: 3s - loss: 0.5403 - acc: 0.747 - ETA: 3s - loss: 0.5382 - acc: 0.749 - ETA: 3s - loss: 0.5359 - acc: 0.751 - ETA: 3s - loss: 0.5349 - acc: 0.752 - ETA: 2s - loss: 0.5357 - acc: 0.751 - ETA: 2s - loss: 0.5337 - acc: 0.752 - ETA: 2s - loss: 0.5344 - acc: 0.751 - ETA: 2s - loss: 0.5336 - acc: 0.752 - ETA: 2s - loss: 0.5331 - acc: 0.753 - ETA: 2s - loss: 0.5339 - acc: 0.752 - ETA: 2s - loss: 0.5349 - acc: 0.751 - ETA: 2s - loss: 0.5340 - acc: 0.752 - ETA: 2s - loss: 0.5322 - acc: 0.753 - ETA: 2s - loss: 0.5333 - acc: 0.752 - ETA: 2s - loss: 0.5329 - acc: 0.752 - ETA: 2s - loss: 0.5327 - acc: 0.752 - ETA: 2s - loss: 0.5325 - acc: 0.752 - ETA: 2s - loss: 0.5333 - acc: 0.752 - ETA: 2s - loss: 0.5338 - acc: 0.752 - ETA: 2s - loss: 0.5344 - acc: 0.751 - ETA: 2s - loss: 0.5344 - acc: 0.752 - ETA: 2s - loss: 0.5341 - acc: 0.752 - ETA: 1s - loss: 0.5331 - acc: 0.752 - ETA: 1s - loss: 0.5321 - acc: 0.753 - ETA: 1s - loss: 0.5320 - acc: 0.753 - ETA: 1s - loss: 0.5327 - acc: 0.753 - ETA: 1s - loss: 0.5326 - acc: 0.753 - ETA: 1s - loss: 0.5319 - acc: 0.754 - ETA: 1s - loss: 0.5316 - acc: 0.754 - ETA: 1s - loss: 0.5318 - acc: 0.753 - ETA: 1s - loss: 0.5312 - acc: 0.754 - ETA: 1s - loss: 0.5304 - acc: 0.755 - ETA: 1s - loss: 0.5299 - acc: 0.755 - ETA: 1s - loss: 0.5302 - acc: 0.755 - ETA: 1s - loss: 0.5303 - acc: 0.755 - ETA: 1s - loss: 0.5313 - acc: 0.755 - ETA: 1s - loss: 0.5310 - acc: 0.755 - ETA: 1s - loss: 0.5305 - acc: 0.755 - ETA: 0s - loss: 0.5304 - acc: 0.756 - ETA: 0s - loss: 0.5303 - acc: 0.755 - ETA: 0s - loss: 0.5300 - acc: 0.756 - ETA: 0s - loss: 0.5296 - acc: 0.756 - ETA: 0s - loss: 0.5298 - acc: 0.756 - ETA: 0s - loss: 0.5296 - acc: 0.756 - ETA: 0s - loss: 0.5291 - acc: 0.756 - ETA: 0s - loss: 0.5292 - acc: 0.756 - ETA: 0s - loss: 0.5293 - acc: 0.756 - ETA: 0s - loss: 0.5293 - acc: 0.756 - ETA: 0s - loss: 0.5290 - acc: 0.756 - ETA: 0s - loss: 0.5292 - acc: 0.756 - ETA: 0s - loss: 0.5290 - acc: 0.756 - ETA: 0s - loss: 0.5290 - acc: 0.757 - ETA: 0s - loss: 0.5289 - acc: 0.757 - ETA: 0s - loss: 0.5288 - acc: 0.757 - ETA: 0s - loss: 0.5284 - acc: 0.757 - 4s 71us/step - loss: 0.5284 - acc: 0.7572\n",
      "Epoch 4/20\n",
      "56094/56094 [==============================] - ETA: 3s - loss: 0.5179 - acc: 0.781 - ETA: 3s - loss: 0.5230 - acc: 0.765 - ETA: 3s - loss: 0.5095 - acc: 0.775 - ETA: 3s - loss: 0.5102 - acc: 0.776 - ETA: 3s - loss: 0.5185 - acc: 0.770 - ETA: 3s - loss: 0.5191 - acc: 0.767 - ETA: 3s - loss: 0.5222 - acc: 0.765 - ETA: 3s - loss: 0.5227 - acc: 0.765 - ETA: 3s - loss: 0.5255 - acc: 0.763 - ETA: 3s - loss: 0.5229 - acc: 0.765 - ETA: 3s - loss: 0.5228 - acc: 0.764 - ETA: 3s - loss: 0.5196 - acc: 0.765 - ETA: 3s - loss: 0.5188 - acc: 0.766 - ETA: 3s - loss: 0.5186 - acc: 0.766 - ETA: 3s - loss: 0.5156 - acc: 0.769 - ETA: 3s - loss: 0.5175 - acc: 0.766 - ETA: 2s - loss: 0.5159 - acc: 0.767 - ETA: 2s - loss: 0.5136 - acc: 0.768 - ETA: 2s - loss: 0.5151 - acc: 0.767 - ETA: 2s - loss: 0.5150 - acc: 0.767 - ETA: 2s - loss: 0.5152 - acc: 0.767 - ETA: 2s - loss: 0.5149 - acc: 0.767 - ETA: 2s - loss: 0.5156 - acc: 0.766 - ETA: 2s - loss: 0.5143 - acc: 0.767 - ETA: 2s - loss: 0.5146 - acc: 0.767 - ETA: 2s - loss: 0.5137 - acc: 0.768 - ETA: 2s - loss: 0.5137 - acc: 0.768 - ETA: 2s - loss: 0.5136 - acc: 0.768 - ETA: 2s - loss: 0.5139 - acc: 0.768 - ETA: 2s - loss: 0.5148 - acc: 0.767 - ETA: 2s - loss: 0.5149 - acc: 0.767 - ETA: 2s - loss: 0.5153 - acc: 0.767 - ETA: 1s - loss: 0.5149 - acc: 0.767 - ETA: 1s - loss: 0.5152 - acc: 0.767 - ETA: 1s - loss: 0.5152 - acc: 0.767 - ETA: 1s - loss: 0.5156 - acc: 0.767 - ETA: 1s - loss: 0.5156 - acc: 0.767 - ETA: 1s - loss: 0.5162 - acc: 0.766 - ETA: 1s - loss: 0.5168 - acc: 0.766 - ETA: 1s - loss: 0.5167 - acc: 0.766 - ETA: 1s - loss: 0.5172 - acc: 0.766 - ETA: 1s - loss: 0.5173 - acc: 0.766 - ETA: 1s - loss: 0.5177 - acc: 0.766 - ETA: 1s - loss: 0.5176 - acc: 0.766 - ETA: 1s - loss: 0.5182 - acc: 0.765 - ETA: 1s - loss: 0.5181 - acc: 0.765 - ETA: 1s - loss: 0.5185 - acc: 0.765 - ETA: 1s - loss: 0.5181 - acc: 0.765 - ETA: 1s - loss: 0.5183 - acc: 0.765 - ETA: 0s - loss: 0.5183 - acc: 0.765 - ETA: 0s - loss: 0.5177 - acc: 0.765 - ETA: 0s - loss: 0.5176 - acc: 0.765 - ETA: 0s - loss: 0.5178 - acc: 0.765 - ETA: 0s - loss: 0.5177 - acc: 0.765 - ETA: 0s - loss: 0.5177 - acc: 0.765 - ETA: 0s - loss: 0.5179 - acc: 0.765 - ETA: 0s - loss: 0.5177 - acc: 0.766 - ETA: 0s - loss: 0.5171 - acc: 0.766 - ETA: 0s - loss: 0.5169 - acc: 0.766 - ETA: 0s - loss: 0.5170 - acc: 0.766 - ETA: 0s - loss: 0.5172 - acc: 0.766 - ETA: 0s - loss: 0.5170 - acc: 0.766 - ETA: 0s - loss: 0.5171 - acc: 0.766 - ETA: 0s - loss: 0.5167 - acc: 0.766 - ETA: 0s - loss: 0.5169 - acc: 0.765 - ETA: 0s - loss: 0.5164 - acc: 0.766 - 4s 68us/step - loss: 0.5168 - acc: 0.7660\n",
      "Epoch 5/20\n",
      "56094/56094 [==============================] - ETA: 3s - loss: 0.5292 - acc: 0.753 - ETA: 3s - loss: 0.5291 - acc: 0.753 - ETA: 3s - loss: 0.5228 - acc: 0.759 - ETA: 3s - loss: 0.5218 - acc: 0.761 - ETA: 3s - loss: 0.5176 - acc: 0.765 - ETA: 3s - loss: 0.5198 - acc: 0.764 - ETA: 3s - loss: 0.5178 - acc: 0.764 - ETA: 3s - loss: 0.5189 - acc: 0.765 - ETA: 3s - loss: 0.5164 - acc: 0.766 - ETA: 3s - loss: 0.5171 - acc: 0.766 - ETA: 3s - loss: 0.5157 - acc: 0.767 - ETA: 3s - loss: 0.5147 - acc: 0.767 - ETA: 2s - loss: 0.5137 - acc: 0.768 - ETA: 2s - loss: 0.5121 - acc: 0.769 - ETA: 2s - loss: 0.5151 - acc: 0.767 - ETA: 2s - loss: 0.5171 - acc: 0.766 - ETA: 2s - loss: 0.5166 - acc: 0.767 - ETA: 2s - loss: 0.5166 - acc: 0.767 - ETA: 2s - loss: 0.5170 - acc: 0.767 - ETA: 2s - loss: 0.5162 - acc: 0.768 - ETA: 2s - loss: 0.5163 - acc: 0.768 - ETA: 2s - loss: 0.5146 - acc: 0.769 - ETA: 2s - loss: 0.5139 - acc: 0.769 - ETA: 2s - loss: 0.5141 - acc: 0.768 - ETA: 2s - loss: 0.5148 - acc: 0.768 - ETA: 2s - loss: 0.5149 - acc: 0.769 - ETA: 2s - loss: 0.5135 - acc: 0.769 - ETA: 2s - loss: 0.5133 - acc: 0.770 - ETA: 2s - loss: 0.5139 - acc: 0.769 - ETA: 2s - loss: 0.5144 - acc: 0.768 - ETA: 1s - loss: 0.5143 - acc: 0.768 - ETA: 1s - loss: 0.5144 - acc: 0.768 - ETA: 1s - loss: 0.5141 - acc: 0.768 - ETA: 1s - loss: 0.5147 - acc: 0.768 - ETA: 1s - loss: 0.5149 - acc: 0.767 - ETA: 1s - loss: 0.5153 - acc: 0.767 - ETA: 1s - loss: 0.5141 - acc: 0.767 - ETA: 1s - loss: 0.5140 - acc: 0.768 - ETA: 1s - loss: 0.5138 - acc: 0.768 - ETA: 1s - loss: 0.5134 - acc: 0.768 - ETA: 1s - loss: 0.5130 - acc: 0.768 - ETA: 1s - loss: 0.5127 - acc: 0.769 - ETA: 1s - loss: 0.5122 - acc: 0.769 - ETA: 1s - loss: 0.5115 - acc: 0.769 - ETA: 1s - loss: 0.5115 - acc: 0.769 - ETA: 1s - loss: 0.5114 - acc: 0.769 - ETA: 1s - loss: 0.5121 - acc: 0.769 - ETA: 0s - loss: 0.5122 - acc: 0.769 - ETA: 0s - loss: 0.5119 - acc: 0.769 - ETA: 0s - loss: 0.5117 - acc: 0.769 - ETA: 0s - loss: 0.5115 - acc: 0.769 - ETA: 0s - loss: 0.5118 - acc: 0.769 - ETA: 0s - loss: 0.5117 - acc: 0.769 - ETA: 0s - loss: 0.5118 - acc: 0.768 - ETA: 0s - loss: 0.5116 - acc: 0.768 - ETA: 0s - loss: 0.5115 - acc: 0.769 - ETA: 0s - loss: 0.5115 - acc: 0.769 - ETA: 0s - loss: 0.5115 - acc: 0.769 - ETA: 0s - loss: 0.5113 - acc: 0.769 - ETA: 0s - loss: 0.5111 - acc: 0.769 - ETA: 0s - loss: 0.5112 - acc: 0.769 - ETA: 0s - loss: 0.5114 - acc: 0.769 - ETA: 0s - loss: 0.5115 - acc: 0.769 - ETA: 0s - loss: 0.5117 - acc: 0.768 - ETA: 0s - loss: 0.5113 - acc: 0.769 - ETA: 0s - loss: 0.5114 - acc: 0.769 - 4s 68us/step - loss: 0.5115 - acc: 0.7690\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56094/56094 [==============================] - ETA: 3s - loss: 0.4765 - acc: 0.781 - ETA: 4s - loss: 0.4855 - acc: 0.785 - ETA: 3s - loss: 0.4962 - acc: 0.776 - ETA: 4s - loss: 0.4982 - acc: 0.778 - ETA: 4s - loss: 0.5023 - acc: 0.777 - ETA: 4s - loss: 0.5037 - acc: 0.776 - ETA: 3s - loss: 0.5073 - acc: 0.772 - ETA: 3s - loss: 0.5075 - acc: 0.771 - ETA: 3s - loss: 0.5098 - acc: 0.769 - ETA: 3s - loss: 0.5076 - acc: 0.771 - ETA: 3s - loss: 0.5057 - acc: 0.773 - ETA: 3s - loss: 0.5053 - acc: 0.774 - ETA: 3s - loss: 0.5064 - acc: 0.774 - ETA: 3s - loss: 0.5063 - acc: 0.773 - ETA: 3s - loss: 0.5065 - acc: 0.771 - ETA: 3s - loss: 0.5063 - acc: 0.771 - ETA: 3s - loss: 0.5055 - acc: 0.772 - ETA: 3s - loss: 0.5070 - acc: 0.771 - ETA: 3s - loss: 0.5072 - acc: 0.771 - ETA: 3s - loss: 0.5061 - acc: 0.772 - ETA: 3s - loss: 0.5055 - acc: 0.772 - ETA: 3s - loss: 0.5065 - acc: 0.772 - ETA: 3s - loss: 0.5068 - acc: 0.772 - ETA: 3s - loss: 0.5058 - acc: 0.773 - ETA: 2s - loss: 0.5064 - acc: 0.772 - ETA: 2s - loss: 0.5063 - acc: 0.773 - ETA: 2s - loss: 0.5076 - acc: 0.772 - ETA: 2s - loss: 0.5074 - acc: 0.772 - ETA: 2s - loss: 0.5071 - acc: 0.773 - ETA: 2s - loss: 0.5065 - acc: 0.773 - ETA: 2s - loss: 0.5062 - acc: 0.773 - ETA: 2s - loss: 0.5075 - acc: 0.773 - ETA: 2s - loss: 0.5080 - acc: 0.772 - ETA: 2s - loss: 0.5086 - acc: 0.771 - ETA: 2s - loss: 0.5090 - acc: 0.771 - ETA: 2s - loss: 0.5093 - acc: 0.771 - ETA: 2s - loss: 0.5090 - acc: 0.771 - ETA: 2s - loss: 0.5089 - acc: 0.771 - ETA: 2s - loss: 0.5090 - acc: 0.770 - ETA: 1s - loss: 0.5091 - acc: 0.770 - ETA: 1s - loss: 0.5082 - acc: 0.770 - ETA: 1s - loss: 0.5082 - acc: 0.771 - ETA: 1s - loss: 0.5077 - acc: 0.771 - ETA: 1s - loss: 0.5075 - acc: 0.771 - ETA: 1s - loss: 0.5077 - acc: 0.770 - ETA: 1s - loss: 0.5077 - acc: 0.770 - ETA: 1s - loss: 0.5082 - acc: 0.770 - ETA: 1s - loss: 0.5070 - acc: 0.771 - ETA: 1s - loss: 0.5073 - acc: 0.770 - ETA: 1s - loss: 0.5078 - acc: 0.770 - ETA: 1s - loss: 0.5077 - acc: 0.770 - ETA: 1s - loss: 0.5068 - acc: 0.771 - ETA: 1s - loss: 0.5072 - acc: 0.770 - ETA: 0s - loss: 0.5070 - acc: 0.770 - ETA: 0s - loss: 0.5068 - acc: 0.770 - ETA: 0s - loss: 0.5070 - acc: 0.770 - ETA: 0s - loss: 0.5069 - acc: 0.770 - ETA: 0s - loss: 0.5067 - acc: 0.770 - ETA: 0s - loss: 0.5068 - acc: 0.770 - ETA: 0s - loss: 0.5067 - acc: 0.769 - ETA: 0s - loss: 0.5071 - acc: 0.769 - ETA: 0s - loss: 0.5073 - acc: 0.769 - ETA: 0s - loss: 0.5072 - acc: 0.769 - ETA: 0s - loss: 0.5075 - acc: 0.769 - ETA: 0s - loss: 0.5078 - acc: 0.768 - ETA: 0s - loss: 0.5075 - acc: 0.769 - ETA: 0s - loss: 0.5073 - acc: 0.769 - ETA: 0s - loss: 0.5067 - acc: 0.769 - ETA: 0s - loss: 0.5064 - acc: 0.769 - 4s 73us/step - loss: 0.5064 - acc: 0.7697\n",
      "Epoch 7/20\n",
      "56094/56094 [==============================] - ETA: 3s - loss: 0.5344 - acc: 0.750 - ETA: 3s - loss: 0.5235 - acc: 0.766 - ETA: 3s - loss: 0.5036 - acc: 0.770 - ETA: 3s - loss: 0.5011 - acc: 0.770 - ETA: 3s - loss: 0.5044 - acc: 0.770 - ETA: 3s - loss: 0.5019 - acc: 0.770 - ETA: 3s - loss: 0.5014 - acc: 0.772 - ETA: 3s - loss: 0.4989 - acc: 0.773 - ETA: 3s - loss: 0.5009 - acc: 0.770 - ETA: 3s - loss: 0.5022 - acc: 0.769 - ETA: 3s - loss: 0.5039 - acc: 0.767 - ETA: 3s - loss: 0.5022 - acc: 0.767 - ETA: 3s - loss: 0.5000 - acc: 0.769 - ETA: 3s - loss: 0.5005 - acc: 0.769 - ETA: 3s - loss: 0.5027 - acc: 0.767 - ETA: 3s - loss: 0.5021 - acc: 0.768 - ETA: 3s - loss: 0.5012 - acc: 0.770 - ETA: 2s - loss: 0.5024 - acc: 0.769 - ETA: 2s - loss: 0.5034 - acc: 0.768 - ETA: 2s - loss: 0.5037 - acc: 0.768 - ETA: 2s - loss: 0.5037 - acc: 0.768 - ETA: 2s - loss: 0.5038 - acc: 0.768 - ETA: 2s - loss: 0.5037 - acc: 0.768 - ETA: 2s - loss: 0.5030 - acc: 0.768 - ETA: 2s - loss: 0.5021 - acc: 0.770 - ETA: 2s - loss: 0.5029 - acc: 0.769 - ETA: 2s - loss: 0.5022 - acc: 0.769 - ETA: 2s - loss: 0.5021 - acc: 0.769 - ETA: 2s - loss: 0.5014 - acc: 0.769 - ETA: 2s - loss: 0.5013 - acc: 0.769 - ETA: 2s - loss: 0.5016 - acc: 0.769 - ETA: 2s - loss: 0.5016 - acc: 0.769 - ETA: 2s - loss: 0.5016 - acc: 0.769 - ETA: 2s - loss: 0.5012 - acc: 0.770 - ETA: 2s - loss: 0.5016 - acc: 0.769 - ETA: 1s - loss: 0.5013 - acc: 0.770 - ETA: 1s - loss: 0.5009 - acc: 0.770 - ETA: 1s - loss: 0.5006 - acc: 0.771 - ETA: 1s - loss: 0.5003 - acc: 0.771 - ETA: 1s - loss: 0.5006 - acc: 0.770 - ETA: 1s - loss: 0.5011 - acc: 0.770 - ETA: 1s - loss: 0.5013 - acc: 0.770 - ETA: 1s - loss: 0.5013 - acc: 0.770 - ETA: 1s - loss: 0.5017 - acc: 0.769 - ETA: 1s - loss: 0.5017 - acc: 0.769 - ETA: 1s - loss: 0.5012 - acc: 0.770 - ETA: 1s - loss: 0.5011 - acc: 0.770 - ETA: 1s - loss: 0.5014 - acc: 0.770 - ETA: 1s - loss: 0.5017 - acc: 0.770 - ETA: 1s - loss: 0.5017 - acc: 0.770 - ETA: 1s - loss: 0.5019 - acc: 0.770 - ETA: 0s - loss: 0.5022 - acc: 0.770 - ETA: 0s - loss: 0.5018 - acc: 0.770 - ETA: 0s - loss: 0.5021 - acc: 0.769 - ETA: 0s - loss: 0.5022 - acc: 0.769 - ETA: 0s - loss: 0.5017 - acc: 0.770 - ETA: 0s - loss: 0.5021 - acc: 0.769 - ETA: 0s - loss: 0.5024 - acc: 0.769 - ETA: 0s - loss: 0.5022 - acc: 0.769 - ETA: 0s - loss: 0.5018 - acc: 0.770 - ETA: 0s - loss: 0.5017 - acc: 0.770 - ETA: 0s - loss: 0.5016 - acc: 0.770 - ETA: 0s - loss: 0.5015 - acc: 0.770 - ETA: 0s - loss: 0.5016 - acc: 0.770 - ETA: 0s - loss: 0.5011 - acc: 0.770 - ETA: 0s - loss: 0.5015 - acc: 0.770 - ETA: 0s - loss: 0.5017 - acc: 0.770 - 4s 70us/step - loss: 0.5016 - acc: 0.7702\n",
      "Epoch 8/20\n",
      "56094/56094 [==============================] - ETA: 3s - loss: 0.5037 - acc: 0.765 - ETA: 3s - loss: 0.4869 - acc: 0.777 - ETA: 3s - loss: 0.5021 - acc: 0.767 - ETA: 3s - loss: 0.5024 - acc: 0.766 - ETA: 3s - loss: 0.5011 - acc: 0.765 - ETA: 3s - loss: 0.4974 - acc: 0.768 - ETA: 3s - loss: 0.5005 - acc: 0.766 - ETA: 3s - loss: 0.5002 - acc: 0.766 - ETA: 3s - loss: 0.4981 - acc: 0.769 - ETA: 3s - loss: 0.4999 - acc: 0.770 - ETA: 3s - loss: 0.5008 - acc: 0.770 - ETA: 3s - loss: 0.4974 - acc: 0.773 - ETA: 3s - loss: 0.4973 - acc: 0.774 - ETA: 3s - loss: 0.4959 - acc: 0.775 - ETA: 3s - loss: 0.4976 - acc: 0.773 - ETA: 3s - loss: 0.4956 - acc: 0.775 - ETA: 3s - loss: 0.4962 - acc: 0.776 - ETA: 3s - loss: 0.4946 - acc: 0.776 - ETA: 3s - loss: 0.4947 - acc: 0.776 - ETA: 2s - loss: 0.4935 - acc: 0.777 - ETA: 2s - loss: 0.4948 - acc: 0.775 - ETA: 2s - loss: 0.4966 - acc: 0.774 - ETA: 2s - loss: 0.4967 - acc: 0.773 - ETA: 2s - loss: 0.4963 - acc: 0.774 - ETA: 2s - loss: 0.4972 - acc: 0.774 - ETA: 2s - loss: 0.4971 - acc: 0.774 - ETA: 2s - loss: 0.4972 - acc: 0.774 - ETA: 2s - loss: 0.4965 - acc: 0.774 - ETA: 2s - loss: 0.4970 - acc: 0.773 - ETA: 2s - loss: 0.4972 - acc: 0.773 - ETA: 2s - loss: 0.4965 - acc: 0.774 - ETA: 2s - loss: 0.4968 - acc: 0.773 - ETA: 2s - loss: 0.4972 - acc: 0.773 - ETA: 2s - loss: 0.4964 - acc: 0.773 - ETA: 2s - loss: 0.4967 - acc: 0.773 - ETA: 1s - loss: 0.4963 - acc: 0.774 - ETA: 1s - loss: 0.4977 - acc: 0.773 - ETA: 1s - loss: 0.4981 - acc: 0.772 - ETA: 1s - loss: 0.4979 - acc: 0.772 - ETA: 1s - loss: 0.4977 - acc: 0.772 - ETA: 1s - loss: 0.4980 - acc: 0.772 - ETA: 1s - loss: 0.4989 - acc: 0.772 - ETA: 1s - loss: 0.4980 - acc: 0.772 - ETA: 1s - loss: 0.4975 - acc: 0.772 - ETA: 1s - loss: 0.4977 - acc: 0.772 - ETA: 1s - loss: 0.4974 - acc: 0.772 - ETA: 1s - loss: 0.4975 - acc: 0.772 - ETA: 1s - loss: 0.4967 - acc: 0.773 - ETA: 1s - loss: 0.4970 - acc: 0.773 - ETA: 1s - loss: 0.4976 - acc: 0.772 - ETA: 0s - loss: 0.4972 - acc: 0.773 - ETA: 0s - loss: 0.4975 - acc: 0.773 - ETA: 0s - loss: 0.4971 - acc: 0.773 - ETA: 0s - loss: 0.4974 - acc: 0.772 - ETA: 0s - loss: 0.4970 - acc: 0.773 - ETA: 0s - loss: 0.4969 - acc: 0.773 - ETA: 0s - loss: 0.4973 - acc: 0.773 - ETA: 0s - loss: 0.4967 - acc: 0.773 - ETA: 0s - loss: 0.4971 - acc: 0.773 - ETA: 0s - loss: 0.4975 - acc: 0.772 - ETA: 0s - loss: 0.4981 - acc: 0.772 - ETA: 0s - loss: 0.4983 - acc: 0.772 - ETA: 0s - loss: 0.4984 - acc: 0.771 - ETA: 0s - loss: 0.4980 - acc: 0.771 - ETA: 0s - loss: 0.4981 - acc: 0.771 - ETA: 0s - loss: 0.4979 - acc: 0.772 - ETA: 0s - loss: 0.4981 - acc: 0.771 - ETA: 0s - loss: 0.4981 - acc: 0.771 - 4s 69us/step - loss: 0.4982 - acc: 0.7717\n",
      "Epoch 9/20\n",
      "56094/56094 [==============================] - ETA: 4s - loss: 0.5169 - acc: 0.765 - ETA: 4s - loss: 0.4935 - acc: 0.768 - ETA: 3s - loss: 0.4967 - acc: 0.771 - ETA: 3s - loss: 0.5001 - acc: 0.767 - ETA: 3s - loss: 0.5011 - acc: 0.764 - ETA: 3s - loss: 0.5012 - acc: 0.763 - ETA: 3s - loss: 0.5017 - acc: 0.765 - ETA: 3s - loss: 0.5003 - acc: 0.767 - ETA: 3s - loss: 0.4978 - acc: 0.769 - ETA: 3s - loss: 0.4954 - acc: 0.772 - ETA: 3s - loss: 0.4953 - acc: 0.771 - ETA: 3s - loss: 0.4942 - acc: 0.772 - ETA: 3s - loss: 0.4938 - acc: 0.771 - ETA: 3s - loss: 0.4938 - acc: 0.770 - ETA: 2s - loss: 0.4957 - acc: 0.769 - ETA: 2s - loss: 0.4953 - acc: 0.770 - ETA: 2s - loss: 0.4959 - acc: 0.769 - ETA: 2s - loss: 0.4977 - acc: 0.768 - ETA: 2s - loss: 0.4976 - acc: 0.768 - ETA: 2s - loss: 0.4981 - acc: 0.768 - ETA: 2s - loss: 0.4969 - acc: 0.769 - ETA: 2s - loss: 0.4965 - acc: 0.769 - ETA: 2s - loss: 0.4961 - acc: 0.769 - ETA: 2s - loss: 0.4960 - acc: 0.770 - ETA: 2s - loss: 0.4974 - acc: 0.769 - ETA: 2s - loss: 0.4975 - acc: 0.769 - ETA: 2s - loss: 0.4971 - acc: 0.769 - ETA: 2s - loss: 0.4976 - acc: 0.769 - ETA: 2s - loss: 0.4979 - acc: 0.769 - ETA: 2s - loss: 0.4979 - acc: 0.768 - ETA: 2s - loss: 0.4987 - acc: 0.768 - ETA: 2s - loss: 0.4986 - acc: 0.769 - ETA: 2s - loss: 0.4989 - acc: 0.769 - ETA: 2s - loss: 0.4993 - acc: 0.768 - ETA: 1s - loss: 0.4983 - acc: 0.769 - ETA: 1s - loss: 0.4980 - acc: 0.769 - ETA: 1s - loss: 0.4976 - acc: 0.769 - ETA: 1s - loss: 0.4976 - acc: 0.769 - ETA: 1s - loss: 0.4976 - acc: 0.769 - ETA: 1s - loss: 0.4985 - acc: 0.768 - ETA: 1s - loss: 0.4979 - acc: 0.769 - ETA: 1s - loss: 0.4977 - acc: 0.769 - ETA: 1s - loss: 0.4983 - acc: 0.768 - ETA: 1s - loss: 0.4981 - acc: 0.768 - ETA: 1s - loss: 0.4974 - acc: 0.769 - ETA: 1s - loss: 0.4971 - acc: 0.769 - ETA: 1s - loss: 0.4974 - acc: 0.769 - ETA: 1s - loss: 0.4970 - acc: 0.769 - ETA: 1s - loss: 0.4967 - acc: 0.770 - ETA: 0s - loss: 0.4965 - acc: 0.770 - ETA: 0s - loss: 0.4965 - acc: 0.770 - ETA: 0s - loss: 0.4971 - acc: 0.769 - ETA: 0s - loss: 0.4970 - acc: 0.769 - ETA: 0s - loss: 0.4967 - acc: 0.770 - ETA: 0s - loss: 0.4969 - acc: 0.769 - ETA: 0s - loss: 0.4973 - acc: 0.769 - ETA: 0s - loss: 0.4971 - acc: 0.769 - ETA: 0s - loss: 0.4973 - acc: 0.769 - ETA: 0s - loss: 0.4971 - acc: 0.769 - ETA: 0s - loss: 0.4972 - acc: 0.769 - ETA: 0s - loss: 0.4971 - acc: 0.769 - ETA: 0s - loss: 0.4971 - acc: 0.770 - ETA: 0s - loss: 0.4967 - acc: 0.770 - ETA: 0s - loss: 0.4963 - acc: 0.770 - ETA: 0s - loss: 0.4962 - acc: 0.770 - ETA: 0s - loss: 0.4964 - acc: 0.770 - 4s 70us/step - loss: 0.4964 - acc: 0.7707\n",
      "Epoch 10/20\n",
      "56094/56094 [==============================] - ETA: 4s - loss: 0.5472 - acc: 0.730 - ETA: 3s - loss: 0.5118 - acc: 0.752 - ETA: 3s - loss: 0.5130 - acc: 0.756 - ETA: 3s - loss: 0.5161 - acc: 0.759 - ETA: 3s - loss: 0.5136 - acc: 0.759 - ETA: 3s - loss: 0.5141 - acc: 0.759 - ETA: 3s - loss: 0.5114 - acc: 0.760 - ETA: 3s - loss: 0.5061 - acc: 0.764 - ETA: 3s - loss: 0.5070 - acc: 0.762 - ETA: 3s - loss: 0.5034 - acc: 0.764 - ETA: 3s - loss: 0.5035 - acc: 0.765 - ETA: 3s - loss: 0.5001 - acc: 0.767 - ETA: 3s - loss: 0.4997 - acc: 0.766 - ETA: 3s - loss: 0.5011 - acc: 0.766 - ETA: 2s - loss: 0.5012 - acc: 0.765 - ETA: 2s - loss: 0.5012 - acc: 0.765 - ETA: 2s - loss: 0.5004 - acc: 0.766 - ETA: 2s - loss: 0.4988 - acc: 0.767 - ETA: 2s - loss: 0.5007 - acc: 0.766 - ETA: 2s - loss: 0.5013 - acc: 0.765 - ETA: 2s - loss: 0.4993 - acc: 0.766 - ETA: 2s - loss: 0.5005 - acc: 0.765 - ETA: 2s - loss: 0.5005 - acc: 0.766 - ETA: 2s - loss: 0.5007 - acc: 0.766 - ETA: 2s - loss: 0.4999 - acc: 0.767 - ETA: 2s - loss: 0.4994 - acc: 0.768 - ETA: 2s - loss: 0.4983 - acc: 0.768 - ETA: 2s - loss: 0.4981 - acc: 0.768 - ETA: 2s - loss: 0.4976 - acc: 0.769 - ETA: 2s - loss: 0.4980 - acc: 0.769 - ETA: 2s - loss: 0.5000 - acc: 0.767 - ETA: 2s - loss: 0.4996 - acc: 0.767 - ETA: 1s - loss: 0.4992 - acc: 0.768 - ETA: 1s - loss: 0.4991 - acc: 0.768 - ETA: 1s - loss: 0.4994 - acc: 0.768 - ETA: 1s - loss: 0.4994 - acc: 0.768 - ETA: 1s - loss: 0.4987 - acc: 0.768 - ETA: 1s - loss: 0.4979 - acc: 0.769 - ETA: 1s - loss: 0.4973 - acc: 0.770 - ETA: 1s - loss: 0.4972 - acc: 0.770 - ETA: 1s - loss: 0.4976 - acc: 0.769 - ETA: 1s - loss: 0.4975 - acc: 0.770 - ETA: 1s - loss: 0.4979 - acc: 0.769 - ETA: 1s - loss: 0.4979 - acc: 0.769 - ETA: 1s - loss: 0.4980 - acc: 0.769 - ETA: 1s - loss: 0.4981 - acc: 0.769 - ETA: 1s - loss: 0.4983 - acc: 0.769 - ETA: 1s - loss: 0.4983 - acc: 0.769 - ETA: 1s - loss: 0.4979 - acc: 0.770 - ETA: 0s - loss: 0.4969 - acc: 0.770 - ETA: 0s - loss: 0.4967 - acc: 0.770 - ETA: 0s - loss: 0.4970 - acc: 0.770 - ETA: 0s - loss: 0.4969 - acc: 0.770 - ETA: 0s - loss: 0.4969 - acc: 0.770 - ETA: 0s - loss: 0.4967 - acc: 0.770 - ETA: 0s - loss: 0.4961 - acc: 0.770 - ETA: 0s - loss: 0.4959 - acc: 0.770 - ETA: 0s - loss: 0.4956 - acc: 0.770 - ETA: 0s - loss: 0.4957 - acc: 0.770 - ETA: 0s - loss: 0.4958 - acc: 0.770 - ETA: 0s - loss: 0.4959 - acc: 0.770 - ETA: 0s - loss: 0.4961 - acc: 0.770 - ETA: 0s - loss: 0.4963 - acc: 0.770 - ETA: 0s - loss: 0.4961 - acc: 0.770 - ETA: 0s - loss: 0.4957 - acc: 0.770 - ETA: 0s - loss: 0.4953 - acc: 0.770 - ETA: 0s - loss: 0.4947 - acc: 0.771 - 4s 71us/step - loss: 0.4946 - acc: 0.7710\n",
      "Epoch 11/20\n",
      "56094/56094 [==============================] - ETA: 4s - loss: 0.5421 - acc: 0.753 - ETA: 4s - loss: 0.5094 - acc: 0.762 - ETA: 3s - loss: 0.4984 - acc: 0.766 - ETA: 3s - loss: 0.5062 - acc: 0.761 - ETA: 3s - loss: 0.5028 - acc: 0.762 - ETA: 3s - loss: 0.5024 - acc: 0.766 - ETA: 3s - loss: 0.5004 - acc: 0.768 - ETA: 3s - loss: 0.4984 - acc: 0.768 - ETA: 3s - loss: 0.4955 - acc: 0.770 - ETA: 3s - loss: 0.4946 - acc: 0.770 - ETA: 3s - loss: 0.4970 - acc: 0.768 - ETA: 3s - loss: 0.4964 - acc: 0.768 - ETA: 3s - loss: 0.4947 - acc: 0.770 - ETA: 3s - loss: 0.4923 - acc: 0.771 - ETA: 2s - loss: 0.4916 - acc: 0.773 - ETA: 2s - loss: 0.4918 - acc: 0.773 - ETA: 2s - loss: 0.4934 - acc: 0.772 - ETA: 2s - loss: 0.4948 - acc: 0.771 - ETA: 2s - loss: 0.4960 - acc: 0.769 - ETA: 2s - loss: 0.4969 - acc: 0.768 - ETA: 2s - loss: 0.4983 - acc: 0.768 - ETA: 2s - loss: 0.4966 - acc: 0.769 - ETA: 2s - loss: 0.4966 - acc: 0.770 - ETA: 2s - loss: 0.4970 - acc: 0.769 - ETA: 2s - loss: 0.4960 - acc: 0.770 - ETA: 2s - loss: 0.4947 - acc: 0.770 - ETA: 2s - loss: 0.4948 - acc: 0.770 - ETA: 2s - loss: 0.4939 - acc: 0.771 - ETA: 2s - loss: 0.4949 - acc: 0.771 - ETA: 2s - loss: 0.4946 - acc: 0.771 - ETA: 2s - loss: 0.4948 - acc: 0.771 - ETA: 2s - loss: 0.4951 - acc: 0.771 - ETA: 1s - loss: 0.4960 - acc: 0.770 - ETA: 1s - loss: 0.4954 - acc: 0.770 - ETA: 1s - loss: 0.4967 - acc: 0.769 - ETA: 1s - loss: 0.4963 - acc: 0.769 - ETA: 1s - loss: 0.4958 - acc: 0.770 - ETA: 1s - loss: 0.4956 - acc: 0.770 - ETA: 1s - loss: 0.4946 - acc: 0.770 - ETA: 1s - loss: 0.4949 - acc: 0.770 - ETA: 1s - loss: 0.4948 - acc: 0.770 - ETA: 1s - loss: 0.4941 - acc: 0.771 - ETA: 1s - loss: 0.4934 - acc: 0.771 - ETA: 1s - loss: 0.4942 - acc: 0.771 - ETA: 1s - loss: 0.4948 - acc: 0.771 - ETA: 1s - loss: 0.4952 - acc: 0.770 - ETA: 1s - loss: 0.4958 - acc: 0.769 - ETA: 1s - loss: 0.4957 - acc: 0.769 - ETA: 1s - loss: 0.4953 - acc: 0.769 - ETA: 0s - loss: 0.4948 - acc: 0.769 - ETA: 0s - loss: 0.4947 - acc: 0.770 - ETA: 0s - loss: 0.4946 - acc: 0.770 - ETA: 0s - loss: 0.4946 - acc: 0.770 - ETA: 0s - loss: 0.4943 - acc: 0.770 - ETA: 0s - loss: 0.4937 - acc: 0.771 - ETA: 0s - loss: 0.4941 - acc: 0.770 - ETA: 0s - loss: 0.4945 - acc: 0.770 - ETA: 0s - loss: 0.4942 - acc: 0.770 - ETA: 0s - loss: 0.4940 - acc: 0.770 - ETA: 0s - loss: 0.4939 - acc: 0.770 - ETA: 0s - loss: 0.4933 - acc: 0.771 - ETA: 0s - loss: 0.4932 - acc: 0.771 - ETA: 0s - loss: 0.4926 - acc: 0.771 - ETA: 0s - loss: 0.4931 - acc: 0.771 - ETA: 0s - loss: 0.4934 - acc: 0.771 - 4s 69us/step - loss: 0.4935 - acc: 0.7711\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56094/56094 [==============================] - ETA: 3s - loss: 0.4813 - acc: 0.769 - ETA: 3s - loss: 0.4986 - acc: 0.763 - ETA: 4s - loss: 0.4847 - acc: 0.770 - ETA: 4s - loss: 0.4909 - acc: 0.771 - ETA: 4s - loss: 0.4920 - acc: 0.770 - ETA: 4s - loss: 0.4912 - acc: 0.770 - ETA: 3s - loss: 0.4892 - acc: 0.771 - ETA: 3s - loss: 0.4903 - acc: 0.770 - ETA: 3s - loss: 0.4909 - acc: 0.771 - ETA: 3s - loss: 0.4940 - acc: 0.771 - ETA: 3s - loss: 0.4920 - acc: 0.773 - ETA: 3s - loss: 0.4921 - acc: 0.772 - ETA: 3s - loss: 0.4927 - acc: 0.771 - ETA: 3s - loss: 0.4907 - acc: 0.772 - ETA: 3s - loss: 0.4921 - acc: 0.772 - ETA: 3s - loss: 0.4935 - acc: 0.771 - ETA: 3s - loss: 0.4915 - acc: 0.772 - ETA: 3s - loss: 0.4927 - acc: 0.771 - ETA: 3s - loss: 0.4936 - acc: 0.771 - ETA: 2s - loss: 0.4935 - acc: 0.771 - ETA: 2s - loss: 0.4939 - acc: 0.770 - ETA: 2s - loss: 0.4940 - acc: 0.770 - ETA: 2s - loss: 0.4935 - acc: 0.771 - ETA: 2s - loss: 0.4929 - acc: 0.771 - ETA: 2s - loss: 0.4941 - acc: 0.769 - ETA: 2s - loss: 0.4948 - acc: 0.769 - ETA: 2s - loss: 0.4961 - acc: 0.768 - ETA: 2s - loss: 0.4969 - acc: 0.767 - ETA: 2s - loss: 0.4982 - acc: 0.766 - ETA: 2s - loss: 0.4984 - acc: 0.766 - ETA: 2s - loss: 0.4975 - acc: 0.767 - ETA: 2s - loss: 0.4985 - acc: 0.767 - ETA: 2s - loss: 0.4988 - acc: 0.767 - ETA: 2s - loss: 0.4984 - acc: 0.767 - ETA: 2s - loss: 0.4978 - acc: 0.767 - ETA: 2s - loss: 0.4973 - acc: 0.768 - ETA: 1s - loss: 0.4967 - acc: 0.768 - ETA: 1s - loss: 0.4963 - acc: 0.769 - ETA: 1s - loss: 0.4964 - acc: 0.769 - ETA: 1s - loss: 0.4956 - acc: 0.769 - ETA: 1s - loss: 0.4957 - acc: 0.769 - ETA: 1s - loss: 0.4957 - acc: 0.769 - ETA: 1s - loss: 0.4948 - acc: 0.770 - ETA: 1s - loss: 0.4951 - acc: 0.770 - ETA: 1s - loss: 0.4948 - acc: 0.770 - ETA: 1s - loss: 0.4946 - acc: 0.770 - ETA: 1s - loss: 0.4946 - acc: 0.770 - ETA: 1s - loss: 0.4946 - acc: 0.770 - ETA: 1s - loss: 0.4943 - acc: 0.770 - ETA: 1s - loss: 0.4942 - acc: 0.770 - ETA: 1s - loss: 0.4939 - acc: 0.770 - ETA: 1s - loss: 0.4936 - acc: 0.770 - ETA: 1s - loss: 0.4935 - acc: 0.770 - ETA: 0s - loss: 0.4935 - acc: 0.770 - ETA: 0s - loss: 0.4931 - acc: 0.770 - ETA: 0s - loss: 0.4937 - acc: 0.770 - ETA: 0s - loss: 0.4937 - acc: 0.770 - ETA: 0s - loss: 0.4934 - acc: 0.770 - ETA: 0s - loss: 0.4933 - acc: 0.770 - ETA: 0s - loss: 0.4930 - acc: 0.771 - ETA: 0s - loss: 0.4931 - acc: 0.770 - ETA: 0s - loss: 0.4929 - acc: 0.770 - ETA: 0s - loss: 0.4924 - acc: 0.771 - ETA: 0s - loss: 0.4921 - acc: 0.771 - ETA: 0s - loss: 0.4920 - acc: 0.771 - ETA: 0s - loss: 0.4918 - acc: 0.771 - ETA: 0s - loss: 0.4918 - acc: 0.771 - ETA: 0s - loss: 0.4919 - acc: 0.771 - ETA: 0s - loss: 0.4924 - acc: 0.770 - 4s 73us/step - loss: 0.4920 - acc: 0.7712\n",
      "Epoch 13/20\n",
      "56094/56094 [==============================] - ETA: 3s - loss: 0.5092 - acc: 0.738 - ETA: 4s - loss: 0.4923 - acc: 0.766 - ETA: 3s - loss: 0.4986 - acc: 0.775 - ETA: 3s - loss: 0.5004 - acc: 0.770 - ETA: 3s - loss: 0.5049 - acc: 0.768 - ETA: 3s - loss: 0.5018 - acc: 0.770 - ETA: 3s - loss: 0.5011 - acc: 0.768 - ETA: 3s - loss: 0.4998 - acc: 0.770 - ETA: 3s - loss: 0.5033 - acc: 0.768 - ETA: 3s - loss: 0.5033 - acc: 0.766 - ETA: 3s - loss: 0.5037 - acc: 0.766 - ETA: 2s - loss: 0.5035 - acc: 0.765 - ETA: 2s - loss: 0.5018 - acc: 0.766 - ETA: 2s - loss: 0.4997 - acc: 0.767 - ETA: 2s - loss: 0.4979 - acc: 0.768 - ETA: 2s - loss: 0.4973 - acc: 0.768 - ETA: 2s - loss: 0.4955 - acc: 0.769 - ETA: 2s - loss: 0.4952 - acc: 0.770 - ETA: 2s - loss: 0.4947 - acc: 0.770 - ETA: 2s - loss: 0.4951 - acc: 0.769 - ETA: 2s - loss: 0.4949 - acc: 0.769 - ETA: 2s - loss: 0.4948 - acc: 0.769 - ETA: 2s - loss: 0.4937 - acc: 0.770 - ETA: 2s - loss: 0.4917 - acc: 0.772 - ETA: 2s - loss: 0.4914 - acc: 0.772 - ETA: 2s - loss: 0.4926 - acc: 0.771 - ETA: 2s - loss: 0.4926 - acc: 0.771 - ETA: 2s - loss: 0.4928 - acc: 0.771 - ETA: 1s - loss: 0.4920 - acc: 0.771 - ETA: 1s - loss: 0.4933 - acc: 0.770 - ETA: 1s - loss: 0.4937 - acc: 0.770 - ETA: 1s - loss: 0.4942 - acc: 0.769 - ETA: 1s - loss: 0.4943 - acc: 0.769 - ETA: 1s - loss: 0.4931 - acc: 0.770 - ETA: 1s - loss: 0.4924 - acc: 0.771 - ETA: 1s - loss: 0.4917 - acc: 0.771 - ETA: 1s - loss: 0.4917 - acc: 0.771 - ETA: 1s - loss: 0.4908 - acc: 0.772 - ETA: 1s - loss: 0.4911 - acc: 0.772 - ETA: 1s - loss: 0.4903 - acc: 0.772 - ETA: 1s - loss: 0.4904 - acc: 0.772 - ETA: 1s - loss: 0.4905 - acc: 0.772 - ETA: 1s - loss: 0.4904 - acc: 0.771 - ETA: 1s - loss: 0.4902 - acc: 0.772 - ETA: 1s - loss: 0.4900 - acc: 0.772 - ETA: 0s - loss: 0.4906 - acc: 0.771 - ETA: 0s - loss: 0.4907 - acc: 0.771 - ETA: 0s - loss: 0.4911 - acc: 0.771 - ETA: 0s - loss: 0.4913 - acc: 0.771 - ETA: 0s - loss: 0.4911 - acc: 0.771 - ETA: 0s - loss: 0.4915 - acc: 0.770 - ETA: 0s - loss: 0.4911 - acc: 0.771 - ETA: 0s - loss: 0.4916 - acc: 0.770 - ETA: 0s - loss: 0.4915 - acc: 0.770 - ETA: 0s - loss: 0.4910 - acc: 0.771 - ETA: 0s - loss: 0.4909 - acc: 0.771 - ETA: 0s - loss: 0.4904 - acc: 0.772 - ETA: 0s - loss: 0.4908 - acc: 0.771 - ETA: 0s - loss: 0.4907 - acc: 0.771 - ETA: 0s - loss: 0.4906 - acc: 0.771 - ETA: 0s - loss: 0.4905 - acc: 0.771 - ETA: 0s - loss: 0.4911 - acc: 0.771 - ETA: 0s - loss: 0.4911 - acc: 0.771 - ETA: 0s - loss: 0.4912 - acc: 0.771 - 4s 68us/step - loss: 0.4913 - acc: 0.7712\n",
      "Epoch 14/20\n",
      "56094/56094 [==============================] - ETA: 4s - loss: 0.4714 - acc: 0.789 - ETA: 4s - loss: 0.5009 - acc: 0.757 - ETA: 3s - loss: 0.4934 - acc: 0.762 - ETA: 3s - loss: 0.4931 - acc: 0.762 - ETA: 3s - loss: 0.4921 - acc: 0.765 - ETA: 3s - loss: 0.4978 - acc: 0.763 - ETA: 3s - loss: 0.4953 - acc: 0.766 - ETA: 3s - loss: 0.4950 - acc: 0.766 - ETA: 3s - loss: 0.4918 - acc: 0.769 - ETA: 3s - loss: 0.4923 - acc: 0.770 - ETA: 3s - loss: 0.4927 - acc: 0.770 - ETA: 3s - loss: 0.4919 - acc: 0.771 - ETA: 3s - loss: 0.4912 - acc: 0.771 - ETA: 3s - loss: 0.4922 - acc: 0.771 - ETA: 2s - loss: 0.4924 - acc: 0.770 - ETA: 2s - loss: 0.4919 - acc: 0.771 - ETA: 2s - loss: 0.4915 - acc: 0.771 - ETA: 2s - loss: 0.4899 - acc: 0.772 - ETA: 2s - loss: 0.4894 - acc: 0.771 - ETA: 2s - loss: 0.4913 - acc: 0.771 - ETA: 2s - loss: 0.4914 - acc: 0.770 - ETA: 2s - loss: 0.4914 - acc: 0.770 - ETA: 2s - loss: 0.4926 - acc: 0.769 - ETA: 2s - loss: 0.4913 - acc: 0.770 - ETA: 2s - loss: 0.4911 - acc: 0.770 - ETA: 2s - loss: 0.4917 - acc: 0.769 - ETA: 2s - loss: 0.4917 - acc: 0.769 - ETA: 2s - loss: 0.4932 - acc: 0.768 - ETA: 2s - loss: 0.4932 - acc: 0.768 - ETA: 2s - loss: 0.4934 - acc: 0.768 - ETA: 2s - loss: 0.4935 - acc: 0.768 - ETA: 1s - loss: 0.4930 - acc: 0.769 - ETA: 1s - loss: 0.4925 - acc: 0.769 - ETA: 1s - loss: 0.4927 - acc: 0.769 - ETA: 1s - loss: 0.4918 - acc: 0.771 - ETA: 1s - loss: 0.4917 - acc: 0.771 - ETA: 1s - loss: 0.4915 - acc: 0.771 - ETA: 1s - loss: 0.4901 - acc: 0.771 - ETA: 1s - loss: 0.4895 - acc: 0.772 - ETA: 1s - loss: 0.4896 - acc: 0.772 - ETA: 1s - loss: 0.4901 - acc: 0.771 - ETA: 1s - loss: 0.4898 - acc: 0.772 - ETA: 1s - loss: 0.4899 - acc: 0.771 - ETA: 1s - loss: 0.4904 - acc: 0.771 - ETA: 1s - loss: 0.4902 - acc: 0.771 - ETA: 1s - loss: 0.4896 - acc: 0.772 - ETA: 1s - loss: 0.4894 - acc: 0.772 - ETA: 1s - loss: 0.4896 - acc: 0.772 - ETA: 0s - loss: 0.4896 - acc: 0.772 - ETA: 0s - loss: 0.4898 - acc: 0.772 - ETA: 0s - loss: 0.4904 - acc: 0.772 - ETA: 0s - loss: 0.4904 - acc: 0.771 - ETA: 0s - loss: 0.4899 - acc: 0.772 - ETA: 0s - loss: 0.4900 - acc: 0.772 - ETA: 0s - loss: 0.4904 - acc: 0.771 - ETA: 0s - loss: 0.4910 - acc: 0.771 - ETA: 0s - loss: 0.4913 - acc: 0.770 - ETA: 0s - loss: 0.4915 - acc: 0.770 - ETA: 0s - loss: 0.4916 - acc: 0.770 - ETA: 0s - loss: 0.4913 - acc: 0.770 - ETA: 0s - loss: 0.4910 - acc: 0.771 - ETA: 0s - loss: 0.4909 - acc: 0.771 - ETA: 0s - loss: 0.4910 - acc: 0.771 - ETA: 0s - loss: 0.4909 - acc: 0.771 - ETA: 0s - loss: 0.4907 - acc: 0.771 - ETA: 0s - loss: 0.4909 - acc: 0.771 - 4s 68us/step - loss: 0.4910 - acc: 0.7712\n",
      "Epoch 15/20\n",
      "56094/56094 [==============================] - ETA: 3s - loss: 0.4955 - acc: 0.757 - ETA: 3s - loss: 0.4884 - acc: 0.768 - ETA: 3s - loss: 0.4906 - acc: 0.767 - ETA: 3s - loss: 0.4879 - acc: 0.771 - ETA: 3s - loss: 0.4859 - acc: 0.771 - ETA: 3s - loss: 0.4836 - acc: 0.770 - ETA: 3s - loss: 0.4839 - acc: 0.770 - ETA: 3s - loss: 0.4836 - acc: 0.772 - ETA: 3s - loss: 0.4849 - acc: 0.772 - ETA: 3s - loss: 0.4851 - acc: 0.773 - ETA: 3s - loss: 0.4859 - acc: 0.772 - ETA: 3s - loss: 0.4829 - acc: 0.774 - ETA: 3s - loss: 0.4849 - acc: 0.772 - ETA: 3s - loss: 0.4837 - acc: 0.773 - ETA: 3s - loss: 0.4841 - acc: 0.772 - ETA: 3s - loss: 0.4868 - acc: 0.769 - ETA: 3s - loss: 0.4858 - acc: 0.771 - ETA: 2s - loss: 0.4834 - acc: 0.772 - ETA: 2s - loss: 0.4843 - acc: 0.772 - ETA: 2s - loss: 0.4830 - acc: 0.773 - ETA: 2s - loss: 0.4831 - acc: 0.773 - ETA: 2s - loss: 0.4838 - acc: 0.773 - ETA: 2s - loss: 0.4845 - acc: 0.773 - ETA: 2s - loss: 0.4852 - acc: 0.773 - ETA: 2s - loss: 0.4852 - acc: 0.773 - ETA: 2s - loss: 0.4877 - acc: 0.771 - ETA: 2s - loss: 0.4879 - acc: 0.771 - ETA: 2s - loss: 0.4889 - acc: 0.771 - ETA: 2s - loss: 0.4893 - acc: 0.770 - ETA: 2s - loss: 0.4891 - acc: 0.770 - ETA: 2s - loss: 0.4894 - acc: 0.771 - ETA: 2s - loss: 0.4897 - acc: 0.771 - ETA: 2s - loss: 0.4896 - acc: 0.771 - ETA: 2s - loss: 0.4894 - acc: 0.771 - ETA: 2s - loss: 0.4903 - acc: 0.770 - ETA: 2s - loss: 0.4908 - acc: 0.770 - ETA: 2s - loss: 0.4911 - acc: 0.770 - ETA: 2s - loss: 0.4904 - acc: 0.770 - ETA: 2s - loss: 0.4897 - acc: 0.771 - ETA: 2s - loss: 0.4904 - acc: 0.770 - ETA: 1s - loss: 0.4902 - acc: 0.770 - ETA: 1s - loss: 0.4903 - acc: 0.770 - ETA: 1s - loss: 0.4902 - acc: 0.771 - ETA: 1s - loss: 0.4899 - acc: 0.771 - ETA: 1s - loss: 0.4897 - acc: 0.771 - ETA: 1s - loss: 0.4900 - acc: 0.771 - ETA: 1s - loss: 0.4900 - acc: 0.771 - ETA: 1s - loss: 0.4905 - acc: 0.770 - ETA: 1s - loss: 0.4904 - acc: 0.771 - ETA: 1s - loss: 0.4905 - acc: 0.770 - ETA: 1s - loss: 0.4903 - acc: 0.770 - ETA: 1s - loss: 0.4900 - acc: 0.771 - ETA: 1s - loss: 0.4899 - acc: 0.771 - ETA: 0s - loss: 0.4899 - acc: 0.771 - ETA: 0s - loss: 0.4895 - acc: 0.771 - ETA: 0s - loss: 0.4898 - acc: 0.771 - ETA: 0s - loss: 0.4897 - acc: 0.771 - ETA: 0s - loss: 0.4894 - acc: 0.771 - ETA: 0s - loss: 0.4895 - acc: 0.771 - ETA: 0s - loss: 0.4898 - acc: 0.771 - ETA: 0s - loss: 0.4892 - acc: 0.771 - ETA: 0s - loss: 0.4898 - acc: 0.771 - ETA: 0s - loss: 0.4902 - acc: 0.770 - ETA: 0s - loss: 0.4900 - acc: 0.771 - ETA: 0s - loss: 0.4901 - acc: 0.771 - ETA: 0s - loss: 0.4903 - acc: 0.771 - ETA: 0s - loss: 0.4904 - acc: 0.771 - ETA: 0s - loss: 0.4900 - acc: 0.771 - ETA: 0s - loss: 0.4906 - acc: 0.771 - ETA: 0s - loss: 0.4905 - acc: 0.771 - ETA: 0s - loss: 0.4905 - acc: 0.771 - 4s 74us/step - loss: 0.4903 - acc: 0.7712\n",
      "Epoch 16/20\n",
      "56094/56094 [==============================] - ETA: 3s - loss: 0.4886 - acc: 0.757 - ETA: 3s - loss: 0.4781 - acc: 0.767 - ETA: 3s - loss: 0.4851 - acc: 0.765 - ETA: 3s - loss: 0.4861 - acc: 0.769 - ETA: 3s - loss: 0.4857 - acc: 0.771 - ETA: 3s - loss: 0.4803 - acc: 0.774 - ETA: 3s - loss: 0.4799 - acc: 0.776 - ETA: 3s - loss: 0.4799 - acc: 0.776 - ETA: 3s - loss: 0.4800 - acc: 0.775 - ETA: 3s - loss: 0.4830 - acc: 0.773 - ETA: 3s - loss: 0.4854 - acc: 0.773 - ETA: 3s - loss: 0.4826 - acc: 0.776 - ETA: 3s - loss: 0.4825 - acc: 0.776 - ETA: 3s - loss: 0.4816 - acc: 0.777 - ETA: 3s - loss: 0.4831 - acc: 0.775 - ETA: 3s - loss: 0.4824 - acc: 0.775 - ETA: 3s - loss: 0.4827 - acc: 0.775 - ETA: 3s - loss: 0.4828 - acc: 0.774 - ETA: 3s - loss: 0.4823 - acc: 0.775 - ETA: 3s - loss: 0.4827 - acc: 0.773 - ETA: 3s - loss: 0.4813 - acc: 0.774 - ETA: 3s - loss: 0.4833 - acc: 0.773 - ETA: 3s - loss: 0.4834 - acc: 0.773 - ETA: 2s - loss: 0.4848 - acc: 0.771 - ETA: 2s - loss: 0.4839 - acc: 0.772 - ETA: 2s - loss: 0.4849 - acc: 0.772 - ETA: 2s - loss: 0.4855 - acc: 0.772 - ETA: 2s - loss: 0.4868 - acc: 0.771 - ETA: 2s - loss: 0.4881 - acc: 0.770 - ETA: 2s - loss: 0.4877 - acc: 0.770 - ETA: 2s - loss: 0.4871 - acc: 0.771 - ETA: 2s - loss: 0.4878 - acc: 0.770 - ETA: 2s - loss: 0.4876 - acc: 0.770 - ETA: 2s - loss: 0.4877 - acc: 0.770 - ETA: 2s - loss: 0.4870 - acc: 0.771 - ETA: 2s - loss: 0.4871 - acc: 0.771 - ETA: 2s - loss: 0.4874 - acc: 0.771 - ETA: 1s - loss: 0.4878 - acc: 0.771 - ETA: 1s - loss: 0.4871 - acc: 0.772 - ETA: 1s - loss: 0.4866 - acc: 0.772 - ETA: 1s - loss: 0.4858 - acc: 0.773 - ETA: 1s - loss: 0.4863 - acc: 0.772 - ETA: 1s - loss: 0.4871 - acc: 0.772 - ETA: 1s - loss: 0.4869 - acc: 0.772 - ETA: 1s - loss: 0.4864 - acc: 0.772 - ETA: 1s - loss: 0.4862 - acc: 0.772 - ETA: 1s - loss: 0.4867 - acc: 0.772 - ETA: 1s - loss: 0.4863 - acc: 0.772 - ETA: 1s - loss: 0.4869 - acc: 0.772 - ETA: 0s - loss: 0.4865 - acc: 0.772 - ETA: 0s - loss: 0.4866 - acc: 0.772 - ETA: 0s - loss: 0.4874 - acc: 0.772 - ETA: 0s - loss: 0.4879 - acc: 0.771 - ETA: 0s - loss: 0.4884 - acc: 0.771 - ETA: 0s - loss: 0.4883 - acc: 0.771 - ETA: 0s - loss: 0.4885 - acc: 0.771 - ETA: 0s - loss: 0.4883 - acc: 0.771 - ETA: 0s - loss: 0.4887 - acc: 0.771 - ETA: 0s - loss: 0.4891 - acc: 0.771 - ETA: 0s - loss: 0.4890 - acc: 0.771 - ETA: 0s - loss: 0.4892 - acc: 0.771 - ETA: 0s - loss: 0.4898 - acc: 0.770 - ETA: 0s - loss: 0.4897 - acc: 0.771 - ETA: 0s - loss: 0.4897 - acc: 0.771 - 4s 68us/step - loss: 0.4893 - acc: 0.7713\n",
      "Epoch 17/20\n",
      "56094/56094 [==============================] - ETA: 3s - loss: 0.4680 - acc: 0.820 - ETA: 3s - loss: 0.4910 - acc: 0.777 - ETA: 3s - loss: 0.4867 - acc: 0.777 - ETA: 3s - loss: 0.4939 - acc: 0.770 - ETA: 3s - loss: 0.4907 - acc: 0.769 - ETA: 3s - loss: 0.4852 - acc: 0.773 - ETA: 3s - loss: 0.4881 - acc: 0.772 - ETA: 3s - loss: 0.4899 - acc: 0.773 - ETA: 2s - loss: 0.4902 - acc: 0.772 - ETA: 2s - loss: 0.4892 - acc: 0.773 - ETA: 2s - loss: 0.4926 - acc: 0.770 - ETA: 2s - loss: 0.4909 - acc: 0.771 - ETA: 2s - loss: 0.4888 - acc: 0.772 - ETA: 2s - loss: 0.4901 - acc: 0.771 - ETA: 2s - loss: 0.4901 - acc: 0.771 - ETA: 2s - loss: 0.4897 - acc: 0.771 - ETA: 2s - loss: 0.4909 - acc: 0.770 - ETA: 2s - loss: 0.4913 - acc: 0.770 - ETA: 2s - loss: 0.4901 - acc: 0.771 - ETA: 2s - loss: 0.4890 - acc: 0.771 - ETA: 2s - loss: 0.4875 - acc: 0.773 - ETA: 2s - loss: 0.4881 - acc: 0.772 - ETA: 1s - loss: 0.4878 - acc: 0.773 - ETA: 1s - loss: 0.4883 - acc: 0.773 - ETA: 1s - loss: 0.4887 - acc: 0.772 - ETA: 1s - loss: 0.4884 - acc: 0.773 - ETA: 1s - loss: 0.4881 - acc: 0.772 - ETA: 1s - loss: 0.4881 - acc: 0.772 - ETA: 1s - loss: 0.4881 - acc: 0.772 - ETA: 1s - loss: 0.4877 - acc: 0.773 - ETA: 1s - loss: 0.4872 - acc: 0.773 - ETA: 1s - loss: 0.4868 - acc: 0.773 - ETA: 1s - loss: 0.4868 - acc: 0.773 - ETA: 1s - loss: 0.4871 - acc: 0.773 - ETA: 1s - loss: 0.4878 - acc: 0.772 - ETA: 1s - loss: 0.4880 - acc: 0.772 - ETA: 1s - loss: 0.4885 - acc: 0.771 - ETA: 1s - loss: 0.4884 - acc: 0.772 - ETA: 1s - loss: 0.4885 - acc: 0.771 - ETA: 0s - loss: 0.4888 - acc: 0.771 - ETA: 0s - loss: 0.4886 - acc: 0.772 - ETA: 0s - loss: 0.4894 - acc: 0.771 - ETA: 0s - loss: 0.4889 - acc: 0.772 - ETA: 0s - loss: 0.4890 - acc: 0.772 - ETA: 0s - loss: 0.4890 - acc: 0.772 - ETA: 0s - loss: 0.4894 - acc: 0.772 - ETA: 0s - loss: 0.4892 - acc: 0.772 - ETA: 0s - loss: 0.4894 - acc: 0.772 - ETA: 0s - loss: 0.4894 - acc: 0.772 - ETA: 0s - loss: 0.4889 - acc: 0.772 - ETA: 0s - loss: 0.4892 - acc: 0.772 - ETA: 0s - loss: 0.4890 - acc: 0.772 - ETA: 0s - loss: 0.4893 - acc: 0.772 - ETA: 0s - loss: 0.4894 - acc: 0.771 - ETA: 0s - loss: 0.4897 - acc: 0.771 - ETA: 0s - loss: 0.4897 - acc: 0.771 - 3s 58us/step - loss: 0.4897 - acc: 0.7714\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56094/56094 [==============================] - ETA: 3s - loss: 0.5410 - acc: 0.738 - ETA: 3s - loss: 0.5108 - acc: 0.764 - ETA: 3s - loss: 0.4974 - acc: 0.770 - ETA: 2s - loss: 0.5044 - acc: 0.765 - ETA: 2s - loss: 0.5053 - acc: 0.762 - ETA: 2s - loss: 0.4976 - acc: 0.766 - ETA: 2s - loss: 0.4945 - acc: 0.768 - ETA: 2s - loss: 0.4942 - acc: 0.768 - ETA: 2s - loss: 0.4968 - acc: 0.765 - ETA: 2s - loss: 0.4943 - acc: 0.768 - ETA: 2s - loss: 0.4941 - acc: 0.768 - ETA: 2s - loss: 0.4937 - acc: 0.768 - ETA: 2s - loss: 0.4933 - acc: 0.769 - ETA: 2s - loss: 0.4948 - acc: 0.768 - ETA: 2s - loss: 0.4943 - acc: 0.769 - ETA: 2s - loss: 0.4949 - acc: 0.769 - ETA: 2s - loss: 0.4946 - acc: 0.770 - ETA: 2s - loss: 0.4954 - acc: 0.769 - ETA: 2s - loss: 0.4943 - acc: 0.769 - ETA: 2s - loss: 0.4948 - acc: 0.769 - ETA: 2s - loss: 0.4947 - acc: 0.768 - ETA: 1s - loss: 0.4938 - acc: 0.769 - ETA: 1s - loss: 0.4928 - acc: 0.770 - ETA: 1s - loss: 0.4930 - acc: 0.769 - ETA: 1s - loss: 0.4940 - acc: 0.769 - ETA: 1s - loss: 0.4935 - acc: 0.769 - ETA: 1s - loss: 0.4932 - acc: 0.769 - ETA: 1s - loss: 0.4936 - acc: 0.769 - ETA: 1s - loss: 0.4925 - acc: 0.770 - ETA: 1s - loss: 0.4920 - acc: 0.770 - ETA: 1s - loss: 0.4914 - acc: 0.770 - ETA: 1s - loss: 0.4908 - acc: 0.771 - ETA: 1s - loss: 0.4910 - acc: 0.771 - ETA: 1s - loss: 0.4907 - acc: 0.771 - ETA: 1s - loss: 0.4913 - acc: 0.771 - ETA: 1s - loss: 0.4914 - acc: 0.771 - ETA: 1s - loss: 0.4912 - acc: 0.771 - ETA: 1s - loss: 0.4912 - acc: 0.771 - ETA: 1s - loss: 0.4910 - acc: 0.771 - ETA: 1s - loss: 0.4916 - acc: 0.771 - ETA: 0s - loss: 0.4916 - acc: 0.770 - ETA: 0s - loss: 0.4913 - acc: 0.771 - ETA: 0s - loss: 0.4913 - acc: 0.771 - ETA: 0s - loss: 0.4911 - acc: 0.771 - ETA: 0s - loss: 0.4908 - acc: 0.771 - ETA: 0s - loss: 0.4907 - acc: 0.771 - ETA: 0s - loss: 0.4910 - acc: 0.771 - ETA: 0s - loss: 0.4908 - acc: 0.771 - ETA: 0s - loss: 0.4901 - acc: 0.771 - ETA: 0s - loss: 0.4902 - acc: 0.771 - ETA: 0s - loss: 0.4896 - acc: 0.771 - ETA: 0s - loss: 0.4899 - acc: 0.771 - ETA: 0s - loss: 0.4901 - acc: 0.771 - ETA: 0s - loss: 0.4898 - acc: 0.771 - ETA: 0s - loss: 0.4900 - acc: 0.771 - ETA: 0s - loss: 0.4901 - acc: 0.771 - ETA: 0s - loss: 0.4897 - acc: 0.771 - ETA: 0s - loss: 0.4897 - acc: 0.771 - 3s 60us/step - loss: 0.4898 - acc: 0.7712\n",
      "Epoch 19/20\n",
      "56094/56094 [==============================] - ETA: 3s - loss: 0.4234 - acc: 0.816 - ETA: 3s - loss: 0.4867 - acc: 0.769 - ETA: 3s - loss: 0.4890 - acc: 0.772 - ETA: 2s - loss: 0.4906 - acc: 0.769 - ETA: 2s - loss: 0.4894 - acc: 0.770 - ETA: 2s - loss: 0.4894 - acc: 0.770 - ETA: 2s - loss: 0.4915 - acc: 0.769 - ETA: 3s - loss: 0.4917 - acc: 0.769 - ETA: 3s - loss: 0.4927 - acc: 0.768 - ETA: 3s - loss: 0.4900 - acc: 0.770 - ETA: 3s - loss: 0.4911 - acc: 0.769 - ETA: 2s - loss: 0.4914 - acc: 0.768 - ETA: 2s - loss: 0.4920 - acc: 0.768 - ETA: 2s - loss: 0.4907 - acc: 0.768 - ETA: 2s - loss: 0.4901 - acc: 0.769 - ETA: 2s - loss: 0.4904 - acc: 0.769 - ETA: 2s - loss: 0.4905 - acc: 0.768 - ETA: 2s - loss: 0.4907 - acc: 0.768 - ETA: 2s - loss: 0.4911 - acc: 0.768 - ETA: 2s - loss: 0.4906 - acc: 0.769 - ETA: 2s - loss: 0.4901 - acc: 0.769 - ETA: 2s - loss: 0.4903 - acc: 0.769 - ETA: 2s - loss: 0.4894 - acc: 0.770 - ETA: 2s - loss: 0.4894 - acc: 0.770 - ETA: 1s - loss: 0.4899 - acc: 0.769 - ETA: 1s - loss: 0.4910 - acc: 0.768 - ETA: 1s - loss: 0.4913 - acc: 0.768 - ETA: 1s - loss: 0.4916 - acc: 0.767 - ETA: 1s - loss: 0.4921 - acc: 0.767 - ETA: 1s - loss: 0.4924 - acc: 0.767 - ETA: 1s - loss: 0.4918 - acc: 0.767 - ETA: 1s - loss: 0.4909 - acc: 0.767 - ETA: 1s - loss: 0.4909 - acc: 0.768 - ETA: 1s - loss: 0.4906 - acc: 0.768 - ETA: 1s - loss: 0.4900 - acc: 0.769 - ETA: 1s - loss: 0.4909 - acc: 0.768 - ETA: 1s - loss: 0.4901 - acc: 0.769 - ETA: 1s - loss: 0.4904 - acc: 0.769 - ETA: 1s - loss: 0.4901 - acc: 0.769 - ETA: 1s - loss: 0.4897 - acc: 0.769 - ETA: 1s - loss: 0.4895 - acc: 0.770 - ETA: 1s - loss: 0.4896 - acc: 0.770 - ETA: 0s - loss: 0.4892 - acc: 0.770 - ETA: 0s - loss: 0.4892 - acc: 0.770 - ETA: 0s - loss: 0.4892 - acc: 0.770 - ETA: 0s - loss: 0.4892 - acc: 0.771 - ETA: 0s - loss: 0.4894 - acc: 0.771 - ETA: 0s - loss: 0.4896 - acc: 0.770 - ETA: 0s - loss: 0.4894 - acc: 0.771 - ETA: 0s - loss: 0.4893 - acc: 0.771 - ETA: 0s - loss: 0.4895 - acc: 0.771 - ETA: 0s - loss: 0.4894 - acc: 0.771 - ETA: 0s - loss: 0.4897 - acc: 0.770 - ETA: 0s - loss: 0.4891 - acc: 0.770 - ETA: 0s - loss: 0.4889 - acc: 0.770 - ETA: 0s - loss: 0.4886 - acc: 0.771 - ETA: 0s - loss: 0.4889 - acc: 0.770 - ETA: 0s - loss: 0.4891 - acc: 0.770 - 3s 60us/step - loss: 0.4892 - acc: 0.7708\n",
      "Epoch 20/20\n",
      "56094/56094 [==============================] - ETA: 5s - loss: 0.4928 - acc: 0.773 - ETA: 4s - loss: 0.4952 - acc: 0.762 - ETA: 4s - loss: 0.5040 - acc: 0.757 - ETA: 4s - loss: 0.4849 - acc: 0.775 - ETA: 4s - loss: 0.4848 - acc: 0.776 - ETA: 3s - loss: 0.4845 - acc: 0.777 - ETA: 3s - loss: 0.4889 - acc: 0.776 - ETA: 3s - loss: 0.4902 - acc: 0.773 - ETA: 3s - loss: 0.4909 - acc: 0.773 - ETA: 3s - loss: 0.4910 - acc: 0.771 - ETA: 3s - loss: 0.4918 - acc: 0.771 - ETA: 3s - loss: 0.4912 - acc: 0.772 - ETA: 3s - loss: 0.4919 - acc: 0.772 - ETA: 3s - loss: 0.4923 - acc: 0.772 - ETA: 3s - loss: 0.4919 - acc: 0.772 - ETA: 3s - loss: 0.4928 - acc: 0.771 - ETA: 3s - loss: 0.4930 - acc: 0.771 - ETA: 2s - loss: 0.4905 - acc: 0.772 - ETA: 2s - loss: 0.4911 - acc: 0.771 - ETA: 2s - loss: 0.4907 - acc: 0.771 - ETA: 2s - loss: 0.4913 - acc: 0.771 - ETA: 2s - loss: 0.4913 - acc: 0.770 - ETA: 2s - loss: 0.4918 - acc: 0.770 - ETA: 2s - loss: 0.4928 - acc: 0.770 - ETA: 2s - loss: 0.4920 - acc: 0.771 - ETA: 2s - loss: 0.4923 - acc: 0.770 - ETA: 2s - loss: 0.4928 - acc: 0.770 - ETA: 2s - loss: 0.4916 - acc: 0.770 - ETA: 1s - loss: 0.4914 - acc: 0.770 - ETA: 1s - loss: 0.4915 - acc: 0.770 - ETA: 1s - loss: 0.4912 - acc: 0.771 - ETA: 1s - loss: 0.4913 - acc: 0.771 - ETA: 1s - loss: 0.4915 - acc: 0.770 - ETA: 1s - loss: 0.4923 - acc: 0.770 - ETA: 1s - loss: 0.4911 - acc: 0.770 - ETA: 1s - loss: 0.4909 - acc: 0.770 - ETA: 1s - loss: 0.4904 - acc: 0.771 - ETA: 1s - loss: 0.4906 - acc: 0.770 - ETA: 1s - loss: 0.4908 - acc: 0.770 - ETA: 1s - loss: 0.4905 - acc: 0.770 - ETA: 1s - loss: 0.4907 - acc: 0.770 - ETA: 1s - loss: 0.4911 - acc: 0.769 - ETA: 1s - loss: 0.4912 - acc: 0.769 - ETA: 0s - loss: 0.4913 - acc: 0.769 - ETA: 0s - loss: 0.4911 - acc: 0.769 - ETA: 0s - loss: 0.4906 - acc: 0.770 - ETA: 0s - loss: 0.4906 - acc: 0.770 - ETA: 0s - loss: 0.4908 - acc: 0.769 - ETA: 0s - loss: 0.4901 - acc: 0.770 - ETA: 0s - loss: 0.4895 - acc: 0.770 - ETA: 0s - loss: 0.4891 - acc: 0.770 - ETA: 0s - loss: 0.4888 - acc: 0.771 - ETA: 0s - loss: 0.4885 - acc: 0.771 - ETA: 0s - loss: 0.4886 - acc: 0.771 - ETA: 0s - loss: 0.4885 - acc: 0.771 - ETA: 0s - loss: 0.4882 - acc: 0.771 - ETA: 0s - loss: 0.4880 - acc: 0.771 - ETA: 0s - loss: 0.4875 - acc: 0.771 - ETA: 0s - loss: 0.4880 - acc: 0.771 - ETA: 0s - loss: 0.4881 - acc: 0.771 - 3s 62us/step - loss: 0.4882 - acc: 0.7712\n",
      "14024/14024 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 41us/step\n",
      "[0.49118000042826127, 0.767969195528569]\n"
     ]
    }
   ],
   "source": [
    "# KERAS\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=2454, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=256)\n",
    "score = model.evaluate(X_test, y_test, batch_size=256)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) add copies of instances from the under-represented class - over-sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2 dataframe with all clicked rows and select the same number of rows as in '1' from '0' data \n",
    "data_1 = data[data['Is_lp_click'] == 1]\n",
    "data_1_add = data[data['Is_lp_click'] == 1].sample(15000, random_state = 32)\n",
    "data_0 = data[data['Is_lp_click'] == 0].sample(50000, random_state = 12)\n",
    "# concat 2 dataframes and shuffle it\n",
    "data_concat = pd.concat([data_1, data_1_add, data_0])\n",
    "data_under = data_concat.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time on encoding: 104.521 s\n"
     ]
    }
   ],
   "source": [
    "# features & labels\n",
    "labels = data_under['Is_lp_click']\n",
    "features = data_under.drop('Is_lp_click', axis = 1)\n",
    "# create the dictionary of unique attributes' lists\n",
    "unique_attr = {}\n",
    "for attr in features.columns:\n",
    "    unique_attr[attr] = features[attr].unique().tolist()\n",
    "unique_attr\n",
    "# features encoding\n",
    "encoder = preprocessing.OneHotEncoder(categories=[unique_attr[i] for i in unique_attr], sparse = False, handle_unknown='ignore')\n",
    "encoder\n",
    "features['CONCAT'] = features.values.tolist()\n",
    "features['CONCAT'].head()\n",
    "t0 = time()\n",
    "features['ENCODED'] = [encoder.fit_transform([i]).flatten() for i in features['CONCAT']]\n",
    "print (\"time on encoding:\", round(time()-t0, 3), \"s\")\n",
    "# create the array with feature vectors\n",
    "features_list = [list(i) for i in features['ENCODED']]\n",
    "features_list_array = np.array(features_list)\n",
    "# create the array with label vector \n",
    "labels_list_array = np.array(labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features_list_array, labels_list_array, random_state=35, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.69      0.73     15014\n",
      "         1.0       0.72      0.81      0.76     15004\n",
      "\n",
      "   micro avg       0.75      0.75      0.75     30018\n",
      "   macro avg       0.75      0.75      0.75     30018\n",
      "weighted avg       0.75      0.75      0.75     30018\n",
      "\n",
      "[[10350  4664]\n",
      " [ 2894 12110]]\n",
      "Accuracy is  74.82177360250516\n",
      "Time on model's work: 20.968 s\n",
      "====================================================================================================\n",
      "GradientBoostingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.62      0.73     15014\n",
      "         1.0       0.71      0.92      0.80     15004\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     30018\n",
      "   macro avg       0.80      0.77      0.76     30018\n",
      "weighted avg       0.80      0.77      0.76     30018\n",
      "\n",
      "[[ 9306  5708]\n",
      " [ 1217 13787]]\n",
      "Accuracy is  76.93050836164967\n",
      "Time on model's work: 1198.169 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.71      0.73     15014\n",
      "         1.0       0.73      0.77      0.75     15004\n",
      "\n",
      "   micro avg       0.74      0.74      0.74     30018\n",
      "   macro avg       0.74      0.74      0.74     30018\n",
      "weighted avg       0.74      0.74      0.74     30018\n",
      "\n",
      "[[10698  4316]\n",
      " [ 3405 11599]]\n",
      "Accuracy is  74.27876607368911\n",
      "Time on model's work: 45.195 s\n",
      "====================================================================================================\n",
      "AdaBoostClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.63      0.73     15014\n",
      "         1.0       0.71      0.91      0.80     15004\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     30018\n",
      "   macro avg       0.79      0.77      0.76     30018\n",
      "weighted avg       0.79      0.77      0.76     30018\n",
      "\n",
      "[[ 9384  5630]\n",
      " [ 1338 13666]]\n",
      "Accuracy is  76.78726097674729\n",
      "Time on model's work: 462.168 s\n",
      "====================================================================================================\n",
      "BaggingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.69      0.73     15014\n",
      "         1.0       0.72      0.81      0.76     15004\n",
      "\n",
      "   micro avg       0.75      0.75      0.75     30018\n",
      "   macro avg       0.75      0.75      0.75     30018\n",
      "weighted avg       0.75      0.75      0.75     30018\n",
      "\n",
      "[[10330  4684]\n",
      " [ 2816 12188]]\n",
      "Accuracy is  75.01499100539675\n",
      "Time on model's work: 237.391 s\n",
      "====================================================================================================\n",
      "DecisionTreeClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.71      0.73     15014\n",
      "         1.0       0.73      0.76      0.75     15004\n",
      "\n",
      "   micro avg       0.74      0.74      0.74     30018\n",
      "   macro avg       0.74      0.74      0.74     30018\n",
      "weighted avg       0.74      0.74      0.74     30018\n",
      "\n",
      "[[10732  4282]\n",
      " [ 3551 11453]]\n",
      "Accuracy is  73.90565660603637\n",
      "Time on model's work: 68.819 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.71      0.73     15014\n",
      "         1.0       0.73      0.77      0.75     15004\n",
      "\n",
      "   micro avg       0.74      0.74      0.74     30018\n",
      "   macro avg       0.74      0.74      0.74     30018\n",
      "weighted avg       0.74      0.74      0.74     30018\n",
      "\n",
      "[[10708  4306]\n",
      " [ 3453 11551]]\n",
      "Accuracy is  74.1521753614498\n",
      "Time on model's work: 2549.128 s\n",
      "====================================================================================================\n",
      "XGBClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.62      0.73     15014\n",
      "         1.0       0.71      0.92      0.80     15004\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     30018\n",
      "   macro avg       0.80      0.77      0.76     30018\n",
      "weighted avg       0.80      0.77      0.76     30018\n",
      "\n",
      "[[ 9299  5715]\n",
      " [ 1212 13792]]\n",
      "Accuracy is  76.92384569258445\n",
      "Time on model's work: 561.64 s\n",
      "====================================================================================================\n",
      "0:\tlearn: 0.6642920\ttotal: 567ms\tremaining: 9m 26s\n",
      "1:\tlearn: 0.6401349\ttotal: 1.05s\tremaining: 8m 45s\n",
      "2:\tlearn: 0.6194234\ttotal: 1.56s\tremaining: 8m 40s\n",
      "3:\tlearn: 0.6022337\ttotal: 2.05s\tremaining: 8m 30s\n",
      "4:\tlearn: 0.5880572\ttotal: 2.45s\tremaining: 8m 6s\n",
      "5:\tlearn: 0.5756420\ttotal: 2.9s\tremaining: 8m 1s\n",
      "6:\tlearn: 0.5646267\ttotal: 3.38s\tremaining: 7m 58s\n",
      "7:\tlearn: 0.5557317\ttotal: 3.78s\tremaining: 7m 48s\n",
      "8:\tlearn: 0.5477799\ttotal: 4.25s\tremaining: 7m 48s\n",
      "9:\tlearn: 0.5411264\ttotal: 4.64s\tremaining: 7m 39s\n",
      "10:\tlearn: 0.5347577\ttotal: 5.09s\tremaining: 7m 37s\n",
      "11:\tlearn: 0.5295889\ttotal: 5.51s\tremaining: 7m 33s\n",
      "12:\tlearn: 0.5251996\ttotal: 5.9s\tremaining: 7m 28s\n",
      "13:\tlearn: 0.5211816\ttotal: 6.34s\tremaining: 7m 26s\n",
      "14:\tlearn: 0.5173752\ttotal: 6.84s\tremaining: 7m 29s\n",
      "15:\tlearn: 0.5141442\ttotal: 7.26s\tremaining: 7m 26s\n",
      "16:\tlearn: 0.5114271\ttotal: 7.67s\tremaining: 7m 23s\n",
      "17:\tlearn: 0.5088964\ttotal: 8.03s\tremaining: 7m 18s\n",
      "18:\tlearn: 0.5067171\ttotal: 8.49s\tremaining: 7m 18s\n",
      "19:\tlearn: 0.5047828\ttotal: 8.91s\tremaining: 7m 16s\n",
      "20:\tlearn: 0.5031874\ttotal: 9.33s\tremaining: 7m 14s\n",
      "21:\tlearn: 0.5016503\ttotal: 9.7s\tremaining: 7m 11s\n",
      "22:\tlearn: 0.5003667\ttotal: 10.1s\tremaining: 7m 10s\n",
      "23:\tlearn: 0.4990617\ttotal: 10.5s\tremaining: 7m 8s\n",
      "24:\tlearn: 0.4980150\ttotal: 11s\tremaining: 7m 7s\n",
      "25:\tlearn: 0.4970752\ttotal: 11.4s\tremaining: 7m 6s\n",
      "26:\tlearn: 0.4962492\ttotal: 11.8s\tremaining: 7m 4s\n",
      "27:\tlearn: 0.4954190\ttotal: 12.2s\tremaining: 7m 3s\n",
      "28:\tlearn: 0.4945706\ttotal: 12.6s\tremaining: 7m 1s\n",
      "29:\tlearn: 0.4939736\ttotal: 13s\tremaining: 7m\n",
      "30:\tlearn: 0.4932962\ttotal: 13.5s\tremaining: 7m\n",
      "31:\tlearn: 0.4926288\ttotal: 14s\tremaining: 7m 4s\n",
      "32:\tlearn: 0.4921397\ttotal: 14.5s\tremaining: 7m 6s\n",
      "33:\tlearn: 0.4917311\ttotal: 15s\tremaining: 7m 5s\n",
      "34:\tlearn: 0.4913902\ttotal: 15.3s\tremaining: 7m 3s\n",
      "35:\tlearn: 0.4909814\ttotal: 15.8s\tremaining: 7m 3s\n",
      "36:\tlearn: 0.4906093\ttotal: 16.3s\tremaining: 7m 3s\n",
      "37:\tlearn: 0.4903512\ttotal: 16.7s\tremaining: 7m 3s\n",
      "38:\tlearn: 0.4900281\ttotal: 17.1s\tremaining: 7m 1s\n",
      "39:\tlearn: 0.4898309\ttotal: 17.5s\tremaining: 6m 59s\n",
      "40:\tlearn: 0.4895979\ttotal: 17.9s\tremaining: 6m 58s\n",
      "41:\tlearn: 0.4892781\ttotal: 18.3s\tremaining: 6m 56s\n",
      "42:\tlearn: 0.4889729\ttotal: 18.7s\tremaining: 6m 57s\n",
      "43:\tlearn: 0.4887848\ttotal: 19.1s\tremaining: 6m 55s\n",
      "44:\tlearn: 0.4886431\ttotal: 19.5s\tremaining: 6m 53s\n",
      "45:\tlearn: 0.4885079\ttotal: 19.8s\tremaining: 6m 50s\n",
      "46:\tlearn: 0.4883210\ttotal: 20.2s\tremaining: 6m 49s\n",
      "47:\tlearn: 0.4882080\ttotal: 20.6s\tremaining: 6m 48s\n",
      "48:\tlearn: 0.4881036\ttotal: 20.9s\tremaining: 6m 46s\n",
      "49:\tlearn: 0.4879544\ttotal: 21.4s\tremaining: 6m 46s\n",
      "50:\tlearn: 0.4878239\ttotal: 21.8s\tremaining: 6m 45s\n",
      "51:\tlearn: 0.4877460\ttotal: 22.1s\tremaining: 6m 43s\n",
      "52:\tlearn: 0.4876439\ttotal: 22.5s\tremaining: 6m 41s\n",
      "53:\tlearn: 0.4875555\ttotal: 22.8s\tremaining: 6m 40s\n",
      "54:\tlearn: 0.4873887\ttotal: 23.2s\tremaining: 6m 38s\n",
      "55:\tlearn: 0.4873213\ttotal: 23.5s\tremaining: 6m 36s\n",
      "56:\tlearn: 0.4872254\ttotal: 23.9s\tremaining: 6m 35s\n",
      "57:\tlearn: 0.4871599\ttotal: 24.3s\tremaining: 6m 34s\n",
      "58:\tlearn: 0.4870782\ttotal: 24.6s\tremaining: 6m 32s\n",
      "59:\tlearn: 0.4869866\ttotal: 25s\tremaining: 6m 31s\n",
      "60:\tlearn: 0.4868769\ttotal: 25.4s\tremaining: 6m 30s\n",
      "61:\tlearn: 0.4868036\ttotal: 25.7s\tremaining: 6m 28s\n",
      "62:\tlearn: 0.4867258\ttotal: 26s\tremaining: 6m 27s\n",
      "63:\tlearn: 0.4866840\ttotal: 26.4s\tremaining: 6m 25s\n",
      "64:\tlearn: 0.4866402\ttotal: 26.7s\tremaining: 6m 23s\n",
      "65:\tlearn: 0.4865741\ttotal: 27s\tremaining: 6m 21s\n",
      "66:\tlearn: 0.4864998\ttotal: 27.4s\tremaining: 6m 20s\n",
      "67:\tlearn: 0.4864110\ttotal: 27.8s\tremaining: 6m 21s\n",
      "68:\tlearn: 0.4863653\ttotal: 28.1s\tremaining: 6m 19s\n",
      "69:\tlearn: 0.4863415\ttotal: 28.3s\tremaining: 6m 16s\n",
      "70:\tlearn: 0.4862834\ttotal: 28.6s\tremaining: 6m 14s\n",
      "71:\tlearn: 0.4862343\ttotal: 28.9s\tremaining: 6m 13s\n",
      "72:\tlearn: 0.4861549\ttotal: 29.3s\tremaining: 6m 11s\n",
      "73:\tlearn: 0.4861288\ttotal: 29.6s\tremaining: 6m 10s\n",
      "74:\tlearn: 0.4860687\ttotal: 30s\tremaining: 6m 10s\n",
      "75:\tlearn: 0.4860343\ttotal: 30.3s\tremaining: 6m 8s\n",
      "76:\tlearn: 0.4859828\ttotal: 30.6s\tremaining: 6m 7s\n",
      "77:\tlearn: 0.4859256\ttotal: 31.1s\tremaining: 6m 7s\n",
      "78:\tlearn: 0.4858720\ttotal: 31.4s\tremaining: 6m 6s\n",
      "79:\tlearn: 0.4858237\ttotal: 31.8s\tremaining: 6m 5s\n",
      "80:\tlearn: 0.4857456\ttotal: 32.2s\tremaining: 6m 5s\n",
      "81:\tlearn: 0.4856862\ttotal: 32.6s\tremaining: 6m 5s\n",
      "82:\tlearn: 0.4856120\ttotal: 33s\tremaining: 6m 5s\n",
      "83:\tlearn: 0.4855412\ttotal: 33.5s\tremaining: 6m 4s\n",
      "84:\tlearn: 0.4855063\ttotal: 33.7s\tremaining: 6m 3s\n",
      "85:\tlearn: 0.4854340\ttotal: 34.1s\tremaining: 6m 2s\n",
      "86:\tlearn: 0.4853936\ttotal: 34.4s\tremaining: 6m 1s\n",
      "87:\tlearn: 0.4853497\ttotal: 34.7s\tremaining: 5m 59s\n",
      "88:\tlearn: 0.4853193\ttotal: 34.9s\tremaining: 5m 57s\n",
      "89:\tlearn: 0.4852958\ttotal: 35.2s\tremaining: 5m 55s\n",
      "90:\tlearn: 0.4852689\ttotal: 35.4s\tremaining: 5m 53s\n",
      "91:\tlearn: 0.4852353\ttotal: 35.7s\tremaining: 5m 52s\n",
      "92:\tlearn: 0.4851898\ttotal: 36.1s\tremaining: 5m 51s\n",
      "93:\tlearn: 0.4851515\ttotal: 36.3s\tremaining: 5m 50s\n",
      "94:\tlearn: 0.4851219\ttotal: 36.6s\tremaining: 5m 48s\n",
      "95:\tlearn: 0.4850833\ttotal: 36.9s\tremaining: 5m 47s\n",
      "96:\tlearn: 0.4850453\ttotal: 37.2s\tremaining: 5m 45s\n",
      "97:\tlearn: 0.4850117\ttotal: 37.4s\tremaining: 5m 44s\n",
      "98:\tlearn: 0.4849774\ttotal: 37.7s\tremaining: 5m 42s\n",
      "99:\tlearn: 0.4849619\ttotal: 37.9s\tremaining: 5m 41s\n",
      "100:\tlearn: 0.4849211\ttotal: 38.2s\tremaining: 5m 40s\n",
      "101:\tlearn: 0.4848786\ttotal: 38.5s\tremaining: 5m 38s\n",
      "102:\tlearn: 0.4848312\ttotal: 38.8s\tremaining: 5m 38s\n",
      "103:\tlearn: 0.4847615\ttotal: 39.2s\tremaining: 5m 37s\n",
      "104:\tlearn: 0.4847327\ttotal: 39.4s\tremaining: 5m 35s\n",
      "105:\tlearn: 0.4846961\ttotal: 39.7s\tremaining: 5m 34s\n",
      "106:\tlearn: 0.4846489\ttotal: 40s\tremaining: 5m 33s\n",
      "107:\tlearn: 0.4846139\ttotal: 40.2s\tremaining: 5m 32s\n",
      "108:\tlearn: 0.4845965\ttotal: 40.5s\tremaining: 5m 30s\n",
      "109:\tlearn: 0.4845646\ttotal: 40.7s\tremaining: 5m 29s\n",
      "110:\tlearn: 0.4845368\ttotal: 41s\tremaining: 5m 28s\n",
      "111:\tlearn: 0.4844863\ttotal: 41.3s\tremaining: 5m 27s\n",
      "112:\tlearn: 0.4844606\ttotal: 41.5s\tremaining: 5m 26s\n",
      "113:\tlearn: 0.4844427\ttotal: 41.8s\tremaining: 5m 24s\n",
      "114:\tlearn: 0.4844253\ttotal: 42s\tremaining: 5m 23s\n",
      "115:\tlearn: 0.4843633\ttotal: 42.3s\tremaining: 5m 22s\n",
      "116:\tlearn: 0.4843347\ttotal: 42.7s\tremaining: 5m 21s\n",
      "117:\tlearn: 0.4842739\ttotal: 43s\tremaining: 5m 21s\n",
      "118:\tlearn: 0.4842620\ttotal: 43.3s\tremaining: 5m 20s\n",
      "119:\tlearn: 0.4842123\ttotal: 43.6s\tremaining: 5m 19s\n",
      "120:\tlearn: 0.4841931\ttotal: 43.8s\tremaining: 5m 18s\n",
      "121:\tlearn: 0.4841657\ttotal: 44.1s\tremaining: 5m 17s\n",
      "122:\tlearn: 0.4841495\ttotal: 44.3s\tremaining: 5m 15s\n",
      "123:\tlearn: 0.4841303\ttotal: 44.6s\tremaining: 5m 14s\n",
      "124:\tlearn: 0.4841175\ttotal: 44.8s\tremaining: 5m 13s\n",
      "125:\tlearn: 0.4840863\ttotal: 45.2s\tremaining: 5m 13s\n",
      "126:\tlearn: 0.4840487\ttotal: 45.6s\tremaining: 5m 13s\n",
      "127:\tlearn: 0.4840083\ttotal: 45.8s\tremaining: 5m 12s\n",
      "128:\tlearn: 0.4839850\ttotal: 46.1s\tremaining: 5m 11s\n",
      "129:\tlearn: 0.4839687\ttotal: 46.4s\tremaining: 5m 10s\n",
      "130:\tlearn: 0.4839545\ttotal: 46.7s\tremaining: 5m 9s\n",
      "131:\tlearn: 0.4839213\ttotal: 47.1s\tremaining: 5m 9s\n",
      "132:\tlearn: 0.4838978\ttotal: 47.4s\tremaining: 5m 8s\n",
      "133:\tlearn: 0.4838782\ttotal: 47.6s\tremaining: 5m 7s\n",
      "134:\tlearn: 0.4838298\ttotal: 48s\tremaining: 5m 7s\n",
      "135:\tlearn: 0.4838026\ttotal: 48.3s\tremaining: 5m 6s\n",
      "136:\tlearn: 0.4837753\ttotal: 48.7s\tremaining: 5m 6s\n",
      "137:\tlearn: 0.4837479\ttotal: 49s\tremaining: 5m 5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138:\tlearn: 0.4837375\ttotal: 49.2s\tremaining: 5m 4s\n",
      "139:\tlearn: 0.4837155\ttotal: 49.4s\tremaining: 5m 3s\n",
      "140:\tlearn: 0.4836834\ttotal: 49.7s\tremaining: 5m 2s\n",
      "141:\tlearn: 0.4836093\ttotal: 50.1s\tremaining: 5m 2s\n",
      "142:\tlearn: 0.4835482\ttotal: 50.5s\tremaining: 5m 2s\n",
      "143:\tlearn: 0.4835140\ttotal: 50.8s\tremaining: 5m 1s\n",
      "144:\tlearn: 0.4834770\ttotal: 51.1s\tremaining: 5m 1s\n",
      "145:\tlearn: 0.4834445\ttotal: 51.4s\tremaining: 5m\n",
      "146:\tlearn: 0.4834176\ttotal: 51.7s\tremaining: 4m 59s\n",
      "147:\tlearn: 0.4833954\ttotal: 52s\tremaining: 4m 59s\n",
      "148:\tlearn: 0.4833802\ttotal: 52.2s\tremaining: 4m 58s\n",
      "149:\tlearn: 0.4833616\ttotal: 52.4s\tremaining: 4m 57s\n",
      "150:\tlearn: 0.4833560\ttotal: 52.7s\tremaining: 4m 56s\n",
      "151:\tlearn: 0.4833229\ttotal: 52.9s\tremaining: 4m 55s\n",
      "152:\tlearn: 0.4832516\ttotal: 53.3s\tremaining: 4m 55s\n",
      "153:\tlearn: 0.4832073\ttotal: 53.7s\tremaining: 4m 54s\n",
      "154:\tlearn: 0.4831749\ttotal: 54s\tremaining: 4m 54s\n",
      "155:\tlearn: 0.4831533\ttotal: 54.2s\tremaining: 4m 53s\n",
      "156:\tlearn: 0.4831363\ttotal: 54.5s\tremaining: 4m 52s\n",
      "157:\tlearn: 0.4831071\ttotal: 54.8s\tremaining: 4m 51s\n",
      "158:\tlearn: 0.4830894\ttotal: 55s\tremaining: 4m 51s\n",
      "159:\tlearn: 0.4830731\ttotal: 55.3s\tremaining: 4m 50s\n",
      "160:\tlearn: 0.4830608\ttotal: 55.5s\tremaining: 4m 49s\n",
      "161:\tlearn: 0.4830430\ttotal: 55.8s\tremaining: 4m 48s\n",
      "162:\tlearn: 0.4830136\ttotal: 56.1s\tremaining: 4m 47s\n",
      "163:\tlearn: 0.4830024\ttotal: 56.3s\tremaining: 4m 47s\n",
      "164:\tlearn: 0.4829732\ttotal: 56.6s\tremaining: 4m 46s\n",
      "165:\tlearn: 0.4829222\ttotal: 56.9s\tremaining: 4m 46s\n",
      "166:\tlearn: 0.4828929\ttotal: 57.2s\tremaining: 4m 45s\n",
      "167:\tlearn: 0.4828637\ttotal: 57.5s\tremaining: 4m 44s\n",
      "168:\tlearn: 0.4828405\ttotal: 57.8s\tremaining: 4m 43s\n",
      "169:\tlearn: 0.4828232\ttotal: 58s\tremaining: 4m 43s\n",
      "170:\tlearn: 0.4827886\ttotal: 58.3s\tremaining: 4m 42s\n",
      "171:\tlearn: 0.4827687\ttotal: 58.6s\tremaining: 4m 41s\n",
      "172:\tlearn: 0.4827509\ttotal: 58.9s\tremaining: 4m 41s\n",
      "173:\tlearn: 0.4827318\ttotal: 59.1s\tremaining: 4m 40s\n",
      "174:\tlearn: 0.4826910\ttotal: 59.4s\tremaining: 4m 40s\n",
      "175:\tlearn: 0.4826603\ttotal: 59.7s\tremaining: 4m 39s\n",
      "176:\tlearn: 0.4826447\ttotal: 59.9s\tremaining: 4m 38s\n",
      "177:\tlearn: 0.4826280\ttotal: 1m\tremaining: 4m 37s\n",
      "178:\tlearn: 0.4825891\ttotal: 1m\tremaining: 4m 37s\n",
      "179:\tlearn: 0.4825766\ttotal: 1m\tremaining: 4m 36s\n",
      "180:\tlearn: 0.4825532\ttotal: 1m 1s\tremaining: 4m 36s\n",
      "181:\tlearn: 0.4825321\ttotal: 1m 1s\tremaining: 4m 35s\n",
      "182:\tlearn: 0.4825038\ttotal: 1m 1s\tremaining: 4m 34s\n",
      "183:\tlearn: 0.4824807\ttotal: 1m 1s\tremaining: 4m 34s\n",
      "184:\tlearn: 0.4824629\ttotal: 1m 2s\tremaining: 4m 33s\n",
      "185:\tlearn: 0.4824383\ttotal: 1m 2s\tremaining: 4m 32s\n",
      "186:\tlearn: 0.4824094\ttotal: 1m 2s\tremaining: 4m 32s\n",
      "187:\tlearn: 0.4823913\ttotal: 1m 2s\tremaining: 4m 31s\n",
      "188:\tlearn: 0.4823608\ttotal: 1m 3s\tremaining: 4m 31s\n",
      "189:\tlearn: 0.4823396\ttotal: 1m 3s\tremaining: 4m 30s\n",
      "190:\tlearn: 0.4823201\ttotal: 1m 3s\tremaining: 4m 30s\n",
      "191:\tlearn: 0.4823010\ttotal: 1m 4s\tremaining: 4m 29s\n",
      "192:\tlearn: 0.4822809\ttotal: 1m 4s\tremaining: 4m 28s\n",
      "193:\tlearn: 0.4822666\ttotal: 1m 4s\tremaining: 4m 28s\n",
      "194:\tlearn: 0.4822503\ttotal: 1m 4s\tremaining: 4m 27s\n",
      "195:\tlearn: 0.4822380\ttotal: 1m 5s\tremaining: 4m 26s\n",
      "196:\tlearn: 0.4822205\ttotal: 1m 5s\tremaining: 4m 26s\n",
      "197:\tlearn: 0.4821920\ttotal: 1m 5s\tremaining: 4m 25s\n",
      "198:\tlearn: 0.4821772\ttotal: 1m 5s\tremaining: 4m 24s\n",
      "199:\tlearn: 0.4821546\ttotal: 1m 6s\tremaining: 4m 24s\n",
      "200:\tlearn: 0.4821387\ttotal: 1m 6s\tremaining: 4m 23s\n",
      "201:\tlearn: 0.4821242\ttotal: 1m 6s\tremaining: 4m 23s\n",
      "202:\tlearn: 0.4821071\ttotal: 1m 6s\tremaining: 4m 22s\n",
      "203:\tlearn: 0.4820947\ttotal: 1m 7s\tremaining: 4m 21s\n",
      "204:\tlearn: 0.4820789\ttotal: 1m 7s\tremaining: 4m 21s\n",
      "205:\tlearn: 0.4820554\ttotal: 1m 7s\tremaining: 4m 20s\n",
      "206:\tlearn: 0.4820323\ttotal: 1m 7s\tremaining: 4m 19s\n",
      "207:\tlearn: 0.4820107\ttotal: 1m 8s\tremaining: 4m 19s\n",
      "208:\tlearn: 0.4819884\ttotal: 1m 8s\tremaining: 4m 18s\n",
      "209:\tlearn: 0.4819734\ttotal: 1m 8s\tremaining: 4m 17s\n",
      "210:\tlearn: 0.4819605\ttotal: 1m 8s\tremaining: 4m 17s\n",
      "211:\tlearn: 0.4819278\ttotal: 1m 9s\tremaining: 4m 16s\n",
      "212:\tlearn: 0.4819058\ttotal: 1m 9s\tremaining: 4m 16s\n",
      "213:\tlearn: 0.4818840\ttotal: 1m 9s\tremaining: 4m 15s\n",
      "214:\tlearn: 0.4818615\ttotal: 1m 9s\tremaining: 4m 15s\n",
      "215:\tlearn: 0.4818450\ttotal: 1m 10s\tremaining: 4m 14s\n",
      "216:\tlearn: 0.4818343\ttotal: 1m 10s\tremaining: 4m 14s\n",
      "217:\tlearn: 0.4818121\ttotal: 1m 10s\tremaining: 4m 13s\n",
      "218:\tlearn: 0.4817992\ttotal: 1m 10s\tremaining: 4m 13s\n",
      "219:\tlearn: 0.4817809\ttotal: 1m 11s\tremaining: 4m 12s\n",
      "220:\tlearn: 0.4817506\ttotal: 1m 11s\tremaining: 4m 12s\n",
      "221:\tlearn: 0.4817289\ttotal: 1m 11s\tremaining: 4m 11s\n",
      "222:\tlearn: 0.4816973\ttotal: 1m 12s\tremaining: 4m 10s\n",
      "223:\tlearn: 0.4816801\ttotal: 1m 12s\tremaining: 4m 10s\n",
      "224:\tlearn: 0.4816698\ttotal: 1m 12s\tremaining: 4m 9s\n",
      "225:\tlearn: 0.4816540\ttotal: 1m 12s\tremaining: 4m 9s\n",
      "226:\tlearn: 0.4816402\ttotal: 1m 13s\tremaining: 4m 8s\n",
      "227:\tlearn: 0.4816156\ttotal: 1m 13s\tremaining: 4m 8s\n",
      "228:\tlearn: 0.4816040\ttotal: 1m 13s\tremaining: 4m 8s\n",
      "229:\tlearn: 0.4815910\ttotal: 1m 13s\tremaining: 4m 7s\n",
      "230:\tlearn: 0.4815724\ttotal: 1m 14s\tremaining: 4m 7s\n",
      "231:\tlearn: 0.4815582\ttotal: 1m 14s\tremaining: 4m 6s\n",
      "232:\tlearn: 0.4815373\ttotal: 1m 14s\tremaining: 4m 5s\n",
      "233:\tlearn: 0.4815205\ttotal: 1m 14s\tremaining: 4m 5s\n",
      "234:\tlearn: 0.4815031\ttotal: 1m 15s\tremaining: 4m 4s\n",
      "235:\tlearn: 0.4814826\ttotal: 1m 15s\tremaining: 4m 4s\n",
      "236:\tlearn: 0.4814545\ttotal: 1m 15s\tremaining: 4m 3s\n",
      "237:\tlearn: 0.4814378\ttotal: 1m 16s\tremaining: 4m 3s\n",
      "238:\tlearn: 0.4814266\ttotal: 1m 16s\tremaining: 4m 2s\n",
      "239:\tlearn: 0.4814166\ttotal: 1m 16s\tremaining: 4m 2s\n",
      "240:\tlearn: 0.4813974\ttotal: 1m 16s\tremaining: 4m 1s\n",
      "241:\tlearn: 0.4813824\ttotal: 1m 17s\tremaining: 4m 1s\n",
      "242:\tlearn: 0.4813714\ttotal: 1m 17s\tremaining: 4m\n",
      "243:\tlearn: 0.4813512\ttotal: 1m 17s\tremaining: 4m\n",
      "244:\tlearn: 0.4813278\ttotal: 1m 17s\tremaining: 3m 59s\n",
      "245:\tlearn: 0.4813131\ttotal: 1m 17s\tremaining: 3m 59s\n",
      "246:\tlearn: 0.4813000\ttotal: 1m 18s\tremaining: 3m 58s\n",
      "247:\tlearn: 0.4812909\ttotal: 1m 18s\tremaining: 3m 57s\n",
      "248:\tlearn: 0.4812820\ttotal: 1m 18s\tremaining: 3m 57s\n",
      "249:\tlearn: 0.4812614\ttotal: 1m 18s\tremaining: 3m 56s\n",
      "250:\tlearn: 0.4812485\ttotal: 1m 19s\tremaining: 3m 56s\n",
      "251:\tlearn: 0.4812004\ttotal: 1m 19s\tremaining: 3m 56s\n",
      "252:\tlearn: 0.4811841\ttotal: 1m 19s\tremaining: 3m 55s\n",
      "253:\tlearn: 0.4811647\ttotal: 1m 20s\tremaining: 3m 55s\n",
      "254:\tlearn: 0.4811413\ttotal: 1m 20s\tremaining: 3m 54s\n",
      "255:\tlearn: 0.4811101\ttotal: 1m 20s\tremaining: 3m 54s\n",
      "256:\tlearn: 0.4810897\ttotal: 1m 20s\tremaining: 3m 53s\n",
      "257:\tlearn: 0.4810789\ttotal: 1m 21s\tremaining: 3m 53s\n",
      "258:\tlearn: 0.4810695\ttotal: 1m 21s\tremaining: 3m 52s\n",
      "259:\tlearn: 0.4810570\ttotal: 1m 21s\tremaining: 3m 52s\n",
      "260:\tlearn: 0.4810432\ttotal: 1m 21s\tremaining: 3m 51s\n",
      "261:\tlearn: 0.4810144\ttotal: 1m 22s\tremaining: 3m 51s\n",
      "262:\tlearn: 0.4809997\ttotal: 1m 22s\tremaining: 3m 50s\n",
      "263:\tlearn: 0.4809750\ttotal: 1m 22s\tremaining: 3m 50s\n",
      "264:\tlearn: 0.4809480\ttotal: 1m 22s\tremaining: 3m 50s\n",
      "265:\tlearn: 0.4809026\ttotal: 1m 23s\tremaining: 3m 49s\n",
      "266:\tlearn: 0.4808952\ttotal: 1m 23s\tremaining: 3m 49s\n",
      "267:\tlearn: 0.4808840\ttotal: 1m 23s\tremaining: 3m 48s\n",
      "268:\tlearn: 0.4808720\ttotal: 1m 24s\tremaining: 3m 48s\n",
      "269:\tlearn: 0.4808423\ttotal: 1m 24s\tremaining: 3m 47s\n",
      "270:\tlearn: 0.4808276\ttotal: 1m 24s\tremaining: 3m 47s\n",
      "271:\tlearn: 0.4808049\ttotal: 1m 24s\tremaining: 3m 47s\n",
      "272:\tlearn: 0.4807874\ttotal: 1m 25s\tremaining: 3m 46s\n",
      "273:\tlearn: 0.4807702\ttotal: 1m 25s\tremaining: 3m 46s\n",
      "274:\tlearn: 0.4807608\ttotal: 1m 25s\tremaining: 3m 45s\n",
      "275:\tlearn: 0.4807468\ttotal: 1m 25s\tremaining: 3m 45s\n",
      "276:\tlearn: 0.4807273\ttotal: 1m 26s\tremaining: 3m 44s\n",
      "277:\tlearn: 0.4806996\ttotal: 1m 26s\tremaining: 3m 44s\n",
      "278:\tlearn: 0.4806891\ttotal: 1m 26s\tremaining: 3m 44s\n",
      "279:\tlearn: 0.4806601\ttotal: 1m 26s\tremaining: 3m 43s\n",
      "280:\tlearn: 0.4806307\ttotal: 1m 27s\tremaining: 3m 43s\n",
      "281:\tlearn: 0.4806122\ttotal: 1m 27s\tremaining: 3m 42s\n",
      "282:\tlearn: 0.4805625\ttotal: 1m 27s\tremaining: 3m 42s\n",
      "283:\tlearn: 0.4805187\ttotal: 1m 28s\tremaining: 3m 42s\n",
      "284:\tlearn: 0.4805067\ttotal: 1m 28s\tremaining: 3m 42s\n",
      "285:\tlearn: 0.4804768\ttotal: 1m 28s\tremaining: 3m 41s\n",
      "286:\tlearn: 0.4804663\ttotal: 1m 29s\tremaining: 3m 41s\n",
      "287:\tlearn: 0.4804447\ttotal: 1m 29s\tremaining: 3m 40s\n",
      "288:\tlearn: 0.4804200\ttotal: 1m 29s\tremaining: 3m 40s\n",
      "289:\tlearn: 0.4804045\ttotal: 1m 29s\tremaining: 3m 40s\n",
      "290:\tlearn: 0.4803842\ttotal: 1m 30s\tremaining: 3m 39s\n",
      "291:\tlearn: 0.4803589\ttotal: 1m 30s\tremaining: 3m 39s\n",
      "292:\tlearn: 0.4803467\ttotal: 1m 30s\tremaining: 3m 39s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293:\tlearn: 0.4803354\ttotal: 1m 31s\tremaining: 3m 38s\n",
      "294:\tlearn: 0.4803097\ttotal: 1m 31s\tremaining: 3m 38s\n",
      "295:\tlearn: 0.4803002\ttotal: 1m 31s\tremaining: 3m 38s\n",
      "296:\tlearn: 0.4802835\ttotal: 1m 32s\tremaining: 3m 38s\n",
      "297:\tlearn: 0.4802666\ttotal: 1m 32s\tremaining: 3m 37s\n",
      "298:\tlearn: 0.4802399\ttotal: 1m 32s\tremaining: 3m 37s\n",
      "299:\tlearn: 0.4802152\ttotal: 1m 32s\tremaining: 3m 36s\n",
      "300:\tlearn: 0.4802043\ttotal: 1m 33s\tremaining: 3m 36s\n",
      "301:\tlearn: 0.4801861\ttotal: 1m 33s\tremaining: 3m 36s\n",
      "302:\tlearn: 0.4801756\ttotal: 1m 33s\tremaining: 3m 35s\n",
      "303:\tlearn: 0.4801657\ttotal: 1m 34s\tremaining: 3m 35s\n",
      "304:\tlearn: 0.4801578\ttotal: 1m 34s\tremaining: 3m 34s\n",
      "305:\tlearn: 0.4801197\ttotal: 1m 34s\tremaining: 3m 34s\n",
      "306:\tlearn: 0.4800960\ttotal: 1m 35s\tremaining: 3m 34s\n",
      "307:\tlearn: 0.4800737\ttotal: 1m 35s\tremaining: 3m 34s\n",
      "308:\tlearn: 0.4800673\ttotal: 1m 35s\tremaining: 3m 33s\n",
      "309:\tlearn: 0.4800492\ttotal: 1m 35s\tremaining: 3m 33s\n",
      "310:\tlearn: 0.4800169\ttotal: 1m 36s\tremaining: 3m 33s\n",
      "311:\tlearn: 0.4800055\ttotal: 1m 36s\tremaining: 3m 32s\n",
      "312:\tlearn: 0.4799792\ttotal: 1m 36s\tremaining: 3m 32s\n",
      "313:\tlearn: 0.4799498\ttotal: 1m 37s\tremaining: 3m 32s\n",
      "314:\tlearn: 0.4799362\ttotal: 1m 37s\tremaining: 3m 31s\n",
      "315:\tlearn: 0.4799081\ttotal: 1m 37s\tremaining: 3m 31s\n",
      "316:\tlearn: 0.4798978\ttotal: 1m 38s\tremaining: 3m 31s\n",
      "317:\tlearn: 0.4798863\ttotal: 1m 38s\tremaining: 3m 31s\n",
      "318:\tlearn: 0.4798587\ttotal: 1m 38s\tremaining: 3m 31s\n",
      "319:\tlearn: 0.4798187\ttotal: 1m 39s\tremaining: 3m 31s\n",
      "320:\tlearn: 0.4798035\ttotal: 1m 39s\tremaining: 3m 30s\n",
      "321:\tlearn: 0.4797810\ttotal: 1m 39s\tremaining: 3m 30s\n",
      "322:\tlearn: 0.4797646\ttotal: 1m 40s\tremaining: 3m 30s\n",
      "323:\tlearn: 0.4797477\ttotal: 1m 40s\tremaining: 3m 29s\n",
      "324:\tlearn: 0.4797400\ttotal: 1m 40s\tremaining: 3m 29s\n",
      "325:\tlearn: 0.4797195\ttotal: 1m 40s\tremaining: 3m 28s\n",
      "326:\tlearn: 0.4796961\ttotal: 1m 41s\tremaining: 3m 28s\n",
      "327:\tlearn: 0.4796842\ttotal: 1m 41s\tremaining: 3m 28s\n",
      "328:\tlearn: 0.4796370\ttotal: 1m 41s\tremaining: 3m 27s\n",
      "329:\tlearn: 0.4796250\ttotal: 1m 42s\tremaining: 3m 27s\n",
      "330:\tlearn: 0.4796046\ttotal: 1m 42s\tremaining: 3m 26s\n",
      "331:\tlearn: 0.4795867\ttotal: 1m 42s\tremaining: 3m 26s\n",
      "332:\tlearn: 0.4795436\ttotal: 1m 42s\tremaining: 3m 26s\n",
      "333:\tlearn: 0.4795276\ttotal: 1m 43s\tremaining: 3m 25s\n",
      "334:\tlearn: 0.4795160\ttotal: 1m 43s\tremaining: 3m 25s\n",
      "335:\tlearn: 0.4795078\ttotal: 1m 43s\tremaining: 3m 24s\n",
      "336:\tlearn: 0.4794743\ttotal: 1m 43s\tremaining: 3m 24s\n",
      "337:\tlearn: 0.4794571\ttotal: 1m 44s\tremaining: 3m 24s\n",
      "338:\tlearn: 0.4794466\ttotal: 1m 44s\tremaining: 3m 23s\n",
      "339:\tlearn: 0.4794347\ttotal: 1m 44s\tremaining: 3m 23s\n",
      "340:\tlearn: 0.4794241\ttotal: 1m 45s\tremaining: 3m 23s\n",
      "341:\tlearn: 0.4794154\ttotal: 1m 45s\tremaining: 3m 22s\n",
      "342:\tlearn: 0.4794019\ttotal: 1m 45s\tremaining: 3m 22s\n",
      "343:\tlearn: 0.4793785\ttotal: 1m 45s\tremaining: 3m 21s\n",
      "344:\tlearn: 0.4793656\ttotal: 1m 46s\tremaining: 3m 21s\n",
      "345:\tlearn: 0.4793440\ttotal: 1m 46s\tremaining: 3m 21s\n",
      "346:\tlearn: 0.4793359\ttotal: 1m 46s\tremaining: 3m 20s\n",
      "347:\tlearn: 0.4793198\ttotal: 1m 46s\tremaining: 3m 20s\n",
      "348:\tlearn: 0.4793108\ttotal: 1m 47s\tremaining: 3m 19s\n",
      "349:\tlearn: 0.4792990\ttotal: 1m 47s\tremaining: 3m 19s\n",
      "350:\tlearn: 0.4792834\ttotal: 1m 47s\tremaining: 3m 19s\n",
      "351:\tlearn: 0.4792730\ttotal: 1m 48s\tremaining: 3m 18s\n",
      "352:\tlearn: 0.4792594\ttotal: 1m 48s\tremaining: 3m 18s\n",
      "353:\tlearn: 0.4792462\ttotal: 1m 48s\tremaining: 3m 18s\n",
      "354:\tlearn: 0.4792350\ttotal: 1m 48s\tremaining: 3m 17s\n",
      "355:\tlearn: 0.4792244\ttotal: 1m 49s\tremaining: 3m 17s\n",
      "356:\tlearn: 0.4792062\ttotal: 1m 49s\tremaining: 3m 17s\n",
      "357:\tlearn: 0.4791946\ttotal: 1m 49s\tremaining: 3m 16s\n",
      "358:\tlearn: 0.4791799\ttotal: 1m 49s\tremaining: 3m 16s\n",
      "359:\tlearn: 0.4791703\ttotal: 1m 50s\tremaining: 3m 15s\n",
      "360:\tlearn: 0.4791599\ttotal: 1m 50s\tremaining: 3m 15s\n",
      "361:\tlearn: 0.4791532\ttotal: 1m 50s\tremaining: 3m 15s\n",
      "362:\tlearn: 0.4791443\ttotal: 1m 50s\tremaining: 3m 14s\n",
      "363:\tlearn: 0.4791097\ttotal: 1m 51s\tremaining: 3m 14s\n",
      "364:\tlearn: 0.4790953\ttotal: 1m 51s\tremaining: 3m 13s\n",
      "365:\tlearn: 0.4790509\ttotal: 1m 51s\tremaining: 3m 13s\n",
      "366:\tlearn: 0.4790122\ttotal: 1m 52s\tremaining: 3m 13s\n",
      "367:\tlearn: 0.4790015\ttotal: 1m 52s\tremaining: 3m 12s\n",
      "368:\tlearn: 0.4789871\ttotal: 1m 52s\tremaining: 3m 12s\n",
      "369:\tlearn: 0.4789755\ttotal: 1m 52s\tremaining: 3m 12s\n",
      "370:\tlearn: 0.4789593\ttotal: 1m 53s\tremaining: 3m 11s\n",
      "371:\tlearn: 0.4789357\ttotal: 1m 53s\tremaining: 3m 11s\n",
      "372:\tlearn: 0.4789042\ttotal: 1m 53s\tremaining: 3m 11s\n",
      "373:\tlearn: 0.4788806\ttotal: 1m 54s\tremaining: 3m 10s\n",
      "374:\tlearn: 0.4788451\ttotal: 1m 54s\tremaining: 3m 10s\n",
      "375:\tlearn: 0.4788255\ttotal: 1m 54s\tremaining: 3m 10s\n",
      "376:\tlearn: 0.4788067\ttotal: 1m 54s\tremaining: 3m 10s\n",
      "377:\tlearn: 0.4787943\ttotal: 1m 55s\tremaining: 3m 9s\n",
      "378:\tlearn: 0.4787813\ttotal: 1m 55s\tremaining: 3m 9s\n",
      "379:\tlearn: 0.4787706\ttotal: 1m 55s\tremaining: 3m 8s\n",
      "380:\tlearn: 0.4787470\ttotal: 1m 56s\tremaining: 3m 8s\n",
      "381:\tlearn: 0.4787400\ttotal: 1m 56s\tremaining: 3m 8s\n",
      "382:\tlearn: 0.4787191\ttotal: 1m 56s\tremaining: 3m 7s\n",
      "383:\tlearn: 0.4786955\ttotal: 1m 56s\tremaining: 3m 7s\n",
      "384:\tlearn: 0.4786889\ttotal: 1m 57s\tremaining: 3m 7s\n",
      "385:\tlearn: 0.4786761\ttotal: 1m 57s\tremaining: 3m 6s\n",
      "386:\tlearn: 0.4786662\ttotal: 1m 57s\tremaining: 3m 6s\n",
      "387:\tlearn: 0.4786507\ttotal: 1m 58s\tremaining: 3m 6s\n",
      "388:\tlearn: 0.4786407\ttotal: 1m 58s\tremaining: 3m 5s\n",
      "389:\tlearn: 0.4786338\ttotal: 1m 58s\tremaining: 3m 5s\n",
      "390:\tlearn: 0.4786272\ttotal: 1m 58s\tremaining: 3m 4s\n",
      "391:\tlearn: 0.4786139\ttotal: 1m 59s\tremaining: 3m 4s\n",
      "392:\tlearn: 0.4786058\ttotal: 1m 59s\tremaining: 3m 4s\n",
      "393:\tlearn: 0.4785842\ttotal: 1m 59s\tremaining: 3m 4s\n",
      "394:\tlearn: 0.4785548\ttotal: 2m\tremaining: 3m 4s\n",
      "395:\tlearn: 0.4785472\ttotal: 2m\tremaining: 3m 3s\n",
      "396:\tlearn: 0.4785383\ttotal: 2m\tremaining: 3m 3s\n",
      "397:\tlearn: 0.4785276\ttotal: 2m\tremaining: 3m 2s\n",
      "398:\tlearn: 0.4785187\ttotal: 2m 1s\tremaining: 3m 2s\n",
      "399:\tlearn: 0.4784918\ttotal: 2m 1s\tremaining: 3m 2s\n",
      "400:\tlearn: 0.4784825\ttotal: 2m 1s\tremaining: 3m 1s\n",
      "401:\tlearn: 0.4784746\ttotal: 2m 1s\tremaining: 3m 1s\n",
      "402:\tlearn: 0.4784679\ttotal: 2m 2s\tremaining: 3m 1s\n",
      "403:\tlearn: 0.4784285\ttotal: 2m 2s\tremaining: 3m\n",
      "404:\tlearn: 0.4784167\ttotal: 2m 2s\tremaining: 3m\n",
      "405:\tlearn: 0.4784040\ttotal: 2m 3s\tremaining: 3m\n",
      "406:\tlearn: 0.4783815\ttotal: 2m 3s\tremaining: 2m 59s\n",
      "407:\tlearn: 0.4783669\ttotal: 2m 3s\tremaining: 2m 59s\n",
      "408:\tlearn: 0.4783417\ttotal: 2m 3s\tremaining: 2m 59s\n",
      "409:\tlearn: 0.4783337\ttotal: 2m 4s\tremaining: 2m 58s\n",
      "410:\tlearn: 0.4783222\ttotal: 2m 4s\tremaining: 2m 58s\n",
      "411:\tlearn: 0.4783112\ttotal: 2m 4s\tremaining: 2m 57s\n",
      "412:\tlearn: 0.4782883\ttotal: 2m 5s\tremaining: 2m 57s\n",
      "413:\tlearn: 0.4782661\ttotal: 2m 5s\tremaining: 2m 57s\n",
      "414:\tlearn: 0.4782501\ttotal: 2m 5s\tremaining: 2m 57s\n",
      "415:\tlearn: 0.4782418\ttotal: 2m 5s\tremaining: 2m 56s\n",
      "416:\tlearn: 0.4782263\ttotal: 2m 6s\tremaining: 2m 56s\n",
      "417:\tlearn: 0.4782107\ttotal: 2m 6s\tremaining: 2m 55s\n",
      "418:\tlearn: 0.4781969\ttotal: 2m 6s\tremaining: 2m 55s\n",
      "419:\tlearn: 0.4781876\ttotal: 2m 6s\tremaining: 2m 55s\n",
      "420:\tlearn: 0.4781769\ttotal: 2m 7s\tremaining: 2m 54s\n",
      "421:\tlearn: 0.4781624\ttotal: 2m 7s\tremaining: 2m 54s\n",
      "422:\tlearn: 0.4781562\ttotal: 2m 7s\tremaining: 2m 53s\n",
      "423:\tlearn: 0.4781502\ttotal: 2m 7s\tremaining: 2m 53s\n",
      "424:\tlearn: 0.4781389\ttotal: 2m 8s\tremaining: 2m 53s\n",
      "425:\tlearn: 0.4781274\ttotal: 2m 8s\tremaining: 2m 52s\n",
      "426:\tlearn: 0.4781219\ttotal: 2m 8s\tremaining: 2m 52s\n",
      "427:\tlearn: 0.4781124\ttotal: 2m 8s\tremaining: 2m 52s\n",
      "428:\tlearn: 0.4780945\ttotal: 2m 9s\tremaining: 2m 51s\n",
      "429:\tlearn: 0.4780737\ttotal: 2m 9s\tremaining: 2m 51s\n",
      "430:\tlearn: 0.4780622\ttotal: 2m 9s\tremaining: 2m 51s\n",
      "431:\tlearn: 0.4780523\ttotal: 2m 9s\tremaining: 2m 50s\n",
      "432:\tlearn: 0.4780410\ttotal: 2m 10s\tremaining: 2m 50s\n",
      "433:\tlearn: 0.4780343\ttotal: 2m 10s\tremaining: 2m 49s\n",
      "434:\tlearn: 0.4779997\ttotal: 2m 10s\tremaining: 2m 49s\n",
      "435:\tlearn: 0.4779854\ttotal: 2m 10s\tremaining: 2m 49s\n",
      "436:\tlearn: 0.4779731\ttotal: 2m 11s\tremaining: 2m 49s\n",
      "437:\tlearn: 0.4779564\ttotal: 2m 11s\tremaining: 2m 48s\n",
      "438:\tlearn: 0.4779457\ttotal: 2m 11s\tremaining: 2m 48s\n",
      "439:\tlearn: 0.4779371\ttotal: 2m 11s\tremaining: 2m 47s\n",
      "440:\tlearn: 0.4779290\ttotal: 2m 12s\tremaining: 2m 47s\n",
      "441:\tlearn: 0.4779130\ttotal: 2m 12s\tremaining: 2m 47s\n",
      "442:\tlearn: 0.4779052\ttotal: 2m 12s\tremaining: 2m 47s\n",
      "443:\tlearn: 0.4778974\ttotal: 2m 13s\tremaining: 2m 46s\n",
      "444:\tlearn: 0.4778835\ttotal: 2m 13s\tremaining: 2m 46s\n",
      "445:\tlearn: 0.4778589\ttotal: 2m 13s\tremaining: 2m 46s\n",
      "446:\tlearn: 0.4778361\ttotal: 2m 14s\tremaining: 2m 45s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447:\tlearn: 0.4778118\ttotal: 2m 14s\tremaining: 2m 45s\n",
      "448:\tlearn: 0.4777935\ttotal: 2m 14s\tremaining: 2m 45s\n",
      "449:\tlearn: 0.4777876\ttotal: 2m 14s\tremaining: 2m 44s\n",
      "450:\tlearn: 0.4777435\ttotal: 2m 15s\tremaining: 2m 44s\n",
      "451:\tlearn: 0.4777258\ttotal: 2m 15s\tremaining: 2m 44s\n",
      "452:\tlearn: 0.4777171\ttotal: 2m 15s\tremaining: 2m 43s\n",
      "453:\tlearn: 0.4777005\ttotal: 2m 16s\tremaining: 2m 43s\n",
      "454:\tlearn: 0.4776853\ttotal: 2m 16s\tremaining: 2m 43s\n",
      "455:\tlearn: 0.4776756\ttotal: 2m 16s\tremaining: 2m 42s\n",
      "456:\tlearn: 0.4776674\ttotal: 2m 16s\tremaining: 2m 42s\n",
      "457:\tlearn: 0.4776558\ttotal: 2m 17s\tremaining: 2m 42s\n",
      "458:\tlearn: 0.4776486\ttotal: 2m 17s\tremaining: 2m 41s\n",
      "459:\tlearn: 0.4776277\ttotal: 2m 17s\tremaining: 2m 41s\n",
      "460:\tlearn: 0.4776186\ttotal: 2m 17s\tremaining: 2m 41s\n",
      "461:\tlearn: 0.4775528\ttotal: 2m 18s\tremaining: 2m 40s\n",
      "462:\tlearn: 0.4775355\ttotal: 2m 18s\tremaining: 2m 40s\n",
      "463:\tlearn: 0.4775096\ttotal: 2m 18s\tremaining: 2m 40s\n",
      "464:\tlearn: 0.4775010\ttotal: 2m 19s\tremaining: 2m 39s\n",
      "465:\tlearn: 0.4774861\ttotal: 2m 19s\tremaining: 2m 39s\n",
      "466:\tlearn: 0.4774733\ttotal: 2m 19s\tremaining: 2m 39s\n",
      "467:\tlearn: 0.4774678\ttotal: 2m 19s\tremaining: 2m 38s\n",
      "468:\tlearn: 0.4774568\ttotal: 2m 20s\tremaining: 2m 38s\n",
      "469:\tlearn: 0.4774486\ttotal: 2m 20s\tremaining: 2m 38s\n",
      "470:\tlearn: 0.4774426\ttotal: 2m 20s\tremaining: 2m 37s\n",
      "471:\tlearn: 0.4774341\ttotal: 2m 20s\tremaining: 2m 37s\n",
      "472:\tlearn: 0.4774189\ttotal: 2m 21s\tremaining: 2m 37s\n",
      "473:\tlearn: 0.4774068\ttotal: 2m 21s\tremaining: 2m 36s\n",
      "474:\tlearn: 0.4773877\ttotal: 2m 21s\tremaining: 2m 36s\n",
      "475:\tlearn: 0.4773754\ttotal: 2m 21s\tremaining: 2m 36s\n",
      "476:\tlearn: 0.4773636\ttotal: 2m 22s\tremaining: 2m 35s\n",
      "477:\tlearn: 0.4773497\ttotal: 2m 22s\tremaining: 2m 35s\n",
      "478:\tlearn: 0.4773318\ttotal: 2m 22s\tremaining: 2m 35s\n",
      "479:\tlearn: 0.4773235\ttotal: 2m 23s\tremaining: 2m 34s\n",
      "480:\tlearn: 0.4773030\ttotal: 2m 23s\tremaining: 2m 34s\n",
      "481:\tlearn: 0.4772967\ttotal: 2m 23s\tremaining: 2m 34s\n",
      "482:\tlearn: 0.4772842\ttotal: 2m 23s\tremaining: 2m 33s\n",
      "483:\tlearn: 0.4772696\ttotal: 2m 24s\tremaining: 2m 33s\n",
      "484:\tlearn: 0.4772616\ttotal: 2m 24s\tremaining: 2m 33s\n",
      "485:\tlearn: 0.4772325\ttotal: 2m 24s\tremaining: 2m 32s\n",
      "486:\tlearn: 0.4772223\ttotal: 2m 24s\tremaining: 2m 32s\n",
      "487:\tlearn: 0.4772150\ttotal: 2m 25s\tremaining: 2m 32s\n",
      "488:\tlearn: 0.4772045\ttotal: 2m 25s\tremaining: 2m 31s\n",
      "489:\tlearn: 0.4771835\ttotal: 2m 25s\tremaining: 2m 31s\n",
      "490:\tlearn: 0.4771777\ttotal: 2m 25s\tremaining: 2m 31s\n",
      "491:\tlearn: 0.4771581\ttotal: 2m 26s\tremaining: 2m 30s\n",
      "492:\tlearn: 0.4771442\ttotal: 2m 26s\tremaining: 2m 30s\n",
      "493:\tlearn: 0.4771302\ttotal: 2m 26s\tremaining: 2m 30s\n",
      "494:\tlearn: 0.4771242\ttotal: 2m 26s\tremaining: 2m 29s\n",
      "495:\tlearn: 0.4771081\ttotal: 2m 27s\tremaining: 2m 29s\n",
      "496:\tlearn: 0.4770828\ttotal: 2m 27s\tremaining: 2m 29s\n",
      "497:\tlearn: 0.4770712\ttotal: 2m 27s\tremaining: 2m 28s\n",
      "498:\tlearn: 0.4770606\ttotal: 2m 28s\tremaining: 2m 28s\n",
      "499:\tlearn: 0.4770424\ttotal: 2m 28s\tremaining: 2m 28s\n",
      "500:\tlearn: 0.4770070\ttotal: 2m 28s\tremaining: 2m 28s\n",
      "501:\tlearn: 0.4769856\ttotal: 2m 28s\tremaining: 2m 27s\n",
      "502:\tlearn: 0.4769452\ttotal: 2m 29s\tremaining: 2m 27s\n",
      "503:\tlearn: 0.4769377\ttotal: 2m 29s\tremaining: 2m 27s\n",
      "504:\tlearn: 0.4769206\ttotal: 2m 29s\tremaining: 2m 26s\n",
      "505:\tlearn: 0.4769124\ttotal: 2m 30s\tremaining: 2m 26s\n",
      "506:\tlearn: 0.4769062\ttotal: 2m 30s\tremaining: 2m 26s\n",
      "507:\tlearn: 0.4768994\ttotal: 2m 30s\tremaining: 2m 25s\n",
      "508:\tlearn: 0.4768868\ttotal: 2m 30s\tremaining: 2m 25s\n",
      "509:\tlearn: 0.4768707\ttotal: 2m 31s\tremaining: 2m 25s\n",
      "510:\tlearn: 0.4768445\ttotal: 2m 31s\tremaining: 2m 24s\n",
      "511:\tlearn: 0.4768361\ttotal: 2m 31s\tremaining: 2m 24s\n",
      "512:\tlearn: 0.4768260\ttotal: 2m 32s\tremaining: 2m 24s\n",
      "513:\tlearn: 0.4768161\ttotal: 2m 32s\tremaining: 2m 24s\n",
      "514:\tlearn: 0.4767879\ttotal: 2m 32s\tremaining: 2m 23s\n",
      "515:\tlearn: 0.4767817\ttotal: 2m 32s\tremaining: 2m 23s\n",
      "516:\tlearn: 0.4767747\ttotal: 2m 33s\tremaining: 2m 23s\n",
      "517:\tlearn: 0.4767647\ttotal: 2m 33s\tremaining: 2m 22s\n",
      "518:\tlearn: 0.4767552\ttotal: 2m 33s\tremaining: 2m 22s\n",
      "519:\tlearn: 0.4767505\ttotal: 2m 33s\tremaining: 2m 22s\n",
      "520:\tlearn: 0.4767319\ttotal: 2m 34s\tremaining: 2m 21s\n",
      "521:\tlearn: 0.4767247\ttotal: 2m 34s\tremaining: 2m 21s\n",
      "522:\tlearn: 0.4767108\ttotal: 2m 34s\tremaining: 2m 21s\n",
      "523:\tlearn: 0.4766945\ttotal: 2m 35s\tremaining: 2m 20s\n",
      "524:\tlearn: 0.4766882\ttotal: 2m 35s\tremaining: 2m 20s\n",
      "525:\tlearn: 0.4766661\ttotal: 2m 35s\tremaining: 2m 20s\n",
      "526:\tlearn: 0.4766536\ttotal: 2m 35s\tremaining: 2m 19s\n",
      "527:\tlearn: 0.4766271\ttotal: 2m 36s\tremaining: 2m 19s\n",
      "528:\tlearn: 0.4766171\ttotal: 2m 36s\tremaining: 2m 19s\n",
      "529:\tlearn: 0.4765933\ttotal: 2m 36s\tremaining: 2m 19s\n",
      "530:\tlearn: 0.4765791\ttotal: 2m 37s\tremaining: 2m 18s\n",
      "531:\tlearn: 0.4765584\ttotal: 2m 37s\tremaining: 2m 18s\n",
      "532:\tlearn: 0.4765502\ttotal: 2m 37s\tremaining: 2m 18s\n",
      "533:\tlearn: 0.4765423\ttotal: 2m 37s\tremaining: 2m 17s\n",
      "534:\tlearn: 0.4765259\ttotal: 2m 38s\tremaining: 2m 17s\n",
      "535:\tlearn: 0.4765126\ttotal: 2m 38s\tremaining: 2m 17s\n",
      "536:\tlearn: 0.4764961\ttotal: 2m 38s\tremaining: 2m 16s\n",
      "537:\tlearn: 0.4764656\ttotal: 2m 39s\tremaining: 2m 16s\n",
      "538:\tlearn: 0.4764456\ttotal: 2m 39s\tremaining: 2m 16s\n",
      "539:\tlearn: 0.4764362\ttotal: 2m 39s\tremaining: 2m 15s\n",
      "540:\tlearn: 0.4764141\ttotal: 2m 39s\tremaining: 2m 15s\n",
      "541:\tlearn: 0.4764083\ttotal: 2m 40s\tremaining: 2m 15s\n",
      "542:\tlearn: 0.4763903\ttotal: 2m 40s\tremaining: 2m 15s\n",
      "543:\tlearn: 0.4763817\ttotal: 2m 40s\tremaining: 2m 14s\n",
      "544:\tlearn: 0.4763456\ttotal: 2m 41s\tremaining: 2m 14s\n",
      "545:\tlearn: 0.4763078\ttotal: 2m 41s\tremaining: 2m 14s\n",
      "546:\tlearn: 0.4762928\ttotal: 2m 41s\tremaining: 2m 13s\n",
      "547:\tlearn: 0.4762806\ttotal: 2m 41s\tremaining: 2m 13s\n",
      "548:\tlearn: 0.4762744\ttotal: 2m 42s\tremaining: 2m 13s\n",
      "549:\tlearn: 0.4762621\ttotal: 2m 42s\tremaining: 2m 12s\n",
      "550:\tlearn: 0.4762537\ttotal: 2m 42s\tremaining: 2m 12s\n",
      "551:\tlearn: 0.4762336\ttotal: 2m 42s\tremaining: 2m 12s\n",
      "552:\tlearn: 0.4762036\ttotal: 2m 43s\tremaining: 2m 12s\n",
      "553:\tlearn: 0.4761949\ttotal: 2m 43s\tremaining: 2m 11s\n",
      "554:\tlearn: 0.4761834\ttotal: 2m 43s\tremaining: 2m 11s\n",
      "555:\tlearn: 0.4761742\ttotal: 2m 44s\tremaining: 2m 11s\n",
      "556:\tlearn: 0.4761619\ttotal: 2m 44s\tremaining: 2m 10s\n",
      "557:\tlearn: 0.4761445\ttotal: 2m 44s\tremaining: 2m 10s\n",
      "558:\tlearn: 0.4761228\ttotal: 2m 45s\tremaining: 2m 10s\n",
      "559:\tlearn: 0.4761109\ttotal: 2m 45s\tremaining: 2m 9s\n",
      "560:\tlearn: 0.4761043\ttotal: 2m 45s\tremaining: 2m 9s\n",
      "561:\tlearn: 0.4760958\ttotal: 2m 45s\tremaining: 2m 9s\n",
      "562:\tlearn: 0.4760900\ttotal: 2m 45s\tremaining: 2m 8s\n",
      "563:\tlearn: 0.4760757\ttotal: 2m 46s\tremaining: 2m 8s\n",
      "564:\tlearn: 0.4760629\ttotal: 2m 46s\tremaining: 2m 8s\n",
      "565:\tlearn: 0.4760373\ttotal: 2m 46s\tremaining: 2m 7s\n",
      "566:\tlearn: 0.4760202\ttotal: 2m 47s\tremaining: 2m 7s\n",
      "567:\tlearn: 0.4760079\ttotal: 2m 47s\tremaining: 2m 7s\n",
      "568:\tlearn: 0.4759919\ttotal: 2m 47s\tremaining: 2m 7s\n",
      "569:\tlearn: 0.4759611\ttotal: 2m 48s\tremaining: 2m 6s\n",
      "570:\tlearn: 0.4759560\ttotal: 2m 48s\tremaining: 2m 6s\n",
      "571:\tlearn: 0.4759268\ttotal: 2m 48s\tremaining: 2m 6s\n",
      "572:\tlearn: 0.4759145\ttotal: 2m 48s\tremaining: 2m 5s\n",
      "573:\tlearn: 0.4759003\ttotal: 2m 49s\tremaining: 2m 5s\n",
      "574:\tlearn: 0.4758945\ttotal: 2m 49s\tremaining: 2m 5s\n",
      "575:\tlearn: 0.4758861\ttotal: 2m 49s\tremaining: 2m 4s\n",
      "576:\tlearn: 0.4758687\ttotal: 2m 50s\tremaining: 2m 4s\n",
      "577:\tlearn: 0.4758617\ttotal: 2m 50s\tremaining: 2m 4s\n",
      "578:\tlearn: 0.4758494\ttotal: 2m 50s\tremaining: 2m 4s\n",
      "579:\tlearn: 0.4758276\ttotal: 2m 50s\tremaining: 2m 3s\n",
      "580:\tlearn: 0.4758229\ttotal: 2m 51s\tremaining: 2m 3s\n",
      "581:\tlearn: 0.4758175\ttotal: 2m 51s\tremaining: 2m 3s\n",
      "582:\tlearn: 0.4757732\ttotal: 2m 51s\tremaining: 2m 2s\n",
      "583:\tlearn: 0.4757615\ttotal: 2m 52s\tremaining: 2m 2s\n",
      "584:\tlearn: 0.4757477\ttotal: 2m 52s\tremaining: 2m 2s\n",
      "585:\tlearn: 0.4757341\ttotal: 2m 52s\tremaining: 2m 1s\n",
      "586:\tlearn: 0.4757193\ttotal: 2m 52s\tremaining: 2m 1s\n",
      "587:\tlearn: 0.4757118\ttotal: 2m 53s\tremaining: 2m 1s\n",
      "588:\tlearn: 0.4756848\ttotal: 2m 53s\tremaining: 2m 1s\n",
      "589:\tlearn: 0.4756608\ttotal: 2m 53s\tremaining: 2m\n",
      "590:\tlearn: 0.4756558\ttotal: 2m 54s\tremaining: 2m\n",
      "591:\tlearn: 0.4756392\ttotal: 2m 54s\tremaining: 2m\n",
      "592:\tlearn: 0.4756313\ttotal: 2m 54s\tremaining: 1m 59s\n",
      "593:\tlearn: 0.4756189\ttotal: 2m 54s\tremaining: 1m 59s\n",
      "594:\tlearn: 0.4756127\ttotal: 2m 55s\tremaining: 1m 59s\n",
      "595:\tlearn: 0.4755999\ttotal: 2m 55s\tremaining: 1m 58s\n",
      "596:\tlearn: 0.4755883\ttotal: 2m 55s\tremaining: 1m 58s\n",
      "597:\tlearn: 0.4755754\ttotal: 2m 56s\tremaining: 1m 58s\n",
      "598:\tlearn: 0.4755595\ttotal: 2m 56s\tremaining: 1m 58s\n",
      "599:\tlearn: 0.4755540\ttotal: 2m 56s\tremaining: 1m 57s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600:\tlearn: 0.4755396\ttotal: 2m 56s\tremaining: 1m 57s\n",
      "601:\tlearn: 0.4755310\ttotal: 2m 57s\tremaining: 1m 57s\n",
      "602:\tlearn: 0.4755196\ttotal: 2m 57s\tremaining: 1m 56s\n",
      "603:\tlearn: 0.4755143\ttotal: 2m 57s\tremaining: 1m 56s\n",
      "604:\tlearn: 0.4754827\ttotal: 2m 57s\tremaining: 1m 56s\n",
      "605:\tlearn: 0.4754510\ttotal: 2m 58s\tremaining: 1m 55s\n",
      "606:\tlearn: 0.4754436\ttotal: 2m 58s\tremaining: 1m 55s\n",
      "607:\tlearn: 0.4754340\ttotal: 2m 58s\tremaining: 1m 55s\n",
      "608:\tlearn: 0.4754233\ttotal: 2m 58s\tremaining: 1m 54s\n",
      "609:\tlearn: 0.4754085\ttotal: 2m 59s\tremaining: 1m 54s\n",
      "610:\tlearn: 0.4753954\ttotal: 2m 59s\tremaining: 1m 54s\n",
      "611:\tlearn: 0.4753915\ttotal: 2m 59s\tremaining: 1m 53s\n",
      "612:\tlearn: 0.4753681\ttotal: 3m\tremaining: 1m 53s\n",
      "613:\tlearn: 0.4753613\ttotal: 3m\tremaining: 1m 53s\n",
      "614:\tlearn: 0.4753493\ttotal: 3m\tremaining: 1m 53s\n",
      "615:\tlearn: 0.4753409\ttotal: 3m\tremaining: 1m 52s\n",
      "616:\tlearn: 0.4753266\ttotal: 3m 1s\tremaining: 1m 52s\n",
      "617:\tlearn: 0.4753094\ttotal: 3m 1s\tremaining: 1m 52s\n",
      "618:\tlearn: 0.4752897\ttotal: 3m 1s\tremaining: 1m 51s\n",
      "619:\tlearn: 0.4752838\ttotal: 3m 1s\tremaining: 1m 51s\n",
      "620:\tlearn: 0.4752301\ttotal: 3m 2s\tremaining: 1m 51s\n",
      "621:\tlearn: 0.4752011\ttotal: 3m 2s\tremaining: 1m 51s\n",
      "622:\tlearn: 0.4751917\ttotal: 3m 2s\tremaining: 1m 50s\n",
      "623:\tlearn: 0.4751818\ttotal: 3m 3s\tremaining: 1m 50s\n",
      "624:\tlearn: 0.4751513\ttotal: 3m 3s\tremaining: 1m 50s\n",
      "625:\tlearn: 0.4751413\ttotal: 3m 3s\tremaining: 1m 49s\n",
      "626:\tlearn: 0.4751086\ttotal: 3m 4s\tremaining: 1m 49s\n",
      "627:\tlearn: 0.4750931\ttotal: 3m 4s\tremaining: 1m 49s\n",
      "628:\tlearn: 0.4750861\ttotal: 3m 4s\tremaining: 1m 48s\n",
      "629:\tlearn: 0.4750737\ttotal: 3m 4s\tremaining: 1m 48s\n",
      "630:\tlearn: 0.4750615\ttotal: 3m 5s\tremaining: 1m 48s\n",
      "631:\tlearn: 0.4750517\ttotal: 3m 5s\tremaining: 1m 47s\n",
      "632:\tlearn: 0.4750439\ttotal: 3m 5s\tremaining: 1m 47s\n",
      "633:\tlearn: 0.4750371\ttotal: 3m 6s\tremaining: 1m 47s\n",
      "634:\tlearn: 0.4750011\ttotal: 3m 6s\tremaining: 1m 47s\n",
      "635:\tlearn: 0.4749840\ttotal: 3m 6s\tremaining: 1m 47s\n",
      "636:\tlearn: 0.4749674\ttotal: 3m 7s\tremaining: 1m 46s\n",
      "637:\tlearn: 0.4749591\ttotal: 3m 7s\tremaining: 1m 46s\n",
      "638:\tlearn: 0.4749397\ttotal: 3m 7s\tremaining: 1m 46s\n",
      "639:\tlearn: 0.4749243\ttotal: 3m 8s\tremaining: 1m 45s\n",
      "640:\tlearn: 0.4749081\ttotal: 3m 8s\tremaining: 1m 45s\n",
      "641:\tlearn: 0.4749014\ttotal: 3m 8s\tremaining: 1m 45s\n",
      "642:\tlearn: 0.4748734\ttotal: 3m 9s\tremaining: 1m 45s\n",
      "643:\tlearn: 0.4748689\ttotal: 3m 9s\tremaining: 1m 44s\n",
      "644:\tlearn: 0.4748602\ttotal: 3m 9s\tremaining: 1m 44s\n",
      "645:\tlearn: 0.4748506\ttotal: 3m 10s\tremaining: 1m 44s\n",
      "646:\tlearn: 0.4748422\ttotal: 3m 10s\tremaining: 1m 43s\n",
      "647:\tlearn: 0.4748191\ttotal: 3m 10s\tremaining: 1m 43s\n",
      "648:\tlearn: 0.4748066\ttotal: 3m 11s\tremaining: 1m 43s\n",
      "649:\tlearn: 0.4747950\ttotal: 3m 11s\tremaining: 1m 43s\n",
      "650:\tlearn: 0.4747805\ttotal: 3m 11s\tremaining: 1m 42s\n",
      "651:\tlearn: 0.4747698\ttotal: 3m 12s\tremaining: 1m 42s\n",
      "652:\tlearn: 0.4747580\ttotal: 3m 12s\tremaining: 1m 42s\n",
      "653:\tlearn: 0.4747489\ttotal: 3m 12s\tremaining: 1m 41s\n",
      "654:\tlearn: 0.4747365\ttotal: 3m 12s\tremaining: 1m 41s\n",
      "655:\tlearn: 0.4747272\ttotal: 3m 13s\tremaining: 1m 41s\n",
      "656:\tlearn: 0.4747215\ttotal: 3m 13s\tremaining: 1m 40s\n",
      "657:\tlearn: 0.4747070\ttotal: 3m 13s\tremaining: 1m 40s\n",
      "658:\tlearn: 0.4746955\ttotal: 3m 13s\tremaining: 1m 40s\n",
      "659:\tlearn: 0.4746876\ttotal: 3m 14s\tremaining: 1m 40s\n",
      "660:\tlearn: 0.4746827\ttotal: 3m 14s\tremaining: 1m 39s\n",
      "661:\tlearn: 0.4746717\ttotal: 3m 14s\tremaining: 1m 39s\n",
      "662:\tlearn: 0.4746599\ttotal: 3m 14s\tremaining: 1m 39s\n",
      "663:\tlearn: 0.4746563\ttotal: 3m 15s\tremaining: 1m 38s\n",
      "664:\tlearn: 0.4746435\ttotal: 3m 15s\tremaining: 1m 38s\n",
      "665:\tlearn: 0.4746334\ttotal: 3m 15s\tremaining: 1m 38s\n",
      "666:\tlearn: 0.4746124\ttotal: 3m 16s\tremaining: 1m 37s\n",
      "667:\tlearn: 0.4746029\ttotal: 3m 16s\tremaining: 1m 37s\n",
      "668:\tlearn: 0.4745882\ttotal: 3m 16s\tremaining: 1m 37s\n",
      "669:\tlearn: 0.4745756\ttotal: 3m 16s\tremaining: 1m 36s\n",
      "670:\tlearn: 0.4745594\ttotal: 3m 17s\tremaining: 1m 36s\n",
      "671:\tlearn: 0.4745483\ttotal: 3m 17s\tremaining: 1m 36s\n",
      "672:\tlearn: 0.4745441\ttotal: 3m 17s\tremaining: 1m 36s\n",
      "673:\tlearn: 0.4745386\ttotal: 3m 17s\tremaining: 1m 35s\n",
      "674:\tlearn: 0.4745331\ttotal: 3m 18s\tremaining: 1m 35s\n",
      "675:\tlearn: 0.4745206\ttotal: 3m 18s\tremaining: 1m 35s\n",
      "676:\tlearn: 0.4745098\ttotal: 3m 18s\tremaining: 1m 34s\n",
      "677:\tlearn: 0.4745027\ttotal: 3m 18s\tremaining: 1m 34s\n",
      "678:\tlearn: 0.4744969\ttotal: 3m 19s\tremaining: 1m 34s\n",
      "679:\tlearn: 0.4744846\ttotal: 3m 19s\tremaining: 1m 33s\n",
      "680:\tlearn: 0.4744629\ttotal: 3m 19s\tremaining: 1m 33s\n",
      "681:\tlearn: 0.4744409\ttotal: 3m 20s\tremaining: 1m 33s\n",
      "682:\tlearn: 0.4744316\ttotal: 3m 20s\tremaining: 1m 33s\n",
      "683:\tlearn: 0.4744260\ttotal: 3m 20s\tremaining: 1m 32s\n",
      "684:\tlearn: 0.4744234\ttotal: 3m 20s\tremaining: 1m 32s\n",
      "685:\tlearn: 0.4744148\ttotal: 3m 21s\tremaining: 1m 32s\n",
      "686:\tlearn: 0.4744081\ttotal: 3m 21s\tremaining: 1m 31s\n",
      "687:\tlearn: 0.4743973\ttotal: 3m 21s\tremaining: 1m 31s\n",
      "688:\tlearn: 0.4743839\ttotal: 3m 21s\tremaining: 1m 31s\n",
      "689:\tlearn: 0.4743793\ttotal: 3m 22s\tremaining: 1m 30s\n",
      "690:\tlearn: 0.4743599\ttotal: 3m 22s\tremaining: 1m 30s\n",
      "691:\tlearn: 0.4743485\ttotal: 3m 22s\tremaining: 1m 30s\n",
      "692:\tlearn: 0.4743380\ttotal: 3m 23s\tremaining: 1m 29s\n",
      "693:\tlearn: 0.4743303\ttotal: 3m 23s\tremaining: 1m 29s\n",
      "694:\tlearn: 0.4743260\ttotal: 3m 23s\tremaining: 1m 29s\n",
      "695:\tlearn: 0.4743131\ttotal: 3m 23s\tremaining: 1m 29s\n",
      "696:\tlearn: 0.4743078\ttotal: 3m 24s\tremaining: 1m 28s\n",
      "697:\tlearn: 0.4743005\ttotal: 3m 24s\tremaining: 1m 28s\n",
      "698:\tlearn: 0.4742920\ttotal: 3m 24s\tremaining: 1m 28s\n",
      "699:\tlearn: 0.4742584\ttotal: 3m 25s\tremaining: 1m 27s\n",
      "700:\tlearn: 0.4742274\ttotal: 3m 25s\tremaining: 1m 27s\n",
      "701:\tlearn: 0.4742216\ttotal: 3m 25s\tremaining: 1m 27s\n",
      "702:\tlearn: 0.4741936\ttotal: 3m 26s\tremaining: 1m 27s\n",
      "703:\tlearn: 0.4741814\ttotal: 3m 26s\tremaining: 1m 26s\n",
      "704:\tlearn: 0.4741731\ttotal: 3m 26s\tremaining: 1m 26s\n",
      "705:\tlearn: 0.4741606\ttotal: 3m 26s\tremaining: 1m 26s\n",
      "706:\tlearn: 0.4741509\ttotal: 3m 27s\tremaining: 1m 25s\n",
      "707:\tlearn: 0.4741356\ttotal: 3m 27s\tremaining: 1m 25s\n",
      "708:\tlearn: 0.4741156\ttotal: 3m 27s\tremaining: 1m 25s\n",
      "709:\tlearn: 0.4740910\ttotal: 3m 28s\tremaining: 1m 24s\n",
      "710:\tlearn: 0.4740825\ttotal: 3m 28s\tremaining: 1m 24s\n",
      "711:\tlearn: 0.4740741\ttotal: 3m 28s\tremaining: 1m 24s\n",
      "712:\tlearn: 0.4740672\ttotal: 3m 28s\tremaining: 1m 24s\n",
      "713:\tlearn: 0.4740525\ttotal: 3m 29s\tremaining: 1m 23s\n",
      "714:\tlearn: 0.4740327\ttotal: 3m 29s\tremaining: 1m 23s\n",
      "715:\tlearn: 0.4740238\ttotal: 3m 29s\tremaining: 1m 23s\n",
      "716:\tlearn: 0.4740146\ttotal: 3m 30s\tremaining: 1m 22s\n",
      "717:\tlearn: 0.4739880\ttotal: 3m 30s\tremaining: 1m 22s\n",
      "718:\tlearn: 0.4739771\ttotal: 3m 30s\tremaining: 1m 22s\n",
      "719:\tlearn: 0.4739595\ttotal: 3m 30s\tremaining: 1m 22s\n",
      "720:\tlearn: 0.4739465\ttotal: 3m 31s\tremaining: 1m 21s\n",
      "721:\tlearn: 0.4739324\ttotal: 3m 31s\tremaining: 1m 21s\n",
      "722:\tlearn: 0.4739282\ttotal: 3m 31s\tremaining: 1m 21s\n",
      "723:\tlearn: 0.4739221\ttotal: 3m 32s\tremaining: 1m 20s\n",
      "724:\tlearn: 0.4739177\ttotal: 3m 32s\tremaining: 1m 20s\n",
      "725:\tlearn: 0.4739130\ttotal: 3m 32s\tremaining: 1m 20s\n",
      "726:\tlearn: 0.4739051\ttotal: 3m 32s\tremaining: 1m 19s\n",
      "727:\tlearn: 0.4738991\ttotal: 3m 33s\tremaining: 1m 19s\n",
      "728:\tlearn: 0.4738920\ttotal: 3m 33s\tremaining: 1m 19s\n",
      "729:\tlearn: 0.4738805\ttotal: 3m 33s\tremaining: 1m 19s\n",
      "730:\tlearn: 0.4738747\ttotal: 3m 33s\tremaining: 1m 18s\n",
      "731:\tlearn: 0.4738694\ttotal: 3m 34s\tremaining: 1m 18s\n",
      "732:\tlearn: 0.4738470\ttotal: 3m 34s\tremaining: 1m 18s\n",
      "733:\tlearn: 0.4738423\ttotal: 3m 34s\tremaining: 1m 17s\n",
      "734:\tlearn: 0.4738305\ttotal: 3m 34s\tremaining: 1m 17s\n",
      "735:\tlearn: 0.4738256\ttotal: 3m 35s\tremaining: 1m 17s\n",
      "736:\tlearn: 0.4738174\ttotal: 3m 35s\tremaining: 1m 16s\n",
      "737:\tlearn: 0.4738118\ttotal: 3m 35s\tremaining: 1m 16s\n",
      "738:\tlearn: 0.4738010\ttotal: 3m 35s\tremaining: 1m 16s\n",
      "739:\tlearn: 0.4737878\ttotal: 3m 36s\tremaining: 1m 15s\n",
      "740:\tlearn: 0.4737788\ttotal: 3m 36s\tremaining: 1m 15s\n",
      "741:\tlearn: 0.4737580\ttotal: 3m 36s\tremaining: 1m 15s\n",
      "742:\tlearn: 0.4737509\ttotal: 3m 37s\tremaining: 1m 15s\n",
      "743:\tlearn: 0.4737457\ttotal: 3m 37s\tremaining: 1m 14s\n",
      "744:\tlearn: 0.4737425\ttotal: 3m 37s\tremaining: 1m 14s\n",
      "745:\tlearn: 0.4737366\ttotal: 3m 37s\tremaining: 1m 14s\n",
      "746:\tlearn: 0.4737239\ttotal: 3m 38s\tremaining: 1m 13s\n",
      "747:\tlearn: 0.4737149\ttotal: 3m 38s\tremaining: 1m 13s\n",
      "748:\tlearn: 0.4737081\ttotal: 3m 38s\tremaining: 1m 13s\n",
      "749:\tlearn: 0.4736750\ttotal: 3m 39s\tremaining: 1m 13s\n",
      "750:\tlearn: 0.4736555\ttotal: 3m 39s\tremaining: 1m 12s\n",
      "751:\tlearn: 0.4736292\ttotal: 3m 39s\tremaining: 1m 12s\n",
      "752:\tlearn: 0.4736211\ttotal: 3m 39s\tremaining: 1m 12s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "753:\tlearn: 0.4736116\ttotal: 3m 40s\tremaining: 1m 11s\n",
      "754:\tlearn: 0.4735992\ttotal: 3m 40s\tremaining: 1m 11s\n",
      "755:\tlearn: 0.4735886\ttotal: 3m 40s\tremaining: 1m 11s\n",
      "756:\tlearn: 0.4735818\ttotal: 3m 41s\tremaining: 1m 10s\n",
      "757:\tlearn: 0.4735782\ttotal: 3m 41s\tremaining: 1m 10s\n",
      "758:\tlearn: 0.4735662\ttotal: 3m 41s\tremaining: 1m 10s\n",
      "759:\tlearn: 0.4735611\ttotal: 3m 41s\tremaining: 1m 10s\n",
      "760:\tlearn: 0.4735549\ttotal: 3m 41s\tremaining: 1m 9s\n",
      "761:\tlearn: 0.4735487\ttotal: 3m 42s\tremaining: 1m 9s\n",
      "762:\tlearn: 0.4735436\ttotal: 3m 42s\tremaining: 1m 9s\n",
      "763:\tlearn: 0.4735239\ttotal: 3m 42s\tremaining: 1m 8s\n",
      "764:\tlearn: 0.4735180\ttotal: 3m 43s\tremaining: 1m 8s\n",
      "765:\tlearn: 0.4735127\ttotal: 3m 43s\tremaining: 1m 8s\n",
      "766:\tlearn: 0.4734564\ttotal: 3m 43s\tremaining: 1m 7s\n",
      "767:\tlearn: 0.4734504\ttotal: 3m 43s\tremaining: 1m 7s\n",
      "768:\tlearn: 0.4734375\ttotal: 3m 44s\tremaining: 1m 7s\n",
      "769:\tlearn: 0.4734296\ttotal: 3m 44s\tremaining: 1m 7s\n",
      "770:\tlearn: 0.4734230\ttotal: 3m 44s\tremaining: 1m 6s\n",
      "771:\tlearn: 0.4733770\ttotal: 3m 44s\tremaining: 1m 6s\n",
      "772:\tlearn: 0.4733568\ttotal: 3m 45s\tremaining: 1m 6s\n",
      "773:\tlearn: 0.4733477\ttotal: 3m 45s\tremaining: 1m 5s\n",
      "774:\tlearn: 0.4733285\ttotal: 3m 45s\tremaining: 1m 5s\n",
      "775:\tlearn: 0.4733230\ttotal: 3m 46s\tremaining: 1m 5s\n",
      "776:\tlearn: 0.4732988\ttotal: 3m 46s\tremaining: 1m 4s\n",
      "777:\tlearn: 0.4732812\ttotal: 3m 46s\tremaining: 1m 4s\n",
      "778:\tlearn: 0.4732702\ttotal: 3m 46s\tremaining: 1m 4s\n",
      "779:\tlearn: 0.4732618\ttotal: 3m 47s\tremaining: 1m 4s\n",
      "780:\tlearn: 0.4732434\ttotal: 3m 47s\tremaining: 1m 3s\n",
      "781:\tlearn: 0.4732367\ttotal: 3m 47s\tremaining: 1m 3s\n",
      "782:\tlearn: 0.4732282\ttotal: 3m 48s\tremaining: 1m 3s\n",
      "783:\tlearn: 0.4732195\ttotal: 3m 48s\tremaining: 1m 2s\n",
      "784:\tlearn: 0.4732100\ttotal: 3m 48s\tremaining: 1m 2s\n",
      "785:\tlearn: 0.4732037\ttotal: 3m 48s\tremaining: 1m 2s\n",
      "786:\tlearn: 0.4731968\ttotal: 3m 48s\tremaining: 1m 1s\n",
      "787:\tlearn: 0.4731908\ttotal: 3m 49s\tremaining: 1m 1s\n",
      "788:\tlearn: 0.4731829\ttotal: 3m 49s\tremaining: 1m 1s\n",
      "789:\tlearn: 0.4731705\ttotal: 3m 49s\tremaining: 1m 1s\n",
      "790:\tlearn: 0.4731477\ttotal: 3m 50s\tremaining: 1m\n",
      "791:\tlearn: 0.4731403\ttotal: 3m 50s\tremaining: 1m\n",
      "792:\tlearn: 0.4731119\ttotal: 3m 50s\tremaining: 1m\n",
      "793:\tlearn: 0.4731034\ttotal: 3m 51s\tremaining: 60s\n",
      "794:\tlearn: 0.4730919\ttotal: 3m 51s\tremaining: 59.7s\n",
      "795:\tlearn: 0.4730888\ttotal: 3m 51s\tremaining: 59.4s\n",
      "796:\tlearn: 0.4730825\ttotal: 3m 51s\tremaining: 59s\n",
      "797:\tlearn: 0.4730638\ttotal: 3m 52s\tremaining: 58.8s\n",
      "798:\tlearn: 0.4730557\ttotal: 3m 52s\tremaining: 58.5s\n",
      "799:\tlearn: 0.4730469\ttotal: 3m 52s\tremaining: 58.2s\n",
      "800:\tlearn: 0.4730119\ttotal: 3m 52s\tremaining: 57.9s\n",
      "801:\tlearn: 0.4729798\ttotal: 3m 53s\tremaining: 57.6s\n",
      "802:\tlearn: 0.4729654\ttotal: 3m 53s\tremaining: 57.3s\n",
      "803:\tlearn: 0.4729599\ttotal: 3m 53s\tremaining: 57s\n",
      "804:\tlearn: 0.4729515\ttotal: 3m 54s\tremaining: 56.8s\n",
      "805:\tlearn: 0.4729395\ttotal: 3m 54s\tremaining: 56.5s\n",
      "806:\tlearn: 0.4729145\ttotal: 3m 54s\tremaining: 56.2s\n",
      "807:\tlearn: 0.4728938\ttotal: 3m 55s\tremaining: 55.9s\n",
      "808:\tlearn: 0.4728851\ttotal: 3m 55s\tremaining: 55.6s\n",
      "809:\tlearn: 0.4728786\ttotal: 3m 55s\tremaining: 55.3s\n",
      "810:\tlearn: 0.4728733\ttotal: 3m 56s\tremaining: 55s\n",
      "811:\tlearn: 0.4728702\ttotal: 3m 56s\tremaining: 54.7s\n",
      "812:\tlearn: 0.4728620\ttotal: 3m 56s\tremaining: 54.4s\n",
      "813:\tlearn: 0.4728565\ttotal: 3m 56s\tremaining: 54.1s\n",
      "814:\tlearn: 0.4728526\ttotal: 3m 57s\tremaining: 53.8s\n",
      "815:\tlearn: 0.4728467\ttotal: 3m 57s\tremaining: 53.5s\n",
      "816:\tlearn: 0.4728423\ttotal: 3m 57s\tremaining: 53.2s\n",
      "817:\tlearn: 0.4728221\ttotal: 3m 57s\tremaining: 52.9s\n",
      "818:\tlearn: 0.4728138\ttotal: 3m 58s\tremaining: 52.6s\n",
      "819:\tlearn: 0.4728062\ttotal: 3m 58s\tremaining: 52.3s\n",
      "820:\tlearn: 0.4727947\ttotal: 3m 58s\tremaining: 52.1s\n",
      "821:\tlearn: 0.4727894\ttotal: 3m 59s\tremaining: 51.8s\n",
      "822:\tlearn: 0.4727837\ttotal: 3m 59s\tremaining: 51.5s\n",
      "823:\tlearn: 0.4727723\ttotal: 3m 59s\tremaining: 51.2s\n",
      "824:\tlearn: 0.4727675\ttotal: 3m 59s\tremaining: 50.9s\n",
      "825:\tlearn: 0.4727615\ttotal: 4m\tremaining: 50.6s\n",
      "826:\tlearn: 0.4727532\ttotal: 4m\tremaining: 50.3s\n",
      "827:\tlearn: 0.4727319\ttotal: 4m\tremaining: 50s\n",
      "828:\tlearn: 0.4727131\ttotal: 4m 1s\tremaining: 49.7s\n",
      "829:\tlearn: 0.4727085\ttotal: 4m 1s\tremaining: 49.4s\n",
      "830:\tlearn: 0.4727009\ttotal: 4m 1s\tremaining: 49.1s\n",
      "831:\tlearn: 0.4726909\ttotal: 4m 1s\tremaining: 48.8s\n",
      "832:\tlearn: 0.4726525\ttotal: 4m 2s\tremaining: 48.6s\n",
      "833:\tlearn: 0.4726181\ttotal: 4m 2s\tremaining: 48.3s\n",
      "834:\tlearn: 0.4726061\ttotal: 4m 2s\tremaining: 48s\n",
      "835:\tlearn: 0.4725962\ttotal: 4m 3s\tremaining: 47.7s\n",
      "836:\tlearn: 0.4725853\ttotal: 4m 3s\tremaining: 47.4s\n",
      "837:\tlearn: 0.4725670\ttotal: 4m 3s\tremaining: 47.1s\n",
      "838:\tlearn: 0.4725504\ttotal: 4m 4s\tremaining: 46.9s\n",
      "839:\tlearn: 0.4725436\ttotal: 4m 4s\tremaining: 46.6s\n",
      "840:\tlearn: 0.4724982\ttotal: 4m 4s\tremaining: 46.3s\n",
      "841:\tlearn: 0.4724792\ttotal: 4m 5s\tremaining: 46s\n",
      "842:\tlearn: 0.4724682\ttotal: 4m 5s\tremaining: 45.7s\n",
      "843:\tlearn: 0.4724599\ttotal: 4m 5s\tremaining: 45.5s\n",
      "844:\tlearn: 0.4724566\ttotal: 4m 6s\tremaining: 45.2s\n",
      "845:\tlearn: 0.4724435\ttotal: 4m 6s\tremaining: 44.9s\n",
      "846:\tlearn: 0.4724334\ttotal: 4m 6s\tremaining: 44.6s\n",
      "847:\tlearn: 0.4724203\ttotal: 4m 6s\tremaining: 44.3s\n",
      "848:\tlearn: 0.4724123\ttotal: 4m 7s\tremaining: 44s\n",
      "849:\tlearn: 0.4724019\ttotal: 4m 7s\tremaining: 43.7s\n",
      "850:\tlearn: 0.4723954\ttotal: 4m 7s\tremaining: 43.4s\n",
      "851:\tlearn: 0.4723659\ttotal: 4m 8s\tremaining: 43.1s\n",
      "852:\tlearn: 0.4723328\ttotal: 4m 8s\tremaining: 42.8s\n",
      "853:\tlearn: 0.4722998\ttotal: 4m 8s\tremaining: 42.5s\n",
      "854:\tlearn: 0.4722752\ttotal: 4m 9s\tremaining: 42.2s\n",
      "855:\tlearn: 0.4722595\ttotal: 4m 9s\tremaining: 42s\n",
      "856:\tlearn: 0.4722464\ttotal: 4m 9s\tremaining: 41.7s\n",
      "857:\tlearn: 0.4722364\ttotal: 4m 9s\tremaining: 41.4s\n",
      "858:\tlearn: 0.4722308\ttotal: 4m 10s\tremaining: 41.1s\n",
      "859:\tlearn: 0.4722096\ttotal: 4m 10s\tremaining: 40.8s\n",
      "860:\tlearn: 0.4722035\ttotal: 4m 10s\tremaining: 40.5s\n",
      "861:\tlearn: 0.4721997\ttotal: 4m 10s\tremaining: 40.2s\n",
      "862:\tlearn: 0.4721842\ttotal: 4m 11s\tremaining: 39.9s\n",
      "863:\tlearn: 0.4721778\ttotal: 4m 11s\tremaining: 39.6s\n",
      "864:\tlearn: 0.4721663\ttotal: 4m 11s\tremaining: 39.3s\n",
      "865:\tlearn: 0.4721606\ttotal: 4m 12s\tremaining: 39s\n",
      "866:\tlearn: 0.4721558\ttotal: 4m 12s\tremaining: 38.7s\n",
      "867:\tlearn: 0.4721460\ttotal: 4m 12s\tremaining: 38.4s\n",
      "868:\tlearn: 0.4721370\ttotal: 4m 12s\tremaining: 38.1s\n",
      "869:\tlearn: 0.4721209\ttotal: 4m 13s\tremaining: 37.8s\n",
      "870:\tlearn: 0.4721127\ttotal: 4m 13s\tremaining: 37.5s\n",
      "871:\tlearn: 0.4720910\ttotal: 4m 13s\tremaining: 37.2s\n",
      "872:\tlearn: 0.4720806\ttotal: 4m 13s\tremaining: 36.9s\n",
      "873:\tlearn: 0.4720625\ttotal: 4m 14s\tremaining: 36.6s\n",
      "874:\tlearn: 0.4720543\ttotal: 4m 14s\tremaining: 36.4s\n",
      "875:\tlearn: 0.4720492\ttotal: 4m 14s\tremaining: 36.1s\n",
      "876:\tlearn: 0.4720434\ttotal: 4m 14s\tremaining: 35.8s\n",
      "877:\tlearn: 0.4720385\ttotal: 4m 15s\tremaining: 35.5s\n",
      "878:\tlearn: 0.4720322\ttotal: 4m 15s\tremaining: 35.2s\n",
      "879:\tlearn: 0.4719949\ttotal: 4m 15s\tremaining: 34.9s\n",
      "880:\tlearn: 0.4719910\ttotal: 4m 15s\tremaining: 34.6s\n",
      "881:\tlearn: 0.4719845\ttotal: 4m 16s\tremaining: 34.3s\n",
      "882:\tlearn: 0.4719729\ttotal: 4m 16s\tremaining: 34s\n",
      "883:\tlearn: 0.4719648\ttotal: 4m 16s\tremaining: 33.7s\n",
      "884:\tlearn: 0.4719406\ttotal: 4m 16s\tremaining: 33.4s\n",
      "885:\tlearn: 0.4719352\ttotal: 4m 17s\tremaining: 33.1s\n",
      "886:\tlearn: 0.4719269\ttotal: 4m 17s\tremaining: 32.8s\n",
      "887:\tlearn: 0.4719160\ttotal: 4m 17s\tremaining: 32.5s\n",
      "888:\tlearn: 0.4719046\ttotal: 4m 17s\tremaining: 32.2s\n",
      "889:\tlearn: 0.4718958\ttotal: 4m 18s\tremaining: 31.9s\n",
      "890:\tlearn: 0.4718892\ttotal: 4m 18s\tremaining: 31.6s\n",
      "891:\tlearn: 0.4718840\ttotal: 4m 18s\tremaining: 31.3s\n",
      "892:\tlearn: 0.4718666\ttotal: 4m 18s\tremaining: 31s\n",
      "893:\tlearn: 0.4718523\ttotal: 4m 19s\tremaining: 30.7s\n",
      "894:\tlearn: 0.4718472\ttotal: 4m 19s\tremaining: 30.4s\n",
      "895:\tlearn: 0.4718382\ttotal: 4m 19s\tremaining: 30.2s\n",
      "896:\tlearn: 0.4718227\ttotal: 4m 20s\tremaining: 29.9s\n",
      "897:\tlearn: 0.4718154\ttotal: 4m 20s\tremaining: 29.6s\n",
      "898:\tlearn: 0.4717985\ttotal: 4m 20s\tremaining: 29.3s\n",
      "899:\tlearn: 0.4717909\ttotal: 4m 20s\tremaining: 29s\n",
      "900:\tlearn: 0.4717861\ttotal: 4m 21s\tremaining: 28.7s\n",
      "901:\tlearn: 0.4717796\ttotal: 4m 21s\tremaining: 28.4s\n",
      "902:\tlearn: 0.4717726\ttotal: 4m 21s\tremaining: 28.1s\n",
      "903:\tlearn: 0.4717645\ttotal: 4m 21s\tremaining: 27.8s\n",
      "904:\tlearn: 0.4717547\ttotal: 4m 22s\tremaining: 27.5s\n",
      "905:\tlearn: 0.4717408\ttotal: 4m 22s\tremaining: 27.2s\n",
      "906:\tlearn: 0.4717358\ttotal: 4m 22s\tremaining: 26.9s\n",
      "907:\tlearn: 0.4717220\ttotal: 4m 22s\tremaining: 26.6s\n",
      "908:\tlearn: 0.4717142\ttotal: 4m 23s\tremaining: 26.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "909:\tlearn: 0.4717107\ttotal: 4m 23s\tremaining: 26.1s\n",
      "910:\tlearn: 0.4717044\ttotal: 4m 23s\tremaining: 25.8s\n",
      "911:\tlearn: 0.4717013\ttotal: 4m 23s\tremaining: 25.5s\n",
      "912:\tlearn: 0.4716944\ttotal: 4m 24s\tremaining: 25.2s\n",
      "913:\tlearn: 0.4716879\ttotal: 4m 24s\tremaining: 24.9s\n",
      "914:\tlearn: 0.4716543\ttotal: 4m 24s\tremaining: 24.6s\n",
      "915:\tlearn: 0.4716474\ttotal: 4m 24s\tremaining: 24.3s\n",
      "916:\tlearn: 0.4716411\ttotal: 4m 25s\tremaining: 24s\n",
      "917:\tlearn: 0.4716287\ttotal: 4m 25s\tremaining: 23.7s\n",
      "918:\tlearn: 0.4716219\ttotal: 4m 25s\tremaining: 23.4s\n",
      "919:\tlearn: 0.4716119\ttotal: 4m 25s\tremaining: 23.1s\n",
      "920:\tlearn: 0.4716025\ttotal: 4m 26s\tremaining: 22.8s\n",
      "921:\tlearn: 0.4715980\ttotal: 4m 26s\tremaining: 22.5s\n",
      "922:\tlearn: 0.4715876\ttotal: 4m 26s\tremaining: 22.3s\n",
      "923:\tlearn: 0.4715756\ttotal: 4m 27s\tremaining: 22s\n",
      "924:\tlearn: 0.4715558\ttotal: 4m 27s\tremaining: 21.7s\n",
      "925:\tlearn: 0.4715512\ttotal: 4m 27s\tremaining: 21.4s\n",
      "926:\tlearn: 0.4715465\ttotal: 4m 27s\tremaining: 21.1s\n",
      "927:\tlearn: 0.4715340\ttotal: 4m 28s\tremaining: 20.8s\n",
      "928:\tlearn: 0.4715208\ttotal: 4m 28s\tremaining: 20.5s\n",
      "929:\tlearn: 0.4715123\ttotal: 4m 28s\tremaining: 20.2s\n",
      "930:\tlearn: 0.4714881\ttotal: 4m 29s\tremaining: 19.9s\n",
      "931:\tlearn: 0.4714808\ttotal: 4m 29s\tremaining: 19.6s\n",
      "932:\tlearn: 0.4714674\ttotal: 4m 29s\tremaining: 19.4s\n",
      "933:\tlearn: 0.4714590\ttotal: 4m 29s\tremaining: 19.1s\n",
      "934:\tlearn: 0.4714553\ttotal: 4m 30s\tremaining: 18.8s\n",
      "935:\tlearn: 0.4714521\ttotal: 4m 30s\tremaining: 18.5s\n",
      "936:\tlearn: 0.4714460\ttotal: 4m 30s\tremaining: 18.2s\n",
      "937:\tlearn: 0.4714426\ttotal: 4m 30s\tremaining: 17.9s\n",
      "938:\tlearn: 0.4714282\ttotal: 4m 31s\tremaining: 17.6s\n",
      "939:\tlearn: 0.4714118\ttotal: 4m 31s\tremaining: 17.3s\n",
      "940:\tlearn: 0.4714072\ttotal: 4m 32s\tremaining: 17.1s\n",
      "941:\tlearn: 0.4713883\ttotal: 4m 32s\tremaining: 16.8s\n",
      "942:\tlearn: 0.4713749\ttotal: 4m 32s\tremaining: 16.5s\n",
      "943:\tlearn: 0.4713680\ttotal: 4m 33s\tremaining: 16.2s\n",
      "944:\tlearn: 0.4713572\ttotal: 4m 33s\tremaining: 15.9s\n",
      "945:\tlearn: 0.4713477\ttotal: 4m 34s\tremaining: 15.7s\n",
      "946:\tlearn: 0.4713424\ttotal: 4m 34s\tremaining: 15.4s\n",
      "947:\tlearn: 0.4713345\ttotal: 4m 35s\tremaining: 15.1s\n",
      "948:\tlearn: 0.4713298\ttotal: 4m 35s\tremaining: 14.8s\n",
      "949:\tlearn: 0.4713196\ttotal: 4m 36s\tremaining: 14.5s\n",
      "950:\tlearn: 0.4713155\ttotal: 4m 36s\tremaining: 14.2s\n",
      "951:\tlearn: 0.4712759\ttotal: 4m 37s\tremaining: 14s\n",
      "952:\tlearn: 0.4712711\ttotal: 4m 37s\tremaining: 13.7s\n",
      "953:\tlearn: 0.4712572\ttotal: 4m 38s\tremaining: 13.4s\n",
      "954:\tlearn: 0.4712450\ttotal: 4m 38s\tremaining: 13.1s\n",
      "955:\tlearn: 0.4712356\ttotal: 4m 38s\tremaining: 12.8s\n",
      "956:\tlearn: 0.4711939\ttotal: 4m 39s\tremaining: 12.6s\n",
      "957:\tlearn: 0.4711891\ttotal: 4m 39s\tremaining: 12.3s\n",
      "958:\tlearn: 0.4711856\ttotal: 4m 40s\tremaining: 12s\n",
      "959:\tlearn: 0.4711827\ttotal: 4m 40s\tremaining: 11.7s\n",
      "960:\tlearn: 0.4711464\ttotal: 4m 40s\tremaining: 11.4s\n",
      "961:\tlearn: 0.4711195\ttotal: 4m 41s\tremaining: 11.1s\n",
      "962:\tlearn: 0.4711143\ttotal: 4m 41s\tremaining: 10.8s\n",
      "963:\tlearn: 0.4710894\ttotal: 4m 42s\tremaining: 10.5s\n",
      "964:\tlearn: 0.4710783\ttotal: 4m 42s\tremaining: 10.2s\n",
      "965:\tlearn: 0.4710655\ttotal: 4m 42s\tremaining: 9.95s\n",
      "966:\tlearn: 0.4710530\ttotal: 4m 42s\tremaining: 9.65s\n",
      "967:\tlearn: 0.4710488\ttotal: 4m 43s\tremaining: 9.36s\n",
      "968:\tlearn: 0.4710362\ttotal: 4m 43s\tremaining: 9.07s\n",
      "969:\tlearn: 0.4710236\ttotal: 4m 43s\tremaining: 8.77s\n",
      "970:\tlearn: 0.4710194\ttotal: 4m 43s\tremaining: 8.48s\n",
      "971:\tlearn: 0.4710084\ttotal: 4m 44s\tremaining: 8.19s\n",
      "972:\tlearn: 0.4710030\ttotal: 4m 44s\tremaining: 7.89s\n",
      "973:\tlearn: 0.4709911\ttotal: 4m 44s\tremaining: 7.6s\n",
      "974:\tlearn: 0.4709868\ttotal: 4m 45s\tremaining: 7.31s\n",
      "975:\tlearn: 0.4709820\ttotal: 4m 45s\tremaining: 7.02s\n",
      "976:\tlearn: 0.4709699\ttotal: 4m 45s\tremaining: 6.72s\n",
      "977:\tlearn: 0.4709647\ttotal: 4m 45s\tremaining: 6.43s\n",
      "978:\tlearn: 0.4709580\ttotal: 4m 46s\tremaining: 6.14s\n",
      "979:\tlearn: 0.4709357\ttotal: 4m 46s\tremaining: 5.84s\n",
      "980:\tlearn: 0.4709232\ttotal: 4m 46s\tremaining: 5.55s\n",
      "981:\tlearn: 0.4709195\ttotal: 4m 46s\tremaining: 5.26s\n",
      "982:\tlearn: 0.4709138\ttotal: 4m 47s\tremaining: 4.97s\n",
      "983:\tlearn: 0.4709071\ttotal: 4m 47s\tremaining: 4.67s\n",
      "984:\tlearn: 0.4708929\ttotal: 4m 47s\tremaining: 4.38s\n",
      "985:\tlearn: 0.4708885\ttotal: 4m 47s\tremaining: 4.09s\n",
      "986:\tlearn: 0.4708839\ttotal: 4m 48s\tremaining: 3.8s\n",
      "987:\tlearn: 0.4708769\ttotal: 4m 48s\tremaining: 3.5s\n",
      "988:\tlearn: 0.4708702\ttotal: 4m 48s\tremaining: 3.21s\n",
      "989:\tlearn: 0.4708613\ttotal: 4m 49s\tremaining: 2.92s\n",
      "990:\tlearn: 0.4708560\ttotal: 4m 49s\tremaining: 2.63s\n",
      "991:\tlearn: 0.4708194\ttotal: 4m 50s\tremaining: 2.34s\n",
      "992:\tlearn: 0.4707910\ttotal: 4m 50s\tremaining: 2.05s\n",
      "993:\tlearn: 0.4707838\ttotal: 4m 50s\tremaining: 1.75s\n",
      "994:\tlearn: 0.4707721\ttotal: 4m 50s\tremaining: 1.46s\n",
      "995:\tlearn: 0.4707592\ttotal: 4m 51s\tremaining: 1.17s\n",
      "996:\tlearn: 0.4707529\ttotal: 4m 51s\tremaining: 877ms\n",
      "997:\tlearn: 0.4707440\ttotal: 4m 51s\tremaining: 585ms\n",
      "998:\tlearn: 0.4707359\ttotal: 4m 52s\tremaining: 292ms\n",
      "999:\tlearn: 0.4707301\ttotal: 4m 52s\tremaining: 0us\n",
      "CatBoostClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.63      0.73     15014\n",
      "         1.0       0.71      0.91      0.80     15004\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     30018\n",
      "   macro avg       0.79      0.77      0.76     30018\n",
      "weighted avg       0.79      0.77      0.76     30018\n",
      "\n",
      "[[ 9406  5608]\n",
      " [ 1304 13700]]\n",
      "Accuracy is  76.97381571057366\n",
      "Time on model's work: 318.586 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.63      0.73     15014\n",
      "         1.0       0.71      0.90      0.80     15004\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     30018\n",
      "   macro avg       0.79      0.77      0.76     30018\n",
      "weighted avg       0.79      0.77      0.76     30018\n",
      "\n",
      "[[ 9529  5485]\n",
      " [ 1478 13526]]\n",
      "Accuracy is  76.80391764941035\n",
      "Time on model's work: 3.672 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.62      0.73     15014\n",
      "         1.0       0.71      0.92      0.80     15004\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     30018\n",
      "   macro avg       0.79      0.77      0.76     30018\n",
      "weighted avg       0.79      0.77      0.76     30018\n",
      "\n",
      "[[ 9337  5677]\n",
      " [ 1265 13739]]\n",
      "Accuracy is  76.87387567459524\n",
      "Time on model's work: 2.009 s\n",
      "====================================================================================================\n",
      "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-54f4fb45672f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclfs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tffm\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, pos_class_weight, n_epochs, show_progress)\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_class_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpos_class_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mused_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_preprocess_sample_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_class_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mused_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mused_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mused_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_progress\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_batch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tffm\\base.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X_, y_, w_, n_epochs, show_progress)\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;31m# iterate over batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mbX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbW\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m                 \u001b[0mfd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_to_feeddict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                 \u001b[0mops_to_run\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clfs = [\n",
    "        ['RandomForestClassifier', RandomForestClassifier()],\n",
    "        ['GradientBoostingClassifier', GradientBoostingClassifier()],\n",
    "        ['ExtraTreesClassifier', ExtraTreesClassifier()],\n",
    "        ['AdaBoostClassifier', AdaBoostClassifier()],\n",
    "        ['BaggingClassifier', BaggingClassifier()],\n",
    "        ['DecisionTreeClassifier', DecisionTreeClassifier()],\n",
    "        ['MLPClassifier', MLPClassifier()],\n",
    "        ['XGBClassifier', XGBClassifier()],\n",
    "        ['CatBoostClassifier', CatBoostClassifier()],\n",
    "        ['LogisticRegression', LogisticRegression()],\n",
    "        ['SGDClassifier', SGDClassifier()],\n",
    "        ['TFFMClassifier', TFFMClassifier()]\n",
    "       ]\n",
    "t = time()\n",
    "for name, clf in clfs:\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    prediction = clf.predict(X_test)\n",
    "    print(name)\n",
    "    print(classification_report(y_test, prediction))\n",
    "    print(confusion_matrix(y_test, prediction))\n",
    "    print('Accuracy is ', accuracy_score(y_test, prediction)*100)\n",
    "    print (\"Time on model's work:\", round(time()-t0, 3), \"s\")\n",
    "    print('='*100)\n",
    "print (\"TOTAL TIME SPENT: \", round(time()-t, 3), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFFM sparse\n",
    "# only CSR format supported\n",
    "X_train_sparse = sp.csr_matrix(X_train)\n",
    "X_test_sparse = sp.csr_matrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:33<00:00,  1.51epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[order=2] accuracy: 0.7661736291558399\n",
      "[[ 9576  5438]\n",
      " [ 1581 13423]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.64      0.73     15014\n",
      "         1.0       0.71      0.89      0.79     15004\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     30018\n",
      "   macro avg       0.78      0.77      0.76     30018\n",
      "weighted avg       0.79      0.77      0.76     30018\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [01:08<00:00,  1.31s/epoch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[order=3] accuracy: 0.7636085015657272\n",
      "[[ 9817  5197]\n",
      " [ 1899 13105]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.65      0.73     15014\n",
      "         1.0       0.72      0.87      0.79     15004\n",
      "\n",
      "   micro avg       0.76      0.76      0.76     30018\n",
      "   macro avg       0.78      0.76      0.76     30018\n",
      "weighted avg       0.78      0.76      0.76     30018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# input_type='sparse' /// rank == 10\n",
    "for order in [2, 3]:\n",
    "    model = TFFMClassifier(\n",
    "        order=order, \n",
    "        rank=10, \n",
    "        optimizer=tf.train.AdamOptimizer(learning_rate=0.001), \n",
    "        n_epochs=50, \n",
    "        batch_size=512,\n",
    "        init_std=0.001,\n",
    "        reg=0.01,\n",
    "        input_type='sparse',\n",
    "        #log_dir='./tmp/logs',\n",
    "        #verbose=1,\n",
    "        seed=42\n",
    "    )\n",
    "    model.fit(X_train_sparse, y_train, show_progress=True)\n",
    "    predictions = model.predict(X_test_sparse)\n",
    "    print('[order={}] accuracy: {}'.format(order, accuracy_score(y_test, predictions)))\n",
    "    # this will close tf.Session and free resources\n",
    "    print(confusion_matrix(y_test,predictions)) \n",
    "    print(classification_report(y_test, predictions))\n",
    "    model.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:30<00:00,  1.67epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[order=3] accuracy: 0.7656073022852955\n",
      "[[ 9270  5744]\n",
      " [ 1292 13712]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.62      0.72     15014\n",
      "         1.0       0.70      0.91      0.80     15004\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     30018\n",
      "   macro avg       0.79      0.77      0.76     30018\n",
      "weighted avg       0.79      0.77      0.76     30018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FtrlOptimizer\n",
    "# input_type='sparse' /// rank == 10\n",
    "model = TFFMClassifier(\n",
    "    order=2, \n",
    "    rank=10, \n",
    "    optimizer=tf.train.FtrlOptimizer(0.01, l2_regularization_strength=0.001, l2_shrinkage_regularization_strength=0.001), \n",
    "    n_epochs=50, \n",
    "    batch_size=1024,\n",
    "    init_std=0.001,\n",
    "    reg=0.01,\n",
    "    input_type='sparse',\n",
    "    #log_dir='./tmp/logs',\n",
    "    #verbose=1,\n",
    "    seed=42\n",
    ")\n",
    "model.fit(X_train_sparse, y_train, show_progress=True)\n",
    "predictions = model.predict(X_test_sparse)\n",
    "print('[order={}] accuracy: {}'.format(order, accuracy_score(y_test, predictions)))\n",
    "# this will close tf.Session and free resources\n",
    "print(confusion_matrix(y_test,predictions)) \n",
    "print(classification_report(y_test, predictions))\n",
    "model.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:29<00:00,  1.69epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[order=3] accuracy: 0.765240855486708\n",
      "[[ 9499  5515]\n",
      " [ 1532 13472]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.63      0.73     15014\n",
      "         1.0       0.71      0.90      0.79     15004\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     30018\n",
      "   macro avg       0.79      0.77      0.76     30018\n",
      "weighted avg       0.79      0.77      0.76     30018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sample_weight='balanced'\n",
    "model = TFFMClassifier(\n",
    "    order=2,\n",
    "    sample_weight='balanced',\n",
    "    rank=10, \n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate=0.001), \n",
    "    n_epochs=50, \n",
    "    batch_size=1024,\n",
    "    init_std=0.001,\n",
    "    reg=0.01,\n",
    "    input_type='sparse',\n",
    "    #log_dir='./tmp/logs',\n",
    "    #verbose=1,\n",
    "    seed=42\n",
    ")\n",
    "model.fit(X_train_sparse, y_train, show_progress=True)\n",
    "predictions = model.predict(X_test_sparse)\n",
    "print('[order={}] accuracy: {}'.format(order, accuracy_score(y_test, predictions)))\n",
    "# this will close tf.Session and free resources\n",
    "print(confusion_matrix(y_test,predictions)) \n",
    "print(classification_report(y_test, predictions))\n",
    "model.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:12<00:00,  1.67epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[order=3] accuracy: 0.7667399560263841\n",
      "[[ 9483  5531]\n",
      " [ 1471 13533]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.63      0.73     15014\n",
      "         1.0       0.71      0.90      0.79     15004\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     30018\n",
      "   macro avg       0.79      0.77      0.76     30018\n",
      "weighted avg       0.79      0.77      0.76     30018\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:12<00:00,  1.65epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[order=3] accuracy: 0.7668065827170365\n",
      "[[ 9148  5866]\n",
      " [ 1134 13870]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.61      0.72     15014\n",
      "         1.0       0.70      0.92      0.80     15004\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     30018\n",
      "   macro avg       0.80      0.77      0.76     30018\n",
      "weighted avg       0.80      0.77      0.76     30018\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:12<00:00,  1.68epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[order=3] accuracy: 0.7590112599107203\n",
      "[[ 8781  6233]\n",
      " [ 1001 14003]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.58      0.71     15014\n",
      "         1.0       0.69      0.93      0.79     15004\n",
      "\n",
      "   micro avg       0.76      0.76      0.76     30018\n",
      "   macro avg       0.79      0.76      0.75     30018\n",
      "weighted avg       0.79      0.76      0.75     30018\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:12<00:00,  1.67epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[order=3] accuracy: 0.7485508694783131\n",
      "[[ 8341  6673]\n",
      " [  875 14129]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.56      0.69     15014\n",
      "         1.0       0.68      0.94      0.79     15004\n",
      "\n",
      "   micro avg       0.75      0.75      0.75     30018\n",
      "   macro avg       0.79      0.75      0.74     30018\n",
      "weighted avg       0.79      0.75      0.74     30018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# weight - optional\n",
    "pos_class_weight = list(map(float,range(1, 5)))\n",
    "for weight in pos_class_weight:\n",
    "    model = TFFMClassifier(\n",
    "        order=2,\n",
    "        pos_class_weight=weight,\n",
    "        rank=10, \n",
    "        optimizer=tf.train.AdamOptimizer(learning_rate=0.001), \n",
    "        n_epochs=20, \n",
    "        batch_size=1024,\n",
    "        init_std=0.001,\n",
    "        reg=0.01,\n",
    "        input_type='sparse',\n",
    "        #log_dir='./tmp/logs',\n",
    "        #verbose=1,\n",
    "        seed=42\n",
    "    )\n",
    "    model.fit(X_train_sparse, y_train, show_progress=True)\n",
    "    predictions = model.predict(X_test_sparse)\n",
    "    print('[order={}] accuracy: {}'.format(order, accuracy_score(y_test, predictions)))\n",
    "    # this will close tf.Session and free resources\n",
    "    print(confusion_matrix(y_test,predictions)) \n",
    "    print(classification_report(y_test, predictions))\n",
    "    model.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [01:01<00:00,  1.68epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[order=3] accuracy: 0.7647411553068159\n",
      "[[ 9134  5880]\n",
      " [ 1182 13822]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.61      0.72     15014\n",
      "         1.0       0.70      0.92      0.80     15004\n",
      "\n",
      "   micro avg       0.76      0.76      0.76     30018\n",
      "   macro avg       0.79      0.76      0.76     30018\n",
      "weighted avg       0.79      0.76      0.76     30018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# weight - 2.0 best\n",
    "model = TFFMClassifier(\n",
    "    order=2,\n",
    "    pos_class_weight=2.0,\n",
    "    rank=10, \n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate=0.001), \n",
    "    n_epochs=100, \n",
    "    batch_size=1024,\n",
    "    init_std=0.001,\n",
    "    reg=0.01,\n",
    "    input_type='sparse',\n",
    "    #log_dir='./tmp/logs',\n",
    "    #verbose=1,\n",
    "    seed=42\n",
    ")\n",
    "model.fit(X_train_sparse, y_train, show_progress=True)\n",
    "predictions = model.predict(X_test_sparse)\n",
    "print('[order={}] accuracy: {}'.format(order, accuracy_score(y_test, predictions)))\n",
    "# this will close tf.Session and free resources\n",
    "print(confusion_matrix(y_test,predictions)) \n",
    "print(classification_report(y_test, predictions))\n",
    "model.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [01:00<00:00,  1.65epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[order=3] accuracy: 0.7683056832567127\n",
      "[[ 9243  5771]\n",
      " [ 1184 13820]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.62      0.73     15014\n",
      "         1.0       0.71      0.92      0.80     15004\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     30018\n",
      "   macro avg       0.80      0.77      0.76     30018\n",
      "weighted avg       0.80      0.77      0.76     30018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BEST OPTIMIZED TFFM\n",
    "model = TFFMClassifier(\n",
    "    order=2, \n",
    "    rank=10,\n",
    "    pos_class_weight=2.0,\n",
    "    optimizer=tf.train.FtrlOptimizer(0.01, l2_regularization_strength=0.001, l2_shrinkage_regularization_strength=0.001), \n",
    "    n_epochs=100, \n",
    "    batch_size=1024,\n",
    "    init_std=0.001,\n",
    "    reg=0.01,\n",
    "    input_type='sparse',\n",
    "    #log_dir='./tmp/logs',\n",
    "    #verbose=1,\n",
    "    seed=42\n",
    ")\n",
    "model.fit(X_train_sparse, y_train, show_progress=True)\n",
    "predictions = model.predict(X_test_sparse)\n",
    "print('[order={}] accuracy: {}'.format(order, accuracy_score(y_test, predictions)))\n",
    "# this will close tf.Session and free resources\n",
    "print(confusion_matrix(y_test,predictions)) \n",
    "print(classification_report(y_test, predictions))\n",
    "model.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "70041/70041 [==============================] - ETA: 1:30 - loss: 1.0082 - acc: 0.543 - ETA: 27s - loss: 0.8918 - acc: 0.527 - ETA: 17s - loss: 0.8464 - acc: 0.50 - ETA: 13s - loss: 0.8150 - acc: 0.50 - ETA: 11s - loss: 0.7941 - acc: 0.51 - ETA: 10s - loss: 0.7803 - acc: 0.51 - ETA: 9s - loss: 0.7692 - acc: 0.5175 - ETA: 8s - loss: 0.7596 - acc: 0.522 - ETA: 8s - loss: 0.7533 - acc: 0.527 - ETA: 7s - loss: 0.7443 - acc: 0.534 - ETA: 7s - loss: 0.7385 - acc: 0.537 - ETA: 7s - loss: 0.7349 - acc: 0.537 - ETA: 6s - loss: 0.7301 - acc: 0.541 - ETA: 6s - loss: 0.7242 - acc: 0.547 - ETA: 6s - loss: 0.7200 - acc: 0.549 - ETA: 6s - loss: 0.7152 - acc: 0.554 - ETA: 6s - loss: 0.7113 - acc: 0.558 - ETA: 5s - loss: 0.7077 - acc: 0.562 - ETA: 5s - loss: 0.7056 - acc: 0.563 - ETA: 5s - loss: 0.7022 - acc: 0.568 - ETA: 5s - loss: 0.6993 - acc: 0.571 - ETA: 5s - loss: 0.6963 - acc: 0.575 - ETA: 5s - loss: 0.6929 - acc: 0.579 - ETA: 5s - loss: 0.6906 - acc: 0.582 - ETA: 5s - loss: 0.6873 - acc: 0.585 - ETA: 4s - loss: 0.6844 - acc: 0.589 - ETA: 4s - loss: 0.6812 - acc: 0.593 - ETA: 4s - loss: 0.6792 - acc: 0.595 - ETA: 4s - loss: 0.6759 - acc: 0.599 - ETA: 4s - loss: 0.6731 - acc: 0.602 - ETA: 4s - loss: 0.6707 - acc: 0.605 - ETA: 4s - loss: 0.6682 - acc: 0.609 - ETA: 4s - loss: 0.6651 - acc: 0.613 - ETA: 4s - loss: 0.6621 - acc: 0.616 - ETA: 4s - loss: 0.6602 - acc: 0.619 - ETA: 4s - loss: 0.6593 - acc: 0.620 - ETA: 4s - loss: 0.6578 - acc: 0.622 - ETA: 4s - loss: 0.6564 - acc: 0.624 - ETA: 3s - loss: 0.6543 - acc: 0.626 - ETA: 3s - loss: 0.6520 - acc: 0.629 - ETA: 3s - loss: 0.6497 - acc: 0.632 - ETA: 3s - loss: 0.6483 - acc: 0.634 - ETA: 3s - loss: 0.6470 - acc: 0.635 - ETA: 3s - loss: 0.6450 - acc: 0.638 - ETA: 3s - loss: 0.6432 - acc: 0.639 - ETA: 3s - loss: 0.6422 - acc: 0.640 - ETA: 3s - loss: 0.6404 - acc: 0.642 - ETA: 3s - loss: 0.6395 - acc: 0.644 - ETA: 3s - loss: 0.6385 - acc: 0.645 - ETA: 3s - loss: 0.6380 - acc: 0.647 - ETA: 2s - loss: 0.6369 - acc: 0.648 - ETA: 2s - loss: 0.6358 - acc: 0.650 - ETA: 2s - loss: 0.6344 - acc: 0.651 - ETA: 2s - loss: 0.6341 - acc: 0.652 - ETA: 2s - loss: 0.6333 - acc: 0.653 - ETA: 2s - loss: 0.6325 - acc: 0.654 - ETA: 2s - loss: 0.6312 - acc: 0.655 - ETA: 2s - loss: 0.6301 - acc: 0.656 - ETA: 2s - loss: 0.6293 - acc: 0.658 - ETA: 2s - loss: 0.6283 - acc: 0.659 - ETA: 2s - loss: 0.6276 - acc: 0.660 - ETA: 2s - loss: 0.6266 - acc: 0.661 - ETA: 2s - loss: 0.6255 - acc: 0.662 - ETA: 2s - loss: 0.6241 - acc: 0.664 - ETA: 2s - loss: 0.6235 - acc: 0.664 - ETA: 2s - loss: 0.6230 - acc: 0.665 - ETA: 1s - loss: 0.6217 - acc: 0.666 - ETA: 1s - loss: 0.6205 - acc: 0.667 - ETA: 1s - loss: 0.6195 - acc: 0.669 - ETA: 1s - loss: 0.6194 - acc: 0.669 - ETA: 1s - loss: 0.6189 - acc: 0.670 - ETA: 1s - loss: 0.6185 - acc: 0.670 - ETA: 1s - loss: 0.6184 - acc: 0.670 - ETA: 1s - loss: 0.6174 - acc: 0.672 - ETA: 1s - loss: 0.6170 - acc: 0.672 - ETA: 1s - loss: 0.6159 - acc: 0.673 - ETA: 1s - loss: 0.6156 - acc: 0.674 - ETA: 1s - loss: 0.6150 - acc: 0.675 - ETA: 1s - loss: 0.6147 - acc: 0.675 - ETA: 1s - loss: 0.6143 - acc: 0.676 - ETA: 1s - loss: 0.6140 - acc: 0.677 - ETA: 1s - loss: 0.6134 - acc: 0.677 - ETA: 1s - loss: 0.6125 - acc: 0.678 - ETA: 1s - loss: 0.6121 - acc: 0.679 - ETA: 0s - loss: 0.6119 - acc: 0.679 - ETA: 0s - loss: 0.6114 - acc: 0.680 - ETA: 0s - loss: 0.6108 - acc: 0.680 - ETA: 0s - loss: 0.6103 - acc: 0.681 - ETA: 0s - loss: 0.6097 - acc: 0.681 - ETA: 0s - loss: 0.6093 - acc: 0.682 - ETA: 0s - loss: 0.6088 - acc: 0.683 - ETA: 0s - loss: 0.6081 - acc: 0.683 - ETA: 0s - loss: 0.6078 - acc: 0.684 - ETA: 0s - loss: 0.6073 - acc: 0.684 - ETA: 0s - loss: 0.6066 - acc: 0.685 - ETA: 0s - loss: 0.6065 - acc: 0.685 - ETA: 0s - loss: 0.6062 - acc: 0.685 - ETA: 0s - loss: 0.6055 - acc: 0.686 - ETA: 0s - loss: 0.6051 - acc: 0.687 - 6s 92us/step - loss: 0.6044 - acc: 0.6878\n",
      "Epoch 2/20\n",
      "70041/70041 [==============================] - ETA: 4s - loss: 0.5123 - acc: 0.765 - ETA: 7s - loss: 0.5208 - acc: 0.764 - ETA: 7s - loss: 0.5408 - acc: 0.753 - ETA: 7s - loss: 0.5411 - acc: 0.748 - ETA: 7s - loss: 0.5471 - acc: 0.740 - ETA: 6s - loss: 0.5471 - acc: 0.741 - ETA: 6s - loss: 0.5532 - acc: 0.736 - ETA: 6s - loss: 0.5495 - acc: 0.738 - ETA: 5s - loss: 0.5532 - acc: 0.735 - ETA: 5s - loss: 0.5549 - acc: 0.734 - ETA: 5s - loss: 0.5578 - acc: 0.732 - ETA: 5s - loss: 0.5576 - acc: 0.734 - ETA: 5s - loss: 0.5566 - acc: 0.734 - ETA: 5s - loss: 0.5561 - acc: 0.735 - ETA: 5s - loss: 0.5544 - acc: 0.737 - ETA: 4s - loss: 0.5566 - acc: 0.734 - ETA: 4s - loss: 0.5548 - acc: 0.736 - ETA: 4s - loss: 0.5540 - acc: 0.737 - ETA: 4s - loss: 0.5545 - acc: 0.737 - ETA: 4s - loss: 0.5550 - acc: 0.736 - ETA: 4s - loss: 0.5542 - acc: 0.737 - ETA: 4s - loss: 0.5536 - acc: 0.738 - ETA: 4s - loss: 0.5545 - acc: 0.738 - ETA: 4s - loss: 0.5544 - acc: 0.738 - ETA: 4s - loss: 0.5539 - acc: 0.738 - ETA: 4s - loss: 0.5530 - acc: 0.739 - ETA: 4s - loss: 0.5515 - acc: 0.740 - ETA: 4s - loss: 0.5509 - acc: 0.741 - ETA: 4s - loss: 0.5505 - acc: 0.742 - ETA: 3s - loss: 0.5494 - acc: 0.742 - ETA: 3s - loss: 0.5486 - acc: 0.743 - ETA: 3s - loss: 0.5486 - acc: 0.744 - ETA: 3s - loss: 0.5496 - acc: 0.744 - ETA: 3s - loss: 0.5485 - acc: 0.745 - ETA: 3s - loss: 0.5478 - acc: 0.745 - ETA: 3s - loss: 0.5483 - acc: 0.744 - ETA: 3s - loss: 0.5482 - acc: 0.745 - ETA: 3s - loss: 0.5485 - acc: 0.744 - ETA: 3s - loss: 0.5484 - acc: 0.744 - ETA: 3s - loss: 0.5483 - acc: 0.744 - ETA: 3s - loss: 0.5482 - acc: 0.744 - ETA: 3s - loss: 0.5475 - acc: 0.745 - ETA: 3s - loss: 0.5479 - acc: 0.745 - ETA: 3s - loss: 0.5486 - acc: 0.744 - ETA: 2s - loss: 0.5487 - acc: 0.744 - ETA: 2s - loss: 0.5477 - acc: 0.745 - ETA: 2s - loss: 0.5476 - acc: 0.745 - ETA: 2s - loss: 0.5471 - acc: 0.745 - ETA: 2s - loss: 0.5475 - acc: 0.745 - ETA: 2s - loss: 0.5480 - acc: 0.744 - ETA: 2s - loss: 0.5479 - acc: 0.744 - ETA: 2s - loss: 0.5475 - acc: 0.745 - ETA: 2s - loss: 0.5472 - acc: 0.745 - ETA: 2s - loss: 0.5463 - acc: 0.746 - ETA: 2s - loss: 0.5458 - acc: 0.746 - ETA: 2s - loss: 0.5456 - acc: 0.746 - ETA: 2s - loss: 0.5456 - acc: 0.747 - ETA: 2s - loss: 0.5456 - acc: 0.747 - ETA: 2s - loss: 0.5455 - acc: 0.747 - ETA: 2s - loss: 0.5454 - acc: 0.747 - ETA: 1s - loss: 0.5456 - acc: 0.747 - ETA: 1s - loss: 0.5454 - acc: 0.747 - ETA: 1s - loss: 0.5452 - acc: 0.747 - ETA: 1s - loss: 0.5449 - acc: 0.747 - ETA: 1s - loss: 0.5451 - acc: 0.747 - ETA: 1s - loss: 0.5455 - acc: 0.746 - ETA: 1s - loss: 0.5456 - acc: 0.746 - ETA: 1s - loss: 0.5455 - acc: 0.746 - ETA: 1s - loss: 0.5453 - acc: 0.746 - ETA: 1s - loss: 0.5450 - acc: 0.747 - ETA: 1s - loss: 0.5446 - acc: 0.747 - ETA: 1s - loss: 0.5443 - acc: 0.747 - ETA: 1s - loss: 0.5439 - acc: 0.747 - ETA: 1s - loss: 0.5438 - acc: 0.747 - ETA: 1s - loss: 0.5437 - acc: 0.747 - ETA: 1s - loss: 0.5436 - acc: 0.748 - ETA: 0s - loss: 0.5433 - acc: 0.748 - ETA: 0s - loss: 0.5428 - acc: 0.748 - ETA: 0s - loss: 0.5427 - acc: 0.748 - ETA: 0s - loss: 0.5426 - acc: 0.748 - ETA: 0s - loss: 0.5423 - acc: 0.749 - ETA: 0s - loss: 0.5422 - acc: 0.749 - ETA: 0s - loss: 0.5424 - acc: 0.749 - ETA: 0s - loss: 0.5424 - acc: 0.749 - ETA: 0s - loss: 0.5423 - acc: 0.749 - ETA: 0s - loss: 0.5421 - acc: 0.749 - ETA: 0s - loss: 0.5421 - acc: 0.749 - ETA: 0s - loss: 0.5419 - acc: 0.749 - ETA: 0s - loss: 0.5418 - acc: 0.749 - ETA: 0s - loss: 0.5415 - acc: 0.749 - ETA: 0s - loss: 0.5417 - acc: 0.749 - ETA: 0s - loss: 0.5412 - acc: 0.749 - 5s 78us/step - loss: 0.5414 - acc: 0.7498\n",
      "Epoch 3/20\n",
      "70041/70041 [==============================] - ETA: 5s - loss: 0.5594 - acc: 0.730 - ETA: 5s - loss: 0.5380 - acc: 0.748 - ETA: 5s - loss: 0.5392 - acc: 0.749 - ETA: 4s - loss: 0.5343 - acc: 0.755 - ETA: 5s - loss: 0.5305 - acc: 0.758 - ETA: 4s - loss: 0.5332 - acc: 0.753 - ETA: 4s - loss: 0.5342 - acc: 0.753 - ETA: 4s - loss: 0.5317 - acc: 0.756 - ETA: 4s - loss: 0.5325 - acc: 0.754 - ETA: 4s - loss: 0.5344 - acc: 0.753 - ETA: 4s - loss: 0.5317 - acc: 0.756 - ETA: 4s - loss: 0.5307 - acc: 0.757 - ETA: 4s - loss: 0.5317 - acc: 0.756 - ETA: 4s - loss: 0.5318 - acc: 0.757 - ETA: 4s - loss: 0.5314 - acc: 0.758 - ETA: 4s - loss: 0.5324 - acc: 0.757 - ETA: 4s - loss: 0.5317 - acc: 0.758 - ETA: 4s - loss: 0.5308 - acc: 0.760 - ETA: 4s - loss: 0.5308 - acc: 0.759 - ETA: 4s - loss: 0.5293 - acc: 0.761 - ETA: 3s - loss: 0.5284 - acc: 0.761 - ETA: 3s - loss: 0.5278 - acc: 0.761 - ETA: 3s - loss: 0.5280 - acc: 0.761 - ETA: 3s - loss: 0.5273 - acc: 0.761 - ETA: 3s - loss: 0.5277 - acc: 0.760 - ETA: 3s - loss: 0.5268 - acc: 0.761 - ETA: 3s - loss: 0.5268 - acc: 0.761 - ETA: 3s - loss: 0.5274 - acc: 0.761 - ETA: 3s - loss: 0.5273 - acc: 0.761 - ETA: 3s - loss: 0.5269 - acc: 0.761 - ETA: 3s - loss: 0.5271 - acc: 0.761 - ETA: 3s - loss: 0.5267 - acc: 0.761 - ETA: 3s - loss: 0.5267 - acc: 0.761 - ETA: 3s - loss: 0.5271 - acc: 0.760 - ETA: 3s - loss: 0.5272 - acc: 0.760 - ETA: 3s - loss: 0.5266 - acc: 0.760 - ETA: 3s - loss: 0.5272 - acc: 0.760 - ETA: 3s - loss: 0.5267 - acc: 0.760 - ETA: 2s - loss: 0.5278 - acc: 0.759 - ETA: 2s - loss: 0.5290 - acc: 0.759 - ETA: 2s - loss: 0.5290 - acc: 0.759 - ETA: 2s - loss: 0.5280 - acc: 0.760 - ETA: 2s - loss: 0.5274 - acc: 0.760 - ETA: 2s - loss: 0.5281 - acc: 0.760 - ETA: 2s - loss: 0.5279 - acc: 0.760 - ETA: 2s - loss: 0.5285 - acc: 0.760 - ETA: 2s - loss: 0.5281 - acc: 0.760 - ETA: 2s - loss: 0.5276 - acc: 0.760 - ETA: 2s - loss: 0.5267 - acc: 0.761 - ETA: 2s - loss: 0.5268 - acc: 0.761 - ETA: 2s - loss: 0.5266 - acc: 0.761 - ETA: 2s - loss: 0.5262 - acc: 0.762 - ETA: 2s - loss: 0.5268 - acc: 0.762 - ETA: 2s - loss: 0.5266 - acc: 0.762 - ETA: 2s - loss: 0.5262 - acc: 0.762 - ETA: 2s - loss: 0.5255 - acc: 0.763 - ETA: 1s - loss: 0.5251 - acc: 0.763 - ETA: 1s - loss: 0.5249 - acc: 0.763 - ETA: 1s - loss: 0.5246 - acc: 0.763 - ETA: 1s - loss: 0.5244 - acc: 0.764 - ETA: 1s - loss: 0.5245 - acc: 0.764 - ETA: 1s - loss: 0.5239 - acc: 0.764 - ETA: 1s - loss: 0.5238 - acc: 0.764 - ETA: 1s - loss: 0.5240 - acc: 0.764 - ETA: 1s - loss: 0.5235 - acc: 0.764 - ETA: 1s - loss: 0.5234 - acc: 0.764 - ETA: 1s - loss: 0.5235 - acc: 0.764 - ETA: 1s - loss: 0.5239 - acc: 0.764 - ETA: 1s - loss: 0.5235 - acc: 0.765 - ETA: 1s - loss: 0.5231 - acc: 0.765 - ETA: 1s - loss: 0.5229 - acc: 0.765 - ETA: 1s - loss: 0.5227 - acc: 0.765 - ETA: 1s - loss: 0.5226 - acc: 0.765 - ETA: 1s - loss: 0.5228 - acc: 0.765 - ETA: 0s - loss: 0.5225 - acc: 0.765 - ETA: 0s - loss: 0.5224 - acc: 0.765 - ETA: 0s - loss: 0.5222 - acc: 0.765 - ETA: 0s - loss: 0.5223 - acc: 0.765 - ETA: 0s - loss: 0.5225 - acc: 0.765 - ETA: 0s - loss: 0.5223 - acc: 0.765 - ETA: 0s - loss: 0.5221 - acc: 0.765 - ETA: 0s - loss: 0.5220 - acc: 0.765 - ETA: 0s - loss: 0.5218 - acc: 0.765 - ETA: 0s - loss: 0.5216 - acc: 0.765 - ETA: 0s - loss: 0.5215 - acc: 0.766 - ETA: 0s - loss: 0.5213 - acc: 0.766 - ETA: 0s - loss: 0.5216 - acc: 0.766 - ETA: 0s - loss: 0.5216 - acc: 0.765 - ETA: 0s - loss: 0.5217 - acc: 0.765 - ETA: 0s - loss: 0.5218 - acc: 0.765 - ETA: 0s - loss: 0.5220 - acc: 0.765 - ETA: 0s - loss: 0.5219 - acc: 0.765 - 5s 73us/step - loss: 0.5218 - acc: 0.7658\n",
      "Epoch 4/20\n",
      "70041/70041 [==============================] - ETA: 5s - loss: 0.5215 - acc: 0.773 - ETA: 4s - loss: 0.4970 - acc: 0.775 - ETA: 4s - loss: 0.5060 - acc: 0.769 - ETA: 5s - loss: 0.5079 - acc: 0.769 - ETA: 5s - loss: 0.5117 - acc: 0.766 - ETA: 5s - loss: 0.5113 - acc: 0.767 - ETA: 4s - loss: 0.5124 - acc: 0.766 - ETA: 4s - loss: 0.5147 - acc: 0.765 - ETA: 4s - loss: 0.5158 - acc: 0.763 - ETA: 4s - loss: 0.5141 - acc: 0.765 - ETA: 4s - loss: 0.5160 - acc: 0.764 - ETA: 4s - loss: 0.5158 - acc: 0.764 - ETA: 4s - loss: 0.5190 - acc: 0.762 - ETA: 4s - loss: 0.5202 - acc: 0.763 - ETA: 4s - loss: 0.5185 - acc: 0.764 - ETA: 4s - loss: 0.5189 - acc: 0.765 - ETA: 4s - loss: 0.5205 - acc: 0.764 - ETA: 4s - loss: 0.5194 - acc: 0.764 - ETA: 4s - loss: 0.5201 - acc: 0.763 - ETA: 4s - loss: 0.5206 - acc: 0.763 - ETA: 3s - loss: 0.5199 - acc: 0.764 - ETA: 3s - loss: 0.5204 - acc: 0.763 - ETA: 3s - loss: 0.5201 - acc: 0.764 - ETA: 3s - loss: 0.5195 - acc: 0.765 - ETA: 3s - loss: 0.5189 - acc: 0.765 - ETA: 3s - loss: 0.5183 - acc: 0.765 - ETA: 3s - loss: 0.5178 - acc: 0.766 - ETA: 3s - loss: 0.5182 - acc: 0.765 - ETA: 3s - loss: 0.5186 - acc: 0.765 - ETA: 3s - loss: 0.5176 - acc: 0.765 - ETA: 3s - loss: 0.5179 - acc: 0.765 - ETA: 3s - loss: 0.5175 - acc: 0.766 - ETA: 3s - loss: 0.5180 - acc: 0.766 - ETA: 3s - loss: 0.5178 - acc: 0.766 - ETA: 3s - loss: 0.5174 - acc: 0.766 - ETA: 3s - loss: 0.5176 - acc: 0.766 - ETA: 3s - loss: 0.5179 - acc: 0.766 - ETA: 3s - loss: 0.5174 - acc: 0.767 - ETA: 3s - loss: 0.5169 - acc: 0.767 - ETA: 3s - loss: 0.5164 - acc: 0.767 - ETA: 3s - loss: 0.5171 - acc: 0.767 - ETA: 3s - loss: 0.5167 - acc: 0.768 - ETA: 2s - loss: 0.5163 - acc: 0.768 - ETA: 2s - loss: 0.5158 - acc: 0.768 - ETA: 2s - loss: 0.5162 - acc: 0.768 - ETA: 2s - loss: 0.5162 - acc: 0.768 - ETA: 2s - loss: 0.5162 - acc: 0.768 - ETA: 2s - loss: 0.5158 - acc: 0.768 - ETA: 2s - loss: 0.5157 - acc: 0.768 - ETA: 2s - loss: 0.5153 - acc: 0.769 - ETA: 2s - loss: 0.5154 - acc: 0.769 - ETA: 2s - loss: 0.5148 - acc: 0.769 - ETA: 2s - loss: 0.5152 - acc: 0.769 - ETA: 2s - loss: 0.5147 - acc: 0.769 - ETA: 2s - loss: 0.5143 - acc: 0.769 - ETA: 2s - loss: 0.5139 - acc: 0.770 - ETA: 2s - loss: 0.5136 - acc: 0.770 - ETA: 2s - loss: 0.5133 - acc: 0.770 - ETA: 2s - loss: 0.5136 - acc: 0.770 - ETA: 2s - loss: 0.5136 - acc: 0.771 - ETA: 2s - loss: 0.5135 - acc: 0.771 - ETA: 2s - loss: 0.5136 - acc: 0.771 - ETA: 1s - loss: 0.5135 - acc: 0.771 - ETA: 1s - loss: 0.5132 - acc: 0.771 - ETA: 1s - loss: 0.5128 - acc: 0.771 - ETA: 1s - loss: 0.5129 - acc: 0.771 - ETA: 1s - loss: 0.5124 - acc: 0.771 - ETA: 1s - loss: 0.5123 - acc: 0.772 - ETA: 1s - loss: 0.5117 - acc: 0.772 - ETA: 1s - loss: 0.5120 - acc: 0.772 - ETA: 1s - loss: 0.5125 - acc: 0.771 - ETA: 1s - loss: 0.5121 - acc: 0.772 - ETA: 1s - loss: 0.5121 - acc: 0.772 - ETA: 1s - loss: 0.5122 - acc: 0.772 - ETA: 1s - loss: 0.5122 - acc: 0.771 - ETA: 1s - loss: 0.5121 - acc: 0.771 - ETA: 1s - loss: 0.5119 - acc: 0.772 - ETA: 1s - loss: 0.5121 - acc: 0.771 - ETA: 1s - loss: 0.5118 - acc: 0.772 - ETA: 0s - loss: 0.5122 - acc: 0.771 - ETA: 0s - loss: 0.5122 - acc: 0.771 - ETA: 0s - loss: 0.5123 - acc: 0.771 - ETA: 0s - loss: 0.5122 - acc: 0.771 - ETA: 0s - loss: 0.5118 - acc: 0.771 - ETA: 0s - loss: 0.5123 - acc: 0.771 - ETA: 0s - loss: 0.5125 - acc: 0.771 - ETA: 0s - loss: 0.5123 - acc: 0.771 - ETA: 0s - loss: 0.5120 - acc: 0.771 - ETA: 0s - loss: 0.5115 - acc: 0.771 - ETA: 0s - loss: 0.5114 - acc: 0.771 - ETA: 0s - loss: 0.5112 - acc: 0.771 - ETA: 0s - loss: 0.5111 - acc: 0.771 - ETA: 0s - loss: 0.5113 - acc: 0.771 - ETA: 0s - loss: 0.5111 - acc: 0.771 - ETA: 0s - loss: 0.5116 - acc: 0.771 - ETA: 0s - loss: 0.5116 - acc: 0.771 - 6s 80us/step - loss: 0.5113 - acc: 0.7713\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70041/70041 [==============================] - ETA: 4s - loss: 0.4471 - acc: 0.824 - ETA: 4s - loss: 0.4801 - acc: 0.787 - ETA: 4s - loss: 0.4874 - acc: 0.785 - ETA: 4s - loss: 0.4974 - acc: 0.775 - ETA: 5s - loss: 0.5046 - acc: 0.770 - ETA: 5s - loss: 0.5027 - acc: 0.769 - ETA: 5s - loss: 0.5022 - acc: 0.768 - ETA: 5s - loss: 0.5055 - acc: 0.766 - ETA: 5s - loss: 0.5031 - acc: 0.768 - ETA: 5s - loss: 0.5030 - acc: 0.768 - ETA: 5s - loss: 0.5040 - acc: 0.769 - ETA: 4s - loss: 0.5034 - acc: 0.770 - ETA: 4s - loss: 0.5036 - acc: 0.770 - ETA: 4s - loss: 0.5045 - acc: 0.769 - ETA: 4s - loss: 0.5048 - acc: 0.770 - ETA: 4s - loss: 0.5048 - acc: 0.769 - ETA: 4s - loss: 0.5043 - acc: 0.769 - ETA: 4s - loss: 0.5057 - acc: 0.768 - ETA: 4s - loss: 0.5075 - acc: 0.767 - ETA: 4s - loss: 0.5062 - acc: 0.768 - ETA: 4s - loss: 0.5055 - acc: 0.769 - ETA: 4s - loss: 0.5041 - acc: 0.770 - ETA: 4s - loss: 0.5036 - acc: 0.770 - ETA: 4s - loss: 0.5033 - acc: 0.771 - ETA: 4s - loss: 0.5026 - acc: 0.772 - ETA: 3s - loss: 0.5021 - acc: 0.772 - ETA: 3s - loss: 0.5023 - acc: 0.772 - ETA: 3s - loss: 0.5023 - acc: 0.772 - ETA: 3s - loss: 0.5021 - acc: 0.772 - ETA: 3s - loss: 0.5025 - acc: 0.773 - ETA: 3s - loss: 0.5022 - acc: 0.772 - ETA: 3s - loss: 0.5014 - acc: 0.773 - ETA: 3s - loss: 0.5022 - acc: 0.773 - ETA: 3s - loss: 0.5019 - acc: 0.773 - ETA: 3s - loss: 0.5020 - acc: 0.772 - ETA: 3s - loss: 0.5025 - acc: 0.772 - ETA: 3s - loss: 0.5028 - acc: 0.772 - ETA: 3s - loss: 0.5024 - acc: 0.772 - ETA: 3s - loss: 0.5029 - acc: 0.772 - ETA: 3s - loss: 0.5026 - acc: 0.772 - ETA: 3s - loss: 0.5023 - acc: 0.772 - ETA: 2s - loss: 0.5020 - acc: 0.772 - ETA: 2s - loss: 0.5025 - acc: 0.772 - ETA: 2s - loss: 0.5025 - acc: 0.772 - ETA: 2s - loss: 0.5026 - acc: 0.772 - ETA: 2s - loss: 0.5024 - acc: 0.772 - ETA: 2s - loss: 0.5032 - acc: 0.772 - ETA: 2s - loss: 0.5035 - acc: 0.772 - ETA: 2s - loss: 0.5034 - acc: 0.772 - ETA: 2s - loss: 0.5033 - acc: 0.772 - ETA: 2s - loss: 0.5030 - acc: 0.772 - ETA: 2s - loss: 0.5032 - acc: 0.772 - ETA: 2s - loss: 0.5030 - acc: 0.772 - ETA: 2s - loss: 0.5033 - acc: 0.772 - ETA: 2s - loss: 0.5028 - acc: 0.772 - ETA: 2s - loss: 0.5025 - acc: 0.773 - ETA: 2s - loss: 0.5024 - acc: 0.773 - ETA: 2s - loss: 0.5026 - acc: 0.773 - ETA: 1s - loss: 0.5023 - acc: 0.773 - ETA: 1s - loss: 0.5025 - acc: 0.772 - ETA: 1s - loss: 0.5026 - acc: 0.772 - ETA: 1s - loss: 0.5030 - acc: 0.772 - ETA: 1s - loss: 0.5033 - acc: 0.772 - ETA: 1s - loss: 0.5035 - acc: 0.771 - ETA: 1s - loss: 0.5032 - acc: 0.772 - ETA: 1s - loss: 0.5032 - acc: 0.772 - ETA: 1s - loss: 0.5034 - acc: 0.772 - ETA: 1s - loss: 0.5032 - acc: 0.772 - ETA: 1s - loss: 0.5033 - acc: 0.772 - ETA: 1s - loss: 0.5041 - acc: 0.771 - ETA: 1s - loss: 0.5040 - acc: 0.771 - ETA: 1s - loss: 0.5036 - acc: 0.771 - ETA: 1s - loss: 0.5033 - acc: 0.771 - ETA: 1s - loss: 0.5030 - acc: 0.772 - ETA: 1s - loss: 0.5032 - acc: 0.772 - ETA: 0s - loss: 0.5033 - acc: 0.771 - ETA: 0s - loss: 0.5035 - acc: 0.772 - ETA: 0s - loss: 0.5040 - acc: 0.771 - ETA: 0s - loss: 0.5041 - acc: 0.771 - ETA: 0s - loss: 0.5042 - acc: 0.771 - ETA: 0s - loss: 0.5042 - acc: 0.771 - ETA: 0s - loss: 0.5043 - acc: 0.771 - ETA: 0s - loss: 0.5043 - acc: 0.771 - ETA: 0s - loss: 0.5043 - acc: 0.771 - ETA: 0s - loss: 0.5049 - acc: 0.771 - ETA: 0s - loss: 0.5048 - acc: 0.771 - ETA: 0s - loss: 0.5044 - acc: 0.771 - ETA: 0s - loss: 0.5045 - acc: 0.771 - ETA: 0s - loss: 0.5042 - acc: 0.771 - ETA: 0s - loss: 0.5041 - acc: 0.771 - ETA: 0s - loss: 0.5040 - acc: 0.772 - ETA: 0s - loss: 0.5039 - acc: 0.772 - 5s 75us/step - loss: 0.5039 - acc: 0.7719\n",
      "Epoch 6/20\n",
      "70041/70041 [==============================] - ETA: 4s - loss: 0.4347 - acc: 0.808 - ETA: 5s - loss: 0.4852 - acc: 0.778 - ETA: 5s - loss: 0.5045 - acc: 0.765 - ETA: 5s - loss: 0.5031 - acc: 0.764 - ETA: 5s - loss: 0.5077 - acc: 0.761 - ETA: 4s - loss: 0.5061 - acc: 0.764 - ETA: 4s - loss: 0.5032 - acc: 0.769 - ETA: 4s - loss: 0.5006 - acc: 0.771 - ETA: 4s - loss: 0.4969 - acc: 0.773 - ETA: 4s - loss: 0.4948 - acc: 0.776 - ETA: 4s - loss: 0.4958 - acc: 0.774 - ETA: 4s - loss: 0.4968 - acc: 0.773 - ETA: 4s - loss: 0.4959 - acc: 0.774 - ETA: 4s - loss: 0.4984 - acc: 0.772 - ETA: 4s - loss: 0.4994 - acc: 0.771 - ETA: 4s - loss: 0.5011 - acc: 0.768 - ETA: 4s - loss: 0.5032 - acc: 0.768 - ETA: 4s - loss: 0.5024 - acc: 0.770 - ETA: 4s - loss: 0.5041 - acc: 0.768 - ETA: 4s - loss: 0.5029 - acc: 0.769 - ETA: 4s - loss: 0.5017 - acc: 0.769 - ETA: 3s - loss: 0.5009 - acc: 0.770 - ETA: 3s - loss: 0.4997 - acc: 0.771 - ETA: 3s - loss: 0.4981 - acc: 0.773 - ETA: 3s - loss: 0.4992 - acc: 0.772 - ETA: 3s - loss: 0.5003 - acc: 0.772 - ETA: 3s - loss: 0.4989 - acc: 0.773 - ETA: 3s - loss: 0.4987 - acc: 0.772 - ETA: 3s - loss: 0.4986 - acc: 0.772 - ETA: 3s - loss: 0.4989 - acc: 0.771 - ETA: 3s - loss: 0.4995 - acc: 0.771 - ETA: 3s - loss: 0.4997 - acc: 0.771 - ETA: 3s - loss: 0.5000 - acc: 0.771 - ETA: 3s - loss: 0.5000 - acc: 0.771 - ETA: 3s - loss: 0.4997 - acc: 0.771 - ETA: 3s - loss: 0.5003 - acc: 0.770 - ETA: 3s - loss: 0.5010 - acc: 0.770 - ETA: 3s - loss: 0.5009 - acc: 0.770 - ETA: 2s - loss: 0.5010 - acc: 0.770 - ETA: 2s - loss: 0.5011 - acc: 0.770 - ETA: 2s - loss: 0.5019 - acc: 0.770 - ETA: 2s - loss: 0.5022 - acc: 0.769 - ETA: 2s - loss: 0.5014 - acc: 0.770 - ETA: 2s - loss: 0.5016 - acc: 0.770 - ETA: 2s - loss: 0.5012 - acc: 0.770 - ETA: 2s - loss: 0.5008 - acc: 0.770 - ETA: 2s - loss: 0.5005 - acc: 0.770 - ETA: 2s - loss: 0.5003 - acc: 0.771 - ETA: 2s - loss: 0.5003 - acc: 0.771 - ETA: 2s - loss: 0.5003 - acc: 0.770 - ETA: 2s - loss: 0.5007 - acc: 0.770 - ETA: 2s - loss: 0.5009 - acc: 0.770 - ETA: 2s - loss: 0.5012 - acc: 0.770 - ETA: 2s - loss: 0.5001 - acc: 0.770 - ETA: 2s - loss: 0.4994 - acc: 0.771 - ETA: 1s - loss: 0.4996 - acc: 0.771 - ETA: 1s - loss: 0.4997 - acc: 0.771 - ETA: 1s - loss: 0.4995 - acc: 0.771 - ETA: 1s - loss: 0.4994 - acc: 0.771 - ETA: 1s - loss: 0.4995 - acc: 0.771 - ETA: 1s - loss: 0.4996 - acc: 0.770 - ETA: 1s - loss: 0.4995 - acc: 0.770 - ETA: 1s - loss: 0.4998 - acc: 0.770 - ETA: 1s - loss: 0.4999 - acc: 0.770 - ETA: 1s - loss: 0.4997 - acc: 0.770 - ETA: 1s - loss: 0.4998 - acc: 0.770 - ETA: 1s - loss: 0.4995 - acc: 0.770 - ETA: 1s - loss: 0.4993 - acc: 0.771 - ETA: 1s - loss: 0.4993 - acc: 0.771 - ETA: 1s - loss: 0.4998 - acc: 0.770 - ETA: 1s - loss: 0.4995 - acc: 0.770 - ETA: 1s - loss: 0.4992 - acc: 0.770 - ETA: 1s - loss: 0.4984 - acc: 0.771 - ETA: 1s - loss: 0.4988 - acc: 0.771 - ETA: 0s - loss: 0.4989 - acc: 0.771 - ETA: 0s - loss: 0.4988 - acc: 0.771 - ETA: 0s - loss: 0.4989 - acc: 0.771 - ETA: 0s - loss: 0.4989 - acc: 0.771 - ETA: 0s - loss: 0.4988 - acc: 0.771 - ETA: 0s - loss: 0.4987 - acc: 0.771 - ETA: 0s - loss: 0.4988 - acc: 0.771 - ETA: 0s - loss: 0.4991 - acc: 0.771 - ETA: 0s - loss: 0.4990 - acc: 0.771 - ETA: 0s - loss: 0.4989 - acc: 0.771 - ETA: 0s - loss: 0.4985 - acc: 0.771 - ETA: 0s - loss: 0.4985 - acc: 0.771 - ETA: 0s - loss: 0.4980 - acc: 0.772 - ETA: 0s - loss: 0.4982 - acc: 0.772 - ETA: 0s - loss: 0.4981 - acc: 0.772 - ETA: 0s - loss: 0.4979 - acc: 0.772 - ETA: 0s - loss: 0.4981 - acc: 0.772 - ETA: 0s - loss: 0.4985 - acc: 0.771 - 5s 73us/step - loss: 0.4984 - acc: 0.7720\n",
      "Epoch 7/20\n",
      "70041/70041 [==============================] - ETA: 5s - loss: 0.5429 - acc: 0.726 - ETA: 4s - loss: 0.4769 - acc: 0.782 - ETA: 4s - loss: 0.4728 - acc: 0.790 - ETA: 4s - loss: 0.4733 - acc: 0.786 - ETA: 4s - loss: 0.4774 - acc: 0.788 - ETA: 4s - loss: 0.4876 - acc: 0.779 - ETA: 4s - loss: 0.4895 - acc: 0.779 - ETA: 4s - loss: 0.4939 - acc: 0.774 - ETA: 4s - loss: 0.4934 - acc: 0.776 - ETA: 4s - loss: 0.4931 - acc: 0.778 - ETA: 4s - loss: 0.4916 - acc: 0.779 - ETA: 4s - loss: 0.4932 - acc: 0.777 - ETA: 4s - loss: 0.4912 - acc: 0.778 - ETA: 4s - loss: 0.4916 - acc: 0.779 - ETA: 4s - loss: 0.4917 - acc: 0.779 - ETA: 4s - loss: 0.4908 - acc: 0.779 - ETA: 4s - loss: 0.4928 - acc: 0.778 - ETA: 4s - loss: 0.4946 - acc: 0.776 - ETA: 4s - loss: 0.4941 - acc: 0.776 - ETA: 4s - loss: 0.4935 - acc: 0.776 - ETA: 3s - loss: 0.4935 - acc: 0.776 - ETA: 3s - loss: 0.4937 - acc: 0.775 - ETA: 3s - loss: 0.4954 - acc: 0.774 - ETA: 3s - loss: 0.4956 - acc: 0.774 - ETA: 3s - loss: 0.4958 - acc: 0.774 - ETA: 3s - loss: 0.4973 - acc: 0.773 - ETA: 3s - loss: 0.4969 - acc: 0.772 - ETA: 3s - loss: 0.4978 - acc: 0.772 - ETA: 3s - loss: 0.4976 - acc: 0.772 - ETA: 3s - loss: 0.4976 - acc: 0.772 - ETA: 3s - loss: 0.4970 - acc: 0.773 - ETA: 3s - loss: 0.4963 - acc: 0.773 - ETA: 3s - loss: 0.4972 - acc: 0.773 - ETA: 3s - loss: 0.4971 - acc: 0.773 - ETA: 3s - loss: 0.4964 - acc: 0.774 - ETA: 3s - loss: 0.4961 - acc: 0.774 - ETA: 3s - loss: 0.4963 - acc: 0.773 - ETA: 3s - loss: 0.4963 - acc: 0.773 - ETA: 2s - loss: 0.4959 - acc: 0.773 - ETA: 2s - loss: 0.4954 - acc: 0.773 - ETA: 2s - loss: 0.4958 - acc: 0.773 - ETA: 2s - loss: 0.4961 - acc: 0.773 - ETA: 2s - loss: 0.4962 - acc: 0.773 - ETA: 2s - loss: 0.4974 - acc: 0.772 - ETA: 2s - loss: 0.4976 - acc: 0.772 - ETA: 2s - loss: 0.4976 - acc: 0.772 - ETA: 2s - loss: 0.4977 - acc: 0.772 - ETA: 2s - loss: 0.4982 - acc: 0.771 - ETA: 2s - loss: 0.4977 - acc: 0.771 - ETA: 2s - loss: 0.4977 - acc: 0.771 - ETA: 2s - loss: 0.4973 - acc: 0.771 - ETA: 2s - loss: 0.4974 - acc: 0.771 - ETA: 2s - loss: 0.4972 - acc: 0.771 - ETA: 2s - loss: 0.4970 - acc: 0.771 - ETA: 2s - loss: 0.4973 - acc: 0.771 - ETA: 1s - loss: 0.4970 - acc: 0.771 - ETA: 1s - loss: 0.4973 - acc: 0.771 - ETA: 1s - loss: 0.4973 - acc: 0.771 - ETA: 1s - loss: 0.4972 - acc: 0.772 - ETA: 1s - loss: 0.4971 - acc: 0.772 - ETA: 1s - loss: 0.4974 - acc: 0.771 - ETA: 1s - loss: 0.4971 - acc: 0.772 - ETA: 1s - loss: 0.4972 - acc: 0.772 - ETA: 1s - loss: 0.4971 - acc: 0.771 - ETA: 1s - loss: 0.4973 - acc: 0.771 - ETA: 1s - loss: 0.4975 - acc: 0.771 - ETA: 1s - loss: 0.4974 - acc: 0.771 - ETA: 1s - loss: 0.4973 - acc: 0.771 - ETA: 1s - loss: 0.4975 - acc: 0.771 - ETA: 1s - loss: 0.4972 - acc: 0.771 - ETA: 1s - loss: 0.4974 - acc: 0.771 - ETA: 1s - loss: 0.4973 - acc: 0.772 - ETA: 1s - loss: 0.4976 - acc: 0.771 - ETA: 0s - loss: 0.4973 - acc: 0.772 - ETA: 0s - loss: 0.4972 - acc: 0.772 - ETA: 0s - loss: 0.4973 - acc: 0.772 - ETA: 0s - loss: 0.4970 - acc: 0.772 - ETA: 0s - loss: 0.4973 - acc: 0.772 - ETA: 0s - loss: 0.4972 - acc: 0.772 - ETA: 0s - loss: 0.4970 - acc: 0.772 - ETA: 0s - loss: 0.4971 - acc: 0.772 - ETA: 0s - loss: 0.4967 - acc: 0.772 - ETA: 0s - loss: 0.4967 - acc: 0.772 - ETA: 0s - loss: 0.4963 - acc: 0.772 - ETA: 0s - loss: 0.4962 - acc: 0.772 - ETA: 0s - loss: 0.4961 - acc: 0.772 - ETA: 0s - loss: 0.4961 - acc: 0.772 - ETA: 0s - loss: 0.4965 - acc: 0.772 - ETA: 0s - loss: 0.4966 - acc: 0.772 - ETA: 0s - loss: 0.4965 - acc: 0.772 - ETA: 0s - loss: 0.4962 - acc: 0.772 - 5s 72us/step - loss: 0.4962 - acc: 0.7727\n",
      "Epoch 8/20\n",
      "70041/70041 [==============================] - ETA: 5s - loss: 0.5350 - acc: 0.742 - ETA: 4s - loss: 0.5183 - acc: 0.747 - ETA: 4s - loss: 0.5102 - acc: 0.755 - ETA: 4s - loss: 0.5052 - acc: 0.760 - ETA: 4s - loss: 0.5009 - acc: 0.763 - ETA: 4s - loss: 0.4954 - acc: 0.768 - ETA: 4s - loss: 0.4937 - acc: 0.770 - ETA: 4s - loss: 0.4958 - acc: 0.768 - ETA: 4s - loss: 0.4950 - acc: 0.769 - ETA: 4s - loss: 0.4980 - acc: 0.768 - ETA: 4s - loss: 0.4989 - acc: 0.768 - ETA: 4s - loss: 0.4991 - acc: 0.769 - ETA: 4s - loss: 0.4991 - acc: 0.769 - ETA: 4s - loss: 0.4983 - acc: 0.771 - ETA: 4s - loss: 0.4980 - acc: 0.772 - ETA: 4s - loss: 0.4999 - acc: 0.770 - ETA: 4s - loss: 0.4987 - acc: 0.771 - ETA: 4s - loss: 0.4986 - acc: 0.771 - ETA: 4s - loss: 0.4984 - acc: 0.770 - ETA: 3s - loss: 0.4961 - acc: 0.772 - ETA: 3s - loss: 0.4966 - acc: 0.772 - ETA: 3s - loss: 0.4949 - acc: 0.773 - ETA: 3s - loss: 0.4953 - acc: 0.772 - ETA: 3s - loss: 0.4964 - acc: 0.771 - ETA: 3s - loss: 0.4965 - acc: 0.771 - ETA: 3s - loss: 0.4970 - acc: 0.771 - ETA: 3s - loss: 0.4975 - acc: 0.771 - ETA: 3s - loss: 0.4971 - acc: 0.771 - ETA: 3s - loss: 0.4958 - acc: 0.772 - ETA: 3s - loss: 0.4955 - acc: 0.772 - ETA: 3s - loss: 0.4950 - acc: 0.773 - ETA: 3s - loss: 0.4949 - acc: 0.773 - ETA: 3s - loss: 0.4946 - acc: 0.773 - ETA: 3s - loss: 0.4946 - acc: 0.773 - ETA: 3s - loss: 0.4939 - acc: 0.773 - ETA: 3s - loss: 0.4944 - acc: 0.773 - ETA: 3s - loss: 0.4950 - acc: 0.772 - ETA: 3s - loss: 0.4956 - acc: 0.772 - ETA: 2s - loss: 0.4956 - acc: 0.772 - ETA: 2s - loss: 0.4960 - acc: 0.772 - ETA: 2s - loss: 0.4953 - acc: 0.772 - ETA: 2s - loss: 0.4951 - acc: 0.772 - ETA: 2s - loss: 0.4953 - acc: 0.772 - ETA: 2s - loss: 0.4950 - acc: 0.772 - ETA: 2s - loss: 0.4951 - acc: 0.772 - ETA: 2s - loss: 0.4945 - acc: 0.772 - ETA: 2s - loss: 0.4946 - acc: 0.772 - ETA: 2s - loss: 0.4942 - acc: 0.772 - ETA: 2s - loss: 0.4932 - acc: 0.773 - ETA: 2s - loss: 0.4930 - acc: 0.773 - ETA: 2s - loss: 0.4933 - acc: 0.773 - ETA: 2s - loss: 0.4937 - acc: 0.772 - ETA: 2s - loss: 0.4935 - acc: 0.773 - ETA: 2s - loss: 0.4935 - acc: 0.773 - ETA: 2s - loss: 0.4937 - acc: 0.773 - ETA: 2s - loss: 0.4936 - acc: 0.773 - ETA: 1s - loss: 0.4934 - acc: 0.773 - ETA: 1s - loss: 0.4936 - acc: 0.773 - ETA: 1s - loss: 0.4930 - acc: 0.773 - ETA: 1s - loss: 0.4936 - acc: 0.773 - ETA: 1s - loss: 0.4936 - acc: 0.773 - ETA: 1s - loss: 0.4938 - acc: 0.773 - ETA: 1s - loss: 0.4939 - acc: 0.772 - ETA: 1s - loss: 0.4947 - acc: 0.772 - ETA: 1s - loss: 0.4944 - acc: 0.772 - ETA: 1s - loss: 0.4943 - acc: 0.772 - ETA: 1s - loss: 0.4945 - acc: 0.772 - ETA: 1s - loss: 0.4943 - acc: 0.772 - ETA: 1s - loss: 0.4939 - acc: 0.773 - ETA: 1s - loss: 0.4943 - acc: 0.772 - ETA: 1s - loss: 0.4940 - acc: 0.773 - ETA: 1s - loss: 0.4940 - acc: 0.773 - ETA: 1s - loss: 0.4937 - acc: 0.773 - ETA: 0s - loss: 0.4938 - acc: 0.773 - ETA: 0s - loss: 0.4942 - acc: 0.773 - ETA: 0s - loss: 0.4939 - acc: 0.773 - ETA: 0s - loss: 0.4938 - acc: 0.773 - ETA: 0s - loss: 0.4936 - acc: 0.773 - ETA: 0s - loss: 0.4937 - acc: 0.773 - ETA: 0s - loss: 0.4935 - acc: 0.773 - ETA: 0s - loss: 0.4935 - acc: 0.773 - ETA: 0s - loss: 0.4934 - acc: 0.773 - ETA: 0s - loss: 0.4935 - acc: 0.773 - ETA: 0s - loss: 0.4938 - acc: 0.773 - ETA: 0s - loss: 0.4938 - acc: 0.773 - ETA: 0s - loss: 0.4937 - acc: 0.773 - ETA: 0s - loss: 0.4936 - acc: 0.773 - ETA: 0s - loss: 0.4938 - acc: 0.773 - ETA: 0s - loss: 0.4939 - acc: 0.773 - ETA: 0s - loss: 0.4948 - acc: 0.772 - ETA: 0s - loss: 0.4944 - acc: 0.773 - 5s 73us/step - loss: 0.4943 - acc: 0.7731\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70041/70041 [==============================] - ETA: 5s - loss: 0.4768 - acc: 0.781 - ETA: 4s - loss: 0.4951 - acc: 0.767 - ETA: 4s - loss: 0.4960 - acc: 0.767 - ETA: 4s - loss: 0.4923 - acc: 0.769 - ETA: 4s - loss: 0.4966 - acc: 0.768 - ETA: 4s - loss: 0.4972 - acc: 0.767 - ETA: 4s - loss: 0.4965 - acc: 0.769 - ETA: 4s - loss: 0.4948 - acc: 0.771 - ETA: 4s - loss: 0.4898 - acc: 0.773 - ETA: 4s - loss: 0.4898 - acc: 0.772 - ETA: 4s - loss: 0.4877 - acc: 0.774 - ETA: 4s - loss: 0.4868 - acc: 0.775 - ETA: 4s - loss: 0.4867 - acc: 0.775 - ETA: 4s - loss: 0.4865 - acc: 0.776 - ETA: 4s - loss: 0.4873 - acc: 0.776 - ETA: 4s - loss: 0.4880 - acc: 0.776 - ETA: 4s - loss: 0.4867 - acc: 0.777 - ETA: 4s - loss: 0.4855 - acc: 0.778 - ETA: 4s - loss: 0.4855 - acc: 0.778 - ETA: 3s - loss: 0.4859 - acc: 0.778 - ETA: 3s - loss: 0.4870 - acc: 0.778 - ETA: 3s - loss: 0.4885 - acc: 0.777 - ETA: 3s - loss: 0.4874 - acc: 0.778 - ETA: 3s - loss: 0.4869 - acc: 0.778 - ETA: 3s - loss: 0.4870 - acc: 0.779 - ETA: 3s - loss: 0.4884 - acc: 0.777 - ETA: 3s - loss: 0.4887 - acc: 0.778 - ETA: 3s - loss: 0.4900 - acc: 0.777 - ETA: 3s - loss: 0.4892 - acc: 0.777 - ETA: 3s - loss: 0.4908 - acc: 0.776 - ETA: 3s - loss: 0.4913 - acc: 0.776 - ETA: 3s - loss: 0.4905 - acc: 0.776 - ETA: 3s - loss: 0.4903 - acc: 0.777 - ETA: 3s - loss: 0.4908 - acc: 0.776 - ETA: 3s - loss: 0.4903 - acc: 0.777 - ETA: 3s - loss: 0.4908 - acc: 0.776 - ETA: 3s - loss: 0.4899 - acc: 0.777 - ETA: 2s - loss: 0.4901 - acc: 0.777 - ETA: 2s - loss: 0.4900 - acc: 0.777 - ETA: 2s - loss: 0.4901 - acc: 0.777 - ETA: 2s - loss: 0.4904 - acc: 0.776 - ETA: 2s - loss: 0.4907 - acc: 0.776 - ETA: 2s - loss: 0.4904 - acc: 0.776 - ETA: 2s - loss: 0.4901 - acc: 0.776 - ETA: 2s - loss: 0.4896 - acc: 0.777 - ETA: 2s - loss: 0.4902 - acc: 0.776 - ETA: 2s - loss: 0.4896 - acc: 0.777 - ETA: 2s - loss: 0.4897 - acc: 0.777 - ETA: 2s - loss: 0.4900 - acc: 0.776 - ETA: 2s - loss: 0.4904 - acc: 0.776 - ETA: 2s - loss: 0.4900 - acc: 0.776 - ETA: 2s - loss: 0.4903 - acc: 0.776 - ETA: 2s - loss: 0.4902 - acc: 0.776 - ETA: 2s - loss: 0.4906 - acc: 0.775 - ETA: 2s - loss: 0.4906 - acc: 0.776 - ETA: 2s - loss: 0.4908 - acc: 0.775 - ETA: 1s - loss: 0.4911 - acc: 0.775 - ETA: 1s - loss: 0.4913 - acc: 0.775 - ETA: 1s - loss: 0.4912 - acc: 0.775 - ETA: 1s - loss: 0.4912 - acc: 0.775 - ETA: 1s - loss: 0.4914 - acc: 0.775 - ETA: 1s - loss: 0.4909 - acc: 0.775 - ETA: 1s - loss: 0.4908 - acc: 0.775 - ETA: 1s - loss: 0.4908 - acc: 0.775 - ETA: 1s - loss: 0.4912 - acc: 0.775 - ETA: 1s - loss: 0.4918 - acc: 0.774 - ETA: 1s - loss: 0.4919 - acc: 0.774 - ETA: 1s - loss: 0.4922 - acc: 0.774 - ETA: 1s - loss: 0.4923 - acc: 0.774 - ETA: 1s - loss: 0.4920 - acc: 0.774 - ETA: 1s - loss: 0.4920 - acc: 0.774 - ETA: 1s - loss: 0.4920 - acc: 0.774 - ETA: 1s - loss: 0.4919 - acc: 0.774 - ETA: 1s - loss: 0.4924 - acc: 0.774 - ETA: 0s - loss: 0.4927 - acc: 0.774 - ETA: 0s - loss: 0.4923 - acc: 0.774 - ETA: 0s - loss: 0.4925 - acc: 0.774 - ETA: 0s - loss: 0.4926 - acc: 0.774 - ETA: 0s - loss: 0.4926 - acc: 0.773 - ETA: 0s - loss: 0.4923 - acc: 0.774 - ETA: 0s - loss: 0.4924 - acc: 0.773 - ETA: 0s - loss: 0.4926 - acc: 0.773 - ETA: 0s - loss: 0.4924 - acc: 0.773 - ETA: 0s - loss: 0.4924 - acc: 0.773 - ETA: 0s - loss: 0.4927 - acc: 0.773 - ETA: 0s - loss: 0.4926 - acc: 0.773 - ETA: 0s - loss: 0.4926 - acc: 0.773 - ETA: 0s - loss: 0.4924 - acc: 0.773 - ETA: 0s - loss: 0.4925 - acc: 0.773 - ETA: 0s - loss: 0.4925 - acc: 0.773 - ETA: 0s - loss: 0.4926 - acc: 0.773 - ETA: 0s - loss: 0.4926 - acc: 0.773 - 5s 73us/step - loss: 0.4927 - acc: 0.7730\n",
      "Epoch 10/20\n",
      "70041/70041 [==============================] - ETA: 5s - loss: 0.4533 - acc: 0.781 - ETA: 4s - loss: 0.4851 - acc: 0.771 - ETA: 4s - loss: 0.4844 - acc: 0.781 - ETA: 4s - loss: 0.4817 - acc: 0.780 - ETA: 4s - loss: 0.4823 - acc: 0.783 - ETA: 4s - loss: 0.4859 - acc: 0.784 - ETA: 4s - loss: 0.4876 - acc: 0.782 - ETA: 4s - loss: 0.4866 - acc: 0.783 - ETA: 4s - loss: 0.4834 - acc: 0.786 - ETA: 4s - loss: 0.4855 - acc: 0.782 - ETA: 4s - loss: 0.4864 - acc: 0.781 - ETA: 4s - loss: 0.4894 - acc: 0.778 - ETA: 4s - loss: 0.4883 - acc: 0.779 - ETA: 4s - loss: 0.4884 - acc: 0.779 - ETA: 4s - loss: 0.4872 - acc: 0.778 - ETA: 4s - loss: 0.4873 - acc: 0.778 - ETA: 4s - loss: 0.4868 - acc: 0.779 - ETA: 4s - loss: 0.4863 - acc: 0.779 - ETA: 4s - loss: 0.4871 - acc: 0.779 - ETA: 4s - loss: 0.4863 - acc: 0.779 - ETA: 4s - loss: 0.4858 - acc: 0.779 - ETA: 4s - loss: 0.4867 - acc: 0.779 - ETA: 3s - loss: 0.4869 - acc: 0.779 - ETA: 3s - loss: 0.4864 - acc: 0.779 - ETA: 3s - loss: 0.4869 - acc: 0.779 - ETA: 3s - loss: 0.4870 - acc: 0.779 - ETA: 3s - loss: 0.4869 - acc: 0.779 - ETA: 3s - loss: 0.4873 - acc: 0.779 - ETA: 3s - loss: 0.4869 - acc: 0.779 - ETA: 3s - loss: 0.4874 - acc: 0.779 - ETA: 3s - loss: 0.4876 - acc: 0.778 - ETA: 3s - loss: 0.4880 - acc: 0.778 - ETA: 3s - loss: 0.4877 - acc: 0.778 - ETA: 3s - loss: 0.4880 - acc: 0.778 - ETA: 3s - loss: 0.4895 - acc: 0.777 - ETA: 3s - loss: 0.4895 - acc: 0.777 - ETA: 3s - loss: 0.4892 - acc: 0.777 - ETA: 3s - loss: 0.4895 - acc: 0.776 - ETA: 3s - loss: 0.4896 - acc: 0.776 - ETA: 2s - loss: 0.4905 - acc: 0.775 - ETA: 2s - loss: 0.4907 - acc: 0.774 - ETA: 2s - loss: 0.4907 - acc: 0.774 - ETA: 2s - loss: 0.4909 - acc: 0.774 - ETA: 2s - loss: 0.4913 - acc: 0.774 - ETA: 2s - loss: 0.4907 - acc: 0.774 - ETA: 2s - loss: 0.4904 - acc: 0.775 - ETA: 2s - loss: 0.4904 - acc: 0.775 - ETA: 2s - loss: 0.4903 - acc: 0.774 - ETA: 2s - loss: 0.4901 - acc: 0.774 - ETA: 2s - loss: 0.4902 - acc: 0.774 - ETA: 2s - loss: 0.4906 - acc: 0.774 - ETA: 2s - loss: 0.4904 - acc: 0.774 - ETA: 2s - loss: 0.4904 - acc: 0.775 - ETA: 2s - loss: 0.4909 - acc: 0.774 - ETA: 2s - loss: 0.4909 - acc: 0.774 - ETA: 2s - loss: 0.4908 - acc: 0.774 - ETA: 1s - loss: 0.4908 - acc: 0.774 - ETA: 1s - loss: 0.4911 - acc: 0.774 - ETA: 1s - loss: 0.4912 - acc: 0.773 - ETA: 1s - loss: 0.4915 - acc: 0.773 - ETA: 1s - loss: 0.4914 - acc: 0.773 - ETA: 1s - loss: 0.4906 - acc: 0.774 - ETA: 1s - loss: 0.4912 - acc: 0.773 - ETA: 1s - loss: 0.4915 - acc: 0.773 - ETA: 1s - loss: 0.4918 - acc: 0.772 - ETA: 1s - loss: 0.4919 - acc: 0.772 - ETA: 1s - loss: 0.4922 - acc: 0.772 - ETA: 1s - loss: 0.4921 - acc: 0.773 - ETA: 1s - loss: 0.4918 - acc: 0.773 - ETA: 1s - loss: 0.4918 - acc: 0.773 - ETA: 1s - loss: 0.4919 - acc: 0.773 - ETA: 1s - loss: 0.4919 - acc: 0.773 - ETA: 1s - loss: 0.4918 - acc: 0.773 - ETA: 1s - loss: 0.4920 - acc: 0.773 - ETA: 0s - loss: 0.4919 - acc: 0.773 - ETA: 0s - loss: 0.4915 - acc: 0.773 - ETA: 0s - loss: 0.4916 - acc: 0.773 - ETA: 0s - loss: 0.4912 - acc: 0.774 - ETA: 0s - loss: 0.4911 - acc: 0.774 - ETA: 0s - loss: 0.4909 - acc: 0.774 - ETA: 0s - loss: 0.4915 - acc: 0.773 - ETA: 0s - loss: 0.4913 - acc: 0.774 - ETA: 0s - loss: 0.4917 - acc: 0.773 - ETA: 0s - loss: 0.4916 - acc: 0.773 - ETA: 0s - loss: 0.4919 - acc: 0.773 - ETA: 0s - loss: 0.4917 - acc: 0.773 - ETA: 0s - loss: 0.4915 - acc: 0.773 - ETA: 0s - loss: 0.4914 - acc: 0.773 - ETA: 0s - loss: 0.4916 - acc: 0.773 - ETA: 0s - loss: 0.4920 - acc: 0.773 - ETA: 0s - loss: 0.4918 - acc: 0.773 - ETA: 0s - loss: 0.4920 - acc: 0.773 - 5s 74us/step - loss: 0.4920 - acc: 0.7734\n",
      "Epoch 11/20\n",
      "70041/70041 [==============================] - ETA: 5s - loss: 0.5013 - acc: 0.793 - ETA: 4s - loss: 0.4955 - acc: 0.776 - ETA: 5s - loss: 0.5002 - acc: 0.765 - ETA: 5s - loss: 0.4893 - acc: 0.775 - ETA: 4s - loss: 0.5001 - acc: 0.764 - ETA: 4s - loss: 0.5007 - acc: 0.765 - ETA: 4s - loss: 0.4950 - acc: 0.772 - ETA: 4s - loss: 0.4944 - acc: 0.773 - ETA: 4s - loss: 0.4909 - acc: 0.774 - ETA: 4s - loss: 0.4884 - acc: 0.775 - ETA: 4s - loss: 0.4905 - acc: 0.773 - ETA: 4s - loss: 0.4892 - acc: 0.774 - ETA: 4s - loss: 0.4894 - acc: 0.775 - ETA: 4s - loss: 0.4909 - acc: 0.773 - ETA: 4s - loss: 0.4896 - acc: 0.774 - ETA: 4s - loss: 0.4916 - acc: 0.773 - ETA: 4s - loss: 0.4919 - acc: 0.772 - ETA: 4s - loss: 0.4914 - acc: 0.773 - ETA: 4s - loss: 0.4932 - acc: 0.772 - ETA: 4s - loss: 0.4919 - acc: 0.773 - ETA: 4s - loss: 0.4900 - acc: 0.775 - ETA: 3s - loss: 0.4905 - acc: 0.775 - ETA: 3s - loss: 0.4917 - acc: 0.773 - ETA: 3s - loss: 0.4916 - acc: 0.774 - ETA: 3s - loss: 0.4911 - acc: 0.774 - ETA: 3s - loss: 0.4910 - acc: 0.774 - ETA: 3s - loss: 0.4903 - acc: 0.775 - ETA: 3s - loss: 0.4900 - acc: 0.775 - ETA: 3s - loss: 0.4891 - acc: 0.775 - ETA: 3s - loss: 0.4891 - acc: 0.776 - ETA: 3s - loss: 0.4884 - acc: 0.776 - ETA: 3s - loss: 0.4879 - acc: 0.776 - ETA: 3s - loss: 0.4886 - acc: 0.776 - ETA: 3s - loss: 0.4890 - acc: 0.776 - ETA: 3s - loss: 0.4899 - acc: 0.775 - ETA: 3s - loss: 0.4905 - acc: 0.774 - ETA: 3s - loss: 0.4906 - acc: 0.774 - ETA: 3s - loss: 0.4912 - acc: 0.773 - ETA: 2s - loss: 0.4909 - acc: 0.774 - ETA: 2s - loss: 0.4905 - acc: 0.774 - ETA: 2s - loss: 0.4906 - acc: 0.774 - ETA: 2s - loss: 0.4909 - acc: 0.774 - ETA: 2s - loss: 0.4907 - acc: 0.774 - ETA: 2s - loss: 0.4907 - acc: 0.774 - ETA: 2s - loss: 0.4907 - acc: 0.774 - ETA: 2s - loss: 0.4908 - acc: 0.774 - ETA: 2s - loss: 0.4905 - acc: 0.774 - ETA: 2s - loss: 0.4910 - acc: 0.773 - ETA: 2s - loss: 0.4919 - acc: 0.772 - ETA: 2s - loss: 0.4916 - acc: 0.772 - ETA: 2s - loss: 0.4920 - acc: 0.772 - ETA: 2s - loss: 0.4919 - acc: 0.773 - ETA: 2s - loss: 0.4922 - acc: 0.772 - ETA: 2s - loss: 0.4922 - acc: 0.773 - ETA: 2s - loss: 0.4923 - acc: 0.772 - ETA: 2s - loss: 0.4927 - acc: 0.772 - ETA: 1s - loss: 0.4929 - acc: 0.772 - ETA: 1s - loss: 0.4931 - acc: 0.772 - ETA: 1s - loss: 0.4924 - acc: 0.772 - ETA: 1s - loss: 0.4925 - acc: 0.773 - ETA: 1s - loss: 0.4933 - acc: 0.772 - ETA: 1s - loss: 0.4933 - acc: 0.772 - ETA: 1s - loss: 0.4934 - acc: 0.772 - ETA: 1s - loss: 0.4935 - acc: 0.772 - ETA: 1s - loss: 0.4934 - acc: 0.772 - ETA: 1s - loss: 0.4930 - acc: 0.772 - ETA: 1s - loss: 0.4928 - acc: 0.772 - ETA: 1s - loss: 0.4928 - acc: 0.772 - ETA: 1s - loss: 0.4929 - acc: 0.772 - ETA: 1s - loss: 0.4931 - acc: 0.772 - ETA: 1s - loss: 0.4930 - acc: 0.772 - ETA: 1s - loss: 0.4935 - acc: 0.772 - ETA: 1s - loss: 0.4932 - acc: 0.772 - ETA: 1s - loss: 0.4931 - acc: 0.772 - ETA: 0s - loss: 0.4933 - acc: 0.772 - ETA: 0s - loss: 0.4932 - acc: 0.772 - ETA: 0s - loss: 0.4932 - acc: 0.772 - ETA: 0s - loss: 0.4933 - acc: 0.772 - ETA: 0s - loss: 0.4934 - acc: 0.772 - ETA: 0s - loss: 0.4933 - acc: 0.772 - ETA: 0s - loss: 0.4930 - acc: 0.772 - ETA: 0s - loss: 0.4928 - acc: 0.772 - ETA: 0s - loss: 0.4928 - acc: 0.772 - ETA: 0s - loss: 0.4927 - acc: 0.772 - ETA: 0s - loss: 0.4930 - acc: 0.772 - ETA: 0s - loss: 0.4927 - acc: 0.772 - ETA: 0s - loss: 0.4924 - acc: 0.772 - ETA: 0s - loss: 0.4924 - acc: 0.772 - ETA: 0s - loss: 0.4921 - acc: 0.773 - ETA: 0s - loss: 0.4923 - acc: 0.773 - ETA: 0s - loss: 0.4920 - acc: 0.773 - 5s 73us/step - loss: 0.4920 - acc: 0.7731\n",
      "Epoch 12/20\n",
      "70041/70041 [==============================] - ETA: 5s - loss: 0.4787 - acc: 0.789 - ETA: 4s - loss: 0.4988 - acc: 0.769 - ETA: 4s - loss: 0.5035 - acc: 0.772 - ETA: 4s - loss: 0.4944 - acc: 0.776 - ETA: 4s - loss: 0.4911 - acc: 0.778 - ETA: 4s - loss: 0.4968 - acc: 0.773 - ETA: 4s - loss: 0.4947 - acc: 0.774 - ETA: 4s - loss: 0.4946 - acc: 0.773 - ETA: 4s - loss: 0.4921 - acc: 0.774 - ETA: 4s - loss: 0.4937 - acc: 0.774 - ETA: 4s - loss: 0.4930 - acc: 0.774 - ETA: 4s - loss: 0.4927 - acc: 0.774 - ETA: 4s - loss: 0.4944 - acc: 0.774 - ETA: 4s - loss: 0.4952 - acc: 0.773 - ETA: 4s - loss: 0.4931 - acc: 0.773 - ETA: 4s - loss: 0.4931 - acc: 0.773 - ETA: 4s - loss: 0.4933 - acc: 0.773 - ETA: 4s - loss: 0.4946 - acc: 0.770 - ETA: 4s - loss: 0.4938 - acc: 0.772 - ETA: 4s - loss: 0.4933 - acc: 0.772 - ETA: 3s - loss: 0.4944 - acc: 0.771 - ETA: 3s - loss: 0.4926 - acc: 0.772 - ETA: 3s - loss: 0.4936 - acc: 0.772 - ETA: 3s - loss: 0.4921 - acc: 0.772 - ETA: 3s - loss: 0.4911 - acc: 0.773 - ETA: 3s - loss: 0.4919 - acc: 0.772 - ETA: 3s - loss: 0.4906 - acc: 0.773 - ETA: 3s - loss: 0.4918 - acc: 0.772 - ETA: 3s - loss: 0.4909 - acc: 0.773 - ETA: 3s - loss: 0.4903 - acc: 0.774 - ETA: 3s - loss: 0.4903 - acc: 0.774 - ETA: 3s - loss: 0.4908 - acc: 0.774 - ETA: 3s - loss: 0.4903 - acc: 0.774 - ETA: 3s - loss: 0.4906 - acc: 0.774 - ETA: 3s - loss: 0.4908 - acc: 0.773 - ETA: 3s - loss: 0.4911 - acc: 0.773 - ETA: 3s - loss: 0.4905 - acc: 0.774 - ETA: 3s - loss: 0.4905 - acc: 0.773 - ETA: 2s - loss: 0.4906 - acc: 0.773 - ETA: 2s - loss: 0.4912 - acc: 0.774 - ETA: 2s - loss: 0.4906 - acc: 0.774 - ETA: 2s - loss: 0.4900 - acc: 0.774 - ETA: 2s - loss: 0.4897 - acc: 0.774 - ETA: 2s - loss: 0.4892 - acc: 0.774 - ETA: 2s - loss: 0.4895 - acc: 0.774 - ETA: 2s - loss: 0.4899 - acc: 0.773 - ETA: 2s - loss: 0.4897 - acc: 0.773 - ETA: 2s - loss: 0.4901 - acc: 0.773 - ETA: 2s - loss: 0.4904 - acc: 0.773 - ETA: 2s - loss: 0.4905 - acc: 0.772 - ETA: 2s - loss: 0.4907 - acc: 0.773 - ETA: 2s - loss: 0.4904 - acc: 0.773 - ETA: 2s - loss: 0.4904 - acc: 0.773 - ETA: 2s - loss: 0.4904 - acc: 0.772 - ETA: 2s - loss: 0.4904 - acc: 0.772 - ETA: 2s - loss: 0.4902 - acc: 0.773 - ETA: 1s - loss: 0.4906 - acc: 0.772 - ETA: 1s - loss: 0.4904 - acc: 0.772 - ETA: 1s - loss: 0.4902 - acc: 0.772 - ETA: 1s - loss: 0.4901 - acc: 0.773 - ETA: 1s - loss: 0.4897 - acc: 0.773 - ETA: 1s - loss: 0.4902 - acc: 0.773 - ETA: 1s - loss: 0.4902 - acc: 0.773 - ETA: 1s - loss: 0.4904 - acc: 0.772 - ETA: 1s - loss: 0.4901 - acc: 0.773 - ETA: 1s - loss: 0.4909 - acc: 0.772 - ETA: 1s - loss: 0.4909 - acc: 0.772 - ETA: 1s - loss: 0.4911 - acc: 0.772 - ETA: 1s - loss: 0.4912 - acc: 0.772 - ETA: 1s - loss: 0.4911 - acc: 0.772 - ETA: 1s - loss: 0.4907 - acc: 0.772 - ETA: 1s - loss: 0.4905 - acc: 0.772 - ETA: 1s - loss: 0.4903 - acc: 0.773 - ETA: 1s - loss: 0.4905 - acc: 0.773 - ETA: 0s - loss: 0.4907 - acc: 0.773 - ETA: 0s - loss: 0.4910 - acc: 0.773 - ETA: 0s - loss: 0.4910 - acc: 0.772 - ETA: 0s - loss: 0.4908 - acc: 0.773 - ETA: 0s - loss: 0.4907 - acc: 0.773 - ETA: 0s - loss: 0.4906 - acc: 0.773 - ETA: 0s - loss: 0.4905 - acc: 0.773 - ETA: 0s - loss: 0.4907 - acc: 0.773 - ETA: 0s - loss: 0.4907 - acc: 0.773 - ETA: 0s - loss: 0.4904 - acc: 0.773 - ETA: 0s - loss: 0.4903 - acc: 0.773 - ETA: 0s - loss: 0.4903 - acc: 0.773 - ETA: 0s - loss: 0.4901 - acc: 0.773 - ETA: 0s - loss: 0.4902 - acc: 0.773 - ETA: 0s - loss: 0.4902 - acc: 0.773 - ETA: 0s - loss: 0.4906 - acc: 0.773 - ETA: 0s - loss: 0.4907 - acc: 0.773 - 5s 73us/step - loss: 0.4906 - acc: 0.7733\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70041/70041 [==============================] - ETA: 5s - loss: 0.5220 - acc: 0.765 - ETA: 5s - loss: 0.4891 - acc: 0.786 - ETA: 4s - loss: 0.4976 - acc: 0.774 - ETA: 4s - loss: 0.4898 - acc: 0.776 - ETA: 4s - loss: 0.4891 - acc: 0.779 - ETA: 4s - loss: 0.4879 - acc: 0.779 - ETA: 4s - loss: 0.4842 - acc: 0.780 - ETA: 4s - loss: 0.4838 - acc: 0.780 - ETA: 4s - loss: 0.4842 - acc: 0.780 - ETA: 4s - loss: 0.4853 - acc: 0.777 - ETA: 4s - loss: 0.4814 - acc: 0.781 - ETA: 4s - loss: 0.4858 - acc: 0.778 - ETA: 4s - loss: 0.4875 - acc: 0.777 - ETA: 4s - loss: 0.4845 - acc: 0.778 - ETA: 4s - loss: 0.4826 - acc: 0.779 - ETA: 4s - loss: 0.4829 - acc: 0.780 - ETA: 4s - loss: 0.4826 - acc: 0.780 - ETA: 4s - loss: 0.4829 - acc: 0.780 - ETA: 4s - loss: 0.4828 - acc: 0.780 - ETA: 4s - loss: 0.4840 - acc: 0.780 - ETA: 4s - loss: 0.4851 - acc: 0.778 - ETA: 3s - loss: 0.4859 - acc: 0.777 - ETA: 3s - loss: 0.4864 - acc: 0.777 - ETA: 3s - loss: 0.4852 - acc: 0.777 - ETA: 3s - loss: 0.4863 - acc: 0.776 - ETA: 3s - loss: 0.4865 - acc: 0.776 - ETA: 3s - loss: 0.4863 - acc: 0.777 - ETA: 3s - loss: 0.4860 - acc: 0.776 - ETA: 3s - loss: 0.4856 - acc: 0.777 - ETA: 3s - loss: 0.4861 - acc: 0.776 - ETA: 3s - loss: 0.4863 - acc: 0.776 - ETA: 3s - loss: 0.4865 - acc: 0.776 - ETA: 3s - loss: 0.4867 - acc: 0.776 - ETA: 3s - loss: 0.4863 - acc: 0.777 - ETA: 3s - loss: 0.4866 - acc: 0.776 - ETA: 3s - loss: 0.4873 - acc: 0.776 - ETA: 3s - loss: 0.4886 - acc: 0.775 - ETA: 3s - loss: 0.4892 - acc: 0.774 - ETA: 2s - loss: 0.4896 - acc: 0.774 - ETA: 2s - loss: 0.4901 - acc: 0.773 - ETA: 2s - loss: 0.4906 - acc: 0.773 - ETA: 2s - loss: 0.4904 - acc: 0.773 - ETA: 2s - loss: 0.4903 - acc: 0.773 - ETA: 2s - loss: 0.4904 - acc: 0.773 - ETA: 2s - loss: 0.4911 - acc: 0.772 - ETA: 2s - loss: 0.4913 - acc: 0.772 - ETA: 2s - loss: 0.4908 - acc: 0.773 - ETA: 2s - loss: 0.4906 - acc: 0.773 - ETA: 2s - loss: 0.4906 - acc: 0.773 - ETA: 2s - loss: 0.4907 - acc: 0.773 - ETA: 2s - loss: 0.4907 - acc: 0.773 - ETA: 2s - loss: 0.4914 - acc: 0.772 - ETA: 2s - loss: 0.4917 - acc: 0.773 - ETA: 2s - loss: 0.4914 - acc: 0.773 - ETA: 2s - loss: 0.4912 - acc: 0.773 - ETA: 2s - loss: 0.4905 - acc: 0.773 - ETA: 1s - loss: 0.4904 - acc: 0.773 - ETA: 1s - loss: 0.4908 - acc: 0.773 - ETA: 1s - loss: 0.4909 - acc: 0.773 - ETA: 1s - loss: 0.4905 - acc: 0.773 - ETA: 1s - loss: 0.4904 - acc: 0.773 - ETA: 1s - loss: 0.4905 - acc: 0.773 - ETA: 1s - loss: 0.4907 - acc: 0.773 - ETA: 1s - loss: 0.4908 - acc: 0.773 - ETA: 1s - loss: 0.4913 - acc: 0.773 - ETA: 1s - loss: 0.4914 - acc: 0.773 - ETA: 1s - loss: 0.4911 - acc: 0.773 - ETA: 1s - loss: 0.4912 - acc: 0.773 - ETA: 1s - loss: 0.4914 - acc: 0.772 - ETA: 1s - loss: 0.4916 - acc: 0.772 - ETA: 1s - loss: 0.4914 - acc: 0.772 - ETA: 1s - loss: 0.4914 - acc: 0.772 - ETA: 1s - loss: 0.4912 - acc: 0.772 - ETA: 1s - loss: 0.4910 - acc: 0.772 - ETA: 0s - loss: 0.4913 - acc: 0.772 - ETA: 0s - loss: 0.4909 - acc: 0.772 - ETA: 0s - loss: 0.4908 - acc: 0.772 - ETA: 0s - loss: 0.4905 - acc: 0.773 - ETA: 0s - loss: 0.4908 - acc: 0.773 - ETA: 0s - loss: 0.4907 - acc: 0.773 - ETA: 0s - loss: 0.4904 - acc: 0.773 - ETA: 0s - loss: 0.4901 - acc: 0.773 - ETA: 0s - loss: 0.4896 - acc: 0.773 - ETA: 0s - loss: 0.4895 - acc: 0.773 - ETA: 0s - loss: 0.4894 - acc: 0.773 - ETA: 0s - loss: 0.4893 - acc: 0.773 - ETA: 0s - loss: 0.4892 - acc: 0.773 - ETA: 0s - loss: 0.4895 - acc: 0.773 - ETA: 0s - loss: 0.4896 - acc: 0.773 - ETA: 0s - loss: 0.4896 - acc: 0.773 - ETA: 0s - loss: 0.4896 - acc: 0.773 - 5s 73us/step - loss: 0.4896 - acc: 0.7733\n",
      "Epoch 14/20\n",
      "70041/70041 [==============================] - ETA: 7s - loss: 0.5517 - acc: 0.742 - ETA: 5s - loss: 0.4957 - acc: 0.765 - ETA: 5s - loss: 0.4945 - acc: 0.774 - ETA: 5s - loss: 0.4923 - acc: 0.776 - ETA: 5s - loss: 0.4986 - acc: 0.770 - ETA: 4s - loss: 0.4968 - acc: 0.772 - ETA: 4s - loss: 0.4946 - acc: 0.774 - ETA: 4s - loss: 0.4906 - acc: 0.776 - ETA: 4s - loss: 0.4907 - acc: 0.776 - ETA: 4s - loss: 0.4883 - acc: 0.777 - ETA: 4s - loss: 0.4884 - acc: 0.775 - ETA: 4s - loss: 0.4903 - acc: 0.774 - ETA: 4s - loss: 0.4895 - acc: 0.775 - ETA: 4s - loss: 0.4907 - acc: 0.773 - ETA: 4s - loss: 0.4913 - acc: 0.772 - ETA: 4s - loss: 0.4917 - acc: 0.772 - ETA: 4s - loss: 0.4914 - acc: 0.772 - ETA: 4s - loss: 0.4907 - acc: 0.773 - ETA: 4s - loss: 0.4897 - acc: 0.773 - ETA: 4s - loss: 0.4923 - acc: 0.772 - ETA: 4s - loss: 0.4925 - acc: 0.772 - ETA: 3s - loss: 0.4921 - acc: 0.772 - ETA: 3s - loss: 0.4922 - acc: 0.772 - ETA: 3s - loss: 0.4914 - acc: 0.774 - ETA: 3s - loss: 0.4902 - acc: 0.774 - ETA: 3s - loss: 0.4900 - acc: 0.774 - ETA: 3s - loss: 0.4904 - acc: 0.773 - ETA: 3s - loss: 0.4902 - acc: 0.774 - ETA: 3s - loss: 0.4907 - acc: 0.774 - ETA: 3s - loss: 0.4903 - acc: 0.774 - ETA: 3s - loss: 0.4906 - acc: 0.774 - ETA: 3s - loss: 0.4912 - acc: 0.773 - ETA: 3s - loss: 0.4910 - acc: 0.773 - ETA: 3s - loss: 0.4909 - acc: 0.773 - ETA: 3s - loss: 0.4908 - acc: 0.773 - ETA: 3s - loss: 0.4913 - acc: 0.772 - ETA: 3s - loss: 0.4905 - acc: 0.773 - ETA: 3s - loss: 0.4896 - acc: 0.774 - ETA: 2s - loss: 0.4884 - acc: 0.775 - ETA: 2s - loss: 0.4878 - acc: 0.775 - ETA: 2s - loss: 0.4881 - acc: 0.775 - ETA: 2s - loss: 0.4884 - acc: 0.774 - ETA: 2s - loss: 0.4885 - acc: 0.774 - ETA: 2s - loss: 0.4875 - acc: 0.775 - ETA: 2s - loss: 0.4877 - acc: 0.775 - ETA: 2s - loss: 0.4876 - acc: 0.775 - ETA: 2s - loss: 0.4879 - acc: 0.775 - ETA: 2s - loss: 0.4878 - acc: 0.775 - ETA: 2s - loss: 0.4885 - acc: 0.774 - ETA: 2s - loss: 0.4888 - acc: 0.774 - ETA: 2s - loss: 0.4894 - acc: 0.773 - ETA: 2s - loss: 0.4902 - acc: 0.773 - ETA: 2s - loss: 0.4896 - acc: 0.773 - ETA: 2s - loss: 0.4899 - acc: 0.773 - ETA: 2s - loss: 0.4904 - acc: 0.773 - ETA: 2s - loss: 0.4904 - acc: 0.772 - ETA: 1s - loss: 0.4902 - acc: 0.773 - ETA: 1s - loss: 0.4901 - acc: 0.773 - ETA: 1s - loss: 0.4901 - acc: 0.773 - ETA: 1s - loss: 0.4903 - acc: 0.773 - ETA: 1s - loss: 0.4900 - acc: 0.773 - ETA: 1s - loss: 0.4905 - acc: 0.773 - ETA: 1s - loss: 0.4911 - acc: 0.772 - ETA: 1s - loss: 0.4910 - acc: 0.772 - ETA: 1s - loss: 0.4908 - acc: 0.772 - ETA: 1s - loss: 0.4906 - acc: 0.772 - ETA: 1s - loss: 0.4903 - acc: 0.773 - ETA: 1s - loss: 0.4904 - acc: 0.773 - ETA: 1s - loss: 0.4901 - acc: 0.773 - ETA: 1s - loss: 0.4904 - acc: 0.773 - ETA: 1s - loss: 0.4900 - acc: 0.773 - ETA: 1s - loss: 0.4904 - acc: 0.773 - ETA: 1s - loss: 0.4899 - acc: 0.773 - ETA: 1s - loss: 0.4903 - acc: 0.773 - ETA: 0s - loss: 0.4902 - acc: 0.773 - ETA: 0s - loss: 0.4902 - acc: 0.773 - ETA: 0s - loss: 0.4896 - acc: 0.773 - ETA: 0s - loss: 0.4894 - acc: 0.773 - ETA: 0s - loss: 0.4897 - acc: 0.773 - ETA: 0s - loss: 0.4898 - acc: 0.773 - ETA: 0s - loss: 0.4897 - acc: 0.773 - ETA: 0s - loss: 0.4898 - acc: 0.773 - ETA: 0s - loss: 0.4899 - acc: 0.773 - ETA: 0s - loss: 0.4900 - acc: 0.773 - ETA: 0s - loss: 0.4899 - acc: 0.772 - ETA: 0s - loss: 0.4899 - acc: 0.773 - ETA: 0s - loss: 0.4897 - acc: 0.773 - ETA: 0s - loss: 0.4898 - acc: 0.773 - ETA: 0s - loss: 0.4899 - acc: 0.773 - ETA: 0s - loss: 0.4899 - acc: 0.773 - ETA: 0s - loss: 0.4897 - acc: 0.773 - 5s 73us/step - loss: 0.4895 - acc: 0.7732\n",
      "Epoch 15/20\n",
      "70041/70041 [==============================] - ETA: 4s - loss: 0.4395 - acc: 0.793 - ETA: 5s - loss: 0.4980 - acc: 0.768 - ETA: 4s - loss: 0.4893 - acc: 0.770 - ETA: 5s - loss: 0.4913 - acc: 0.767 - ETA: 4s - loss: 0.4899 - acc: 0.769 - ETA: 4s - loss: 0.4939 - acc: 0.768 - ETA: 4s - loss: 0.4960 - acc: 0.766 - ETA: 4s - loss: 0.4955 - acc: 0.764 - ETA: 4s - loss: 0.4970 - acc: 0.766 - ETA: 4s - loss: 0.4957 - acc: 0.768 - ETA: 4s - loss: 0.4965 - acc: 0.768 - ETA: 4s - loss: 0.4978 - acc: 0.767 - ETA: 4s - loss: 0.4956 - acc: 0.769 - ETA: 4s - loss: 0.4962 - acc: 0.770 - ETA: 4s - loss: 0.4950 - acc: 0.771 - ETA: 4s - loss: 0.4948 - acc: 0.771 - ETA: 4s - loss: 0.4940 - acc: 0.771 - ETA: 4s - loss: 0.4951 - acc: 0.771 - ETA: 4s - loss: 0.4951 - acc: 0.771 - ETA: 4s - loss: 0.4950 - acc: 0.771 - ETA: 4s - loss: 0.4953 - acc: 0.770 - ETA: 4s - loss: 0.4942 - acc: 0.770 - ETA: 3s - loss: 0.4932 - acc: 0.771 - ETA: 3s - loss: 0.4939 - acc: 0.770 - ETA: 3s - loss: 0.4935 - acc: 0.770 - ETA: 3s - loss: 0.4937 - acc: 0.770 - ETA: 3s - loss: 0.4935 - acc: 0.770 - ETA: 3s - loss: 0.4929 - acc: 0.771 - ETA: 3s - loss: 0.4925 - acc: 0.771 - ETA: 3s - loss: 0.4915 - acc: 0.771 - ETA: 3s - loss: 0.4905 - acc: 0.772 - ETA: 3s - loss: 0.4898 - acc: 0.772 - ETA: 3s - loss: 0.4903 - acc: 0.772 - ETA: 3s - loss: 0.4904 - acc: 0.772 - ETA: 3s - loss: 0.4901 - acc: 0.772 - ETA: 3s - loss: 0.4896 - acc: 0.772 - ETA: 3s - loss: 0.4902 - acc: 0.772 - ETA: 3s - loss: 0.4901 - acc: 0.772 - ETA: 3s - loss: 0.4912 - acc: 0.771 - ETA: 2s - loss: 0.4899 - acc: 0.772 - ETA: 2s - loss: 0.4908 - acc: 0.772 - ETA: 2s - loss: 0.4905 - acc: 0.772 - ETA: 2s - loss: 0.4905 - acc: 0.773 - ETA: 2s - loss: 0.4895 - acc: 0.773 - ETA: 2s - loss: 0.4892 - acc: 0.773 - ETA: 2s - loss: 0.4883 - acc: 0.774 - ETA: 2s - loss: 0.4880 - acc: 0.774 - ETA: 2s - loss: 0.4882 - acc: 0.774 - ETA: 2s - loss: 0.4882 - acc: 0.774 - ETA: 2s - loss: 0.4881 - acc: 0.774 - ETA: 2s - loss: 0.4881 - acc: 0.774 - ETA: 2s - loss: 0.4878 - acc: 0.774 - ETA: 2s - loss: 0.4872 - acc: 0.775 - ETA: 2s - loss: 0.4867 - acc: 0.775 - ETA: 2s - loss: 0.4863 - acc: 0.776 - ETA: 2s - loss: 0.4868 - acc: 0.775 - ETA: 1s - loss: 0.4867 - acc: 0.775 - ETA: 1s - loss: 0.4869 - acc: 0.775 - ETA: 1s - loss: 0.4863 - acc: 0.775 - ETA: 1s - loss: 0.4862 - acc: 0.775 - ETA: 1s - loss: 0.4861 - acc: 0.776 - ETA: 1s - loss: 0.4866 - acc: 0.775 - ETA: 1s - loss: 0.4867 - acc: 0.775 - ETA: 1s - loss: 0.4871 - acc: 0.775 - ETA: 1s - loss: 0.4870 - acc: 0.775 - ETA: 1s - loss: 0.4873 - acc: 0.774 - ETA: 1s - loss: 0.4870 - acc: 0.775 - ETA: 1s - loss: 0.4872 - acc: 0.774 - ETA: 1s - loss: 0.4874 - acc: 0.774 - ETA: 1s - loss: 0.4876 - acc: 0.774 - ETA: 1s - loss: 0.4877 - acc: 0.774 - ETA: 1s - loss: 0.4881 - acc: 0.774 - ETA: 1s - loss: 0.4885 - acc: 0.774 - ETA: 1s - loss: 0.4884 - acc: 0.774 - ETA: 0s - loss: 0.4889 - acc: 0.773 - ETA: 0s - loss: 0.4893 - acc: 0.773 - ETA: 0s - loss: 0.4895 - acc: 0.773 - ETA: 0s - loss: 0.4893 - acc: 0.773 - ETA: 0s - loss: 0.4892 - acc: 0.773 - ETA: 0s - loss: 0.4896 - acc: 0.773 - ETA: 0s - loss: 0.4900 - acc: 0.772 - ETA: 0s - loss: 0.4902 - acc: 0.772 - ETA: 0s - loss: 0.4900 - acc: 0.772 - ETA: 0s - loss: 0.4894 - acc: 0.773 - ETA: 0s - loss: 0.4891 - acc: 0.773 - ETA: 0s - loss: 0.4891 - acc: 0.773 - ETA: 0s - loss: 0.4888 - acc: 0.773 - ETA: 0s - loss: 0.4892 - acc: 0.773 - ETA: 0s - loss: 0.4888 - acc: 0.773 - ETA: 0s - loss: 0.4894 - acc: 0.773 - ETA: 0s - loss: 0.4895 - acc: 0.773 - ETA: 0s - loss: 0.4896 - acc: 0.773 - 5s 76us/step - loss: 0.4895 - acc: 0.7733\n",
      "Epoch 16/20\n",
      "70041/70041 [==============================] - ETA: 5s - loss: 0.4997 - acc: 0.796 - ETA: 5s - loss: 0.4996 - acc: 0.775 - ETA: 6s - loss: 0.4941 - acc: 0.772 - ETA: 6s - loss: 0.4984 - acc: 0.768 - ETA: 5s - loss: 0.4949 - acc: 0.769 - ETA: 5s - loss: 0.4973 - acc: 0.768 - ETA: 5s - loss: 0.4969 - acc: 0.768 - ETA: 5s - loss: 0.4962 - acc: 0.769 - ETA: 5s - loss: 0.4963 - acc: 0.769 - ETA: 5s - loss: 0.4963 - acc: 0.767 - ETA: 4s - loss: 0.4954 - acc: 0.767 - ETA: 4s - loss: 0.4929 - acc: 0.769 - ETA: 4s - loss: 0.4941 - acc: 0.769 - ETA: 4s - loss: 0.4932 - acc: 0.770 - ETA: 4s - loss: 0.4951 - acc: 0.769 - ETA: 4s - loss: 0.4944 - acc: 0.768 - ETA: 4s - loss: 0.4925 - acc: 0.769 - ETA: 4s - loss: 0.4913 - acc: 0.770 - ETA: 4s - loss: 0.4930 - acc: 0.769 - ETA: 4s - loss: 0.4937 - acc: 0.769 - ETA: 4s - loss: 0.4943 - acc: 0.768 - ETA: 4s - loss: 0.4935 - acc: 0.769 - ETA: 4s - loss: 0.4938 - acc: 0.769 - ETA: 4s - loss: 0.4945 - acc: 0.768 - ETA: 4s - loss: 0.4945 - acc: 0.768 - ETA: 4s - loss: 0.4945 - acc: 0.767 - ETA: 4s - loss: 0.4934 - acc: 0.768 - ETA: 4s - loss: 0.4933 - acc: 0.768 - ETA: 4s - loss: 0.4947 - acc: 0.768 - ETA: 4s - loss: 0.4954 - acc: 0.767 - ETA: 4s - loss: 0.4952 - acc: 0.767 - ETA: 4s - loss: 0.4964 - acc: 0.767 - ETA: 4s - loss: 0.4969 - acc: 0.767 - ETA: 4s - loss: 0.4956 - acc: 0.768 - ETA: 4s - loss: 0.4944 - acc: 0.769 - ETA: 4s - loss: 0.4941 - acc: 0.769 - ETA: 3s - loss: 0.4941 - acc: 0.768 - ETA: 3s - loss: 0.4936 - acc: 0.769 - ETA: 3s - loss: 0.4934 - acc: 0.769 - ETA: 3s - loss: 0.4925 - acc: 0.769 - ETA: 3s - loss: 0.4919 - acc: 0.770 - ETA: 3s - loss: 0.4920 - acc: 0.769 - ETA: 3s - loss: 0.4918 - acc: 0.770 - ETA: 3s - loss: 0.4921 - acc: 0.769 - ETA: 3s - loss: 0.4916 - acc: 0.770 - ETA: 3s - loss: 0.4917 - acc: 0.769 - ETA: 3s - loss: 0.4918 - acc: 0.769 - ETA: 3s - loss: 0.4917 - acc: 0.769 - ETA: 3s - loss: 0.4911 - acc: 0.770 - ETA: 3s - loss: 0.4915 - acc: 0.770 - ETA: 2s - loss: 0.4907 - acc: 0.770 - ETA: 2s - loss: 0.4903 - acc: 0.771 - ETA: 2s - loss: 0.4913 - acc: 0.770 - ETA: 2s - loss: 0.4912 - acc: 0.770 - ETA: 2s - loss: 0.4910 - acc: 0.770 - ETA: 2s - loss: 0.4909 - acc: 0.770 - ETA: 2s - loss: 0.4904 - acc: 0.771 - ETA: 2s - loss: 0.4904 - acc: 0.771 - ETA: 2s - loss: 0.4904 - acc: 0.771 - ETA: 2s - loss: 0.4902 - acc: 0.771 - ETA: 2s - loss: 0.4904 - acc: 0.771 - ETA: 2s - loss: 0.4903 - acc: 0.771 - ETA: 2s - loss: 0.4899 - acc: 0.771 - ETA: 2s - loss: 0.4895 - acc: 0.771 - ETA: 1s - loss: 0.4896 - acc: 0.771 - ETA: 1s - loss: 0.4900 - acc: 0.771 - ETA: 1s - loss: 0.4899 - acc: 0.771 - ETA: 1s - loss: 0.4898 - acc: 0.771 - ETA: 1s - loss: 0.4895 - acc: 0.771 - ETA: 1s - loss: 0.4893 - acc: 0.772 - ETA: 1s - loss: 0.4892 - acc: 0.772 - ETA: 1s - loss: 0.4893 - acc: 0.772 - ETA: 1s - loss: 0.4894 - acc: 0.772 - ETA: 1s - loss: 0.4893 - acc: 0.772 - ETA: 1s - loss: 0.4890 - acc: 0.772 - ETA: 1s - loss: 0.4888 - acc: 0.772 - ETA: 1s - loss: 0.4884 - acc: 0.773 - ETA: 1s - loss: 0.4880 - acc: 0.773 - ETA: 1s - loss: 0.4877 - acc: 0.773 - ETA: 0s - loss: 0.4883 - acc: 0.773 - ETA: 0s - loss: 0.4886 - acc: 0.773 - ETA: 0s - loss: 0.4887 - acc: 0.773 - ETA: 0s - loss: 0.4886 - acc: 0.773 - ETA: 0s - loss: 0.4887 - acc: 0.773 - ETA: 0s - loss: 0.4882 - acc: 0.773 - ETA: 0s - loss: 0.4882 - acc: 0.773 - ETA: 0s - loss: 0.4882 - acc: 0.773 - ETA: 0s - loss: 0.4879 - acc: 0.773 - ETA: 0s - loss: 0.4876 - acc: 0.773 - ETA: 0s - loss: 0.4879 - acc: 0.773 - ETA: 0s - loss: 0.4880 - acc: 0.773 - ETA: 0s - loss: 0.4879 - acc: 0.773 - ETA: 0s - loss: 0.4877 - acc: 0.773 - ETA: 0s - loss: 0.4880 - acc: 0.773 - ETA: 0s - loss: 0.4881 - acc: 0.773 - 6s 81us/step - loss: 0.4879 - acc: 0.7733\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70041/70041 [==============================] - ETA: 5s - loss: 0.5460 - acc: 0.730 - ETA: 4s - loss: 0.5164 - acc: 0.753 - ETA: 4s - loss: 0.5085 - acc: 0.760 - ETA: 4s - loss: 0.5044 - acc: 0.757 - ETA: 4s - loss: 0.5006 - acc: 0.760 - ETA: 4s - loss: 0.4973 - acc: 0.762 - ETA: 4s - loss: 0.4937 - acc: 0.765 - ETA: 4s - loss: 0.4929 - acc: 0.766 - ETA: 4s - loss: 0.4934 - acc: 0.767 - ETA: 4s - loss: 0.4917 - acc: 0.769 - ETA: 4s - loss: 0.4924 - acc: 0.769 - ETA: 4s - loss: 0.4915 - acc: 0.769 - ETA: 4s - loss: 0.4922 - acc: 0.768 - ETA: 4s - loss: 0.4944 - acc: 0.765 - ETA: 4s - loss: 0.4929 - acc: 0.767 - ETA: 4s - loss: 0.4929 - acc: 0.766 - ETA: 4s - loss: 0.4914 - acc: 0.767 - ETA: 4s - loss: 0.4928 - acc: 0.767 - ETA: 4s - loss: 0.4923 - acc: 0.767 - ETA: 4s - loss: 0.4908 - acc: 0.769 - ETA: 4s - loss: 0.4898 - acc: 0.770 - ETA: 4s - loss: 0.4883 - acc: 0.771 - ETA: 3s - loss: 0.4890 - acc: 0.771 - ETA: 3s - loss: 0.4899 - acc: 0.771 - ETA: 3s - loss: 0.4896 - acc: 0.771 - ETA: 3s - loss: 0.4894 - acc: 0.771 - ETA: 3s - loss: 0.4899 - acc: 0.771 - ETA: 3s - loss: 0.4903 - acc: 0.771 - ETA: 3s - loss: 0.4897 - acc: 0.771 - ETA: 3s - loss: 0.4903 - acc: 0.771 - ETA: 3s - loss: 0.4911 - acc: 0.770 - ETA: 3s - loss: 0.4923 - acc: 0.769 - ETA: 3s - loss: 0.4921 - acc: 0.769 - ETA: 3s - loss: 0.4919 - acc: 0.769 - ETA: 3s - loss: 0.4923 - acc: 0.769 - ETA: 3s - loss: 0.4930 - acc: 0.768 - ETA: 3s - loss: 0.4930 - acc: 0.768 - ETA: 3s - loss: 0.4928 - acc: 0.768 - ETA: 3s - loss: 0.4924 - acc: 0.769 - ETA: 2s - loss: 0.4913 - acc: 0.770 - ETA: 2s - loss: 0.4906 - acc: 0.770 - ETA: 2s - loss: 0.4902 - acc: 0.771 - ETA: 2s - loss: 0.4898 - acc: 0.771 - ETA: 2s - loss: 0.4893 - acc: 0.772 - ETA: 2s - loss: 0.4893 - acc: 0.772 - ETA: 2s - loss: 0.4891 - acc: 0.772 - ETA: 2s - loss: 0.4884 - acc: 0.773 - ETA: 2s - loss: 0.4891 - acc: 0.773 - ETA: 2s - loss: 0.4891 - acc: 0.772 - ETA: 2s - loss: 0.4886 - acc: 0.772 - ETA: 2s - loss: 0.4886 - acc: 0.772 - ETA: 2s - loss: 0.4889 - acc: 0.772 - ETA: 2s - loss: 0.4886 - acc: 0.772 - ETA: 2s - loss: 0.4885 - acc: 0.772 - ETA: 2s - loss: 0.4892 - acc: 0.772 - ETA: 2s - loss: 0.4895 - acc: 0.772 - ETA: 2s - loss: 0.4893 - acc: 0.772 - ETA: 1s - loss: 0.4889 - acc: 0.772 - ETA: 1s - loss: 0.4886 - acc: 0.772 - ETA: 1s - loss: 0.4891 - acc: 0.771 - ETA: 1s - loss: 0.4888 - acc: 0.772 - ETA: 1s - loss: 0.4888 - acc: 0.772 - ETA: 1s - loss: 0.4889 - acc: 0.772 - ETA: 1s - loss: 0.4891 - acc: 0.771 - ETA: 1s - loss: 0.4892 - acc: 0.772 - ETA: 1s - loss: 0.4897 - acc: 0.771 - ETA: 1s - loss: 0.4894 - acc: 0.771 - ETA: 1s - loss: 0.4891 - acc: 0.772 - ETA: 1s - loss: 0.4891 - acc: 0.772 - ETA: 1s - loss: 0.4888 - acc: 0.772 - ETA: 1s - loss: 0.4890 - acc: 0.772 - ETA: 1s - loss: 0.4889 - acc: 0.772 - ETA: 1s - loss: 0.4887 - acc: 0.772 - ETA: 1s - loss: 0.4891 - acc: 0.772 - ETA: 1s - loss: 0.4892 - acc: 0.772 - ETA: 0s - loss: 0.4893 - acc: 0.772 - ETA: 0s - loss: 0.4892 - acc: 0.772 - ETA: 0s - loss: 0.4889 - acc: 0.772 - ETA: 0s - loss: 0.4886 - acc: 0.772 - ETA: 0s - loss: 0.4892 - acc: 0.772 - ETA: 0s - loss: 0.4891 - acc: 0.772 - ETA: 0s - loss: 0.4893 - acc: 0.772 - ETA: 0s - loss: 0.4891 - acc: 0.772 - ETA: 0s - loss: 0.4891 - acc: 0.772 - ETA: 0s - loss: 0.4890 - acc: 0.772 - ETA: 0s - loss: 0.4889 - acc: 0.772 - ETA: 0s - loss: 0.4888 - acc: 0.772 - ETA: 0s - loss: 0.4888 - acc: 0.772 - ETA: 0s - loss: 0.4884 - acc: 0.773 - ETA: 0s - loss: 0.4883 - acc: 0.773 - ETA: 0s - loss: 0.4882 - acc: 0.773 - ETA: 0s - loss: 0.4882 - acc: 0.773 - 5s 74us/step - loss: 0.4885 - acc: 0.7731\n",
      "Epoch 18/20\n",
      "70041/70041 [==============================] - ETA: 6s - loss: 0.4778 - acc: 0.781 - ETA: 5s - loss: 0.4916 - acc: 0.771 - ETA: 5s - loss: 0.4784 - acc: 0.779 - ETA: 5s - loss: 0.4747 - acc: 0.785 - ETA: 4s - loss: 0.4715 - acc: 0.789 - ETA: 4s - loss: 0.4797 - acc: 0.783 - ETA: 4s - loss: 0.4822 - acc: 0.783 - ETA: 4s - loss: 0.4834 - acc: 0.781 - ETA: 4s - loss: 0.4810 - acc: 0.783 - ETA: 4s - loss: 0.4827 - acc: 0.782 - ETA: 4s - loss: 0.4819 - acc: 0.781 - ETA: 4s - loss: 0.4836 - acc: 0.780 - ETA: 4s - loss: 0.4837 - acc: 0.780 - ETA: 4s - loss: 0.4846 - acc: 0.780 - ETA: 4s - loss: 0.4850 - acc: 0.779 - ETA: 4s - loss: 0.4871 - acc: 0.779 - ETA: 4s - loss: 0.4879 - acc: 0.778 - ETA: 4s - loss: 0.4887 - acc: 0.777 - ETA: 4s - loss: 0.4885 - acc: 0.777 - ETA: 4s - loss: 0.4904 - acc: 0.776 - ETA: 4s - loss: 0.4902 - acc: 0.775 - ETA: 4s - loss: 0.4894 - acc: 0.776 - ETA: 3s - loss: 0.4880 - acc: 0.777 - ETA: 3s - loss: 0.4885 - acc: 0.777 - ETA: 3s - loss: 0.4883 - acc: 0.777 - ETA: 3s - loss: 0.4883 - acc: 0.777 - ETA: 3s - loss: 0.4878 - acc: 0.777 - ETA: 3s - loss: 0.4881 - acc: 0.776 - ETA: 3s - loss: 0.4873 - acc: 0.777 - ETA: 3s - loss: 0.4874 - acc: 0.776 - ETA: 3s - loss: 0.4884 - acc: 0.775 - ETA: 3s - loss: 0.4883 - acc: 0.775 - ETA: 3s - loss: 0.4878 - acc: 0.775 - ETA: 3s - loss: 0.4879 - acc: 0.775 - ETA: 3s - loss: 0.4876 - acc: 0.774 - ETA: 3s - loss: 0.4892 - acc: 0.774 - ETA: 3s - loss: 0.4899 - acc: 0.773 - ETA: 3s - loss: 0.4899 - acc: 0.773 - ETA: 3s - loss: 0.4901 - acc: 0.773 - ETA: 2s - loss: 0.4902 - acc: 0.773 - ETA: 2s - loss: 0.4901 - acc: 0.773 - ETA: 2s - loss: 0.4891 - acc: 0.774 - ETA: 2s - loss: 0.4886 - acc: 0.774 - ETA: 2s - loss: 0.4886 - acc: 0.774 - ETA: 2s - loss: 0.4889 - acc: 0.774 - ETA: 2s - loss: 0.4886 - acc: 0.775 - ETA: 2s - loss: 0.4883 - acc: 0.775 - ETA: 2s - loss: 0.4888 - acc: 0.775 - ETA: 2s - loss: 0.4886 - acc: 0.775 - ETA: 2s - loss: 0.4887 - acc: 0.774 - ETA: 2s - loss: 0.4890 - acc: 0.774 - ETA: 2s - loss: 0.4891 - acc: 0.774 - ETA: 2s - loss: 0.4893 - acc: 0.774 - ETA: 2s - loss: 0.4892 - acc: 0.774 - ETA: 2s - loss: 0.4891 - acc: 0.774 - ETA: 2s - loss: 0.4884 - acc: 0.775 - ETA: 1s - loss: 0.4883 - acc: 0.775 - ETA: 1s - loss: 0.4885 - acc: 0.775 - ETA: 1s - loss: 0.4889 - acc: 0.774 - ETA: 1s - loss: 0.4893 - acc: 0.774 - ETA: 1s - loss: 0.4892 - acc: 0.774 - ETA: 1s - loss: 0.4893 - acc: 0.774 - ETA: 1s - loss: 0.4895 - acc: 0.774 - ETA: 1s - loss: 0.4894 - acc: 0.774 - ETA: 1s - loss: 0.4899 - acc: 0.774 - ETA: 1s - loss: 0.4897 - acc: 0.774 - ETA: 1s - loss: 0.4899 - acc: 0.773 - ETA: 1s - loss: 0.4899 - acc: 0.773 - ETA: 1s - loss: 0.4897 - acc: 0.773 - ETA: 1s - loss: 0.4899 - acc: 0.773 - ETA: 1s - loss: 0.4899 - acc: 0.773 - ETA: 1s - loss: 0.4899 - acc: 0.773 - ETA: 1s - loss: 0.4895 - acc: 0.773 - ETA: 1s - loss: 0.4893 - acc: 0.773 - ETA: 0s - loss: 0.4894 - acc: 0.773 - ETA: 0s - loss: 0.4893 - acc: 0.773 - ETA: 0s - loss: 0.4890 - acc: 0.773 - ETA: 0s - loss: 0.4890 - acc: 0.773 - ETA: 0s - loss: 0.4889 - acc: 0.773 - ETA: 0s - loss: 0.4891 - acc: 0.773 - ETA: 0s - loss: 0.4889 - acc: 0.773 - ETA: 0s - loss: 0.4890 - acc: 0.773 - ETA: 0s - loss: 0.4887 - acc: 0.773 - ETA: 0s - loss: 0.4889 - acc: 0.773 - ETA: 0s - loss: 0.4888 - acc: 0.773 - ETA: 0s - loss: 0.4887 - acc: 0.773 - ETA: 0s - loss: 0.4887 - acc: 0.773 - ETA: 0s - loss: 0.4887 - acc: 0.773 - ETA: 0s - loss: 0.4886 - acc: 0.773 - ETA: 0s - loss: 0.4885 - acc: 0.773 - ETA: 0s - loss: 0.4886 - acc: 0.773 - ETA: 0s - loss: 0.4886 - acc: 0.773 - 5s 75us/step - loss: 0.4885 - acc: 0.7734\n",
      "Epoch 19/20\n",
      "70041/70041 [==============================] - ETA: 5s - loss: 0.4822 - acc: 0.789 - ETA: 5s - loss: 0.4781 - acc: 0.777 - ETA: 5s - loss: 0.4714 - acc: 0.785 - ETA: 5s - loss: 0.4886 - acc: 0.771 - ETA: 5s - loss: 0.4998 - acc: 0.760 - ETA: 5s - loss: 0.4964 - acc: 0.760 - ETA: 5s - loss: 0.4972 - acc: 0.762 - ETA: 5s - loss: 0.4980 - acc: 0.761 - ETA: 5s - loss: 0.4962 - acc: 0.764 - ETA: 5s - loss: 0.4951 - acc: 0.765 - ETA: 5s - loss: 0.4926 - acc: 0.768 - ETA: 5s - loss: 0.4906 - acc: 0.771 - ETA: 5s - loss: 0.4903 - acc: 0.772 - ETA: 5s - loss: 0.4912 - acc: 0.772 - ETA: 5s - loss: 0.4919 - acc: 0.772 - ETA: 5s - loss: 0.4905 - acc: 0.773 - ETA: 5s - loss: 0.4906 - acc: 0.772 - ETA: 5s - loss: 0.4909 - acc: 0.772 - ETA: 4s - loss: 0.4891 - acc: 0.774 - ETA: 4s - loss: 0.4900 - acc: 0.773 - ETA: 4s - loss: 0.4903 - acc: 0.773 - ETA: 4s - loss: 0.4905 - acc: 0.773 - ETA: 4s - loss: 0.4891 - acc: 0.774 - ETA: 4s - loss: 0.4890 - acc: 0.774 - ETA: 4s - loss: 0.4884 - acc: 0.775 - ETA: 4s - loss: 0.4887 - acc: 0.775 - ETA: 4s - loss: 0.4885 - acc: 0.774 - ETA: 4s - loss: 0.4873 - acc: 0.775 - ETA: 4s - loss: 0.4875 - acc: 0.774 - ETA: 4s - loss: 0.4869 - acc: 0.774 - ETA: 4s - loss: 0.4861 - acc: 0.775 - ETA: 3s - loss: 0.4871 - acc: 0.774 - ETA: 3s - loss: 0.4876 - acc: 0.773 - ETA: 3s - loss: 0.4878 - acc: 0.774 - ETA: 3s - loss: 0.4872 - acc: 0.774 - ETA: 3s - loss: 0.4876 - acc: 0.773 - ETA: 3s - loss: 0.4868 - acc: 0.774 - ETA: 3s - loss: 0.4869 - acc: 0.773 - ETA: 3s - loss: 0.4867 - acc: 0.774 - ETA: 3s - loss: 0.4873 - acc: 0.773 - ETA: 3s - loss: 0.4870 - acc: 0.774 - ETA: 3s - loss: 0.4875 - acc: 0.774 - ETA: 3s - loss: 0.4881 - acc: 0.773 - ETA: 3s - loss: 0.4879 - acc: 0.773 - ETA: 3s - loss: 0.4872 - acc: 0.774 - ETA: 3s - loss: 0.4873 - acc: 0.774 - ETA: 3s - loss: 0.4873 - acc: 0.773 - ETA: 2s - loss: 0.4869 - acc: 0.774 - ETA: 2s - loss: 0.4875 - acc: 0.774 - ETA: 2s - loss: 0.4863 - acc: 0.775 - ETA: 2s - loss: 0.4860 - acc: 0.775 - ETA: 2s - loss: 0.4865 - acc: 0.775 - ETA: 2s - loss: 0.4867 - acc: 0.775 - ETA: 2s - loss: 0.4870 - acc: 0.774 - ETA: 2s - loss: 0.4873 - acc: 0.774 - ETA: 2s - loss: 0.4867 - acc: 0.775 - ETA: 2s - loss: 0.4866 - acc: 0.775 - ETA: 2s - loss: 0.4865 - acc: 0.774 - ETA: 2s - loss: 0.4862 - acc: 0.774 - ETA: 2s - loss: 0.4860 - acc: 0.775 - ETA: 2s - loss: 0.4862 - acc: 0.775 - ETA: 2s - loss: 0.4863 - acc: 0.774 - ETA: 1s - loss: 0.4865 - acc: 0.774 - ETA: 1s - loss: 0.4866 - acc: 0.774 - ETA: 1s - loss: 0.4870 - acc: 0.773 - ETA: 1s - loss: 0.4870 - acc: 0.774 - ETA: 1s - loss: 0.4870 - acc: 0.774 - ETA: 1s - loss: 0.4870 - acc: 0.774 - ETA: 1s - loss: 0.4867 - acc: 0.774 - ETA: 1s - loss: 0.4865 - acc: 0.774 - ETA: 1s - loss: 0.4865 - acc: 0.774 - ETA: 1s - loss: 0.4866 - acc: 0.774 - ETA: 1s - loss: 0.4871 - acc: 0.773 - ETA: 1s - loss: 0.4870 - acc: 0.774 - ETA: 1s - loss: 0.4869 - acc: 0.774 - ETA: 1s - loss: 0.4871 - acc: 0.773 - ETA: 1s - loss: 0.4870 - acc: 0.773 - ETA: 1s - loss: 0.4872 - acc: 0.773 - ETA: 1s - loss: 0.4873 - acc: 0.773 - ETA: 1s - loss: 0.4873 - acc: 0.773 - ETA: 0s - loss: 0.4875 - acc: 0.773 - ETA: 0s - loss: 0.4874 - acc: 0.773 - ETA: 0s - loss: 0.4874 - acc: 0.773 - ETA: 0s - loss: 0.4872 - acc: 0.773 - ETA: 0s - loss: 0.4872 - acc: 0.773 - ETA: 0s - loss: 0.4871 - acc: 0.773 - ETA: 0s - loss: 0.4872 - acc: 0.773 - ETA: 0s - loss: 0.4874 - acc: 0.773 - ETA: 0s - loss: 0.4873 - acc: 0.773 - ETA: 0s - loss: 0.4876 - acc: 0.773 - ETA: 0s - loss: 0.4878 - acc: 0.773 - ETA: 0s - loss: 0.4880 - acc: 0.773 - ETA: 0s - loss: 0.4883 - acc: 0.772 - ETA: 0s - loss: 0.4883 - acc: 0.772 - ETA: 0s - loss: 0.4879 - acc: 0.773 - ETA: 0s - loss: 0.4875 - acc: 0.773 - 6s 82us/step - loss: 0.4876 - acc: 0.7732\n",
      "Epoch 20/20\n",
      "70041/70041 [==============================] - ETA: 5s - loss: 0.4533 - acc: 0.793 - ETA: 5s - loss: 0.4708 - acc: 0.766 - ETA: 4s - loss: 0.4859 - acc: 0.770 - ETA: 4s - loss: 0.4883 - acc: 0.772 - ETA: 4s - loss: 0.4947 - acc: 0.768 - ETA: 4s - loss: 0.4924 - acc: 0.768 - ETA: 4s - loss: 0.4933 - acc: 0.768 - ETA: 4s - loss: 0.4900 - acc: 0.770 - ETA: 4s - loss: 0.4919 - acc: 0.771 - ETA: 4s - loss: 0.4913 - acc: 0.772 - ETA: 4s - loss: 0.4922 - acc: 0.772 - ETA: 4s - loss: 0.4895 - acc: 0.774 - ETA: 4s - loss: 0.4915 - acc: 0.773 - ETA: 4s - loss: 0.4887 - acc: 0.774 - ETA: 4s - loss: 0.4881 - acc: 0.775 - ETA: 4s - loss: 0.4858 - acc: 0.776 - ETA: 4s - loss: 0.4868 - acc: 0.776 - ETA: 4s - loss: 0.4868 - acc: 0.775 - ETA: 4s - loss: 0.4877 - acc: 0.775 - ETA: 4s - loss: 0.4875 - acc: 0.774 - ETA: 4s - loss: 0.4871 - acc: 0.774 - ETA: 4s - loss: 0.4884 - acc: 0.774 - ETA: 3s - loss: 0.4886 - acc: 0.774 - ETA: 3s - loss: 0.4884 - acc: 0.774 - ETA: 3s - loss: 0.4871 - acc: 0.775 - ETA: 3s - loss: 0.4881 - acc: 0.774 - ETA: 3s - loss: 0.4878 - acc: 0.774 - ETA: 3s - loss: 0.4884 - acc: 0.774 - ETA: 3s - loss: 0.4875 - acc: 0.774 - ETA: 3s - loss: 0.4873 - acc: 0.775 - ETA: 3s - loss: 0.4881 - acc: 0.774 - ETA: 3s - loss: 0.4882 - acc: 0.774 - ETA: 3s - loss: 0.4886 - acc: 0.774 - ETA: 3s - loss: 0.4887 - acc: 0.774 - ETA: 3s - loss: 0.4885 - acc: 0.774 - ETA: 3s - loss: 0.4891 - acc: 0.773 - ETA: 3s - loss: 0.4891 - acc: 0.773 - ETA: 3s - loss: 0.4898 - acc: 0.772 - ETA: 3s - loss: 0.4897 - acc: 0.773 - ETA: 2s - loss: 0.4893 - acc: 0.773 - ETA: 2s - loss: 0.4886 - acc: 0.774 - ETA: 2s - loss: 0.4886 - acc: 0.773 - ETA: 2s - loss: 0.4882 - acc: 0.773 - ETA: 2s - loss: 0.4878 - acc: 0.774 - ETA: 2s - loss: 0.4871 - acc: 0.774 - ETA: 2s - loss: 0.4878 - acc: 0.774 - ETA: 2s - loss: 0.4876 - acc: 0.774 - ETA: 2s - loss: 0.4877 - acc: 0.774 - ETA: 2s - loss: 0.4883 - acc: 0.773 - ETA: 2s - loss: 0.4888 - acc: 0.773 - ETA: 2s - loss: 0.4889 - acc: 0.772 - ETA: 2s - loss: 0.4886 - acc: 0.772 - ETA: 2s - loss: 0.4883 - acc: 0.773 - ETA: 2s - loss: 0.4880 - acc: 0.773 - ETA: 2s - loss: 0.4885 - acc: 0.773 - ETA: 2s - loss: 0.4880 - acc: 0.773 - ETA: 1s - loss: 0.4877 - acc: 0.773 - ETA: 1s - loss: 0.4882 - acc: 0.773 - ETA: 1s - loss: 0.4881 - acc: 0.774 - ETA: 1s - loss: 0.4879 - acc: 0.773 - ETA: 1s - loss: 0.4878 - acc: 0.774 - ETA: 1s - loss: 0.4879 - acc: 0.773 - ETA: 1s - loss: 0.4880 - acc: 0.773 - ETA: 1s - loss: 0.4878 - acc: 0.773 - ETA: 1s - loss: 0.4878 - acc: 0.773 - ETA: 1s - loss: 0.4879 - acc: 0.773 - ETA: 1s - loss: 0.4882 - acc: 0.773 - ETA: 1s - loss: 0.4882 - acc: 0.773 - ETA: 1s - loss: 0.4882 - acc: 0.773 - ETA: 1s - loss: 0.4882 - acc: 0.773 - ETA: 1s - loss: 0.4879 - acc: 0.774 - ETA: 1s - loss: 0.4881 - acc: 0.773 - ETA: 1s - loss: 0.4879 - acc: 0.774 - ETA: 1s - loss: 0.4875 - acc: 0.774 - ETA: 0s - loss: 0.4876 - acc: 0.774 - ETA: 0s - loss: 0.4873 - acc: 0.774 - ETA: 0s - loss: 0.4874 - acc: 0.774 - ETA: 0s - loss: 0.4875 - acc: 0.774 - ETA: 0s - loss: 0.4874 - acc: 0.774 - ETA: 0s - loss: 0.4873 - acc: 0.774 - ETA: 0s - loss: 0.4875 - acc: 0.774 - ETA: 0s - loss: 0.4874 - acc: 0.774 - ETA: 0s - loss: 0.4875 - acc: 0.773 - ETA: 0s - loss: 0.4876 - acc: 0.773 - ETA: 0s - loss: 0.4875 - acc: 0.773 - ETA: 0s - loss: 0.4875 - acc: 0.773 - ETA: 0s - loss: 0.4876 - acc: 0.773 - ETA: 0s - loss: 0.4870 - acc: 0.773 - ETA: 0s - loss: 0.4873 - acc: 0.773 - ETA: 0s - loss: 0.4872 - acc: 0.773 - ETA: 0s - loss: 0.4872 - acc: 0.773 - ETA: 0s - loss: 0.4874 - acc: 0.773 - 5s 74us/step - loss: 0.4875 - acc: 0.7733\n",
      "30018/30018 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 47us/step\n",
      "[0.4910358894124674, 0.7691052034929136]\n"
     ]
    }
   ],
   "source": [
    "# KERAS\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=2626, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=256)\n",
    "score = model.evaluate(X_test, y_test, batch_size=256)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best results: \n",
    "1. Undersampling\n",
    "GradientBoostingClassifier\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.89      0.62      0.73      6986\n",
    "         1.0       0.71      0.92      0.80      7038\n",
    "\n",
    "   micro avg       0.77      0.77      0.77     14024\n",
    "   macro avg       0.80      0.77      0.76     14024\n",
    "weighted avg       0.80      0.77      0.76     14024\n",
    "\n",
    "[[4304 2682]\n",
    " [ 537 6501]]\n",
    "Accuracy is  77.04649172846548\n",
    "Time on model's work: 736.97 s\n",
    "============================\n",
    "XGBClassifier\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.89      0.61      0.73      6986\n",
    "         1.0       0.71      0.93      0.80      7038\n",
    "\n",
    "   micro avg       0.77      0.77      0.77     14024\n",
    "   macro avg       0.80      0.77      0.76     14024\n",
    "weighted avg       0.80      0.77      0.76     14024\n",
    "\n",
    "[[4288 2698]\n",
    " [ 524 6514]]\n",
    "Accuracy is  77.02509982886481\n",
    "Time on model's work: 480.744 s\n",
    "============================\n",
    "TFFMClassifier\n",
    "[order=2] accuracy: 0.7678265830005705\n",
    "[[4326 2715]\n",
    " [ 541 6442]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.89      0.61      0.73      7041\n",
    "         1.0       0.70      0.92      0.80      6983\n",
    "\n",
    "   micro avg       0.77      0.77      0.77     14024\n",
    "   macro avg       0.80      0.77      0.76     14024\n",
    "weighted avg       0.80      0.77      0.76     14024\n",
    "\n",
    "2. Oversampling\n",
    "GradientBoostingClassifier\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.88      0.62      0.73     15014\n",
    "         1.0       0.71      0.92      0.80     15004\n",
    "\n",
    "   micro avg       0.77      0.77      0.77     30018\n",
    "   macro avg       0.80      0.77      0.76     30018\n",
    "weighted avg       0.80      0.77      0.76     30018\n",
    "\n",
    "[[ 9306  5708]\n",
    " [ 1217 13787]]\n",
    "Accuracy is  76.93050836164967\n",
    "Time on model's work: 1198.169 s\n",
    "============================\n",
    "XGBClassifier\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.88      0.62      0.73     15014\n",
    "         1.0       0.71      0.92      0.80     15004\n",
    "\n",
    "   micro avg       0.77      0.77      0.77     30018\n",
    "   macro avg       0.80      0.77      0.76     30018\n",
    "weighted avg       0.80      0.77      0.76     30018\n",
    "\n",
    "[[ 9299  5715]\n",
    " [ 1212 13792]]\n",
    "Accuracy is  76.92384569258445\n",
    "Time on model's work: 561.64 s\n",
    "============================\n",
    "CatBoostClassifier\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.88      0.63      0.73     15014\n",
    "         1.0       0.71      0.91      0.80     15004\n",
    "\n",
    "   micro avg       0.77      0.77      0.77     30018\n",
    "   macro avg       0.79      0.77      0.76     30018\n",
    "weighted avg       0.79      0.77      0.76     30018\n",
    "\n",
    "[[ 9406  5608]\n",
    " [ 1304 13700]]\n",
    "Accuracy is  76.97381571057366\n",
    "Time on model's work: 318.586 s\n",
    "============================\n",
    "TFFMClassifier\n",
    "[order=2] accuracy: 0.7683056832567127\n",
    "[[ 9243  5771]\n",
    " [ 1184 13820]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.89      0.62      0.73     15014\n",
    "         1.0       0.71      0.92      0.80     15004\n",
    "\n",
    "   micro avg       0.77      0.77      0.77     30018\n",
    "   macro avg       0.80      0.77      0.76     30018\n",
    "weighted avg       0.80      0.77      0.76     30018\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling strategy shows better results than oversampling by time, accuracy, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Imbalanced variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy 0.7042175138171767\n",
      "Accuracy: 0.7163\n",
      "[[6662 2612]\n",
      " [ 225  501]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.72      0.82      9274\n",
      "         1.0       0.16      0.69      0.26       726\n",
      "\n",
      "   micro avg       0.72      0.72      0.72     10000\n",
      "   macro avg       0.56      0.70      0.54     10000\n",
      "weighted avg       0.91      0.72      0.78     10000\n",
      "\n",
      "Time on model's work: 53.088 s\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "t = time()\n",
    "bbc = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
    "                                sampling_strategy='auto',\n",
    "                                replacement=False,\n",
    "                                random_state=0)\n",
    "bbc.fit(X_train, y_train) \n",
    "predictions = bbc.predict(X_test)\n",
    "print('Balanced accuracy', balanced_accuracy_score(y_test, predictions)) \n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, predictions)))\n",
    "print(confusion_matrix(y_test,predictions)) \n",
    "print(classification_report(y_test, predictions))\n",
    "print (\"Time on model's work:\", round(time()-t, 3), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy 0.729588214570668\n",
      "Accuracy: 0.6986\n",
      "[[6430 2844]\n",
      " [ 170  556]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.69      0.81      9274\n",
      "         1.0       0.16      0.77      0.27       726\n",
      "\n",
      "   micro avg       0.70      0.70      0.70     10000\n",
      "   macro avg       0.57      0.73      0.54     10000\n",
      "weighted avg       0.92      0.70      0.77     10000\n",
      "\n",
      "Time on model's work: 496.02 s\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "t = time()\n",
    "brf = BalancedBaggingClassifier(n_estimators=100, random_state=0)\n",
    "brf.fit(X_train, y_train) \n",
    "predictions = brf.predict(X_test)\n",
    "print('Balanced accuracy', balanced_accuracy_score(y_test, predictions)) \n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, predictions)))\n",
    "print(confusion_matrix(y_test,predictions)) \n",
    "print(classification_report(y_test, predictions))\n",
    "print (\"Time on model's work:\", round(time()-t, 3), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy 0.5633828630770227\n",
      "Accuracy: 0.7824\n",
      "[[7601 1673]\n",
      " [ 503  223]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.82      0.87      9274\n",
      "         1.0       0.12      0.31      0.17       726\n",
      "\n",
      "   micro avg       0.78      0.78      0.78     10000\n",
      "   macro avg       0.53      0.56      0.52     10000\n",
      "weighted avg       0.88      0.78      0.82     10000\n",
      "\n",
      "Time on model's work: 42.791 s\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "t = time()\n",
    "rusboost = RUSBoostClassifier(random_state=10)\n",
    "rusboost.fit(X_train, y_train)  \n",
    "predictions = rusboost.predict(X_test)\n",
    "print('Balanced accuracy', balanced_accuracy_score(y_test, predictions)) \n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, predictions)))\n",
    "print(confusion_matrix(y_test,predictions)) \n",
    "print(classification_report(y_test, predictions))\n",
    "print (\"Time on model's work:\", round(time()-t, 3), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy 0.7680685241657266\n",
      "Accuracy: 0.6487\n",
      "[[5828 3446]\n",
      " [  67  659]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.63      0.77      9274\n",
      "         1.0       0.16      0.91      0.27       726\n",
      "\n",
      "   micro avg       0.65      0.65      0.65     10000\n",
      "   macro avg       0.57      0.77      0.52     10000\n",
      "weighted avg       0.93      0.65      0.73     10000\n",
      "\n",
      "Time on model's work: 135.531 s\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "t = time()\n",
    "eec = EasyEnsembleClassifier(random_state=11)\n",
    "eec.fit(X_train, y_train) \n",
    "predictions = eec.predict(X_test)\n",
    "print('Balanced accuracy', balanced_accuracy_score(y_test, predictions)) \n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, predictions)))\n",
    "print(confusion_matrix(y_test,predictions)) \n",
    "print(classification_report(y_test, predictions))\n",
    "print (\"Time on model's work:\", round(time()-t, 3), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 46269), (1.0, 3731)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(Counter(labels_list_array).items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 3731), (1.0, 3731)]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "X_resampled, y_resampled = rus.fit_resample(features_list_array, labels_list_array)\n",
    "print(sorted(Counter(y_resampled).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, random_state=35, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.72      0.74       760\n",
      "         1.0       0.73      0.77      0.75       733\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      1493\n",
      "   macro avg       0.75      0.75      0.75      1493\n",
      "weighted avg       0.75      0.75      0.75      1493\n",
      "\n",
      "[[546 214]\n",
      " [165 568]]\n",
      "Accuracy is  74.61486939048895\n",
      "Time on model's work: 0.85 s\n",
      "====================================================================================================\n",
      "GradientBoostingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.64      0.74       760\n",
      "         1.0       0.71      0.91      0.79       733\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      1493\n",
      "   macro avg       0.79      0.77      0.77      1493\n",
      "weighted avg       0.79      0.77      0.77      1493\n",
      "\n",
      "[[484 276]\n",
      " [ 69 664]]\n",
      "Accuracy is  76.8921634293369\n",
      "Time on model's work: 36.23 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.72      0.73       760\n",
      "         1.0       0.72      0.74      0.73       733\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      1493\n",
      "   macro avg       0.73      0.73      0.73      1493\n",
      "weighted avg       0.73      0.73      0.73      1493\n",
      "\n",
      "[[550 210]\n",
      " [188 545]]\n",
      "Accuracy is  73.34226389819156\n",
      "Time on model's work: 1.31 s\n",
      "====================================================================================================\n",
      "AdaBoostClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.66      0.75       760\n",
      "         1.0       0.72      0.89      0.79       733\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      1493\n",
      "   macro avg       0.79      0.78      0.77      1493\n",
      "weighted avg       0.79      0.77      0.77      1493\n",
      "\n",
      "[[504 256]\n",
      " [ 82 651]]\n",
      "Accuracy is  77.36101808439383\n",
      "Time on model's work: 8.838 s\n",
      "====================================================================================================\n",
      "BaggingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.71      0.73       760\n",
      "         1.0       0.72      0.76      0.74       733\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      1493\n",
      "   macro avg       0.74      0.74      0.73      1493\n",
      "weighted avg       0.74      0.73      0.73      1493\n",
      "\n",
      "[[541 219]\n",
      " [177 556]]\n",
      "Accuracy is  73.47622237106496\n",
      "Time on model's work: 6.363 s\n",
      "====================================================================================================\n",
      "DecisionTreeClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      0.71      0.71       760\n",
      "         1.0       0.70      0.68      0.69       733\n",
      "\n",
      "   micro avg       0.70      0.70      0.70      1493\n",
      "   macro avg       0.70      0.70      0.70      1493\n",
      "weighted avg       0.70      0.70      0.70      1493\n",
      "\n",
      "[[543 217]\n",
      " [231 502]]\n",
      "Accuracy is  69.99330207635633\n",
      "Time on model's work: 2.407 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.67      0.70       760\n",
      "         1.0       0.69      0.75      0.72       733\n",
      "\n",
      "   micro avg       0.71      0.71      0.71      1493\n",
      "   macro avg       0.71      0.71      0.71      1493\n",
      "weighted avg       0.71      0.71      0.71      1493\n",
      "\n",
      "[[511 249]\n",
      " [180 553]]\n",
      "Accuracy is  71.26590756865372\n",
      "Time on model's work: 116.734 s\n",
      "====================================================================================================\n",
      "XGBClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.64      0.74       760\n",
      "         1.0       0.71      0.91      0.80       733\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      1493\n",
      "   macro avg       0.80      0.77      0.77      1493\n",
      "weighted avg       0.80      0.77      0.77      1493\n",
      "\n",
      "[[483 277]\n",
      " [ 64 669]]\n",
      "Accuracy is  77.16008037508372\n",
      "Time on model's work: 40.978 s\n",
      "====================================================================================================\n",
      "0:\tlearn: 0.6801145\ttotal: 111ms\tremaining: 1m 50s\n",
      "1:\tlearn: 0.6679328\ttotal: 172ms\tremaining: 1m 25s\n",
      "2:\tlearn: 0.6566767\ttotal: 231ms\tremaining: 1m 16s\n",
      "3:\tlearn: 0.6461351\ttotal: 290ms\tremaining: 1m 12s\n",
      "4:\tlearn: 0.6364687\ttotal: 346ms\tremaining: 1m 8s\n",
      "5:\tlearn: 0.6273448\ttotal: 406ms\tremaining: 1m 7s\n",
      "6:\tlearn: 0.6189822\ttotal: 465ms\tremaining: 1m 5s\n",
      "7:\tlearn: 0.6111484\ttotal: 524ms\tremaining: 1m 4s\n",
      "8:\tlearn: 0.6038646\ttotal: 580ms\tremaining: 1m 3s\n",
      "9:\tlearn: 0.5971783\ttotal: 639ms\tremaining: 1m 3s\n",
      "10:\tlearn: 0.5908874\ttotal: 697ms\tremaining: 1m 2s\n",
      "11:\tlearn: 0.5851256\ttotal: 761ms\tremaining: 1m 2s\n",
      "12:\tlearn: 0.5811857\ttotal: 821ms\tremaining: 1m 2s\n",
      "13:\tlearn: 0.5759571\ttotal: 882ms\tremaining: 1m 2s\n",
      "14:\tlearn: 0.5711129\ttotal: 942ms\tremaining: 1m 1s\n",
      "15:\tlearn: 0.5665328\ttotal: 1s\tremaining: 1m 1s\n",
      "16:\tlearn: 0.5633767\ttotal: 1.06s\tremaining: 1m 1s\n",
      "17:\tlearn: 0.5593525\ttotal: 1.12s\tremaining: 1m 1s\n",
      "18:\tlearn: 0.5556650\ttotal: 1.18s\tremaining: 1m\n",
      "19:\tlearn: 0.5522110\ttotal: 1.23s\tremaining: 1m\n",
      "20:\tlearn: 0.5495358\ttotal: 1.29s\tremaining: 1m\n",
      "21:\tlearn: 0.5464261\ttotal: 1.35s\tremaining: 1m\n",
      "22:\tlearn: 0.5435765\ttotal: 1.41s\tremaining: 59.9s\n",
      "23:\tlearn: 0.5409172\ttotal: 1.47s\tremaining: 59.9s\n",
      "24:\tlearn: 0.5383524\ttotal: 1.53s\tremaining: 59.8s\n",
      "25:\tlearn: 0.5363983\ttotal: 1.59s\tremaining: 59.6s\n",
      "26:\tlearn: 0.5341995\ttotal: 1.65s\tremaining: 59.6s\n",
      "27:\tlearn: 0.5325112\ttotal: 1.72s\tremaining: 59.6s\n",
      "28:\tlearn: 0.5308785\ttotal: 1.78s\tremaining: 59.5s\n",
      "29:\tlearn: 0.5289521\ttotal: 1.83s\tremaining: 59.3s\n",
      "30:\tlearn: 0.5274569\ttotal: 1.89s\tremaining: 59.2s\n",
      "31:\tlearn: 0.5261791\ttotal: 1.95s\tremaining: 59.1s\n",
      "32:\tlearn: 0.5248631\ttotal: 2.01s\tremaining: 59s\n",
      "33:\tlearn: 0.5236192\ttotal: 2.07s\tremaining: 58.9s\n",
      "34:\tlearn: 0.5221048\ttotal: 2.14s\tremaining: 58.9s\n",
      "35:\tlearn: 0.5209727\ttotal: 2.2s\tremaining: 58.9s\n",
      "36:\tlearn: 0.5199312\ttotal: 2.26s\tremaining: 58.7s\n",
      "37:\tlearn: 0.5189951\ttotal: 2.31s\tremaining: 58.4s\n",
      "38:\tlearn: 0.5181905\ttotal: 2.37s\tremaining: 58.3s\n",
      "39:\tlearn: 0.5174168\ttotal: 2.42s\tremaining: 58.2s\n",
      "40:\tlearn: 0.5164690\ttotal: 2.49s\tremaining: 58.2s\n",
      "41:\tlearn: 0.5156239\ttotal: 2.56s\tremaining: 58.4s\n",
      "42:\tlearn: 0.5148695\ttotal: 2.62s\tremaining: 58.3s\n",
      "43:\tlearn: 0.5141124\ttotal: 2.68s\tremaining: 58.2s\n",
      "44:\tlearn: 0.5134548\ttotal: 2.74s\tremaining: 58.1s\n",
      "45:\tlearn: 0.5125602\ttotal: 2.8s\tremaining: 58s\n",
      "46:\tlearn: 0.5120346\ttotal: 2.86s\tremaining: 57.9s\n",
      "47:\tlearn: 0.5110413\ttotal: 2.92s\tremaining: 57.9s\n",
      "48:\tlearn: 0.5102478\ttotal: 2.98s\tremaining: 57.8s\n",
      "49:\tlearn: 0.5098423\ttotal: 3.04s\tremaining: 57.7s\n",
      "50:\tlearn: 0.5092727\ttotal: 3.1s\tremaining: 57.8s\n",
      "51:\tlearn: 0.5087064\ttotal: 3.17s\tremaining: 57.7s\n",
      "52:\tlearn: 0.5080493\ttotal: 3.23s\tremaining: 57.7s\n",
      "53:\tlearn: 0.5075360\ttotal: 3.28s\tremaining: 57.4s\n",
      "54:\tlearn: 0.5071853\ttotal: 3.34s\tremaining: 57.4s\n",
      "55:\tlearn: 0.5067470\ttotal: 3.4s\tremaining: 57.3s\n",
      "56:\tlearn: 0.5064395\ttotal: 3.46s\tremaining: 57.3s\n",
      "57:\tlearn: 0.5060868\ttotal: 3.52s\tremaining: 57.2s\n",
      "58:\tlearn: 0.5057213\ttotal: 3.58s\tremaining: 57.2s\n",
      "59:\tlearn: 0.5053224\ttotal: 3.65s\tremaining: 57.1s\n",
      "60:\tlearn: 0.5049884\ttotal: 3.71s\tremaining: 57.1s\n",
      "61:\tlearn: 0.5045933\ttotal: 3.77s\tremaining: 57s\n",
      "62:\tlearn: 0.5040678\ttotal: 3.83s\tremaining: 57s\n",
      "63:\tlearn: 0.5037310\ttotal: 3.9s\tremaining: 57s\n",
      "64:\tlearn: 0.5034242\ttotal: 3.96s\tremaining: 56.9s\n",
      "65:\tlearn: 0.5031365\ttotal: 4.02s\tremaining: 56.9s\n",
      "66:\tlearn: 0.5029394\ttotal: 4.08s\tremaining: 56.8s\n",
      "67:\tlearn: 0.5026173\ttotal: 4.14s\tremaining: 56.8s\n",
      "68:\tlearn: 0.5024265\ttotal: 4.21s\tremaining: 56.7s\n",
      "69:\tlearn: 0.5019988\ttotal: 4.27s\tremaining: 56.7s\n",
      "70:\tlearn: 0.5016106\ttotal: 4.33s\tremaining: 56.6s\n",
      "71:\tlearn: 0.5013075\ttotal: 4.39s\tremaining: 56.5s\n",
      "72:\tlearn: 0.5011089\ttotal: 4.44s\tremaining: 56.4s\n",
      "73:\tlearn: 0.5009424\ttotal: 4.5s\tremaining: 56.4s\n",
      "74:\tlearn: 0.5006472\ttotal: 4.56s\tremaining: 56.3s\n",
      "75:\tlearn: 0.5003052\ttotal: 4.62s\tremaining: 56.2s\n",
      "76:\tlearn: 0.5001410\ttotal: 4.68s\tremaining: 56.1s\n",
      "77:\tlearn: 0.4998854\ttotal: 4.74s\tremaining: 56.1s\n",
      "78:\tlearn: 0.4996147\ttotal: 4.81s\tremaining: 56.1s\n",
      "79:\tlearn: 0.4994954\ttotal: 4.87s\tremaining: 56s\n",
      "80:\tlearn: 0.4992809\ttotal: 4.93s\tremaining: 55.9s\n",
      "81:\tlearn: 0.4990370\ttotal: 4.99s\tremaining: 55.8s\n",
      "82:\tlearn: 0.4988552\ttotal: 5.05s\tremaining: 55.8s\n",
      "83:\tlearn: 0.4987942\ttotal: 5.1s\tremaining: 55.6s\n",
      "84:\tlearn: 0.4986681\ttotal: 5.16s\tremaining: 55.6s\n",
      "85:\tlearn: 0.4985470\ttotal: 5.22s\tremaining: 55.5s\n",
      "86:\tlearn: 0.4984070\ttotal: 5.28s\tremaining: 55.4s\n",
      "87:\tlearn: 0.4982122\ttotal: 5.34s\tremaining: 55.4s\n",
      "88:\tlearn: 0.4981497\ttotal: 5.4s\tremaining: 55.3s\n",
      "89:\tlearn: 0.4980264\ttotal: 5.46s\tremaining: 55.2s\n",
      "90:\tlearn: 0.4979430\ttotal: 5.52s\tremaining: 55.2s\n",
      "91:\tlearn: 0.4978041\ttotal: 5.58s\tremaining: 55.1s\n",
      "92:\tlearn: 0.4976137\ttotal: 5.64s\tremaining: 55.1s\n",
      "93:\tlearn: 0.4975242\ttotal: 5.7s\tremaining: 55s\n",
      "94:\tlearn: 0.4973639\ttotal: 5.76s\tremaining: 54.9s\n",
      "95:\tlearn: 0.4973029\ttotal: 5.82s\tremaining: 54.8s\n",
      "96:\tlearn: 0.4971960\ttotal: 5.88s\tremaining: 54.7s\n",
      "97:\tlearn: 0.4970368\ttotal: 5.94s\tremaining: 54.6s\n",
      "98:\tlearn: 0.4968406\ttotal: 6s\tremaining: 54.6s\n",
      "99:\tlearn: 0.4967388\ttotal: 6.06s\tremaining: 54.5s\n",
      "100:\tlearn: 0.4965769\ttotal: 6.12s\tremaining: 54.5s\n",
      "101:\tlearn: 0.4963904\ttotal: 6.18s\tremaining: 54.5s\n",
      "102:\tlearn: 0.4963102\ttotal: 6.24s\tremaining: 54.4s\n",
      "103:\tlearn: 0.4962497\ttotal: 6.31s\tremaining: 54.4s\n",
      "104:\tlearn: 0.4961376\ttotal: 6.37s\tremaining: 54.3s\n",
      "105:\tlearn: 0.4960948\ttotal: 6.43s\tremaining: 54.2s\n",
      "106:\tlearn: 0.4959639\ttotal: 6.49s\tremaining: 54.1s\n",
      "107:\tlearn: 0.4958101\ttotal: 6.54s\tremaining: 54.1s\n",
      "108:\tlearn: 0.4957518\ttotal: 6.61s\tremaining: 54s\n",
      "109:\tlearn: 0.4956510\ttotal: 6.66s\tremaining: 53.9s\n",
      "110:\tlearn: 0.4956044\ttotal: 6.71s\tremaining: 53.8s\n",
      "111:\tlearn: 0.4955257\ttotal: 6.77s\tremaining: 53.7s\n",
      "112:\tlearn: 0.4954438\ttotal: 6.83s\tremaining: 53.6s\n",
      "113:\tlearn: 0.4953902\ttotal: 6.89s\tremaining: 53.5s\n",
      "114:\tlearn: 0.4953123\ttotal: 6.95s\tremaining: 53.5s\n",
      "115:\tlearn: 0.4952129\ttotal: 7.02s\tremaining: 53.5s\n",
      "116:\tlearn: 0.4951256\ttotal: 7.09s\tremaining: 53.5s\n",
      "117:\tlearn: 0.4950510\ttotal: 7.16s\tremaining: 53.5s\n",
      "118:\tlearn: 0.4949756\ttotal: 7.23s\tremaining: 53.5s\n",
      "119:\tlearn: 0.4949348\ttotal: 7.28s\tremaining: 53.4s\n",
      "120:\tlearn: 0.4949113\ttotal: 7.34s\tremaining: 53.3s\n",
      "121:\tlearn: 0.4948902\ttotal: 7.4s\tremaining: 53.3s\n",
      "122:\tlearn: 0.4947523\ttotal: 7.47s\tremaining: 53.2s\n",
      "123:\tlearn: 0.4946987\ttotal: 7.52s\tremaining: 53.1s\n",
      "124:\tlearn: 0.4946019\ttotal: 7.58s\tremaining: 53.1s\n",
      "125:\tlearn: 0.4944912\ttotal: 7.65s\tremaining: 53.1s\n",
      "126:\tlearn: 0.4944444\ttotal: 7.72s\tremaining: 53.1s\n",
      "127:\tlearn: 0.4943660\ttotal: 7.8s\tremaining: 53.2s\n",
      "128:\tlearn: 0.4942008\ttotal: 7.87s\tremaining: 53.2s\n",
      "129:\tlearn: 0.4941436\ttotal: 7.93s\tremaining: 53.1s\n",
      "130:\tlearn: 0.4941254\ttotal: 7.99s\tremaining: 53s\n",
      "131:\tlearn: 0.4940970\ttotal: 8.05s\tremaining: 52.9s\n",
      "132:\tlearn: 0.4940212\ttotal: 8.1s\tremaining: 52.8s\n",
      "133:\tlearn: 0.4939819\ttotal: 8.18s\tremaining: 52.8s\n",
      "134:\tlearn: 0.4939194\ttotal: 8.24s\tremaining: 52.8s\n",
      "135:\tlearn: 0.4938254\ttotal: 8.31s\tremaining: 52.8s\n",
      "136:\tlearn: 0.4937557\ttotal: 8.37s\tremaining: 52.7s\n",
      "137:\tlearn: 0.4936813\ttotal: 8.43s\tremaining: 52.7s\n",
      "138:\tlearn: 0.4936195\ttotal: 8.49s\tremaining: 52.6s\n",
      "139:\tlearn: 0.4935611\ttotal: 8.55s\tremaining: 52.5s\n",
      "140:\tlearn: 0.4934393\ttotal: 8.61s\tremaining: 52.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141:\tlearn: 0.4933810\ttotal: 8.69s\tremaining: 52.5s\n",
      "142:\tlearn: 0.4932554\ttotal: 8.74s\tremaining: 52.4s\n",
      "143:\tlearn: 0.4931723\ttotal: 8.81s\tremaining: 52.4s\n",
      "144:\tlearn: 0.4931048\ttotal: 8.88s\tremaining: 52.3s\n",
      "145:\tlearn: 0.4930253\ttotal: 8.95s\tremaining: 52.3s\n",
      "146:\tlearn: 0.4929187\ttotal: 9.02s\tremaining: 52.3s\n",
      "147:\tlearn: 0.4928799\ttotal: 9.07s\tremaining: 52.2s\n",
      "148:\tlearn: 0.4928128\ttotal: 9.13s\tremaining: 52.2s\n",
      "149:\tlearn: 0.4927473\ttotal: 9.2s\tremaining: 52.2s\n",
      "150:\tlearn: 0.4926888\ttotal: 9.27s\tremaining: 52.1s\n",
      "151:\tlearn: 0.4926768\ttotal: 9.34s\tremaining: 52.1s\n",
      "152:\tlearn: 0.4926024\ttotal: 9.4s\tremaining: 52s\n",
      "153:\tlearn: 0.4925505\ttotal: 9.46s\tremaining: 51.9s\n",
      "154:\tlearn: 0.4924370\ttotal: 9.52s\tremaining: 51.9s\n",
      "155:\tlearn: 0.4923651\ttotal: 9.57s\tremaining: 51.8s\n",
      "156:\tlearn: 0.4923028\ttotal: 9.63s\tremaining: 51.7s\n",
      "157:\tlearn: 0.4922485\ttotal: 9.68s\tremaining: 51.6s\n",
      "158:\tlearn: 0.4921704\ttotal: 9.74s\tremaining: 51.5s\n",
      "159:\tlearn: 0.4921220\ttotal: 9.79s\tremaining: 51.4s\n",
      "160:\tlearn: 0.4920852\ttotal: 9.86s\tremaining: 51.4s\n",
      "161:\tlearn: 0.4920559\ttotal: 9.91s\tremaining: 51.3s\n",
      "162:\tlearn: 0.4920000\ttotal: 9.97s\tremaining: 51.2s\n",
      "163:\tlearn: 0.4919073\ttotal: 10s\tremaining: 51.1s\n",
      "164:\tlearn: 0.4918149\ttotal: 10.1s\tremaining: 51s\n",
      "165:\tlearn: 0.4918018\ttotal: 10.1s\tremaining: 50.9s\n",
      "166:\tlearn: 0.4917336\ttotal: 10.2s\tremaining: 50.9s\n",
      "167:\tlearn: 0.4916978\ttotal: 10.2s\tremaining: 50.8s\n",
      "168:\tlearn: 0.4916104\ttotal: 10.3s\tremaining: 50.7s\n",
      "169:\tlearn: 0.4915723\ttotal: 10.4s\tremaining: 50.6s\n",
      "170:\tlearn: 0.4915510\ttotal: 10.4s\tremaining: 50.5s\n",
      "171:\tlearn: 0.4915267\ttotal: 10.5s\tremaining: 50.4s\n",
      "172:\tlearn: 0.4915071\ttotal: 10.5s\tremaining: 50.4s\n",
      "173:\tlearn: 0.4914067\ttotal: 10.6s\tremaining: 50.4s\n",
      "174:\tlearn: 0.4913116\ttotal: 10.7s\tremaining: 50.3s\n",
      "175:\tlearn: 0.4912485\ttotal: 10.7s\tremaining: 50.2s\n",
      "176:\tlearn: 0.4911968\ttotal: 10.8s\tremaining: 50.1s\n",
      "177:\tlearn: 0.4911590\ttotal: 10.8s\tremaining: 50s\n",
      "178:\tlearn: 0.4911018\ttotal: 10.9s\tremaining: 49.9s\n",
      "179:\tlearn: 0.4910700\ttotal: 10.9s\tremaining: 49.9s\n",
      "180:\tlearn: 0.4910529\ttotal: 11s\tremaining: 49.8s\n",
      "181:\tlearn: 0.4910033\ttotal: 11.1s\tremaining: 49.7s\n",
      "182:\tlearn: 0.4909623\ttotal: 11.1s\tremaining: 49.6s\n",
      "183:\tlearn: 0.4908971\ttotal: 11.2s\tremaining: 49.5s\n",
      "184:\tlearn: 0.4908943\ttotal: 11.2s\tremaining: 49.5s\n",
      "185:\tlearn: 0.4908342\ttotal: 11.3s\tremaining: 49.4s\n",
      "186:\tlearn: 0.4907726\ttotal: 11.4s\tremaining: 49.4s\n",
      "187:\tlearn: 0.4907197\ttotal: 11.4s\tremaining: 49.3s\n",
      "188:\tlearn: 0.4906532\ttotal: 11.5s\tremaining: 49.2s\n",
      "189:\tlearn: 0.4906217\ttotal: 11.5s\tremaining: 49.2s\n",
      "190:\tlearn: 0.4905395\ttotal: 11.6s\tremaining: 49.1s\n",
      "191:\tlearn: 0.4904985\ttotal: 11.6s\tremaining: 49s\n",
      "192:\tlearn: 0.4904553\ttotal: 11.7s\tremaining: 48.9s\n",
      "193:\tlearn: 0.4904137\ttotal: 11.8s\tremaining: 48.9s\n",
      "194:\tlearn: 0.4903935\ttotal: 11.8s\tremaining: 48.8s\n",
      "195:\tlearn: 0.4903449\ttotal: 11.9s\tremaining: 48.8s\n",
      "196:\tlearn: 0.4903054\ttotal: 11.9s\tremaining: 48.7s\n",
      "197:\tlearn: 0.4902951\ttotal: 12s\tremaining: 48.6s\n",
      "198:\tlearn: 0.4902146\ttotal: 12.1s\tremaining: 48.6s\n",
      "199:\tlearn: 0.4901757\ttotal: 12.1s\tremaining: 48.5s\n",
      "200:\tlearn: 0.4901607\ttotal: 12.2s\tremaining: 48.4s\n",
      "201:\tlearn: 0.4901494\ttotal: 12.2s\tremaining: 48.3s\n",
      "202:\tlearn: 0.4901356\ttotal: 12.3s\tremaining: 48.2s\n",
      "203:\tlearn: 0.4900843\ttotal: 12.3s\tremaining: 48.2s\n",
      "204:\tlearn: 0.4900719\ttotal: 12.4s\tremaining: 48.1s\n",
      "205:\tlearn: 0.4900068\ttotal: 12.5s\tremaining: 48s\n",
      "206:\tlearn: 0.4899790\ttotal: 12.5s\tremaining: 47.9s\n",
      "207:\tlearn: 0.4898851\ttotal: 12.6s\tremaining: 47.9s\n",
      "208:\tlearn: 0.4898752\ttotal: 12.6s\tremaining: 47.8s\n",
      "209:\tlearn: 0.4898660\ttotal: 12.7s\tremaining: 47.7s\n",
      "210:\tlearn: 0.4898015\ttotal: 12.7s\tremaining: 47.7s\n",
      "211:\tlearn: 0.4897495\ttotal: 12.8s\tremaining: 47.6s\n",
      "212:\tlearn: 0.4897043\ttotal: 12.9s\tremaining: 47.6s\n",
      "213:\tlearn: 0.4896735\ttotal: 12.9s\tremaining: 47.5s\n",
      "214:\tlearn: 0.4896073\ttotal: 13s\tremaining: 47.4s\n",
      "215:\tlearn: 0.4895966\ttotal: 13s\tremaining: 47.4s\n",
      "216:\tlearn: 0.4895626\ttotal: 13.1s\tremaining: 47.3s\n",
      "217:\tlearn: 0.4895451\ttotal: 13.2s\tremaining: 47.2s\n",
      "218:\tlearn: 0.4894171\ttotal: 13.2s\tremaining: 47.2s\n",
      "219:\tlearn: 0.4892475\ttotal: 13.3s\tremaining: 47.1s\n",
      "220:\tlearn: 0.4891921\ttotal: 13.4s\tremaining: 47.1s\n",
      "221:\tlearn: 0.4891399\ttotal: 13.4s\tremaining: 47s\n",
      "222:\tlearn: 0.4891356\ttotal: 13.5s\tremaining: 47s\n",
      "223:\tlearn: 0.4890282\ttotal: 13.5s\tremaining: 46.9s\n",
      "224:\tlearn: 0.4889940\ttotal: 13.6s\tremaining: 46.8s\n",
      "225:\tlearn: 0.4889840\ttotal: 13.6s\tremaining: 46.7s\n",
      "226:\tlearn: 0.4889074\ttotal: 13.7s\tremaining: 46.7s\n",
      "227:\tlearn: 0.4888786\ttotal: 13.8s\tremaining: 46.6s\n",
      "228:\tlearn: 0.4888308\ttotal: 13.8s\tremaining: 46.5s\n",
      "229:\tlearn: 0.4887662\ttotal: 13.9s\tremaining: 46.5s\n",
      "230:\tlearn: 0.4887093\ttotal: 13.9s\tremaining: 46.4s\n",
      "231:\tlearn: 0.4886057\ttotal: 14s\tremaining: 46.3s\n",
      "232:\tlearn: 0.4885733\ttotal: 14s\tremaining: 46.2s\n",
      "233:\tlearn: 0.4885377\ttotal: 14.1s\tremaining: 46.2s\n",
      "234:\tlearn: 0.4885098\ttotal: 14.2s\tremaining: 46.1s\n",
      "235:\tlearn: 0.4884712\ttotal: 14.2s\tremaining: 46s\n",
      "236:\tlearn: 0.4884290\ttotal: 14.3s\tremaining: 45.9s\n",
      "237:\tlearn: 0.4883954\ttotal: 14.3s\tremaining: 45.9s\n",
      "238:\tlearn: 0.4883821\ttotal: 14.4s\tremaining: 45.8s\n",
      "239:\tlearn: 0.4883207\ttotal: 14.5s\tremaining: 45.8s\n",
      "240:\tlearn: 0.4882714\ttotal: 14.5s\tremaining: 45.7s\n",
      "241:\tlearn: 0.4882513\ttotal: 14.6s\tremaining: 45.6s\n",
      "242:\tlearn: 0.4882458\ttotal: 14.6s\tremaining: 45.6s\n",
      "243:\tlearn: 0.4882183\ttotal: 14.7s\tremaining: 45.5s\n",
      "244:\tlearn: 0.4881576\ttotal: 14.7s\tremaining: 45.4s\n",
      "245:\tlearn: 0.4881199\ttotal: 14.8s\tremaining: 45.4s\n",
      "246:\tlearn: 0.4881008\ttotal: 14.9s\tremaining: 45.3s\n",
      "247:\tlearn: 0.4880358\ttotal: 14.9s\tremaining: 45.2s\n",
      "248:\tlearn: 0.4880097\ttotal: 15s\tremaining: 45.2s\n",
      "249:\tlearn: 0.4880025\ttotal: 15s\tremaining: 45.1s\n",
      "250:\tlearn: 0.4878850\ttotal: 15.1s\tremaining: 45s\n",
      "251:\tlearn: 0.4878403\ttotal: 15.1s\tremaining: 45s\n",
      "252:\tlearn: 0.4878222\ttotal: 15.2s\tremaining: 44.9s\n",
      "253:\tlearn: 0.4878162\ttotal: 15.3s\tremaining: 44.8s\n",
      "254:\tlearn: 0.4877976\ttotal: 15.3s\tremaining: 44.8s\n",
      "255:\tlearn: 0.4877382\ttotal: 15.4s\tremaining: 44.7s\n",
      "256:\tlearn: 0.4876692\ttotal: 15.5s\tremaining: 44.7s\n",
      "257:\tlearn: 0.4876546\ttotal: 15.5s\tremaining: 44.7s\n",
      "258:\tlearn: 0.4876342\ttotal: 15.6s\tremaining: 44.6s\n",
      "259:\tlearn: 0.4875899\ttotal: 15.7s\tremaining: 44.6s\n",
      "260:\tlearn: 0.4875728\ttotal: 15.7s\tremaining: 44.5s\n",
      "261:\tlearn: 0.4874974\ttotal: 15.8s\tremaining: 44.4s\n",
      "262:\tlearn: 0.4874284\ttotal: 15.8s\tremaining: 44.4s\n",
      "263:\tlearn: 0.4873937\ttotal: 15.9s\tremaining: 44.3s\n",
      "264:\tlearn: 0.4873900\ttotal: 16s\tremaining: 44.3s\n",
      "265:\tlearn: 0.4873430\ttotal: 16s\tremaining: 44.2s\n",
      "266:\tlearn: 0.4873001\ttotal: 16.1s\tremaining: 44.2s\n",
      "267:\tlearn: 0.4872622\ttotal: 16.2s\tremaining: 44.2s\n",
      "268:\tlearn: 0.4872264\ttotal: 16.2s\tremaining: 44.1s\n",
      "269:\tlearn: 0.4871849\ttotal: 16.3s\tremaining: 44.1s\n",
      "270:\tlearn: 0.4871526\ttotal: 16.4s\tremaining: 44s\n",
      "271:\tlearn: 0.4870700\ttotal: 16.4s\tremaining: 44s\n",
      "272:\tlearn: 0.4870365\ttotal: 16.5s\tremaining: 43.9s\n",
      "273:\tlearn: 0.4869632\ttotal: 16.6s\tremaining: 43.9s\n",
      "274:\tlearn: 0.4869431\ttotal: 16.6s\tremaining: 43.8s\n",
      "275:\tlearn: 0.4869085\ttotal: 16.7s\tremaining: 43.7s\n",
      "276:\tlearn: 0.4868711\ttotal: 16.7s\tremaining: 43.6s\n",
      "277:\tlearn: 0.4868194\ttotal: 16.8s\tremaining: 43.6s\n",
      "278:\tlearn: 0.4867487\ttotal: 16.8s\tremaining: 43.5s\n",
      "279:\tlearn: 0.4867310\ttotal: 16.9s\tremaining: 43.4s\n",
      "280:\tlearn: 0.4866760\ttotal: 17s\tremaining: 43.4s\n",
      "281:\tlearn: 0.4866485\ttotal: 17s\tremaining: 43.4s\n",
      "282:\tlearn: 0.4866150\ttotal: 17.1s\tremaining: 43.3s\n",
      "283:\tlearn: 0.4865713\ttotal: 17.1s\tremaining: 43.2s\n",
      "284:\tlearn: 0.4865314\ttotal: 17.2s\tremaining: 43.2s\n",
      "285:\tlearn: 0.4865063\ttotal: 17.3s\tremaining: 43.1s\n",
      "286:\tlearn: 0.4864738\ttotal: 17.3s\tremaining: 43s\n",
      "287:\tlearn: 0.4864216\ttotal: 17.4s\tremaining: 43s\n",
      "288:\tlearn: 0.4863262\ttotal: 17.4s\tremaining: 42.9s\n",
      "289:\tlearn: 0.4862604\ttotal: 17.5s\tremaining: 42.8s\n",
      "290:\tlearn: 0.4862123\ttotal: 17.6s\tremaining: 42.8s\n",
      "291:\tlearn: 0.4861712\ttotal: 17.6s\tremaining: 42.7s\n",
      "292:\tlearn: 0.4861621\ttotal: 17.7s\tremaining: 42.6s\n",
      "293:\tlearn: 0.4860371\ttotal: 17.7s\tremaining: 42.6s\n",
      "294:\tlearn: 0.4860212\ttotal: 17.8s\tremaining: 42.5s\n",
      "295:\tlearn: 0.4859903\ttotal: 17.9s\tremaining: 42.5s\n",
      "296:\tlearn: 0.4859289\ttotal: 17.9s\tremaining: 42.4s\n",
      "297:\tlearn: 0.4858683\ttotal: 18s\tremaining: 42.4s\n",
      "298:\tlearn: 0.4858645\ttotal: 18s\tremaining: 42.3s\n",
      "299:\tlearn: 0.4858134\ttotal: 18.1s\tremaining: 42.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300:\tlearn: 0.4857744\ttotal: 18.2s\tremaining: 42.2s\n",
      "301:\tlearn: 0.4857711\ttotal: 18.2s\tremaining: 42.1s\n",
      "302:\tlearn: 0.4857171\ttotal: 18.3s\tremaining: 42s\n",
      "303:\tlearn: 0.4856918\ttotal: 18.3s\tremaining: 41.9s\n",
      "304:\tlearn: 0.4856772\ttotal: 18.4s\tremaining: 41.9s\n",
      "305:\tlearn: 0.4856430\ttotal: 18.4s\tremaining: 41.8s\n",
      "306:\tlearn: 0.4856031\ttotal: 18.5s\tremaining: 41.7s\n",
      "307:\tlearn: 0.4855250\ttotal: 18.5s\tremaining: 41.6s\n",
      "308:\tlearn: 0.4854745\ttotal: 18.6s\tremaining: 41.6s\n",
      "309:\tlearn: 0.4854435\ttotal: 18.6s\tremaining: 41.5s\n",
      "310:\tlearn: 0.4853953\ttotal: 18.7s\tremaining: 41.4s\n",
      "311:\tlearn: 0.4853701\ttotal: 18.8s\tremaining: 41.4s\n",
      "312:\tlearn: 0.4853476\ttotal: 18.8s\tremaining: 41.3s\n",
      "313:\tlearn: 0.4853265\ttotal: 18.9s\tremaining: 41.3s\n",
      "314:\tlearn: 0.4853066\ttotal: 18.9s\tremaining: 41.2s\n",
      "315:\tlearn: 0.4852809\ttotal: 19s\tremaining: 41.1s\n",
      "316:\tlearn: 0.4852734\ttotal: 19.1s\tremaining: 41s\n",
      "317:\tlearn: 0.4852247\ttotal: 19.1s\tremaining: 41s\n",
      "318:\tlearn: 0.4851663\ttotal: 19.2s\tremaining: 40.9s\n",
      "319:\tlearn: 0.4851291\ttotal: 19.2s\tremaining: 40.8s\n",
      "320:\tlearn: 0.4851011\ttotal: 19.3s\tremaining: 40.8s\n",
      "321:\tlearn: 0.4850126\ttotal: 19.3s\tremaining: 40.7s\n",
      "322:\tlearn: 0.4850053\ttotal: 19.4s\tremaining: 40.6s\n",
      "323:\tlearn: 0.4849146\ttotal: 19.4s\tremaining: 40.6s\n",
      "324:\tlearn: 0.4848497\ttotal: 19.5s\tremaining: 40.5s\n",
      "325:\tlearn: 0.4847784\ttotal: 19.6s\tremaining: 40.4s\n",
      "326:\tlearn: 0.4847284\ttotal: 19.6s\tremaining: 40.4s\n",
      "327:\tlearn: 0.4846131\ttotal: 19.7s\tremaining: 40.3s\n",
      "328:\tlearn: 0.4845383\ttotal: 19.7s\tremaining: 40.2s\n",
      "329:\tlearn: 0.4844902\ttotal: 19.8s\tremaining: 40.2s\n",
      "330:\tlearn: 0.4844697\ttotal: 19.8s\tremaining: 40.1s\n",
      "331:\tlearn: 0.4843966\ttotal: 19.9s\tremaining: 40s\n",
      "332:\tlearn: 0.4843755\ttotal: 19.9s\tremaining: 39.9s\n",
      "333:\tlearn: 0.4843692\ttotal: 20s\tremaining: 39.8s\n",
      "334:\tlearn: 0.4843543\ttotal: 20s\tremaining: 39.8s\n",
      "335:\tlearn: 0.4843254\ttotal: 20.1s\tremaining: 39.7s\n",
      "336:\tlearn: 0.4842859\ttotal: 20.1s\tremaining: 39.6s\n",
      "337:\tlearn: 0.4841915\ttotal: 20.2s\tremaining: 39.6s\n",
      "338:\tlearn: 0.4841284\ttotal: 20.3s\tremaining: 39.5s\n",
      "339:\tlearn: 0.4840935\ttotal: 20.3s\tremaining: 39.4s\n",
      "340:\tlearn: 0.4840342\ttotal: 20.4s\tremaining: 39.4s\n",
      "341:\tlearn: 0.4839573\ttotal: 20.4s\tremaining: 39.3s\n",
      "342:\tlearn: 0.4838832\ttotal: 20.5s\tremaining: 39.2s\n",
      "343:\tlearn: 0.4837676\ttotal: 20.5s\tremaining: 39.2s\n",
      "344:\tlearn: 0.4837297\ttotal: 20.6s\tremaining: 39.1s\n",
      "345:\tlearn: 0.4837118\ttotal: 20.6s\tremaining: 39s\n",
      "346:\tlearn: 0.4836892\ttotal: 20.7s\tremaining: 39s\n",
      "347:\tlearn: 0.4836505\ttotal: 20.8s\tremaining: 38.9s\n",
      "348:\tlearn: 0.4836278\ttotal: 20.8s\tremaining: 38.8s\n",
      "349:\tlearn: 0.4836121\ttotal: 20.9s\tremaining: 38.7s\n",
      "350:\tlearn: 0.4835595\ttotal: 20.9s\tremaining: 38.7s\n",
      "351:\tlearn: 0.4834673\ttotal: 21s\tremaining: 38.6s\n",
      "352:\tlearn: 0.4833968\ttotal: 21s\tremaining: 38.6s\n",
      "353:\tlearn: 0.4833188\ttotal: 21.1s\tremaining: 38.5s\n",
      "354:\tlearn: 0.4832914\ttotal: 21.2s\tremaining: 38.5s\n",
      "355:\tlearn: 0.4832636\ttotal: 21.2s\tremaining: 38.4s\n",
      "356:\tlearn: 0.4831932\ttotal: 21.3s\tremaining: 38.3s\n",
      "357:\tlearn: 0.4831727\ttotal: 21.3s\tremaining: 38.3s\n",
      "358:\tlearn: 0.4831578\ttotal: 21.4s\tremaining: 38.2s\n",
      "359:\tlearn: 0.4831363\ttotal: 21.4s\tremaining: 38.1s\n",
      "360:\tlearn: 0.4831059\ttotal: 21.5s\tremaining: 38s\n",
      "361:\tlearn: 0.4830557\ttotal: 21.5s\tremaining: 38s\n",
      "362:\tlearn: 0.4829879\ttotal: 21.6s\tremaining: 37.9s\n",
      "363:\tlearn: 0.4829392\ttotal: 21.7s\tremaining: 37.8s\n",
      "364:\tlearn: 0.4828889\ttotal: 21.7s\tremaining: 37.8s\n",
      "365:\tlearn: 0.4828135\ttotal: 21.8s\tremaining: 37.7s\n",
      "366:\tlearn: 0.4827541\ttotal: 21.8s\tremaining: 37.7s\n",
      "367:\tlearn: 0.4827068\ttotal: 21.9s\tremaining: 37.6s\n",
      "368:\tlearn: 0.4826239\ttotal: 21.9s\tremaining: 37.5s\n",
      "369:\tlearn: 0.4825948\ttotal: 22s\tremaining: 37.5s\n",
      "370:\tlearn: 0.4825264\ttotal: 22.1s\tremaining: 37.4s\n",
      "371:\tlearn: 0.4822803\ttotal: 22.1s\tremaining: 37.4s\n",
      "372:\tlearn: 0.4821916\ttotal: 22.2s\tremaining: 37.3s\n",
      "373:\tlearn: 0.4821639\ttotal: 22.2s\tremaining: 37.2s\n",
      "374:\tlearn: 0.4820960\ttotal: 22.3s\tremaining: 37.2s\n",
      "375:\tlearn: 0.4820219\ttotal: 22.4s\tremaining: 37.1s\n",
      "376:\tlearn: 0.4820101\ttotal: 22.4s\tremaining: 37s\n",
      "377:\tlearn: 0.4819597\ttotal: 22.5s\tremaining: 37s\n",
      "378:\tlearn: 0.4818885\ttotal: 22.5s\tremaining: 36.9s\n",
      "379:\tlearn: 0.4818202\ttotal: 22.6s\tremaining: 36.8s\n",
      "380:\tlearn: 0.4815824\ttotal: 22.6s\tremaining: 36.8s\n",
      "381:\tlearn: 0.4815319\ttotal: 22.7s\tremaining: 36.7s\n",
      "382:\tlearn: 0.4814793\ttotal: 22.7s\tremaining: 36.6s\n",
      "383:\tlearn: 0.4813842\ttotal: 22.8s\tremaining: 36.6s\n",
      "384:\tlearn: 0.4812891\ttotal: 22.9s\tremaining: 36.5s\n",
      "385:\tlearn: 0.4810968\ttotal: 22.9s\tremaining: 36.5s\n",
      "386:\tlearn: 0.4810286\ttotal: 23s\tremaining: 36.4s\n",
      "387:\tlearn: 0.4809689\ttotal: 23s\tremaining: 36.3s\n",
      "388:\tlearn: 0.4808721\ttotal: 23.1s\tremaining: 36.3s\n",
      "389:\tlearn: 0.4808273\ttotal: 23.2s\tremaining: 36.2s\n",
      "390:\tlearn: 0.4807617\ttotal: 23.2s\tremaining: 36.2s\n",
      "391:\tlearn: 0.4807189\ttotal: 23.3s\tremaining: 36.1s\n",
      "392:\tlearn: 0.4806931\ttotal: 23.3s\tremaining: 36s\n",
      "393:\tlearn: 0.4806405\ttotal: 23.4s\tremaining: 36s\n",
      "394:\tlearn: 0.4806030\ttotal: 23.4s\tremaining: 35.9s\n",
      "395:\tlearn: 0.4805418\ttotal: 23.5s\tremaining: 35.8s\n",
      "396:\tlearn: 0.4804935\ttotal: 23.6s\tremaining: 35.8s\n",
      "397:\tlearn: 0.4804635\ttotal: 23.6s\tremaining: 35.7s\n",
      "398:\tlearn: 0.4804389\ttotal: 23.7s\tremaining: 35.6s\n",
      "399:\tlearn: 0.4803804\ttotal: 23.7s\tremaining: 35.6s\n",
      "400:\tlearn: 0.4803677\ttotal: 23.8s\tremaining: 35.5s\n",
      "401:\tlearn: 0.4803289\ttotal: 23.8s\tremaining: 35.5s\n",
      "402:\tlearn: 0.4802267\ttotal: 23.9s\tremaining: 35.4s\n",
      "403:\tlearn: 0.4801856\ttotal: 24s\tremaining: 35.4s\n",
      "404:\tlearn: 0.4801307\ttotal: 24s\tremaining: 35.3s\n",
      "405:\tlearn: 0.4800740\ttotal: 24.1s\tremaining: 35.3s\n",
      "406:\tlearn: 0.4800113\ttotal: 24.2s\tremaining: 35.2s\n",
      "407:\tlearn: 0.4799528\ttotal: 24.2s\tremaining: 35.2s\n",
      "408:\tlearn: 0.4799058\ttotal: 24.3s\tremaining: 35.1s\n",
      "409:\tlearn: 0.4798443\ttotal: 24.4s\tremaining: 35.1s\n",
      "410:\tlearn: 0.4798258\ttotal: 24.4s\tremaining: 35s\n",
      "411:\tlearn: 0.4797799\ttotal: 24.5s\tremaining: 35s\n",
      "412:\tlearn: 0.4796952\ttotal: 24.6s\tremaining: 34.9s\n",
      "413:\tlearn: 0.4796428\ttotal: 24.6s\tremaining: 34.8s\n",
      "414:\tlearn: 0.4795750\ttotal: 24.7s\tremaining: 34.8s\n",
      "415:\tlearn: 0.4794987\ttotal: 24.7s\tremaining: 34.7s\n",
      "416:\tlearn: 0.4794343\ttotal: 24.8s\tremaining: 34.7s\n",
      "417:\tlearn: 0.4793963\ttotal: 24.9s\tremaining: 34.6s\n",
      "418:\tlearn: 0.4793724\ttotal: 24.9s\tremaining: 34.6s\n",
      "419:\tlearn: 0.4792798\ttotal: 25s\tremaining: 34.5s\n",
      "420:\tlearn: 0.4791336\ttotal: 25.1s\tremaining: 34.5s\n",
      "421:\tlearn: 0.4791195\ttotal: 25.1s\tremaining: 34.4s\n",
      "422:\tlearn: 0.4790839\ttotal: 25.2s\tremaining: 34.4s\n",
      "423:\tlearn: 0.4790652\ttotal: 25.2s\tremaining: 34.3s\n",
      "424:\tlearn: 0.4790105\ttotal: 25.3s\tremaining: 34.2s\n",
      "425:\tlearn: 0.4789896\ttotal: 25.4s\tremaining: 34.2s\n",
      "426:\tlearn: 0.4789033\ttotal: 25.4s\tremaining: 34.1s\n",
      "427:\tlearn: 0.4788423\ttotal: 25.5s\tremaining: 34.1s\n",
      "428:\tlearn: 0.4788043\ttotal: 25.6s\tremaining: 34s\n",
      "429:\tlearn: 0.4787502\ttotal: 25.6s\tremaining: 34s\n",
      "430:\tlearn: 0.4787143\ttotal: 25.7s\tremaining: 33.9s\n",
      "431:\tlearn: 0.4786673\ttotal: 25.8s\tremaining: 33.9s\n",
      "432:\tlearn: 0.4786504\ttotal: 25.8s\tremaining: 33.8s\n",
      "433:\tlearn: 0.4785417\ttotal: 25.9s\tremaining: 33.7s\n",
      "434:\tlearn: 0.4785313\ttotal: 25.9s\tremaining: 33.7s\n",
      "435:\tlearn: 0.4785049\ttotal: 26s\tremaining: 33.6s\n",
      "436:\tlearn: 0.4784495\ttotal: 26s\tremaining: 33.6s\n",
      "437:\tlearn: 0.4783732\ttotal: 26.1s\tremaining: 33.5s\n",
      "438:\tlearn: 0.4783342\ttotal: 26.2s\tremaining: 33.4s\n",
      "439:\tlearn: 0.4782712\ttotal: 26.2s\tremaining: 33.4s\n",
      "440:\tlearn: 0.4782298\ttotal: 26.3s\tremaining: 33.3s\n",
      "441:\tlearn: 0.4781621\ttotal: 26.3s\tremaining: 33.3s\n",
      "442:\tlearn: 0.4781369\ttotal: 26.4s\tremaining: 33.2s\n",
      "443:\tlearn: 0.4781155\ttotal: 26.5s\tremaining: 33.2s\n",
      "444:\tlearn: 0.4781021\ttotal: 26.5s\tremaining: 33.1s\n",
      "445:\tlearn: 0.4780719\ttotal: 26.6s\tremaining: 33s\n",
      "446:\tlearn: 0.4780067\ttotal: 26.7s\tremaining: 33s\n",
      "447:\tlearn: 0.4779418\ttotal: 26.7s\tremaining: 32.9s\n",
      "448:\tlearn: 0.4778876\ttotal: 26.8s\tremaining: 32.9s\n",
      "449:\tlearn: 0.4778432\ttotal: 26.8s\tremaining: 32.8s\n",
      "450:\tlearn: 0.4778212\ttotal: 26.9s\tremaining: 32.8s\n",
      "451:\tlearn: 0.4777705\ttotal: 27s\tremaining: 32.7s\n",
      "452:\tlearn: 0.4777147\ttotal: 27s\tremaining: 32.6s\n",
      "453:\tlearn: 0.4776628\ttotal: 27.1s\tremaining: 32.6s\n",
      "454:\tlearn: 0.4776127\ttotal: 27.2s\tremaining: 32.5s\n",
      "455:\tlearn: 0.4775800\ttotal: 27.2s\tremaining: 32.5s\n",
      "456:\tlearn: 0.4775195\ttotal: 27.3s\tremaining: 32.4s\n",
      "457:\tlearn: 0.4774812\ttotal: 27.4s\tremaining: 32.4s\n",
      "458:\tlearn: 0.4774516\ttotal: 27.4s\tremaining: 32.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459:\tlearn: 0.4774000\ttotal: 27.5s\tremaining: 32.3s\n",
      "460:\tlearn: 0.4773601\ttotal: 27.5s\tremaining: 32.2s\n",
      "461:\tlearn: 0.4771399\ttotal: 27.6s\tremaining: 32.2s\n",
      "462:\tlearn: 0.4770914\ttotal: 27.7s\tremaining: 32.1s\n",
      "463:\tlearn: 0.4770487\ttotal: 27.8s\tremaining: 32.1s\n",
      "464:\tlearn: 0.4769844\ttotal: 27.8s\tremaining: 32s\n",
      "465:\tlearn: 0.4769263\ttotal: 27.9s\tremaining: 31.9s\n",
      "466:\tlearn: 0.4768528\ttotal: 27.9s\tremaining: 31.9s\n",
      "467:\tlearn: 0.4768082\ttotal: 28s\tremaining: 31.8s\n",
      "468:\tlearn: 0.4767891\ttotal: 28s\tremaining: 31.8s\n",
      "469:\tlearn: 0.4767177\ttotal: 28.1s\tremaining: 31.7s\n",
      "470:\tlearn: 0.4766124\ttotal: 28.2s\tremaining: 31.7s\n",
      "471:\tlearn: 0.4765602\ttotal: 28.2s\tremaining: 31.6s\n",
      "472:\tlearn: 0.4764654\ttotal: 28.3s\tremaining: 31.5s\n",
      "473:\tlearn: 0.4763658\ttotal: 28.4s\tremaining: 31.5s\n",
      "474:\tlearn: 0.4763216\ttotal: 28.4s\tremaining: 31.4s\n",
      "475:\tlearn: 0.4762818\ttotal: 28.5s\tremaining: 31.3s\n",
      "476:\tlearn: 0.4762082\ttotal: 28.5s\tremaining: 31.3s\n",
      "477:\tlearn: 0.4761767\ttotal: 28.6s\tremaining: 31.2s\n",
      "478:\tlearn: 0.4761609\ttotal: 28.7s\tremaining: 31.2s\n",
      "479:\tlearn: 0.4760863\ttotal: 28.7s\tremaining: 31.1s\n",
      "480:\tlearn: 0.4760271\ttotal: 28.8s\tremaining: 31.1s\n",
      "481:\tlearn: 0.4758305\ttotal: 28.9s\tremaining: 31s\n",
      "482:\tlearn: 0.4757735\ttotal: 28.9s\tremaining: 31s\n",
      "483:\tlearn: 0.4757085\ttotal: 29s\tremaining: 30.9s\n",
      "484:\tlearn: 0.4756720\ttotal: 29s\tremaining: 30.8s\n",
      "485:\tlearn: 0.4756386\ttotal: 29.1s\tremaining: 30.8s\n",
      "486:\tlearn: 0.4755828\ttotal: 29.2s\tremaining: 30.7s\n",
      "487:\tlearn: 0.4755727\ttotal: 29.2s\tremaining: 30.6s\n",
      "488:\tlearn: 0.4755309\ttotal: 29.3s\tremaining: 30.6s\n",
      "489:\tlearn: 0.4754284\ttotal: 29.3s\tremaining: 30.5s\n",
      "490:\tlearn: 0.4753910\ttotal: 29.4s\tremaining: 30.5s\n",
      "491:\tlearn: 0.4753697\ttotal: 29.5s\tremaining: 30.4s\n",
      "492:\tlearn: 0.4753031\ttotal: 29.5s\tremaining: 30.4s\n",
      "493:\tlearn: 0.4752492\ttotal: 29.6s\tremaining: 30.3s\n",
      "494:\tlearn: 0.4751765\ttotal: 29.6s\tremaining: 30.2s\n",
      "495:\tlearn: 0.4751138\ttotal: 29.7s\tremaining: 30.2s\n",
      "496:\tlearn: 0.4750717\ttotal: 29.8s\tremaining: 30.1s\n",
      "497:\tlearn: 0.4750243\ttotal: 29.8s\tremaining: 30s\n",
      "498:\tlearn: 0.4749486\ttotal: 29.9s\tremaining: 30s\n",
      "499:\tlearn: 0.4749197\ttotal: 29.9s\tremaining: 29.9s\n",
      "500:\tlearn: 0.4748894\ttotal: 30s\tremaining: 29.9s\n",
      "501:\tlearn: 0.4748386\ttotal: 30s\tremaining: 29.8s\n",
      "502:\tlearn: 0.4748005\ttotal: 30.1s\tremaining: 29.7s\n",
      "503:\tlearn: 0.4747876\ttotal: 30.2s\tremaining: 29.7s\n",
      "504:\tlearn: 0.4747600\ttotal: 30.2s\tremaining: 29.6s\n",
      "505:\tlearn: 0.4747081\ttotal: 30.3s\tremaining: 29.6s\n",
      "506:\tlearn: 0.4746481\ttotal: 30.4s\tremaining: 29.5s\n",
      "507:\tlearn: 0.4746178\ttotal: 30.4s\tremaining: 29.5s\n",
      "508:\tlearn: 0.4745611\ttotal: 30.5s\tremaining: 29.4s\n",
      "509:\tlearn: 0.4745140\ttotal: 30.5s\tremaining: 29.3s\n",
      "510:\tlearn: 0.4744924\ttotal: 30.6s\tremaining: 29.3s\n",
      "511:\tlearn: 0.4744689\ttotal: 30.7s\tremaining: 29.2s\n",
      "512:\tlearn: 0.4743643\ttotal: 30.7s\tremaining: 29.2s\n",
      "513:\tlearn: 0.4742832\ttotal: 30.8s\tremaining: 29.1s\n",
      "514:\tlearn: 0.4742496\ttotal: 30.8s\tremaining: 29s\n",
      "515:\tlearn: 0.4742253\ttotal: 30.9s\tremaining: 29s\n",
      "516:\tlearn: 0.4741666\ttotal: 30.9s\tremaining: 28.9s\n",
      "517:\tlearn: 0.4741333\ttotal: 31s\tremaining: 28.9s\n",
      "518:\tlearn: 0.4740877\ttotal: 31.1s\tremaining: 28.8s\n",
      "519:\tlearn: 0.4740497\ttotal: 31.1s\tremaining: 28.7s\n",
      "520:\tlearn: 0.4739911\ttotal: 31.2s\tremaining: 28.7s\n",
      "521:\tlearn: 0.4739827\ttotal: 31.2s\tremaining: 28.6s\n",
      "522:\tlearn: 0.4739486\ttotal: 31.3s\tremaining: 28.5s\n",
      "523:\tlearn: 0.4738823\ttotal: 31.3s\tremaining: 28.5s\n",
      "524:\tlearn: 0.4738472\ttotal: 31.4s\tremaining: 28.4s\n",
      "525:\tlearn: 0.4738115\ttotal: 31.5s\tremaining: 28.3s\n",
      "526:\tlearn: 0.4737708\ttotal: 31.5s\tremaining: 28.3s\n",
      "527:\tlearn: 0.4737260\ttotal: 31.6s\tremaining: 28.2s\n",
      "528:\tlearn: 0.4735856\ttotal: 31.6s\tremaining: 28.2s\n",
      "529:\tlearn: 0.4735031\ttotal: 31.7s\tremaining: 28.1s\n",
      "530:\tlearn: 0.4734756\ttotal: 31.8s\tremaining: 28.1s\n",
      "531:\tlearn: 0.4734384\ttotal: 31.8s\tremaining: 28s\n",
      "532:\tlearn: 0.4733835\ttotal: 31.9s\tremaining: 27.9s\n",
      "533:\tlearn: 0.4733432\ttotal: 31.9s\tremaining: 27.9s\n",
      "534:\tlearn: 0.4732874\ttotal: 32s\tremaining: 27.8s\n",
      "535:\tlearn: 0.4732563\ttotal: 32s\tremaining: 27.7s\n",
      "536:\tlearn: 0.4732246\ttotal: 32.1s\tremaining: 27.7s\n",
      "537:\tlearn: 0.4731752\ttotal: 32.2s\tremaining: 27.6s\n",
      "538:\tlearn: 0.4731329\ttotal: 32.2s\tremaining: 27.5s\n",
      "539:\tlearn: 0.4730703\ttotal: 32.3s\tremaining: 27.5s\n",
      "540:\tlearn: 0.4730431\ttotal: 32.3s\tremaining: 27.4s\n",
      "541:\tlearn: 0.4730143\ttotal: 32.4s\tremaining: 27.4s\n",
      "542:\tlearn: 0.4729652\ttotal: 32.5s\tremaining: 27.3s\n",
      "543:\tlearn: 0.4729450\ttotal: 32.5s\tremaining: 27.3s\n",
      "544:\tlearn: 0.4729243\ttotal: 32.6s\tremaining: 27.2s\n",
      "545:\tlearn: 0.4728898\ttotal: 32.6s\tremaining: 27.1s\n",
      "546:\tlearn: 0.4728716\ttotal: 32.7s\tremaining: 27.1s\n",
      "547:\tlearn: 0.4728439\ttotal: 32.7s\tremaining: 27s\n",
      "548:\tlearn: 0.4728087\ttotal: 32.8s\tremaining: 26.9s\n",
      "549:\tlearn: 0.4727605\ttotal: 32.9s\tremaining: 26.9s\n",
      "550:\tlearn: 0.4727142\ttotal: 32.9s\tremaining: 26.8s\n",
      "551:\tlearn: 0.4726418\ttotal: 33s\tremaining: 26.8s\n",
      "552:\tlearn: 0.4726029\ttotal: 33s\tremaining: 26.7s\n",
      "553:\tlearn: 0.4725937\ttotal: 33.1s\tremaining: 26.6s\n",
      "554:\tlearn: 0.4725306\ttotal: 33.2s\tremaining: 26.6s\n",
      "555:\tlearn: 0.4725138\ttotal: 33.2s\tremaining: 26.5s\n",
      "556:\tlearn: 0.4724835\ttotal: 33.3s\tremaining: 26.5s\n",
      "557:\tlearn: 0.4724450\ttotal: 33.3s\tremaining: 26.4s\n",
      "558:\tlearn: 0.4724066\ttotal: 33.4s\tremaining: 26.3s\n",
      "559:\tlearn: 0.4723754\ttotal: 33.4s\tremaining: 26.3s\n",
      "560:\tlearn: 0.4723510\ttotal: 33.5s\tremaining: 26.2s\n",
      "561:\tlearn: 0.4722232\ttotal: 33.6s\tremaining: 26.2s\n",
      "562:\tlearn: 0.4721991\ttotal: 33.6s\tremaining: 26.1s\n",
      "563:\tlearn: 0.4721721\ttotal: 33.7s\tremaining: 26s\n",
      "564:\tlearn: 0.4720857\ttotal: 33.7s\tremaining: 26s\n",
      "565:\tlearn: 0.4719919\ttotal: 33.8s\tremaining: 25.9s\n",
      "566:\tlearn: 0.4719666\ttotal: 33.9s\tremaining: 25.9s\n",
      "567:\tlearn: 0.4719291\ttotal: 33.9s\tremaining: 25.8s\n",
      "568:\tlearn: 0.4718964\ttotal: 34s\tremaining: 25.7s\n",
      "569:\tlearn: 0.4718612\ttotal: 34s\tremaining: 25.7s\n",
      "570:\tlearn: 0.4718033\ttotal: 34.1s\tremaining: 25.6s\n",
      "571:\tlearn: 0.4717646\ttotal: 34.2s\tremaining: 25.6s\n",
      "572:\tlearn: 0.4717318\ttotal: 34.2s\tremaining: 25.5s\n",
      "573:\tlearn: 0.4716335\ttotal: 34.3s\tremaining: 25.4s\n",
      "574:\tlearn: 0.4716177\ttotal: 34.3s\tremaining: 25.4s\n",
      "575:\tlearn: 0.4715019\ttotal: 34.4s\tremaining: 25.3s\n",
      "576:\tlearn: 0.4714233\ttotal: 34.5s\tremaining: 25.3s\n",
      "577:\tlearn: 0.4713342\ttotal: 34.5s\tremaining: 25.2s\n",
      "578:\tlearn: 0.4713080\ttotal: 34.6s\tremaining: 25.1s\n",
      "579:\tlearn: 0.4712632\ttotal: 34.6s\tremaining: 25.1s\n",
      "580:\tlearn: 0.4712532\ttotal: 34.7s\tremaining: 25s\n",
      "581:\tlearn: 0.4712033\ttotal: 34.8s\tremaining: 25s\n",
      "582:\tlearn: 0.4711756\ttotal: 34.8s\tremaining: 24.9s\n",
      "583:\tlearn: 0.4711584\ttotal: 34.9s\tremaining: 24.8s\n",
      "584:\tlearn: 0.4711387\ttotal: 34.9s\tremaining: 24.8s\n",
      "585:\tlearn: 0.4710639\ttotal: 35s\tremaining: 24.7s\n",
      "586:\tlearn: 0.4710612\ttotal: 35s\tremaining: 24.6s\n",
      "587:\tlearn: 0.4710161\ttotal: 35.1s\tremaining: 24.6s\n",
      "588:\tlearn: 0.4709677\ttotal: 35.2s\tremaining: 24.5s\n",
      "589:\tlearn: 0.4709017\ttotal: 35.2s\tremaining: 24.5s\n",
      "590:\tlearn: 0.4708712\ttotal: 35.3s\tremaining: 24.4s\n",
      "591:\tlearn: 0.4708529\ttotal: 35.3s\tremaining: 24.3s\n",
      "592:\tlearn: 0.4708329\ttotal: 35.4s\tremaining: 24.3s\n",
      "593:\tlearn: 0.4707933\ttotal: 35.5s\tremaining: 24.2s\n",
      "594:\tlearn: 0.4707725\ttotal: 35.5s\tremaining: 24.2s\n",
      "595:\tlearn: 0.4707531\ttotal: 35.6s\tremaining: 24.1s\n",
      "596:\tlearn: 0.4707262\ttotal: 35.6s\tremaining: 24s\n",
      "597:\tlearn: 0.4707051\ttotal: 35.7s\tremaining: 24s\n",
      "598:\tlearn: 0.4706355\ttotal: 35.7s\tremaining: 23.9s\n",
      "599:\tlearn: 0.4706031\ttotal: 35.8s\tremaining: 23.9s\n",
      "600:\tlearn: 0.4705166\ttotal: 35.9s\tremaining: 23.8s\n",
      "601:\tlearn: 0.4704553\ttotal: 35.9s\tremaining: 23.7s\n",
      "602:\tlearn: 0.4704085\ttotal: 36s\tremaining: 23.7s\n",
      "603:\tlearn: 0.4703864\ttotal: 36s\tremaining: 23.6s\n",
      "604:\tlearn: 0.4703505\ttotal: 36.1s\tremaining: 23.6s\n",
      "605:\tlearn: 0.4702960\ttotal: 36.2s\tremaining: 23.5s\n",
      "606:\tlearn: 0.4702745\ttotal: 36.2s\tremaining: 23.5s\n",
      "607:\tlearn: 0.4702177\ttotal: 36.3s\tremaining: 23.4s\n",
      "608:\tlearn: 0.4701651\ttotal: 36.3s\tremaining: 23.3s\n",
      "609:\tlearn: 0.4701404\ttotal: 36.4s\tremaining: 23.3s\n",
      "610:\tlearn: 0.4701261\ttotal: 36.5s\tremaining: 23.2s\n",
      "611:\tlearn: 0.4700964\ttotal: 36.5s\tremaining: 23.2s\n",
      "612:\tlearn: 0.4700112\ttotal: 36.6s\tremaining: 23.1s\n",
      "613:\tlearn: 0.4699689\ttotal: 36.7s\tremaining: 23s\n",
      "614:\tlearn: 0.4699480\ttotal: 36.7s\tremaining: 23s\n",
      "615:\tlearn: 0.4699050\ttotal: 36.8s\tremaining: 22.9s\n",
      "616:\tlearn: 0.4698654\ttotal: 36.8s\tremaining: 22.9s\n",
      "617:\tlearn: 0.4698325\ttotal: 36.9s\tremaining: 22.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "618:\tlearn: 0.4698167\ttotal: 37s\tremaining: 22.7s\n",
      "619:\tlearn: 0.4697288\ttotal: 37s\tremaining: 22.7s\n",
      "620:\tlearn: 0.4697010\ttotal: 37.1s\tremaining: 22.6s\n",
      "621:\tlearn: 0.4696515\ttotal: 37.1s\tremaining: 22.6s\n",
      "622:\tlearn: 0.4696050\ttotal: 37.2s\tremaining: 22.5s\n",
      "623:\tlearn: 0.4695582\ttotal: 37.2s\tremaining: 22.4s\n",
      "624:\tlearn: 0.4695124\ttotal: 37.3s\tremaining: 22.4s\n",
      "625:\tlearn: 0.4694933\ttotal: 37.4s\tremaining: 22.3s\n",
      "626:\tlearn: 0.4693546\ttotal: 37.4s\tremaining: 22.3s\n",
      "627:\tlearn: 0.4693005\ttotal: 37.5s\tremaining: 22.2s\n",
      "628:\tlearn: 0.4692683\ttotal: 37.5s\tremaining: 22.1s\n",
      "629:\tlearn: 0.4691403\ttotal: 37.6s\tremaining: 22.1s\n",
      "630:\tlearn: 0.4691148\ttotal: 37.6s\tremaining: 22s\n",
      "631:\tlearn: 0.4690822\ttotal: 37.7s\tremaining: 22s\n",
      "632:\tlearn: 0.4690409\ttotal: 37.8s\tremaining: 21.9s\n",
      "633:\tlearn: 0.4690215\ttotal: 37.8s\tremaining: 21.8s\n",
      "634:\tlearn: 0.4689965\ttotal: 37.9s\tremaining: 21.8s\n",
      "635:\tlearn: 0.4689778\ttotal: 37.9s\tremaining: 21.7s\n",
      "636:\tlearn: 0.4688750\ttotal: 38s\tremaining: 21.7s\n",
      "637:\tlearn: 0.4688273\ttotal: 38.1s\tremaining: 21.6s\n",
      "638:\tlearn: 0.4687961\ttotal: 38.1s\tremaining: 21.5s\n",
      "639:\tlearn: 0.4687736\ttotal: 38.2s\tremaining: 21.5s\n",
      "640:\tlearn: 0.4686787\ttotal: 38.2s\tremaining: 21.4s\n",
      "641:\tlearn: 0.4686597\ttotal: 38.3s\tremaining: 21.3s\n",
      "642:\tlearn: 0.4686389\ttotal: 38.3s\tremaining: 21.3s\n",
      "643:\tlearn: 0.4685976\ttotal: 38.4s\tremaining: 21.2s\n",
      "644:\tlearn: 0.4685820\ttotal: 38.5s\tremaining: 21.2s\n",
      "645:\tlearn: 0.4685363\ttotal: 38.5s\tremaining: 21.1s\n",
      "646:\tlearn: 0.4685208\ttotal: 38.6s\tremaining: 21s\n",
      "647:\tlearn: 0.4684924\ttotal: 38.6s\tremaining: 21s\n",
      "648:\tlearn: 0.4684656\ttotal: 38.7s\tremaining: 20.9s\n",
      "649:\tlearn: 0.4684369\ttotal: 38.7s\tremaining: 20.9s\n",
      "650:\tlearn: 0.4684260\ttotal: 38.8s\tremaining: 20.8s\n",
      "651:\tlearn: 0.4684099\ttotal: 38.9s\tremaining: 20.7s\n",
      "652:\tlearn: 0.4683025\ttotal: 38.9s\tremaining: 20.7s\n",
      "653:\tlearn: 0.4682797\ttotal: 39s\tremaining: 20.6s\n",
      "654:\tlearn: 0.4682562\ttotal: 39s\tremaining: 20.6s\n",
      "655:\tlearn: 0.4682276\ttotal: 39.1s\tremaining: 20.5s\n",
      "656:\tlearn: 0.4682166\ttotal: 39.1s\tremaining: 20.4s\n",
      "657:\tlearn: 0.4681794\ttotal: 39.2s\tremaining: 20.4s\n",
      "658:\tlearn: 0.4681723\ttotal: 39.2s\tremaining: 20.3s\n",
      "659:\tlearn: 0.4681570\ttotal: 39.3s\tremaining: 20.2s\n",
      "660:\tlearn: 0.4681170\ttotal: 39.4s\tremaining: 20.2s\n",
      "661:\tlearn: 0.4680641\ttotal: 39.4s\tremaining: 20.1s\n",
      "662:\tlearn: 0.4680325\ttotal: 39.5s\tremaining: 20.1s\n",
      "663:\tlearn: 0.4680202\ttotal: 39.6s\tremaining: 20s\n",
      "664:\tlearn: 0.4679905\ttotal: 39.6s\tremaining: 20s\n",
      "665:\tlearn: 0.4679689\ttotal: 39.7s\tremaining: 19.9s\n",
      "666:\tlearn: 0.4679022\ttotal: 39.7s\tremaining: 19.8s\n",
      "667:\tlearn: 0.4678630\ttotal: 39.8s\tremaining: 19.8s\n",
      "668:\tlearn: 0.4678267\ttotal: 39.9s\tremaining: 19.7s\n",
      "669:\tlearn: 0.4677844\ttotal: 39.9s\tremaining: 19.7s\n",
      "670:\tlearn: 0.4677536\ttotal: 40s\tremaining: 19.6s\n",
      "671:\tlearn: 0.4676783\ttotal: 40.1s\tremaining: 19.6s\n",
      "672:\tlearn: 0.4676525\ttotal: 40.1s\tremaining: 19.5s\n",
      "673:\tlearn: 0.4676215\ttotal: 40.2s\tremaining: 19.4s\n",
      "674:\tlearn: 0.4675851\ttotal: 40.3s\tremaining: 19.4s\n",
      "675:\tlearn: 0.4675633\ttotal: 40.3s\tremaining: 19.3s\n",
      "676:\tlearn: 0.4675490\ttotal: 40.4s\tremaining: 19.3s\n",
      "677:\tlearn: 0.4675069\ttotal: 40.4s\tremaining: 19.2s\n",
      "678:\tlearn: 0.4674813\ttotal: 40.5s\tremaining: 19.1s\n",
      "679:\tlearn: 0.4674738\ttotal: 40.6s\tremaining: 19.1s\n",
      "680:\tlearn: 0.4674560\ttotal: 40.6s\tremaining: 19s\n",
      "681:\tlearn: 0.4674292\ttotal: 40.7s\tremaining: 19s\n",
      "682:\tlearn: 0.4674087\ttotal: 40.7s\tremaining: 18.9s\n",
      "683:\tlearn: 0.4672993\ttotal: 40.8s\tremaining: 18.8s\n",
      "684:\tlearn: 0.4672494\ttotal: 40.8s\tremaining: 18.8s\n",
      "685:\tlearn: 0.4672024\ttotal: 40.9s\tremaining: 18.7s\n",
      "686:\tlearn: 0.4671585\ttotal: 41s\tremaining: 18.7s\n",
      "687:\tlearn: 0.4671344\ttotal: 41.1s\tremaining: 18.6s\n",
      "688:\tlearn: 0.4670995\ttotal: 41.2s\tremaining: 18.6s\n",
      "689:\tlearn: 0.4670787\ttotal: 41.3s\tremaining: 18.5s\n",
      "690:\tlearn: 0.4670568\ttotal: 41.3s\tremaining: 18.5s\n",
      "691:\tlearn: 0.4670250\ttotal: 41.4s\tremaining: 18.4s\n",
      "692:\tlearn: 0.4669866\ttotal: 41.5s\tremaining: 18.4s\n",
      "693:\tlearn: 0.4669471\ttotal: 41.6s\tremaining: 18.3s\n",
      "694:\tlearn: 0.4669202\ttotal: 41.7s\tremaining: 18.3s\n",
      "695:\tlearn: 0.4669091\ttotal: 41.7s\tremaining: 18.2s\n",
      "696:\tlearn: 0.4668937\ttotal: 41.8s\tremaining: 18.2s\n",
      "697:\tlearn: 0.4668595\ttotal: 41.9s\tremaining: 18.1s\n",
      "698:\tlearn: 0.4668366\ttotal: 41.9s\tremaining: 18.1s\n",
      "699:\tlearn: 0.4667946\ttotal: 42s\tremaining: 18s\n",
      "700:\tlearn: 0.4667739\ttotal: 42.1s\tremaining: 17.9s\n",
      "701:\tlearn: 0.4667611\ttotal: 42.2s\tremaining: 17.9s\n",
      "702:\tlearn: 0.4667349\ttotal: 42.2s\tremaining: 17.8s\n",
      "703:\tlearn: 0.4667184\ttotal: 42.3s\tremaining: 17.8s\n",
      "704:\tlearn: 0.4666933\ttotal: 42.4s\tremaining: 17.7s\n",
      "705:\tlearn: 0.4666648\ttotal: 42.5s\tremaining: 17.7s\n",
      "706:\tlearn: 0.4666395\ttotal: 42.5s\tremaining: 17.6s\n",
      "707:\tlearn: 0.4666059\ttotal: 42.6s\tremaining: 17.6s\n",
      "708:\tlearn: 0.4665781\ttotal: 42.7s\tremaining: 17.5s\n",
      "709:\tlearn: 0.4665340\ttotal: 42.7s\tremaining: 17.5s\n",
      "710:\tlearn: 0.4664988\ttotal: 42.8s\tremaining: 17.4s\n",
      "711:\tlearn: 0.4664936\ttotal: 42.9s\tremaining: 17.3s\n",
      "712:\tlearn: 0.4664777\ttotal: 43s\tremaining: 17.3s\n",
      "713:\tlearn: 0.4664422\ttotal: 43s\tremaining: 17.2s\n",
      "714:\tlearn: 0.4664175\ttotal: 43.1s\tremaining: 17.2s\n",
      "715:\tlearn: 0.4663692\ttotal: 43.2s\tremaining: 17.1s\n",
      "716:\tlearn: 0.4663474\ttotal: 43.3s\tremaining: 17.1s\n",
      "717:\tlearn: 0.4663392\ttotal: 43.3s\tremaining: 17s\n",
      "718:\tlearn: 0.4663216\ttotal: 43.4s\tremaining: 17s\n",
      "719:\tlearn: 0.4662516\ttotal: 43.5s\tremaining: 16.9s\n",
      "720:\tlearn: 0.4662201\ttotal: 43.5s\tremaining: 16.8s\n",
      "721:\tlearn: 0.4661942\ttotal: 43.6s\tremaining: 16.8s\n",
      "722:\tlearn: 0.4661827\ttotal: 43.7s\tremaining: 16.7s\n",
      "723:\tlearn: 0.4661670\ttotal: 43.7s\tremaining: 16.7s\n",
      "724:\tlearn: 0.4661599\ttotal: 43.8s\tremaining: 16.6s\n",
      "725:\tlearn: 0.4661381\ttotal: 43.8s\tremaining: 16.5s\n",
      "726:\tlearn: 0.4661277\ttotal: 43.9s\tremaining: 16.5s\n",
      "727:\tlearn: 0.4660842\ttotal: 44s\tremaining: 16.4s\n",
      "728:\tlearn: 0.4660529\ttotal: 44s\tremaining: 16.4s\n",
      "729:\tlearn: 0.4660198\ttotal: 44.1s\tremaining: 16.3s\n",
      "730:\tlearn: 0.4660029\ttotal: 44.2s\tremaining: 16.3s\n",
      "731:\tlearn: 0.4659717\ttotal: 44.3s\tremaining: 16.2s\n",
      "732:\tlearn: 0.4659007\ttotal: 44.3s\tremaining: 16.1s\n",
      "733:\tlearn: 0.4657652\ttotal: 44.4s\tremaining: 16.1s\n",
      "734:\tlearn: 0.4657475\ttotal: 44.4s\tremaining: 16s\n",
      "735:\tlearn: 0.4657304\ttotal: 44.5s\tremaining: 16s\n",
      "736:\tlearn: 0.4655979\ttotal: 44.6s\tremaining: 15.9s\n",
      "737:\tlearn: 0.4655869\ttotal: 44.6s\tremaining: 15.8s\n",
      "738:\tlearn: 0.4655593\ttotal: 44.7s\tremaining: 15.8s\n",
      "739:\tlearn: 0.4655382\ttotal: 44.7s\tremaining: 15.7s\n",
      "740:\tlearn: 0.4654492\ttotal: 44.8s\tremaining: 15.7s\n",
      "741:\tlearn: 0.4654378\ttotal: 44.9s\tremaining: 15.6s\n",
      "742:\tlearn: 0.4654170\ttotal: 44.9s\tremaining: 15.5s\n",
      "743:\tlearn: 0.4653610\ttotal: 45s\tremaining: 15.5s\n",
      "744:\tlearn: 0.4653378\ttotal: 45.1s\tremaining: 15.4s\n",
      "745:\tlearn: 0.4653206\ttotal: 45.1s\tremaining: 15.4s\n",
      "746:\tlearn: 0.4652669\ttotal: 45.2s\tremaining: 15.3s\n",
      "747:\tlearn: 0.4652566\ttotal: 45.3s\tremaining: 15.3s\n",
      "748:\tlearn: 0.4652500\ttotal: 45.3s\tremaining: 15.2s\n",
      "749:\tlearn: 0.4651940\ttotal: 45.4s\tremaining: 15.1s\n",
      "750:\tlearn: 0.4651703\ttotal: 45.5s\tremaining: 15.1s\n",
      "751:\tlearn: 0.4651484\ttotal: 45.5s\tremaining: 15s\n",
      "752:\tlearn: 0.4651430\ttotal: 45.6s\tremaining: 15s\n",
      "753:\tlearn: 0.4651327\ttotal: 45.7s\tremaining: 14.9s\n",
      "754:\tlearn: 0.4650567\ttotal: 45.8s\tremaining: 14.8s\n",
      "755:\tlearn: 0.4650412\ttotal: 45.8s\tremaining: 14.8s\n",
      "756:\tlearn: 0.4650245\ttotal: 45.9s\tremaining: 14.7s\n",
      "757:\tlearn: 0.4649991\ttotal: 46s\tremaining: 14.7s\n",
      "758:\tlearn: 0.4649833\ttotal: 46s\tremaining: 14.6s\n",
      "759:\tlearn: 0.4649715\ttotal: 46.1s\tremaining: 14.6s\n",
      "760:\tlearn: 0.4649398\ttotal: 46.1s\tremaining: 14.5s\n",
      "761:\tlearn: 0.4649245\ttotal: 46.2s\tremaining: 14.4s\n",
      "762:\tlearn: 0.4648574\ttotal: 46.3s\tremaining: 14.4s\n",
      "763:\tlearn: 0.4647816\ttotal: 46.4s\tremaining: 14.3s\n",
      "764:\tlearn: 0.4647488\ttotal: 46.4s\tremaining: 14.3s\n",
      "765:\tlearn: 0.4647178\ttotal: 46.5s\tremaining: 14.2s\n",
      "766:\tlearn: 0.4647023\ttotal: 46.5s\tremaining: 14.1s\n",
      "767:\tlearn: 0.4646670\ttotal: 46.6s\tremaining: 14.1s\n",
      "768:\tlearn: 0.4646491\ttotal: 46.7s\tremaining: 14s\n",
      "769:\tlearn: 0.4645937\ttotal: 46.8s\tremaining: 14s\n",
      "770:\tlearn: 0.4645765\ttotal: 46.8s\tremaining: 13.9s\n",
      "771:\tlearn: 0.4645542\ttotal: 46.9s\tremaining: 13.8s\n",
      "772:\tlearn: 0.4645148\ttotal: 46.9s\tremaining: 13.8s\n",
      "773:\tlearn: 0.4644890\ttotal: 47s\tremaining: 13.7s\n",
      "774:\tlearn: 0.4644664\ttotal: 47.1s\tremaining: 13.7s\n",
      "775:\tlearn: 0.4644366\ttotal: 47.1s\tremaining: 13.6s\n",
      "776:\tlearn: 0.4644334\ttotal: 47.2s\tremaining: 13.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "777:\tlearn: 0.4644164\ttotal: 47.3s\tremaining: 13.5s\n",
      "778:\tlearn: 0.4643623\ttotal: 47.3s\tremaining: 13.4s\n",
      "779:\tlearn: 0.4643512\ttotal: 47.4s\tremaining: 13.4s\n",
      "780:\tlearn: 0.4643165\ttotal: 47.4s\tremaining: 13.3s\n",
      "781:\tlearn: 0.4642967\ttotal: 47.5s\tremaining: 13.2s\n",
      "782:\tlearn: 0.4642919\ttotal: 47.6s\tremaining: 13.2s\n",
      "783:\tlearn: 0.4642708\ttotal: 47.6s\tremaining: 13.1s\n",
      "784:\tlearn: 0.4642480\ttotal: 47.7s\tremaining: 13.1s\n",
      "785:\tlearn: 0.4642121\ttotal: 47.8s\tremaining: 13s\n",
      "786:\tlearn: 0.4641990\ttotal: 47.8s\tremaining: 12.9s\n",
      "787:\tlearn: 0.4641622\ttotal: 47.9s\tremaining: 12.9s\n",
      "788:\tlearn: 0.4641289\ttotal: 47.9s\tremaining: 12.8s\n",
      "789:\tlearn: 0.4641180\ttotal: 48s\tremaining: 12.8s\n",
      "790:\tlearn: 0.4641005\ttotal: 48s\tremaining: 12.7s\n",
      "791:\tlearn: 0.4640799\ttotal: 48.1s\tremaining: 12.6s\n",
      "792:\tlearn: 0.4640395\ttotal: 48.1s\tremaining: 12.6s\n",
      "793:\tlearn: 0.4640302\ttotal: 48.2s\tremaining: 12.5s\n",
      "794:\tlearn: 0.4640090\ttotal: 48.3s\tremaining: 12.4s\n",
      "795:\tlearn: 0.4639912\ttotal: 48.3s\tremaining: 12.4s\n",
      "796:\tlearn: 0.4639797\ttotal: 48.4s\tremaining: 12.3s\n",
      "797:\tlearn: 0.4639632\ttotal: 48.4s\tremaining: 12.3s\n",
      "798:\tlearn: 0.4639113\ttotal: 48.5s\tremaining: 12.2s\n",
      "799:\tlearn: 0.4638772\ttotal: 48.5s\tremaining: 12.1s\n",
      "800:\tlearn: 0.4638550\ttotal: 48.6s\tremaining: 12.1s\n",
      "801:\tlearn: 0.4638517\ttotal: 48.7s\tremaining: 12s\n",
      "802:\tlearn: 0.4638263\ttotal: 48.7s\tremaining: 11.9s\n",
      "803:\tlearn: 0.4638122\ttotal: 48.8s\tremaining: 11.9s\n",
      "804:\tlearn: 0.4637876\ttotal: 48.8s\tremaining: 11.8s\n",
      "805:\tlearn: 0.4637780\ttotal: 48.9s\tremaining: 11.8s\n",
      "806:\tlearn: 0.4637608\ttotal: 48.9s\tremaining: 11.7s\n",
      "807:\tlearn: 0.4637199\ttotal: 49s\tremaining: 11.6s\n",
      "808:\tlearn: 0.4637029\ttotal: 49.1s\tremaining: 11.6s\n",
      "809:\tlearn: 0.4636776\ttotal: 49.1s\tremaining: 11.5s\n",
      "810:\tlearn: 0.4636641\ttotal: 49.2s\tremaining: 11.5s\n",
      "811:\tlearn: 0.4636437\ttotal: 49.2s\tremaining: 11.4s\n",
      "812:\tlearn: 0.4636278\ttotal: 49.3s\tremaining: 11.3s\n",
      "813:\tlearn: 0.4636116\ttotal: 49.3s\tremaining: 11.3s\n",
      "814:\tlearn: 0.4635372\ttotal: 49.4s\tremaining: 11.2s\n",
      "815:\tlearn: 0.4635265\ttotal: 49.4s\tremaining: 11.1s\n",
      "816:\tlearn: 0.4634766\ttotal: 49.5s\tremaining: 11.1s\n",
      "817:\tlearn: 0.4634569\ttotal: 49.6s\tremaining: 11s\n",
      "818:\tlearn: 0.4634521\ttotal: 49.6s\tremaining: 11s\n",
      "819:\tlearn: 0.4634126\ttotal: 49.7s\tremaining: 10.9s\n",
      "820:\tlearn: 0.4633716\ttotal: 49.7s\tremaining: 10.8s\n",
      "821:\tlearn: 0.4633612\ttotal: 49.8s\tremaining: 10.8s\n",
      "822:\tlearn: 0.4633383\ttotal: 49.8s\tremaining: 10.7s\n",
      "823:\tlearn: 0.4633225\ttotal: 49.9s\tremaining: 10.7s\n",
      "824:\tlearn: 0.4633121\ttotal: 50s\tremaining: 10.6s\n",
      "825:\tlearn: 0.4632364\ttotal: 50s\tremaining: 10.5s\n",
      "826:\tlearn: 0.4632134\ttotal: 50.1s\tremaining: 10.5s\n",
      "827:\tlearn: 0.4632108\ttotal: 50.1s\tremaining: 10.4s\n",
      "828:\tlearn: 0.4631954\ttotal: 50.2s\tremaining: 10.4s\n",
      "829:\tlearn: 0.4631490\ttotal: 50.2s\tremaining: 10.3s\n",
      "830:\tlearn: 0.4631301\ttotal: 50.3s\tremaining: 10.2s\n",
      "831:\tlearn: 0.4630694\ttotal: 50.4s\tremaining: 10.2s\n",
      "832:\tlearn: 0.4629464\ttotal: 50.4s\tremaining: 10.1s\n",
      "833:\tlearn: 0.4627827\ttotal: 50.5s\tremaining: 10s\n",
      "834:\tlearn: 0.4627534\ttotal: 50.5s\tremaining: 9.99s\n",
      "835:\tlearn: 0.4627433\ttotal: 50.6s\tremaining: 9.93s\n",
      "836:\tlearn: 0.4627161\ttotal: 50.7s\tremaining: 9.86s\n",
      "837:\tlearn: 0.4626906\ttotal: 50.7s\tremaining: 9.8s\n",
      "838:\tlearn: 0.4626789\ttotal: 50.8s\tremaining: 9.74s\n",
      "839:\tlearn: 0.4626155\ttotal: 50.8s\tremaining: 9.68s\n",
      "840:\tlearn: 0.4626062\ttotal: 50.9s\tremaining: 9.62s\n",
      "841:\tlearn: 0.4625680\ttotal: 51s\tremaining: 9.56s\n",
      "842:\tlearn: 0.4625531\ttotal: 51s\tremaining: 9.5s\n",
      "843:\tlearn: 0.4625410\ttotal: 51.1s\tremaining: 9.44s\n",
      "844:\tlearn: 0.4625193\ttotal: 51.1s\tremaining: 9.38s\n",
      "845:\tlearn: 0.4624934\ttotal: 51.2s\tremaining: 9.32s\n",
      "846:\tlearn: 0.4624827\ttotal: 51.2s\tremaining: 9.26s\n",
      "847:\tlearn: 0.4623936\ttotal: 51.3s\tremaining: 9.19s\n",
      "848:\tlearn: 0.4623874\ttotal: 51.4s\tremaining: 9.13s\n",
      "849:\tlearn: 0.4623861\ttotal: 51.4s\tremaining: 9.07s\n",
      "850:\tlearn: 0.4623687\ttotal: 51.5s\tremaining: 9.01s\n",
      "851:\tlearn: 0.4623436\ttotal: 51.5s\tremaining: 8.95s\n",
      "852:\tlearn: 0.4623350\ttotal: 51.6s\tremaining: 8.89s\n",
      "853:\tlearn: 0.4623280\ttotal: 51.6s\tremaining: 8.83s\n",
      "854:\tlearn: 0.4623065\ttotal: 51.7s\tremaining: 8.77s\n",
      "855:\tlearn: 0.4622979\ttotal: 51.8s\tremaining: 8.71s\n",
      "856:\tlearn: 0.4622429\ttotal: 51.9s\tremaining: 8.66s\n",
      "857:\tlearn: 0.4621354\ttotal: 52s\tremaining: 8.6s\n",
      "858:\tlearn: 0.4620094\ttotal: 52s\tremaining: 8.54s\n",
      "859:\tlearn: 0.4620057\ttotal: 52.1s\tremaining: 8.48s\n",
      "860:\tlearn: 0.4619849\ttotal: 52.1s\tremaining: 8.42s\n",
      "861:\tlearn: 0.4619665\ttotal: 52.2s\tremaining: 8.36s\n",
      "862:\tlearn: 0.4619107\ttotal: 52.3s\tremaining: 8.3s\n",
      "863:\tlearn: 0.4618923\ttotal: 52.4s\tremaining: 8.24s\n",
      "864:\tlearn: 0.4618752\ttotal: 52.4s\tremaining: 8.19s\n",
      "865:\tlearn: 0.4618444\ttotal: 52.5s\tremaining: 8.13s\n",
      "866:\tlearn: 0.4618203\ttotal: 52.6s\tremaining: 8.07s\n",
      "867:\tlearn: 0.4617913\ttotal: 52.7s\tremaining: 8.01s\n",
      "868:\tlearn: 0.4617772\ttotal: 52.7s\tremaining: 7.95s\n",
      "869:\tlearn: 0.4617662\ttotal: 52.8s\tremaining: 7.89s\n",
      "870:\tlearn: 0.4617407\ttotal: 52.9s\tremaining: 7.83s\n",
      "871:\tlearn: 0.4617176\ttotal: 52.9s\tremaining: 7.77s\n",
      "872:\tlearn: 0.4616878\ttotal: 53s\tremaining: 7.71s\n",
      "873:\tlearn: 0.4616692\ttotal: 53.1s\tremaining: 7.65s\n",
      "874:\tlearn: 0.4616434\ttotal: 53.1s\tremaining: 7.59s\n",
      "875:\tlearn: 0.4616337\ttotal: 53.2s\tremaining: 7.53s\n",
      "876:\tlearn: 0.4615867\ttotal: 53.3s\tremaining: 7.47s\n",
      "877:\tlearn: 0.4615716\ttotal: 53.3s\tremaining: 7.41s\n",
      "878:\tlearn: 0.4615564\ttotal: 53.4s\tremaining: 7.35s\n",
      "879:\tlearn: 0.4615249\ttotal: 53.5s\tremaining: 7.29s\n",
      "880:\tlearn: 0.4615186\ttotal: 53.5s\tremaining: 7.23s\n",
      "881:\tlearn: 0.4615094\ttotal: 53.6s\tremaining: 7.17s\n",
      "882:\tlearn: 0.4614915\ttotal: 53.7s\tremaining: 7.11s\n",
      "883:\tlearn: 0.4614716\ttotal: 53.8s\tremaining: 7.05s\n",
      "884:\tlearn: 0.4614641\ttotal: 53.8s\tremaining: 6.99s\n",
      "885:\tlearn: 0.4614562\ttotal: 53.9s\tremaining: 6.93s\n",
      "886:\tlearn: 0.4614372\ttotal: 53.9s\tremaining: 6.87s\n",
      "887:\tlearn: 0.4614249\ttotal: 54s\tremaining: 6.81s\n",
      "888:\tlearn: 0.4613611\ttotal: 54.1s\tremaining: 6.75s\n",
      "889:\tlearn: 0.4613278\ttotal: 54.2s\tremaining: 6.69s\n",
      "890:\tlearn: 0.4613023\ttotal: 54.2s\tremaining: 6.63s\n",
      "891:\tlearn: 0.4612876\ttotal: 54.3s\tremaining: 6.57s\n",
      "892:\tlearn: 0.4612507\ttotal: 54.4s\tremaining: 6.51s\n",
      "893:\tlearn: 0.4612139\ttotal: 54.4s\tremaining: 6.45s\n",
      "894:\tlearn: 0.4611969\ttotal: 54.5s\tremaining: 6.39s\n",
      "895:\tlearn: 0.4611814\ttotal: 54.6s\tremaining: 6.33s\n",
      "896:\tlearn: 0.4611481\ttotal: 54.6s\tremaining: 6.27s\n",
      "897:\tlearn: 0.4611178\ttotal: 54.7s\tremaining: 6.21s\n",
      "898:\tlearn: 0.4610984\ttotal: 54.8s\tremaining: 6.15s\n",
      "899:\tlearn: 0.4610771\ttotal: 54.8s\tremaining: 6.09s\n",
      "900:\tlearn: 0.4610663\ttotal: 54.9s\tremaining: 6.03s\n",
      "901:\tlearn: 0.4609910\ttotal: 55s\tremaining: 5.97s\n",
      "902:\tlearn: 0.4609693\ttotal: 55.1s\tremaining: 5.91s\n",
      "903:\tlearn: 0.4608804\ttotal: 55.1s\tremaining: 5.85s\n",
      "904:\tlearn: 0.4608456\ttotal: 55.2s\tremaining: 5.79s\n",
      "905:\tlearn: 0.4608211\ttotal: 55.3s\tremaining: 5.73s\n",
      "906:\tlearn: 0.4608086\ttotal: 55.3s\tremaining: 5.67s\n",
      "907:\tlearn: 0.4607798\ttotal: 55.4s\tremaining: 5.61s\n",
      "908:\tlearn: 0.4607623\ttotal: 55.5s\tremaining: 5.55s\n",
      "909:\tlearn: 0.4607527\ttotal: 55.5s\tremaining: 5.49s\n",
      "910:\tlearn: 0.4607429\ttotal: 55.6s\tremaining: 5.43s\n",
      "911:\tlearn: 0.4607052\ttotal: 55.6s\tremaining: 5.37s\n",
      "912:\tlearn: 0.4606797\ttotal: 55.7s\tremaining: 5.31s\n",
      "913:\tlearn: 0.4606277\ttotal: 55.8s\tremaining: 5.25s\n",
      "914:\tlearn: 0.4606168\ttotal: 55.9s\tremaining: 5.19s\n",
      "915:\tlearn: 0.4605762\ttotal: 55.9s\tremaining: 5.13s\n",
      "916:\tlearn: 0.4605528\ttotal: 56s\tremaining: 5.07s\n",
      "917:\tlearn: 0.4605462\ttotal: 56s\tremaining: 5.01s\n",
      "918:\tlearn: 0.4605161\ttotal: 56.1s\tremaining: 4.95s\n",
      "919:\tlearn: 0.4604866\ttotal: 56.2s\tremaining: 4.89s\n",
      "920:\tlearn: 0.4604733\ttotal: 56.3s\tremaining: 4.83s\n",
      "921:\tlearn: 0.4604660\ttotal: 56.3s\tremaining: 4.76s\n",
      "922:\tlearn: 0.4604600\ttotal: 56.4s\tremaining: 4.7s\n",
      "923:\tlearn: 0.4604464\ttotal: 56.4s\tremaining: 4.64s\n",
      "924:\tlearn: 0.4604018\ttotal: 56.5s\tremaining: 4.58s\n",
      "925:\tlearn: 0.4603902\ttotal: 56.6s\tremaining: 4.52s\n",
      "926:\tlearn: 0.4603709\ttotal: 56.6s\tremaining: 4.46s\n",
      "927:\tlearn: 0.4603524\ttotal: 56.7s\tremaining: 4.4s\n",
      "928:\tlearn: 0.4603240\ttotal: 56.8s\tremaining: 4.34s\n",
      "929:\tlearn: 0.4603106\ttotal: 56.8s\tremaining: 4.28s\n",
      "930:\tlearn: 0.4602952\ttotal: 56.9s\tremaining: 4.21s\n",
      "931:\tlearn: 0.4602604\ttotal: 56.9s\tremaining: 4.15s\n",
      "932:\tlearn: 0.4602456\ttotal: 57s\tremaining: 4.09s\n",
      "933:\tlearn: 0.4602071\ttotal: 57.1s\tremaining: 4.03s\n",
      "934:\tlearn: 0.4601911\ttotal: 57.1s\tremaining: 3.97s\n",
      "935:\tlearn: 0.4601772\ttotal: 57.2s\tremaining: 3.91s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "936:\tlearn: 0.4601396\ttotal: 57.2s\tremaining: 3.85s\n",
      "937:\tlearn: 0.4601085\ttotal: 57.3s\tremaining: 3.79s\n",
      "938:\tlearn: 0.4600601\ttotal: 57.4s\tremaining: 3.73s\n",
      "939:\tlearn: 0.4600427\ttotal: 57.4s\tremaining: 3.66s\n",
      "940:\tlearn: 0.4600327\ttotal: 57.5s\tremaining: 3.6s\n",
      "941:\tlearn: 0.4599979\ttotal: 57.5s\tremaining: 3.54s\n",
      "942:\tlearn: 0.4599841\ttotal: 57.6s\tremaining: 3.48s\n",
      "943:\tlearn: 0.4599754\ttotal: 57.6s\tremaining: 3.42s\n",
      "944:\tlearn: 0.4599459\ttotal: 57.7s\tremaining: 3.36s\n",
      "945:\tlearn: 0.4599291\ttotal: 57.8s\tremaining: 3.3s\n",
      "946:\tlearn: 0.4598938\ttotal: 57.8s\tremaining: 3.23s\n",
      "947:\tlearn: 0.4598587\ttotal: 57.9s\tremaining: 3.17s\n",
      "948:\tlearn: 0.4597615\ttotal: 57.9s\tremaining: 3.11s\n",
      "949:\tlearn: 0.4597471\ttotal: 58s\tremaining: 3.05s\n",
      "950:\tlearn: 0.4597312\ttotal: 58s\tremaining: 2.99s\n",
      "951:\tlearn: 0.4597102\ttotal: 58.1s\tremaining: 2.93s\n",
      "952:\tlearn: 0.4596861\ttotal: 58.2s\tremaining: 2.87s\n",
      "953:\tlearn: 0.4596746\ttotal: 58.2s\tremaining: 2.81s\n",
      "954:\tlearn: 0.4596570\ttotal: 58.3s\tremaining: 2.75s\n",
      "955:\tlearn: 0.4596445\ttotal: 58.3s\tremaining: 2.68s\n",
      "956:\tlearn: 0.4596290\ttotal: 58.4s\tremaining: 2.62s\n",
      "957:\tlearn: 0.4596004\ttotal: 58.5s\tremaining: 2.56s\n",
      "958:\tlearn: 0.4595818\ttotal: 58.5s\tremaining: 2.5s\n",
      "959:\tlearn: 0.4595699\ttotal: 58.6s\tremaining: 2.44s\n",
      "960:\tlearn: 0.4593623\ttotal: 58.6s\tremaining: 2.38s\n",
      "961:\tlearn: 0.4593453\ttotal: 58.7s\tremaining: 2.32s\n",
      "962:\tlearn: 0.4591735\ttotal: 58.7s\tremaining: 2.26s\n",
      "963:\tlearn: 0.4590919\ttotal: 58.8s\tremaining: 2.2s\n",
      "964:\tlearn: 0.4590779\ttotal: 58.9s\tremaining: 2.14s\n",
      "965:\tlearn: 0.4590582\ttotal: 59s\tremaining: 2.08s\n",
      "966:\tlearn: 0.4590271\ttotal: 59s\tremaining: 2.01s\n",
      "967:\tlearn: 0.4590084\ttotal: 59.1s\tremaining: 1.95s\n",
      "968:\tlearn: 0.4589557\ttotal: 59.1s\tremaining: 1.89s\n",
      "969:\tlearn: 0.4589424\ttotal: 59.2s\tremaining: 1.83s\n",
      "970:\tlearn: 0.4589118\ttotal: 59.3s\tremaining: 1.77s\n",
      "971:\tlearn: 0.4588998\ttotal: 59.3s\tremaining: 1.71s\n",
      "972:\tlearn: 0.4588794\ttotal: 59.4s\tremaining: 1.65s\n",
      "973:\tlearn: 0.4588634\ttotal: 59.5s\tremaining: 1.59s\n",
      "974:\tlearn: 0.4588412\ttotal: 59.5s\tremaining: 1.53s\n",
      "975:\tlearn: 0.4588192\ttotal: 59.6s\tremaining: 1.47s\n",
      "976:\tlearn: 0.4587893\ttotal: 59.7s\tremaining: 1.4s\n",
      "977:\tlearn: 0.4587768\ttotal: 59.7s\tremaining: 1.34s\n",
      "978:\tlearn: 0.4587433\ttotal: 59.8s\tremaining: 1.28s\n",
      "979:\tlearn: 0.4587323\ttotal: 59.9s\tremaining: 1.22s\n",
      "980:\tlearn: 0.4587162\ttotal: 59.9s\tremaining: 1.16s\n",
      "981:\tlearn: 0.4587030\ttotal: 60s\tremaining: 1.1s\n",
      "982:\tlearn: 0.4586403\ttotal: 1m\tremaining: 1.04s\n",
      "983:\tlearn: 0.4586190\ttotal: 1m\tremaining: 978ms\n",
      "984:\tlearn: 0.4586056\ttotal: 1m\tremaining: 917ms\n",
      "985:\tlearn: 0.4585026\ttotal: 1m\tremaining: 856ms\n",
      "986:\tlearn: 0.4584803\ttotal: 1m\tremaining: 795ms\n",
      "987:\tlearn: 0.4584680\ttotal: 1m\tremaining: 734ms\n",
      "988:\tlearn: 0.4584567\ttotal: 1m\tremaining: 672ms\n",
      "989:\tlearn: 0.4584363\ttotal: 1m\tremaining: 611ms\n",
      "990:\tlearn: 0.4584229\ttotal: 1m\tremaining: 550ms\n",
      "991:\tlearn: 0.4583996\ttotal: 1m\tremaining: 489ms\n",
      "992:\tlearn: 0.4583800\ttotal: 1m\tremaining: 428ms\n",
      "993:\tlearn: 0.4583676\ttotal: 1m\tremaining: 367ms\n",
      "994:\tlearn: 0.4583542\ttotal: 1m\tremaining: 306ms\n",
      "995:\tlearn: 0.4583488\ttotal: 1m\tremaining: 245ms\n",
      "996:\tlearn: 0.4583359\ttotal: 1m\tremaining: 183ms\n",
      "997:\tlearn: 0.4582234\ttotal: 1m 1s\tremaining: 122ms\n",
      "998:\tlearn: 0.4582140\ttotal: 1m 1s\tremaining: 61.2ms\n",
      "999:\tlearn: 0.4581617\ttotal: 1m 1s\tremaining: 0us\n",
      "CatBoostClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.64      0.74       760\n",
      "         1.0       0.71      0.90      0.79       733\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      1493\n",
      "   macro avg       0.79      0.77      0.76      1493\n",
      "weighted avg       0.79      0.77      0.76      1493\n",
      "\n",
      "[[485 275]\n",
      " [ 72 661]]\n",
      "Accuracy is  76.75820495646349\n",
      "Time on model's work: 63.881 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.66      0.75       760\n",
      "         1.0       0.72      0.89      0.79       733\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      1493\n",
      "   macro avg       0.79      0.78      0.77      1493\n",
      "weighted avg       0.79      0.77      0.77      1493\n",
      "\n",
      "[[504 256]\n",
      " [ 81 652]]\n",
      "Accuracy is  77.42799732083054\n",
      "Time on model's work: 0.144 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.65      0.68       760\n",
      "         1.0       0.67      0.73      0.70       733\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      1493\n",
      "   macro avg       0.69      0.69      0.69      1493\n",
      "weighted avg       0.69      0.69      0.69      1493\n",
      "\n",
      "[[492 268]\n",
      " [197 536]]\n",
      "Accuracy is  68.85465505693234\n",
      "Time on model's work: 0.182 s\n",
      "====================================================================================================\n",
      "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n",
      "TFFMClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.68      0.73       760\n",
      "         1.0       0.71      0.81      0.76       733\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      1493\n",
      "   macro avg       0.75      0.75      0.74      1493\n",
      "weighted avg       0.75      0.75      0.74      1493\n",
      "\n",
      "[[519 241]\n",
      " [139 594]]\n",
      "Accuracy is  74.54789015405224\n",
      "Time on model's work: 18.646 s\n",
      "====================================================================================================\n",
      "TOTAL TIME SPENT:  296.608 s\n"
     ]
    }
   ],
   "source": [
    "clfs = [\n",
    "        ['RandomForestClassifier', RandomForestClassifier()],\n",
    "        ['GradientBoostingClassifier', GradientBoostingClassifier()],\n",
    "        ['ExtraTreesClassifier', ExtraTreesClassifier()],\n",
    "        ['AdaBoostClassifier', AdaBoostClassifier()],\n",
    "        ['BaggingClassifier', BaggingClassifier()],\n",
    "        ['DecisionTreeClassifier', DecisionTreeClassifier()],\n",
    "        ['MLPClassifier', MLPClassifier()],\n",
    "        ['XGBClassifier', XGBClassifier()],\n",
    "        ['CatBoostClassifier', CatBoostClassifier()],\n",
    "        ['LogisticRegression', LogisticRegression()],\n",
    "        ['SGDClassifier', SGDClassifier()],\n",
    "        ['TFFMClassifier', TFFMClassifier()]\n",
    "       ]\n",
    "t = time()\n",
    "for name, clf in clfs:\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    prediction = clf.predict(X_test)\n",
    "    print(name)\n",
    "    print(classification_report(y_test, prediction))\n",
    "    print(confusion_matrix(y_test, prediction))\n",
    "    print('Accuracy is ', accuracy_score(y_test, prediction)*100)\n",
    "    print (\"Time on model's work:\", round(time()-t0, 3), \"s\")\n",
    "    print('='*100)\n",
    "print (\"TOTAL TIME SPENT: \", round(time()-t, 3), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [00:05<00:00, 19.61epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7387809778968519\n",
      "[[456 304]\n",
      " [ 86 647]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.60      0.70       760\n",
      "         1.0       0.68      0.88      0.77       733\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      1493\n",
      "   macro avg       0.76      0.74      0.73      1493\n",
      "weighted avg       0.76      0.74      0.73      1493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TFFM sparse\n",
    "# only CSR format supported\n",
    "X_train_sparse = sp.csr_matrix(X_train)\n",
    "X_test_sparse = sp.csr_matrix(X_test)\n",
    "# BEST OPTIMIZED TFFM\n",
    "model = TFFMClassifier(\n",
    "    order=2, \n",
    "    rank=10,\n",
    "    pos_class_weight=2.0,\n",
    "    optimizer=tf.train.FtrlOptimizer(0.01, l2_regularization_strength=0.001, l2_shrinkage_regularization_strength=0.001), \n",
    "    n_epochs=100, \n",
    "    batch_size=1024,\n",
    "    init_std=0.001,\n",
    "    reg=0.01,\n",
    "    input_type='sparse',\n",
    "    #log_dir='./tmp/logs',\n",
    "    #verbose=1,\n",
    "    seed=42\n",
    ")\n",
    "model.fit(X_train_sparse, y_train, show_progress=True)\n",
    "predictions = model.predict(X_test_sparse)\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, predictions)))\n",
    "# this will close tf.Session and free resources\n",
    "print(confusion_matrix(y_test,predictions)) \n",
    "print(classification_report(y_test, predictions))\n",
    "model.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NearMiss (version = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 3726), (1.0, 3726)]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "nm1 = NearMiss(version=1)\n",
    "X_resampled_nm1, y_resampled1 = nm1.fit_resample(features_list_array, labels_list_array)\n",
    "print(sorted(Counter(y_resampled1).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled_nm1, y_resampled1, random_state=35, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.94      0.88       763\n",
      "         1.0       0.92      0.79      0.85       728\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      1491\n",
      "   macro avg       0.87      0.86      0.87      1491\n",
      "weighted avg       0.87      0.87      0.87      1491\n",
      "\n",
      "[[714  49]\n",
      " [150 578]]\n",
      "Accuracy is  86.65325285043595\n",
      "Time on model's work: 0.767 s\n",
      "====================================================================================================\n",
      "GradientBoostingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.87      0.87       763\n",
      "         1.0       0.86      0.86      0.86       728\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      1491\n",
      "   macro avg       0.86      0.86      0.86      1491\n",
      "weighted avg       0.86      0.86      0.86      1491\n",
      "\n",
      "[[661 102]\n",
      " [100 628]]\n",
      "Accuracy is  86.45204560697518\n",
      "Time on model's work: 31.618 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.96      0.88       763\n",
      "         1.0       0.94      0.78      0.85       728\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      1491\n",
      "   macro avg       0.88      0.87      0.87      1491\n",
      "weighted avg       0.88      0.87      0.87      1491\n",
      "\n",
      "[[730  33]\n",
      " [163 565]]\n",
      "Accuracy is  86.85446009389672\n",
      "Time on model's work: 1.506 s\n",
      "====================================================================================================\n",
      "AdaBoostClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.86      0.86       763\n",
      "         1.0       0.85      0.85      0.85       728\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      1491\n",
      "   macro avg       0.86      0.86      0.86      1491\n",
      "weighted avg       0.86      0.86      0.86      1491\n",
      "\n",
      "[[656 107]\n",
      " [108 620]]\n",
      "Accuracy is  85.58014755197854\n",
      "Time on model's work: 9.615 s\n",
      "====================================================================================================\n",
      "BaggingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.94      0.87       763\n",
      "         1.0       0.93      0.78      0.85       728\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      1491\n",
      "   macro avg       0.87      0.86      0.86      1491\n",
      "weighted avg       0.87      0.86      0.86      1491\n",
      "\n",
      "[[717  46]\n",
      " [159 569]]\n",
      "Accuracy is  86.25083836351442\n",
      "Time on model's work: 4.363 s\n",
      "====================================================================================================\n",
      "DecisionTreeClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.96      0.87       763\n",
      "         1.0       0.94      0.75      0.83       728\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      1491\n",
      "   macro avg       0.87      0.85      0.85      1491\n",
      "weighted avg       0.87      0.86      0.85      1491\n",
      "\n",
      "[[730  33]\n",
      " [183 545]]\n",
      "Accuracy is  85.51307847082495\n",
      "Time on model's work: 0.72 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.92      0.88       763\n",
      "         1.0       0.91      0.81      0.85       728\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      1491\n",
      "   macro avg       0.87      0.86      0.87      1491\n",
      "weighted avg       0.87      0.87      0.87      1491\n",
      "\n",
      "[[704  59]\n",
      " [141 587]]\n",
      "Accuracy is  86.58618376928237\n",
      "Time on model's work: 121.631 s\n",
      "====================================================================================================\n",
      "XGBClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.85      0.87       763\n",
      "         1.0       0.85      0.89      0.87       728\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      1491\n",
      "   macro avg       0.87      0.87      0.87      1491\n",
      "weighted avg       0.87      0.87      0.87      1491\n",
      "\n",
      "[[649 114]\n",
      " [ 82 646]]\n",
      "Accuracy is  86.85446009389672\n",
      "Time on model's work: 43.209 s\n",
      "====================================================================================================\n",
      "0:\tlearn: 0.6791880\ttotal: 102ms\tremaining: 1m 41s\n",
      "1:\tlearn: 0.6662516\ttotal: 181ms\tremaining: 1m 30s\n",
      "2:\tlearn: 0.6509329\ttotal: 253ms\tremaining: 1m 24s\n",
      "3:\tlearn: 0.6330583\ttotal: 319ms\tremaining: 1m 19s\n",
      "4:\tlearn: 0.6210736\ttotal: 394ms\tremaining: 1m 18s\n",
      "5:\tlearn: 0.6093855\ttotal: 460ms\tremaining: 1m 16s\n",
      "6:\tlearn: 0.5970175\ttotal: 523ms\tremaining: 1m 14s\n",
      "7:\tlearn: 0.5878154\ttotal: 585ms\tremaining: 1m 12s\n",
      "8:\tlearn: 0.5801930\ttotal: 651ms\tremaining: 1m 11s\n",
      "9:\tlearn: 0.5685404\ttotal: 723ms\tremaining: 1m 11s\n",
      "10:\tlearn: 0.5610870\ttotal: 798ms\tremaining: 1m 11s\n",
      "11:\tlearn: 0.5539601\ttotal: 872ms\tremaining: 1m 11s\n",
      "12:\tlearn: 0.5459435\ttotal: 953ms\tremaining: 1m 12s\n",
      "13:\tlearn: 0.5383305\ttotal: 1.04s\tremaining: 1m 13s\n",
      "14:\tlearn: 0.5312646\ttotal: 1.11s\tremaining: 1m 13s\n",
      "15:\tlearn: 0.5251989\ttotal: 1.2s\tremaining: 1m 13s\n",
      "16:\tlearn: 0.5198025\ttotal: 1.29s\tremaining: 1m 14s\n",
      "17:\tlearn: 0.5157032\ttotal: 1.36s\tremaining: 1m 14s\n",
      "18:\tlearn: 0.5098289\ttotal: 1.42s\tremaining: 1m 13s\n",
      "19:\tlearn: 0.5030152\ttotal: 1.49s\tremaining: 1m 13s\n",
      "20:\tlearn: 0.4982078\ttotal: 1.56s\tremaining: 1m 12s\n",
      "21:\tlearn: 0.4946033\ttotal: 1.62s\tremaining: 1m 12s\n",
      "22:\tlearn: 0.4906562\ttotal: 1.68s\tremaining: 1m 11s\n",
      "23:\tlearn: 0.4865106\ttotal: 1.74s\tremaining: 1m 10s\n",
      "24:\tlearn: 0.4813546\ttotal: 1.81s\tremaining: 1m 10s\n",
      "25:\tlearn: 0.4778052\ttotal: 1.87s\tremaining: 1m 10s\n",
      "26:\tlearn: 0.4729296\ttotal: 1.94s\tremaining: 1m 9s\n",
      "27:\tlearn: 0.4694193\ttotal: 2s\tremaining: 1m 9s\n",
      "28:\tlearn: 0.4662427\ttotal: 2.06s\tremaining: 1m 9s\n",
      "29:\tlearn: 0.4628264\ttotal: 2.13s\tremaining: 1m 8s\n",
      "30:\tlearn: 0.4588759\ttotal: 2.2s\tremaining: 1m 8s\n",
      "31:\tlearn: 0.4557308\ttotal: 2.27s\tremaining: 1m 8s\n",
      "32:\tlearn: 0.4521984\ttotal: 2.34s\tremaining: 1m 8s\n",
      "33:\tlearn: 0.4485365\ttotal: 2.41s\tremaining: 1m 8s\n",
      "34:\tlearn: 0.4459625\ttotal: 2.48s\tremaining: 1m 8s\n",
      "35:\tlearn: 0.4433009\ttotal: 2.55s\tremaining: 1m 8s\n",
      "36:\tlearn: 0.4417139\ttotal: 2.61s\tremaining: 1m 7s\n",
      "37:\tlearn: 0.4375617\ttotal: 2.68s\tremaining: 1m 7s\n",
      "38:\tlearn: 0.4346485\ttotal: 2.74s\tremaining: 1m 7s\n",
      "39:\tlearn: 0.4321322\ttotal: 2.8s\tremaining: 1m 7s\n",
      "40:\tlearn: 0.4283143\ttotal: 2.88s\tremaining: 1m 7s\n",
      "41:\tlearn: 0.4258121\ttotal: 2.94s\tremaining: 1m 7s\n",
      "42:\tlearn: 0.4246207\ttotal: 3s\tremaining: 1m 6s\n",
      "43:\tlearn: 0.4225420\ttotal: 3.07s\tremaining: 1m 6s\n",
      "44:\tlearn: 0.4203359\ttotal: 3.14s\tremaining: 1m 6s\n",
      "45:\tlearn: 0.4191264\ttotal: 3.2s\tremaining: 1m 6s\n",
      "46:\tlearn: 0.4163937\ttotal: 3.26s\tremaining: 1m 6s\n",
      "47:\tlearn: 0.4144691\ttotal: 3.33s\tremaining: 1m 6s\n",
      "48:\tlearn: 0.4125072\ttotal: 3.4s\tremaining: 1m 5s\n",
      "49:\tlearn: 0.4103338\ttotal: 3.47s\tremaining: 1m 5s\n",
      "50:\tlearn: 0.4093005\ttotal: 3.53s\tremaining: 1m 5s\n",
      "51:\tlearn: 0.4075817\ttotal: 3.6s\tremaining: 1m 5s\n",
      "52:\tlearn: 0.4063819\ttotal: 3.66s\tremaining: 1m 5s\n",
      "53:\tlearn: 0.4039684\ttotal: 3.75s\tremaining: 1m 5s\n",
      "54:\tlearn: 0.4026829\ttotal: 3.81s\tremaining: 1m 5s\n",
      "55:\tlearn: 0.4012643\ttotal: 3.87s\tremaining: 1m 5s\n",
      "56:\tlearn: 0.3999747\ttotal: 3.94s\tremaining: 1m 5s\n",
      "57:\tlearn: 0.3984168\ttotal: 4s\tremaining: 1m 4s\n",
      "58:\tlearn: 0.3970943\ttotal: 4.06s\tremaining: 1m 4s\n",
      "59:\tlearn: 0.3964401\ttotal: 4.12s\tremaining: 1m 4s\n",
      "60:\tlearn: 0.3956112\ttotal: 4.18s\tremaining: 1m 4s\n",
      "61:\tlearn: 0.3943418\ttotal: 4.24s\tremaining: 1m 4s\n",
      "62:\tlearn: 0.3932465\ttotal: 4.31s\tremaining: 1m 4s\n",
      "63:\tlearn: 0.3924165\ttotal: 4.38s\tremaining: 1m 4s\n",
      "64:\tlearn: 0.3915005\ttotal: 4.44s\tremaining: 1m 3s\n",
      "65:\tlearn: 0.3903112\ttotal: 4.5s\tremaining: 1m 3s\n",
      "66:\tlearn: 0.3894937\ttotal: 4.56s\tremaining: 1m 3s\n",
      "67:\tlearn: 0.3886943\ttotal: 4.62s\tremaining: 1m 3s\n",
      "68:\tlearn: 0.3881324\ttotal: 4.68s\tremaining: 1m 3s\n",
      "69:\tlearn: 0.3874597\ttotal: 4.74s\tremaining: 1m 2s\n",
      "70:\tlearn: 0.3867327\ttotal: 4.81s\tremaining: 1m 2s\n",
      "71:\tlearn: 0.3858521\ttotal: 4.87s\tremaining: 1m 2s\n",
      "72:\tlearn: 0.3853827\ttotal: 4.93s\tremaining: 1m 2s\n",
      "73:\tlearn: 0.3845966\ttotal: 5s\tremaining: 1m 2s\n",
      "74:\tlearn: 0.3839638\ttotal: 5.05s\tremaining: 1m 2s\n",
      "75:\tlearn: 0.3826364\ttotal: 5.12s\tremaining: 1m 2s\n",
      "76:\tlearn: 0.3816737\ttotal: 5.18s\tremaining: 1m 2s\n",
      "77:\tlearn: 0.3806557\ttotal: 5.24s\tremaining: 1m 1s\n",
      "78:\tlearn: 0.3798348\ttotal: 5.31s\tremaining: 1m 1s\n",
      "79:\tlearn: 0.3795918\ttotal: 5.37s\tremaining: 1m 1s\n",
      "80:\tlearn: 0.3790858\ttotal: 5.43s\tremaining: 1m 1s\n",
      "81:\tlearn: 0.3781232\ttotal: 5.5s\tremaining: 1m 1s\n",
      "82:\tlearn: 0.3776351\ttotal: 5.56s\tremaining: 1m 1s\n",
      "83:\tlearn: 0.3772875\ttotal: 5.62s\tremaining: 1m 1s\n",
      "84:\tlearn: 0.3757850\ttotal: 5.69s\tremaining: 1m 1s\n",
      "85:\tlearn: 0.3746154\ttotal: 5.75s\tremaining: 1m 1s\n",
      "86:\tlearn: 0.3741938\ttotal: 5.81s\tremaining: 1m 1s\n",
      "87:\tlearn: 0.3738585\ttotal: 5.88s\tremaining: 1m\n",
      "88:\tlearn: 0.3733682\ttotal: 5.94s\tremaining: 1m\n",
      "89:\tlearn: 0.3726426\ttotal: 6.01s\tremaining: 1m\n",
      "90:\tlearn: 0.3718574\ttotal: 6.08s\tremaining: 1m\n",
      "91:\tlearn: 0.3716648\ttotal: 6.13s\tremaining: 1m\n",
      "92:\tlearn: 0.3711498\ttotal: 6.2s\tremaining: 1m\n",
      "93:\tlearn: 0.3708685\ttotal: 6.25s\tremaining: 1m\n",
      "94:\tlearn: 0.3702260\ttotal: 6.32s\tremaining: 1m\n",
      "95:\tlearn: 0.3697827\ttotal: 6.38s\tremaining: 1m\n",
      "96:\tlearn: 0.3690920\ttotal: 6.44s\tremaining: 59.9s\n",
      "97:\tlearn: 0.3685661\ttotal: 6.5s\tremaining: 59.8s\n",
      "98:\tlearn: 0.3677797\ttotal: 6.56s\tremaining: 59.7s\n",
      "99:\tlearn: 0.3672461\ttotal: 6.62s\tremaining: 59.6s\n",
      "100:\tlearn: 0.3668261\ttotal: 6.68s\tremaining: 59.5s\n",
      "101:\tlearn: 0.3660319\ttotal: 6.75s\tremaining: 59.4s\n",
      "102:\tlearn: 0.3657665\ttotal: 6.8s\tremaining: 59.3s\n",
      "103:\tlearn: 0.3653749\ttotal: 6.86s\tremaining: 59.1s\n",
      "104:\tlearn: 0.3652571\ttotal: 6.92s\tremaining: 59s\n",
      "105:\tlearn: 0.3647080\ttotal: 6.98s\tremaining: 58.9s\n",
      "106:\tlearn: 0.3640721\ttotal: 7.05s\tremaining: 58.8s\n",
      "107:\tlearn: 0.3635560\ttotal: 7.11s\tremaining: 58.7s\n",
      "108:\tlearn: 0.3632121\ttotal: 7.17s\tremaining: 58.6s\n",
      "109:\tlearn: 0.3630588\ttotal: 7.23s\tremaining: 58.5s\n",
      "110:\tlearn: 0.3626645\ttotal: 7.29s\tremaining: 58.4s\n",
      "111:\tlearn: 0.3624822\ttotal: 7.36s\tremaining: 58.3s\n",
      "112:\tlearn: 0.3621374\ttotal: 7.42s\tremaining: 58.2s\n",
      "113:\tlearn: 0.3617204\ttotal: 7.48s\tremaining: 58.1s\n",
      "114:\tlearn: 0.3612310\ttotal: 7.54s\tremaining: 58s\n",
      "115:\tlearn: 0.3604563\ttotal: 7.6s\tremaining: 57.9s\n",
      "116:\tlearn: 0.3601376\ttotal: 7.67s\tremaining: 57.9s\n",
      "117:\tlearn: 0.3594213\ttotal: 7.72s\tremaining: 57.7s\n",
      "118:\tlearn: 0.3588963\ttotal: 7.79s\tremaining: 57.6s\n",
      "119:\tlearn: 0.3586017\ttotal: 7.85s\tremaining: 57.5s\n",
      "120:\tlearn: 0.3583242\ttotal: 7.9s\tremaining: 57.4s\n",
      "121:\tlearn: 0.3582265\ttotal: 7.96s\tremaining: 57.3s\n",
      "122:\tlearn: 0.3578547\ttotal: 8.03s\tremaining: 57.2s\n",
      "123:\tlearn: 0.3575269\ttotal: 8.09s\tremaining: 57.2s\n",
      "124:\tlearn: 0.3574483\ttotal: 8.14s\tremaining: 57s\n",
      "125:\tlearn: 0.3571429\ttotal: 8.21s\tremaining: 56.9s\n",
      "126:\tlearn: 0.3567775\ttotal: 8.27s\tremaining: 56.9s\n",
      "127:\tlearn: 0.3564035\ttotal: 8.33s\tremaining: 56.7s\n",
      "128:\tlearn: 0.3559225\ttotal: 8.39s\tremaining: 56.7s\n",
      "129:\tlearn: 0.3554646\ttotal: 8.45s\tremaining: 56.6s\n",
      "130:\tlearn: 0.3549106\ttotal: 8.51s\tremaining: 56.5s\n",
      "131:\tlearn: 0.3544891\ttotal: 8.57s\tremaining: 56.4s\n",
      "132:\tlearn: 0.3541492\ttotal: 8.64s\tremaining: 56.3s\n",
      "133:\tlearn: 0.3539271\ttotal: 8.7s\tremaining: 56.2s\n",
      "134:\tlearn: 0.3535995\ttotal: 8.76s\tremaining: 56.1s\n",
      "135:\tlearn: 0.3532331\ttotal: 8.82s\tremaining: 56s\n",
      "136:\tlearn: 0.3529573\ttotal: 8.88s\tremaining: 55.9s\n",
      "137:\tlearn: 0.3525452\ttotal: 8.94s\tremaining: 55.8s\n",
      "138:\tlearn: 0.3520905\ttotal: 9s\tremaining: 55.8s\n",
      "139:\tlearn: 0.3518774\ttotal: 9.07s\tremaining: 55.7s\n",
      "140:\tlearn: 0.3515046\ttotal: 9.13s\tremaining: 55.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141:\tlearn: 0.3513225\ttotal: 9.19s\tremaining: 55.5s\n",
      "142:\tlearn: 0.3510411\ttotal: 9.24s\tremaining: 55.4s\n",
      "143:\tlearn: 0.3508829\ttotal: 9.3s\tremaining: 55.3s\n",
      "144:\tlearn: 0.3505285\ttotal: 9.35s\tremaining: 55.1s\n",
      "145:\tlearn: 0.3502733\ttotal: 9.41s\tremaining: 55s\n",
      "146:\tlearn: 0.3501829\ttotal: 9.46s\tremaining: 54.9s\n",
      "147:\tlearn: 0.3496840\ttotal: 9.52s\tremaining: 54.8s\n",
      "148:\tlearn: 0.3494905\ttotal: 9.57s\tremaining: 54.7s\n",
      "149:\tlearn: 0.3488890\ttotal: 9.63s\tremaining: 54.6s\n",
      "150:\tlearn: 0.3485076\ttotal: 9.69s\tremaining: 54.5s\n",
      "151:\tlearn: 0.3483814\ttotal: 9.74s\tremaining: 54.4s\n",
      "152:\tlearn: 0.3483428\ttotal: 9.8s\tremaining: 54.2s\n",
      "153:\tlearn: 0.3482157\ttotal: 9.85s\tremaining: 54.1s\n",
      "154:\tlearn: 0.3476590\ttotal: 9.91s\tremaining: 54s\n",
      "155:\tlearn: 0.3474480\ttotal: 9.97s\tremaining: 53.9s\n",
      "156:\tlearn: 0.3472572\ttotal: 10s\tremaining: 53.9s\n",
      "157:\tlearn: 0.3471438\ttotal: 10.1s\tremaining: 53.9s\n",
      "158:\tlearn: 0.3469168\ttotal: 10.2s\tremaining: 53.8s\n",
      "159:\tlearn: 0.3465948\ttotal: 10.2s\tremaining: 53.7s\n",
      "160:\tlearn: 0.3463659\ttotal: 10.3s\tremaining: 53.6s\n",
      "161:\tlearn: 0.3462742\ttotal: 10.4s\tremaining: 53.6s\n",
      "162:\tlearn: 0.3462148\ttotal: 10.4s\tremaining: 53.5s\n",
      "163:\tlearn: 0.3460150\ttotal: 10.5s\tremaining: 53.4s\n",
      "164:\tlearn: 0.3457210\ttotal: 10.5s\tremaining: 53.3s\n",
      "165:\tlearn: 0.3452963\ttotal: 10.6s\tremaining: 53.2s\n",
      "166:\tlearn: 0.3450376\ttotal: 10.6s\tremaining: 53.1s\n",
      "167:\tlearn: 0.3446495\ttotal: 10.7s\tremaining: 53s\n",
      "168:\tlearn: 0.3442994\ttotal: 10.8s\tremaining: 52.9s\n",
      "169:\tlearn: 0.3439525\ttotal: 10.8s\tremaining: 52.8s\n",
      "170:\tlearn: 0.3439187\ttotal: 10.9s\tremaining: 52.7s\n",
      "171:\tlearn: 0.3437794\ttotal: 10.9s\tremaining: 52.6s\n",
      "172:\tlearn: 0.3436111\ttotal: 11s\tremaining: 52.5s\n",
      "173:\tlearn: 0.3433782\ttotal: 11s\tremaining: 52.4s\n",
      "174:\tlearn: 0.3431414\ttotal: 11.1s\tremaining: 52.4s\n",
      "175:\tlearn: 0.3428333\ttotal: 11.2s\tremaining: 52.3s\n",
      "176:\tlearn: 0.3426402\ttotal: 11.2s\tremaining: 52.2s\n",
      "177:\tlearn: 0.3422685\ttotal: 11.3s\tremaining: 52.1s\n",
      "178:\tlearn: 0.3421342\ttotal: 11.3s\tremaining: 52s\n",
      "179:\tlearn: 0.3420163\ttotal: 11.4s\tremaining: 51.9s\n",
      "180:\tlearn: 0.3417675\ttotal: 11.5s\tremaining: 51.8s\n",
      "181:\tlearn: 0.3415305\ttotal: 11.5s\tremaining: 51.8s\n",
      "182:\tlearn: 0.3413793\ttotal: 11.6s\tremaining: 51.6s\n",
      "183:\tlearn: 0.3410290\ttotal: 11.6s\tremaining: 51.6s\n",
      "184:\tlearn: 0.3409125\ttotal: 11.7s\tremaining: 51.5s\n",
      "185:\tlearn: 0.3407169\ttotal: 11.7s\tremaining: 51.4s\n",
      "186:\tlearn: 0.3401598\ttotal: 11.8s\tremaining: 51.3s\n",
      "187:\tlearn: 0.3399865\ttotal: 11.9s\tremaining: 51.2s\n",
      "188:\tlearn: 0.3397015\ttotal: 11.9s\tremaining: 51.1s\n",
      "189:\tlearn: 0.3396202\ttotal: 12s\tremaining: 51s\n",
      "190:\tlearn: 0.3395701\ttotal: 12s\tremaining: 51s\n",
      "191:\tlearn: 0.3392782\ttotal: 12.1s\tremaining: 50.9s\n",
      "192:\tlearn: 0.3390024\ttotal: 12.2s\tremaining: 50.9s\n",
      "193:\tlearn: 0.3387912\ttotal: 12.2s\tremaining: 50.8s\n",
      "194:\tlearn: 0.3385367\ttotal: 12.3s\tremaining: 50.7s\n",
      "195:\tlearn: 0.3383938\ttotal: 12.3s\tremaining: 50.7s\n",
      "196:\tlearn: 0.3382140\ttotal: 12.4s\tremaining: 50.6s\n",
      "197:\tlearn: 0.3380447\ttotal: 12.5s\tremaining: 50.5s\n",
      "198:\tlearn: 0.3376581\ttotal: 12.5s\tremaining: 50.4s\n",
      "199:\tlearn: 0.3374866\ttotal: 12.6s\tremaining: 50.3s\n",
      "200:\tlearn: 0.3372731\ttotal: 12.6s\tremaining: 50.2s\n",
      "201:\tlearn: 0.3370381\ttotal: 12.7s\tremaining: 50.2s\n",
      "202:\tlearn: 0.3368001\ttotal: 12.8s\tremaining: 50.1s\n",
      "203:\tlearn: 0.3367132\ttotal: 12.8s\tremaining: 50s\n",
      "204:\tlearn: 0.3364605\ttotal: 12.9s\tremaining: 50s\n",
      "205:\tlearn: 0.3363106\ttotal: 13s\tremaining: 49.9s\n",
      "206:\tlearn: 0.3362349\ttotal: 13s\tremaining: 49.8s\n",
      "207:\tlearn: 0.3361166\ttotal: 13.1s\tremaining: 49.8s\n",
      "208:\tlearn: 0.3358560\ttotal: 13.1s\tremaining: 49.7s\n",
      "209:\tlearn: 0.3357116\ttotal: 13.2s\tremaining: 49.6s\n",
      "210:\tlearn: 0.3356387\ttotal: 13.2s\tremaining: 49.5s\n",
      "211:\tlearn: 0.3354275\ttotal: 13.3s\tremaining: 49.5s\n",
      "212:\tlearn: 0.3352310\ttotal: 13.4s\tremaining: 49.4s\n",
      "213:\tlearn: 0.3350417\ttotal: 13.4s\tremaining: 49.3s\n",
      "214:\tlearn: 0.3348158\ttotal: 13.5s\tremaining: 49.2s\n",
      "215:\tlearn: 0.3346112\ttotal: 13.5s\tremaining: 49.2s\n",
      "216:\tlearn: 0.3344388\ttotal: 13.6s\tremaining: 49.1s\n",
      "217:\tlearn: 0.3343732\ttotal: 13.7s\tremaining: 49s\n",
      "218:\tlearn: 0.3341934\ttotal: 13.7s\tremaining: 48.9s\n",
      "219:\tlearn: 0.3340097\ttotal: 13.8s\tremaining: 48.9s\n",
      "220:\tlearn: 0.3338335\ttotal: 13.8s\tremaining: 48.8s\n",
      "221:\tlearn: 0.3336757\ttotal: 13.9s\tremaining: 48.7s\n",
      "222:\tlearn: 0.3335069\ttotal: 13.9s\tremaining: 48.6s\n",
      "223:\tlearn: 0.3332670\ttotal: 14s\tremaining: 48.5s\n",
      "224:\tlearn: 0.3331581\ttotal: 14.1s\tremaining: 48.4s\n",
      "225:\tlearn: 0.3330422\ttotal: 14.1s\tremaining: 48.4s\n",
      "226:\tlearn: 0.3329545\ttotal: 14.2s\tremaining: 48.3s\n",
      "227:\tlearn: 0.3327632\ttotal: 14.2s\tremaining: 48.2s\n",
      "228:\tlearn: 0.3325673\ttotal: 14.3s\tremaining: 48.2s\n",
      "229:\tlearn: 0.3323630\ttotal: 14.4s\tremaining: 48.1s\n",
      "230:\tlearn: 0.3323169\ttotal: 14.4s\tremaining: 48s\n",
      "231:\tlearn: 0.3322417\ttotal: 14.5s\tremaining: 47.9s\n",
      "232:\tlearn: 0.3320842\ttotal: 14.5s\tremaining: 47.9s\n",
      "233:\tlearn: 0.3320204\ttotal: 14.6s\tremaining: 47.8s\n",
      "234:\tlearn: 0.3317701\ttotal: 14.7s\tremaining: 47.7s\n",
      "235:\tlearn: 0.3316043\ttotal: 14.7s\tremaining: 47.6s\n",
      "236:\tlearn: 0.3313706\ttotal: 14.8s\tremaining: 47.6s\n",
      "237:\tlearn: 0.3311906\ttotal: 14.8s\tremaining: 47.5s\n",
      "238:\tlearn: 0.3310182\ttotal: 14.9s\tremaining: 47.4s\n",
      "239:\tlearn: 0.3308828\ttotal: 14.9s\tremaining: 47.3s\n",
      "240:\tlearn: 0.3307295\ttotal: 15s\tremaining: 47.2s\n",
      "241:\tlearn: 0.3306307\ttotal: 15.1s\tremaining: 47.2s\n",
      "242:\tlearn: 0.3305933\ttotal: 15.1s\tremaining: 47.1s\n",
      "243:\tlearn: 0.3305134\ttotal: 15.2s\tremaining: 47s\n",
      "244:\tlearn: 0.3303985\ttotal: 15.2s\tremaining: 46.9s\n",
      "245:\tlearn: 0.3303199\ttotal: 15.3s\tremaining: 46.9s\n",
      "246:\tlearn: 0.3302349\ttotal: 15.3s\tremaining: 46.8s\n",
      "247:\tlearn: 0.3301332\ttotal: 15.4s\tremaining: 46.7s\n",
      "248:\tlearn: 0.3299744\ttotal: 15.5s\tremaining: 46.7s\n",
      "249:\tlearn: 0.3298537\ttotal: 15.5s\tremaining: 46.6s\n",
      "250:\tlearn: 0.3295943\ttotal: 15.6s\tremaining: 46.6s\n",
      "251:\tlearn: 0.3294406\ttotal: 15.7s\tremaining: 46.5s\n",
      "252:\tlearn: 0.3293595\ttotal: 15.7s\tremaining: 46.4s\n",
      "253:\tlearn: 0.3292842\ttotal: 15.8s\tremaining: 46.4s\n",
      "254:\tlearn: 0.3289941\ttotal: 15.8s\tremaining: 46.3s\n",
      "255:\tlearn: 0.3289166\ttotal: 15.9s\tremaining: 46.2s\n",
      "256:\tlearn: 0.3287290\ttotal: 16s\tremaining: 46.1s\n",
      "257:\tlearn: 0.3285984\ttotal: 16s\tremaining: 46s\n",
      "258:\tlearn: 0.3284190\ttotal: 16.1s\tremaining: 46s\n",
      "259:\tlearn: 0.3283361\ttotal: 16.1s\tremaining: 45.9s\n",
      "260:\tlearn: 0.3282050\ttotal: 16.2s\tremaining: 45.8s\n",
      "261:\tlearn: 0.3280351\ttotal: 16.2s\tremaining: 45.7s\n",
      "262:\tlearn: 0.3278502\ttotal: 16.3s\tremaining: 45.7s\n",
      "263:\tlearn: 0.3277348\ttotal: 16.4s\tremaining: 45.6s\n",
      "264:\tlearn: 0.3276383\ttotal: 16.4s\tremaining: 45.5s\n",
      "265:\tlearn: 0.3275150\ttotal: 16.5s\tremaining: 45.4s\n",
      "266:\tlearn: 0.3274805\ttotal: 16.5s\tremaining: 45.4s\n",
      "267:\tlearn: 0.3273612\ttotal: 16.6s\tremaining: 45.3s\n",
      "268:\tlearn: 0.3271527\ttotal: 16.6s\tremaining: 45.2s\n",
      "269:\tlearn: 0.3270804\ttotal: 16.7s\tremaining: 45.2s\n",
      "270:\tlearn: 0.3269650\ttotal: 16.8s\tremaining: 45.1s\n",
      "271:\tlearn: 0.3267845\ttotal: 16.8s\tremaining: 45s\n",
      "272:\tlearn: 0.3267490\ttotal: 16.9s\tremaining: 45s\n",
      "273:\tlearn: 0.3266462\ttotal: 16.9s\tremaining: 44.9s\n",
      "274:\tlearn: 0.3265154\ttotal: 17s\tremaining: 44.8s\n",
      "275:\tlearn: 0.3264119\ttotal: 17.1s\tremaining: 44.7s\n",
      "276:\tlearn: 0.3262373\ttotal: 17.1s\tremaining: 44.7s\n",
      "277:\tlearn: 0.3260576\ttotal: 17.2s\tremaining: 44.6s\n",
      "278:\tlearn: 0.3258902\ttotal: 17.2s\tremaining: 44.5s\n",
      "279:\tlearn: 0.3257195\ttotal: 17.3s\tremaining: 44.5s\n",
      "280:\tlearn: 0.3255293\ttotal: 17.3s\tremaining: 44.4s\n",
      "281:\tlearn: 0.3254236\ttotal: 17.4s\tremaining: 44.3s\n",
      "282:\tlearn: 0.3253976\ttotal: 17.5s\tremaining: 44.2s\n",
      "283:\tlearn: 0.3253088\ttotal: 17.5s\tremaining: 44.2s\n",
      "284:\tlearn: 0.3251846\ttotal: 17.6s\tremaining: 44.1s\n",
      "285:\tlearn: 0.3249570\ttotal: 17.6s\tremaining: 44s\n",
      "286:\tlearn: 0.3245597\ttotal: 17.7s\tremaining: 44s\n",
      "287:\tlearn: 0.3244763\ttotal: 17.8s\tremaining: 43.9s\n",
      "288:\tlearn: 0.3244258\ttotal: 17.8s\tremaining: 43.8s\n",
      "289:\tlearn: 0.3242463\ttotal: 17.9s\tremaining: 43.8s\n",
      "290:\tlearn: 0.3241636\ttotal: 17.9s\tremaining: 43.7s\n",
      "291:\tlearn: 0.3239561\ttotal: 18s\tremaining: 43.6s\n",
      "292:\tlearn: 0.3238456\ttotal: 18.1s\tremaining: 43.6s\n",
      "293:\tlearn: 0.3236583\ttotal: 18.1s\tremaining: 43.5s\n",
      "294:\tlearn: 0.3235057\ttotal: 18.2s\tremaining: 43.5s\n",
      "295:\tlearn: 0.3233477\ttotal: 18.2s\tremaining: 43.4s\n",
      "296:\tlearn: 0.3231974\ttotal: 18.3s\tremaining: 43.3s\n",
      "297:\tlearn: 0.3231455\ttotal: 18.4s\tremaining: 43.3s\n",
      "298:\tlearn: 0.3230677\ttotal: 18.4s\tremaining: 43.2s\n",
      "299:\tlearn: 0.3229623\ttotal: 18.5s\tremaining: 43.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300:\tlearn: 0.3227087\ttotal: 18.5s\tremaining: 43s\n",
      "301:\tlearn: 0.3226231\ttotal: 18.6s\tremaining: 43s\n",
      "302:\tlearn: 0.3224588\ttotal: 18.7s\tremaining: 42.9s\n",
      "303:\tlearn: 0.3221291\ttotal: 18.7s\tremaining: 42.9s\n",
      "304:\tlearn: 0.3220876\ttotal: 18.8s\tremaining: 42.8s\n",
      "305:\tlearn: 0.3220567\ttotal: 18.8s\tremaining: 42.7s\n",
      "306:\tlearn: 0.3220140\ttotal: 18.9s\tremaining: 42.6s\n",
      "307:\tlearn: 0.3218906\ttotal: 18.9s\tremaining: 42.5s\n",
      "308:\tlearn: 0.3216571\ttotal: 19s\tremaining: 42.5s\n",
      "309:\tlearn: 0.3214431\ttotal: 19s\tremaining: 42.4s\n",
      "310:\tlearn: 0.3212921\ttotal: 19.1s\tremaining: 42.3s\n",
      "311:\tlearn: 0.3211574\ttotal: 19.2s\tremaining: 42.3s\n",
      "312:\tlearn: 0.3210193\ttotal: 19.2s\tremaining: 42.2s\n",
      "313:\tlearn: 0.3208479\ttotal: 19.3s\tremaining: 42.1s\n",
      "314:\tlearn: 0.3207815\ttotal: 19.3s\tremaining: 42s\n",
      "315:\tlearn: 0.3205829\ttotal: 19.4s\tremaining: 42s\n",
      "316:\tlearn: 0.3205239\ttotal: 19.4s\tremaining: 41.9s\n",
      "317:\tlearn: 0.3203518\ttotal: 19.5s\tremaining: 41.8s\n",
      "318:\tlearn: 0.3203219\ttotal: 19.6s\tremaining: 41.8s\n",
      "319:\tlearn: 0.3202362\ttotal: 19.6s\tremaining: 41.7s\n",
      "320:\tlearn: 0.3201380\ttotal: 19.7s\tremaining: 41.6s\n",
      "321:\tlearn: 0.3200183\ttotal: 19.7s\tremaining: 41.6s\n",
      "322:\tlearn: 0.3198655\ttotal: 19.8s\tremaining: 41.5s\n",
      "323:\tlearn: 0.3195613\ttotal: 19.9s\tremaining: 41.4s\n",
      "324:\tlearn: 0.3194308\ttotal: 19.9s\tremaining: 41.4s\n",
      "325:\tlearn: 0.3193237\ttotal: 20s\tremaining: 41.3s\n",
      "326:\tlearn: 0.3191984\ttotal: 20s\tremaining: 41.2s\n",
      "327:\tlearn: 0.3191232\ttotal: 20.1s\tremaining: 41.1s\n",
      "328:\tlearn: 0.3190611\ttotal: 20.1s\tremaining: 41.1s\n",
      "329:\tlearn: 0.3189841\ttotal: 20.2s\tremaining: 41s\n",
      "330:\tlearn: 0.3187402\ttotal: 20.3s\tremaining: 40.9s\n",
      "331:\tlearn: 0.3186436\ttotal: 20.3s\tremaining: 40.9s\n",
      "332:\tlearn: 0.3185059\ttotal: 20.4s\tremaining: 40.8s\n",
      "333:\tlearn: 0.3184631\ttotal: 20.4s\tremaining: 40.7s\n",
      "334:\tlearn: 0.3184091\ttotal: 20.5s\tremaining: 40.6s\n",
      "335:\tlearn: 0.3182394\ttotal: 20.5s\tremaining: 40.6s\n",
      "336:\tlearn: 0.3181118\ttotal: 20.6s\tremaining: 40.5s\n",
      "337:\tlearn: 0.3180263\ttotal: 20.6s\tremaining: 40.4s\n",
      "338:\tlearn: 0.3178459\ttotal: 20.7s\tremaining: 40.4s\n",
      "339:\tlearn: 0.3177806\ttotal: 20.8s\tremaining: 40.3s\n",
      "340:\tlearn: 0.3176832\ttotal: 20.8s\tremaining: 40.2s\n",
      "341:\tlearn: 0.3175973\ttotal: 20.9s\tremaining: 40.1s\n",
      "342:\tlearn: 0.3172611\ttotal: 20.9s\tremaining: 40.1s\n",
      "343:\tlearn: 0.3169525\ttotal: 21s\tremaining: 40s\n",
      "344:\tlearn: 0.3168566\ttotal: 21s\tremaining: 39.9s\n",
      "345:\tlearn: 0.3166694\ttotal: 21.1s\tremaining: 39.9s\n",
      "346:\tlearn: 0.3164786\ttotal: 21.2s\tremaining: 39.8s\n",
      "347:\tlearn: 0.3163165\ttotal: 21.2s\tremaining: 39.8s\n",
      "348:\tlearn: 0.3161140\ttotal: 21.3s\tremaining: 39.7s\n",
      "349:\tlearn: 0.3158536\ttotal: 21.3s\tremaining: 39.6s\n",
      "350:\tlearn: 0.3156624\ttotal: 21.4s\tremaining: 39.6s\n",
      "351:\tlearn: 0.3154896\ttotal: 21.4s\tremaining: 39.5s\n",
      "352:\tlearn: 0.3153168\ttotal: 21.5s\tremaining: 39.4s\n",
      "353:\tlearn: 0.3150859\ttotal: 21.6s\tremaining: 39.4s\n",
      "354:\tlearn: 0.3149280\ttotal: 21.6s\tremaining: 39.3s\n",
      "355:\tlearn: 0.3148600\ttotal: 21.7s\tremaining: 39.2s\n",
      "356:\tlearn: 0.3147275\ttotal: 21.7s\tremaining: 39.2s\n",
      "357:\tlearn: 0.3144744\ttotal: 21.8s\tremaining: 39.1s\n",
      "358:\tlearn: 0.3143988\ttotal: 21.9s\tremaining: 39s\n",
      "359:\tlearn: 0.3141071\ttotal: 21.9s\tremaining: 39s\n",
      "360:\tlearn: 0.3139968\ttotal: 22s\tremaining: 38.9s\n",
      "361:\tlearn: 0.3137240\ttotal: 22s\tremaining: 38.8s\n",
      "362:\tlearn: 0.3134449\ttotal: 22.1s\tremaining: 38.8s\n",
      "363:\tlearn: 0.3133489\ttotal: 22.1s\tremaining: 38.7s\n",
      "364:\tlearn: 0.3132723\ttotal: 22.2s\tremaining: 38.6s\n",
      "365:\tlearn: 0.3130919\ttotal: 22.3s\tremaining: 38.6s\n",
      "366:\tlearn: 0.3129067\ttotal: 22.3s\tremaining: 38.5s\n",
      "367:\tlearn: 0.3126838\ttotal: 22.4s\tremaining: 38.4s\n",
      "368:\tlearn: 0.3124468\ttotal: 22.4s\tremaining: 38.4s\n",
      "369:\tlearn: 0.3122806\ttotal: 22.5s\tremaining: 38.3s\n",
      "370:\tlearn: 0.3122302\ttotal: 22.6s\tremaining: 38.2s\n",
      "371:\tlearn: 0.3120798\ttotal: 22.6s\tremaining: 38.2s\n",
      "372:\tlearn: 0.3119605\ttotal: 22.7s\tremaining: 38.1s\n",
      "373:\tlearn: 0.3118365\ttotal: 22.7s\tremaining: 38s\n",
      "374:\tlearn: 0.3117359\ttotal: 22.8s\tremaining: 38s\n",
      "375:\tlearn: 0.3116416\ttotal: 22.8s\tremaining: 37.9s\n",
      "376:\tlearn: 0.3115390\ttotal: 22.9s\tremaining: 37.8s\n",
      "377:\tlearn: 0.3114271\ttotal: 23s\tremaining: 37.8s\n",
      "378:\tlearn: 0.3112281\ttotal: 23s\tremaining: 37.7s\n",
      "379:\tlearn: 0.3109814\ttotal: 23.1s\tremaining: 37.7s\n",
      "380:\tlearn: 0.3109338\ttotal: 23.1s\tremaining: 37.6s\n",
      "381:\tlearn: 0.3108059\ttotal: 23.2s\tremaining: 37.5s\n",
      "382:\tlearn: 0.3106825\ttotal: 23.3s\tremaining: 37.5s\n",
      "383:\tlearn: 0.3105219\ttotal: 23.3s\tremaining: 37.4s\n",
      "384:\tlearn: 0.3104627\ttotal: 23.4s\tremaining: 37.3s\n",
      "385:\tlearn: 0.3102322\ttotal: 23.4s\tremaining: 37.3s\n",
      "386:\tlearn: 0.3101225\ttotal: 23.5s\tremaining: 37.2s\n",
      "387:\tlearn: 0.3099129\ttotal: 23.5s\tremaining: 37.1s\n",
      "388:\tlearn: 0.3097360\ttotal: 23.6s\tremaining: 37.1s\n",
      "389:\tlearn: 0.3096619\ttotal: 23.7s\tremaining: 37s\n",
      "390:\tlearn: 0.3096395\ttotal: 23.7s\tremaining: 37s\n",
      "391:\tlearn: 0.3095552\ttotal: 23.8s\tremaining: 36.9s\n",
      "392:\tlearn: 0.3094356\ttotal: 23.9s\tremaining: 36.9s\n",
      "393:\tlearn: 0.3092820\ttotal: 23.9s\tremaining: 36.8s\n",
      "394:\tlearn: 0.3091443\ttotal: 24s\tremaining: 36.8s\n",
      "395:\tlearn: 0.3090750\ttotal: 24.1s\tremaining: 36.7s\n",
      "396:\tlearn: 0.3089605\ttotal: 24.1s\tremaining: 36.6s\n",
      "397:\tlearn: 0.3088341\ttotal: 24.2s\tremaining: 36.6s\n",
      "398:\tlearn: 0.3087921\ttotal: 24.3s\tremaining: 36.5s\n",
      "399:\tlearn: 0.3086020\ttotal: 24.3s\tremaining: 36.5s\n",
      "400:\tlearn: 0.3085018\ttotal: 24.4s\tremaining: 36.4s\n",
      "401:\tlearn: 0.3083185\ttotal: 24.5s\tremaining: 36.4s\n",
      "402:\tlearn: 0.3081682\ttotal: 24.5s\tremaining: 36.3s\n",
      "403:\tlearn: 0.3080191\ttotal: 24.6s\tremaining: 36.3s\n",
      "404:\tlearn: 0.3078781\ttotal: 24.7s\tremaining: 36.2s\n",
      "405:\tlearn: 0.3077569\ttotal: 24.7s\tremaining: 36.2s\n",
      "406:\tlearn: 0.3074680\ttotal: 24.8s\tremaining: 36.1s\n",
      "407:\tlearn: 0.3073385\ttotal: 24.9s\tremaining: 36.1s\n",
      "408:\tlearn: 0.3071881\ttotal: 24.9s\tremaining: 36s\n",
      "409:\tlearn: 0.3070723\ttotal: 25s\tremaining: 35.9s\n",
      "410:\tlearn: 0.3069328\ttotal: 25s\tremaining: 35.9s\n",
      "411:\tlearn: 0.3067572\ttotal: 25.1s\tremaining: 35.8s\n",
      "412:\tlearn: 0.3066297\ttotal: 25.2s\tremaining: 35.8s\n",
      "413:\tlearn: 0.3065277\ttotal: 25.2s\tremaining: 35.7s\n",
      "414:\tlearn: 0.3063656\ttotal: 25.3s\tremaining: 35.7s\n",
      "415:\tlearn: 0.3062384\ttotal: 25.4s\tremaining: 35.6s\n",
      "416:\tlearn: 0.3061682\ttotal: 25.4s\tremaining: 35.5s\n",
      "417:\tlearn: 0.3060851\ttotal: 25.5s\tremaining: 35.5s\n",
      "418:\tlearn: 0.3059412\ttotal: 25.5s\tremaining: 35.4s\n",
      "419:\tlearn: 0.3057986\ttotal: 25.6s\tremaining: 35.3s\n",
      "420:\tlearn: 0.3056917\ttotal: 25.6s\tremaining: 35.3s\n",
      "421:\tlearn: 0.3055262\ttotal: 25.7s\tremaining: 35.2s\n",
      "422:\tlearn: 0.3053862\ttotal: 25.8s\tremaining: 35.1s\n",
      "423:\tlearn: 0.3053142\ttotal: 25.8s\tremaining: 35.1s\n",
      "424:\tlearn: 0.3052279\ttotal: 25.9s\tremaining: 35s\n",
      "425:\tlearn: 0.3050994\ttotal: 25.9s\tremaining: 35s\n",
      "426:\tlearn: 0.3049500\ttotal: 26s\tremaining: 34.9s\n",
      "427:\tlearn: 0.3048515\ttotal: 26.1s\tremaining: 34.8s\n",
      "428:\tlearn: 0.3047026\ttotal: 26.1s\tremaining: 34.8s\n",
      "429:\tlearn: 0.3045452\ttotal: 26.2s\tremaining: 34.7s\n",
      "430:\tlearn: 0.3044854\ttotal: 26.3s\tremaining: 34.7s\n",
      "431:\tlearn: 0.3044126\ttotal: 26.4s\tremaining: 34.7s\n",
      "432:\tlearn: 0.3043440\ttotal: 26.5s\tremaining: 34.7s\n",
      "433:\tlearn: 0.3041872\ttotal: 26.6s\tremaining: 34.6s\n",
      "434:\tlearn: 0.3040350\ttotal: 26.6s\tremaining: 34.6s\n",
      "435:\tlearn: 0.3039414\ttotal: 26.7s\tremaining: 34.5s\n",
      "436:\tlearn: 0.3036712\ttotal: 26.8s\tremaining: 34.5s\n",
      "437:\tlearn: 0.3035862\ttotal: 26.9s\tremaining: 34.5s\n",
      "438:\tlearn: 0.3035042\ttotal: 27s\tremaining: 34.5s\n",
      "439:\tlearn: 0.3032906\ttotal: 27.1s\tremaining: 34.5s\n",
      "440:\tlearn: 0.3032066\ttotal: 27.2s\tremaining: 34.4s\n",
      "441:\tlearn: 0.3031240\ttotal: 27.2s\tremaining: 34.4s\n",
      "442:\tlearn: 0.3030771\ttotal: 27.3s\tremaining: 34.4s\n",
      "443:\tlearn: 0.3030127\ttotal: 27.4s\tremaining: 34.3s\n",
      "444:\tlearn: 0.3029499\ttotal: 27.5s\tremaining: 34.3s\n",
      "445:\tlearn: 0.3028751\ttotal: 27.6s\tremaining: 34.2s\n",
      "446:\tlearn: 0.3027271\ttotal: 27.7s\tremaining: 34.2s\n",
      "447:\tlearn: 0.3026543\ttotal: 27.7s\tremaining: 34.2s\n",
      "448:\tlearn: 0.3026058\ttotal: 27.8s\tremaining: 34.1s\n",
      "449:\tlearn: 0.3025381\ttotal: 27.9s\tremaining: 34.1s\n",
      "450:\tlearn: 0.3023193\ttotal: 27.9s\tremaining: 34s\n",
      "451:\tlearn: 0.3022023\ttotal: 28s\tremaining: 33.9s\n",
      "452:\tlearn: 0.3020736\ttotal: 28.1s\tremaining: 33.9s\n",
      "453:\tlearn: 0.3020287\ttotal: 28.2s\tremaining: 33.9s\n",
      "454:\tlearn: 0.3017400\ttotal: 28.3s\tremaining: 33.9s\n",
      "455:\tlearn: 0.3016276\ttotal: 28.4s\tremaining: 33.8s\n",
      "456:\tlearn: 0.3015720\ttotal: 28.4s\tremaining: 33.8s\n",
      "457:\tlearn: 0.3014308\ttotal: 28.5s\tremaining: 33.7s\n",
      "458:\tlearn: 0.3013220\ttotal: 28.6s\tremaining: 33.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459:\tlearn: 0.3012686\ttotal: 28.6s\tremaining: 33.6s\n",
      "460:\tlearn: 0.3011333\ttotal: 28.7s\tremaining: 33.5s\n",
      "461:\tlearn: 0.3010775\ttotal: 28.7s\tremaining: 33.5s\n",
      "462:\tlearn: 0.3009366\ttotal: 28.8s\tremaining: 33.4s\n",
      "463:\tlearn: 0.3008546\ttotal: 28.9s\tremaining: 33.3s\n",
      "464:\tlearn: 0.3007408\ttotal: 28.9s\tremaining: 33.3s\n",
      "465:\tlearn: 0.3006119\ttotal: 29s\tremaining: 33.2s\n",
      "466:\tlearn: 0.3005690\ttotal: 29.1s\tremaining: 33.2s\n",
      "467:\tlearn: 0.3004567\ttotal: 29.1s\tremaining: 33.1s\n",
      "468:\tlearn: 0.3003810\ttotal: 29.2s\tremaining: 33s\n",
      "469:\tlearn: 0.3002935\ttotal: 29.3s\tremaining: 33s\n",
      "470:\tlearn: 0.3002239\ttotal: 29.3s\tremaining: 32.9s\n",
      "471:\tlearn: 0.3001646\ttotal: 29.4s\tremaining: 32.9s\n",
      "472:\tlearn: 0.3000687\ttotal: 29.4s\tremaining: 32.8s\n",
      "473:\tlearn: 0.2999766\ttotal: 29.5s\tremaining: 32.7s\n",
      "474:\tlearn: 0.2999206\ttotal: 29.6s\tremaining: 32.7s\n",
      "475:\tlearn: 0.2998358\ttotal: 29.6s\tremaining: 32.6s\n",
      "476:\tlearn: 0.2997554\ttotal: 29.7s\tremaining: 32.5s\n",
      "477:\tlearn: 0.2997197\ttotal: 29.7s\tremaining: 32.5s\n",
      "478:\tlearn: 0.2996201\ttotal: 29.8s\tremaining: 32.4s\n",
      "479:\tlearn: 0.2995792\ttotal: 29.9s\tremaining: 32.3s\n",
      "480:\tlearn: 0.2994363\ttotal: 29.9s\tremaining: 32.3s\n",
      "481:\tlearn: 0.2993217\ttotal: 30s\tremaining: 32.2s\n",
      "482:\tlearn: 0.2992354\ttotal: 30s\tremaining: 32.1s\n",
      "483:\tlearn: 0.2991857\ttotal: 30.1s\tremaining: 32.1s\n",
      "484:\tlearn: 0.2989976\ttotal: 30.1s\tremaining: 32s\n",
      "485:\tlearn: 0.2989265\ttotal: 30.2s\tremaining: 32s\n",
      "486:\tlearn: 0.2988468\ttotal: 30.3s\tremaining: 31.9s\n",
      "487:\tlearn: 0.2987287\ttotal: 30.3s\tremaining: 31.8s\n",
      "488:\tlearn: 0.2986431\ttotal: 30.4s\tremaining: 31.7s\n",
      "489:\tlearn: 0.2985084\ttotal: 30.4s\tremaining: 31.7s\n",
      "490:\tlearn: 0.2984389\ttotal: 30.5s\tremaining: 31.6s\n",
      "491:\tlearn: 0.2983060\ttotal: 30.6s\tremaining: 31.6s\n",
      "492:\tlearn: 0.2981823\ttotal: 30.7s\tremaining: 31.5s\n",
      "493:\tlearn: 0.2981308\ttotal: 30.7s\tremaining: 31.5s\n",
      "494:\tlearn: 0.2980134\ttotal: 30.8s\tremaining: 31.4s\n",
      "495:\tlearn: 0.2979724\ttotal: 30.8s\tremaining: 31.3s\n",
      "496:\tlearn: 0.2978941\ttotal: 30.9s\tremaining: 31.3s\n",
      "497:\tlearn: 0.2978283\ttotal: 31s\tremaining: 31.2s\n",
      "498:\tlearn: 0.2977073\ttotal: 31s\tremaining: 31.2s\n",
      "499:\tlearn: 0.2976658\ttotal: 31.1s\tremaining: 31.1s\n",
      "500:\tlearn: 0.2976209\ttotal: 31.2s\tremaining: 31s\n",
      "501:\tlearn: 0.2975385\ttotal: 31.2s\tremaining: 31s\n",
      "502:\tlearn: 0.2974925\ttotal: 31.3s\tremaining: 30.9s\n",
      "503:\tlearn: 0.2974501\ttotal: 31.4s\tremaining: 30.9s\n",
      "504:\tlearn: 0.2973922\ttotal: 31.4s\tremaining: 30.8s\n",
      "505:\tlearn: 0.2973359\ttotal: 31.5s\tremaining: 30.7s\n",
      "506:\tlearn: 0.2972947\ttotal: 31.6s\tremaining: 30.7s\n",
      "507:\tlearn: 0.2972239\ttotal: 31.6s\tremaining: 30.6s\n",
      "508:\tlearn: 0.2971763\ttotal: 31.7s\tremaining: 30.6s\n",
      "509:\tlearn: 0.2970984\ttotal: 31.7s\tremaining: 30.5s\n",
      "510:\tlearn: 0.2970442\ttotal: 31.8s\tremaining: 30.4s\n",
      "511:\tlearn: 0.2969158\ttotal: 31.9s\tremaining: 30.4s\n",
      "512:\tlearn: 0.2968015\ttotal: 31.9s\tremaining: 30.3s\n",
      "513:\tlearn: 0.2966752\ttotal: 32s\tremaining: 30.3s\n",
      "514:\tlearn: 0.2966195\ttotal: 32.1s\tremaining: 30.2s\n",
      "515:\tlearn: 0.2965573\ttotal: 32.1s\tremaining: 30.1s\n",
      "516:\tlearn: 0.2964850\ttotal: 32.2s\tremaining: 30.1s\n",
      "517:\tlearn: 0.2964108\ttotal: 32.3s\tremaining: 30s\n",
      "518:\tlearn: 0.2963709\ttotal: 32.3s\tremaining: 30s\n",
      "519:\tlearn: 0.2963194\ttotal: 32.4s\tremaining: 29.9s\n",
      "520:\tlearn: 0.2962500\ttotal: 32.4s\tremaining: 29.8s\n",
      "521:\tlearn: 0.2961794\ttotal: 32.5s\tremaining: 29.8s\n",
      "522:\tlearn: 0.2961565\ttotal: 32.6s\tremaining: 29.7s\n",
      "523:\tlearn: 0.2960741\ttotal: 32.6s\tremaining: 29.6s\n",
      "524:\tlearn: 0.2959792\ttotal: 32.7s\tremaining: 29.6s\n",
      "525:\tlearn: 0.2958856\ttotal: 32.8s\tremaining: 29.5s\n",
      "526:\tlearn: 0.2957881\ttotal: 32.8s\tremaining: 29.5s\n",
      "527:\tlearn: 0.2957620\ttotal: 32.9s\tremaining: 29.4s\n",
      "528:\tlearn: 0.2956736\ttotal: 33s\tremaining: 29.4s\n",
      "529:\tlearn: 0.2956353\ttotal: 33s\tremaining: 29.3s\n",
      "530:\tlearn: 0.2956038\ttotal: 33.1s\tremaining: 29.2s\n",
      "531:\tlearn: 0.2955421\ttotal: 33.2s\tremaining: 29.2s\n",
      "532:\tlearn: 0.2954211\ttotal: 33.3s\tremaining: 29.1s\n",
      "533:\tlearn: 0.2953751\ttotal: 33.3s\tremaining: 29.1s\n",
      "534:\tlearn: 0.2953404\ttotal: 33.4s\tremaining: 29s\n",
      "535:\tlearn: 0.2951850\ttotal: 33.5s\tremaining: 29s\n",
      "536:\tlearn: 0.2951326\ttotal: 33.5s\tremaining: 28.9s\n",
      "537:\tlearn: 0.2950335\ttotal: 33.6s\tremaining: 28.9s\n",
      "538:\tlearn: 0.2950117\ttotal: 33.7s\tremaining: 28.8s\n",
      "539:\tlearn: 0.2949834\ttotal: 33.7s\tremaining: 28.7s\n",
      "540:\tlearn: 0.2949435\ttotal: 33.8s\tremaining: 28.7s\n",
      "541:\tlearn: 0.2948798\ttotal: 33.8s\tremaining: 28.6s\n",
      "542:\tlearn: 0.2948506\ttotal: 33.9s\tremaining: 28.5s\n",
      "543:\tlearn: 0.2947628\ttotal: 34s\tremaining: 28.5s\n",
      "544:\tlearn: 0.2946736\ttotal: 34s\tremaining: 28.4s\n",
      "545:\tlearn: 0.2946273\ttotal: 34.1s\tremaining: 28.4s\n",
      "546:\tlearn: 0.2946079\ttotal: 34.2s\tremaining: 28.3s\n",
      "547:\tlearn: 0.2945093\ttotal: 34.2s\tremaining: 28.2s\n",
      "548:\tlearn: 0.2944427\ttotal: 34.3s\tremaining: 28.2s\n",
      "549:\tlearn: 0.2944140\ttotal: 34.4s\tremaining: 28.1s\n",
      "550:\tlearn: 0.2943646\ttotal: 34.4s\tremaining: 28.1s\n",
      "551:\tlearn: 0.2943269\ttotal: 34.5s\tremaining: 28s\n",
      "552:\tlearn: 0.2942984\ttotal: 34.6s\tremaining: 27.9s\n",
      "553:\tlearn: 0.2941507\ttotal: 34.6s\tremaining: 27.9s\n",
      "554:\tlearn: 0.2940629\ttotal: 34.7s\tremaining: 27.8s\n",
      "555:\tlearn: 0.2940089\ttotal: 34.8s\tremaining: 27.8s\n",
      "556:\tlearn: 0.2939657\ttotal: 34.8s\tremaining: 27.7s\n",
      "557:\tlearn: 0.2939410\ttotal: 34.9s\tremaining: 27.6s\n",
      "558:\tlearn: 0.2938726\ttotal: 35s\tremaining: 27.6s\n",
      "559:\tlearn: 0.2938469\ttotal: 35s\tremaining: 27.5s\n",
      "560:\tlearn: 0.2937892\ttotal: 35.1s\tremaining: 27.5s\n",
      "561:\tlearn: 0.2937407\ttotal: 35.2s\tremaining: 27.4s\n",
      "562:\tlearn: 0.2936394\ttotal: 35.2s\tremaining: 27.3s\n",
      "563:\tlearn: 0.2935988\ttotal: 35.3s\tremaining: 27.3s\n",
      "564:\tlearn: 0.2935432\ttotal: 35.4s\tremaining: 27.2s\n",
      "565:\tlearn: 0.2934497\ttotal: 35.4s\tremaining: 27.2s\n",
      "566:\tlearn: 0.2933698\ttotal: 35.5s\tremaining: 27.1s\n",
      "567:\tlearn: 0.2933272\ttotal: 35.6s\tremaining: 27.1s\n",
      "568:\tlearn: 0.2932186\ttotal: 35.6s\tremaining: 27s\n",
      "569:\tlearn: 0.2930300\ttotal: 35.7s\tremaining: 26.9s\n",
      "570:\tlearn: 0.2929860\ttotal: 35.8s\tremaining: 26.9s\n",
      "571:\tlearn: 0.2929558\ttotal: 35.8s\tremaining: 26.8s\n",
      "572:\tlearn: 0.2928901\ttotal: 35.9s\tremaining: 26.8s\n",
      "573:\tlearn: 0.2928273\ttotal: 36s\tremaining: 26.7s\n",
      "574:\tlearn: 0.2927400\ttotal: 36.1s\tremaining: 26.7s\n",
      "575:\tlearn: 0.2927009\ttotal: 36.1s\tremaining: 26.6s\n",
      "576:\tlearn: 0.2926764\ttotal: 36.2s\tremaining: 26.6s\n",
      "577:\tlearn: 0.2925888\ttotal: 36.3s\tremaining: 26.5s\n",
      "578:\tlearn: 0.2925509\ttotal: 36.4s\tremaining: 26.4s\n",
      "579:\tlearn: 0.2924931\ttotal: 36.4s\tremaining: 26.4s\n",
      "580:\tlearn: 0.2924138\ttotal: 36.5s\tremaining: 26.3s\n",
      "581:\tlearn: 0.2923340\ttotal: 36.6s\tremaining: 26.3s\n",
      "582:\tlearn: 0.2922525\ttotal: 36.6s\tremaining: 26.2s\n",
      "583:\tlearn: 0.2922115\ttotal: 36.7s\tremaining: 26.1s\n",
      "584:\tlearn: 0.2921881\ttotal: 36.8s\tremaining: 26.1s\n",
      "585:\tlearn: 0.2920981\ttotal: 36.8s\tremaining: 26s\n",
      "586:\tlearn: 0.2920921\ttotal: 36.9s\tremaining: 26s\n",
      "587:\tlearn: 0.2920878\ttotal: 37s\tremaining: 25.9s\n",
      "588:\tlearn: 0.2920541\ttotal: 37s\tremaining: 25.9s\n",
      "589:\tlearn: 0.2919878\ttotal: 37.1s\tremaining: 25.8s\n",
      "590:\tlearn: 0.2919428\ttotal: 37.2s\tremaining: 25.7s\n",
      "591:\tlearn: 0.2918609\ttotal: 37.3s\tremaining: 25.7s\n",
      "592:\tlearn: 0.2917802\ttotal: 37.3s\tremaining: 25.6s\n",
      "593:\tlearn: 0.2917510\ttotal: 37.4s\tremaining: 25.6s\n",
      "594:\tlearn: 0.2916546\ttotal: 37.5s\tremaining: 25.5s\n",
      "595:\tlearn: 0.2915349\ttotal: 37.6s\tremaining: 25.5s\n",
      "596:\tlearn: 0.2914753\ttotal: 37.7s\tremaining: 25.4s\n",
      "597:\tlearn: 0.2914513\ttotal: 37.7s\tremaining: 25.4s\n",
      "598:\tlearn: 0.2913836\ttotal: 37.8s\tremaining: 25.3s\n",
      "599:\tlearn: 0.2913364\ttotal: 37.9s\tremaining: 25.3s\n",
      "600:\tlearn: 0.2912929\ttotal: 37.9s\tremaining: 25.2s\n",
      "601:\tlearn: 0.2912332\ttotal: 38s\tremaining: 25.1s\n",
      "602:\tlearn: 0.2911725\ttotal: 38.1s\tremaining: 25.1s\n",
      "603:\tlearn: 0.2911342\ttotal: 38.1s\tremaining: 25s\n",
      "604:\tlearn: 0.2910054\ttotal: 38.2s\tremaining: 24.9s\n",
      "605:\tlearn: 0.2909756\ttotal: 38.2s\tremaining: 24.9s\n",
      "606:\tlearn: 0.2909607\ttotal: 38.3s\tremaining: 24.8s\n",
      "607:\tlearn: 0.2909351\ttotal: 38.4s\tremaining: 24.7s\n",
      "608:\tlearn: 0.2908810\ttotal: 38.4s\tremaining: 24.7s\n",
      "609:\tlearn: 0.2908511\ttotal: 38.5s\tremaining: 24.6s\n",
      "610:\tlearn: 0.2908050\ttotal: 38.6s\tremaining: 24.5s\n",
      "611:\tlearn: 0.2907360\ttotal: 38.6s\tremaining: 24.5s\n",
      "612:\tlearn: 0.2907029\ttotal: 38.7s\tremaining: 24.4s\n",
      "613:\tlearn: 0.2905886\ttotal: 38.8s\tremaining: 24.4s\n",
      "614:\tlearn: 0.2905460\ttotal: 38.8s\tremaining: 24.3s\n",
      "615:\tlearn: 0.2904195\ttotal: 38.9s\tremaining: 24.3s\n",
      "616:\tlearn: 0.2903910\ttotal: 39s\tremaining: 24.2s\n",
      "617:\tlearn: 0.2903631\ttotal: 39s\tremaining: 24.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "618:\tlearn: 0.2903434\ttotal: 39.1s\tremaining: 24.1s\n",
      "619:\tlearn: 0.2903233\ttotal: 39.2s\tremaining: 24s\n",
      "620:\tlearn: 0.2903010\ttotal: 39.2s\tremaining: 23.9s\n",
      "621:\tlearn: 0.2902733\ttotal: 39.3s\tremaining: 23.9s\n",
      "622:\tlearn: 0.2902269\ttotal: 39.3s\tremaining: 23.8s\n",
      "623:\tlearn: 0.2902147\ttotal: 39.4s\tremaining: 23.7s\n",
      "624:\tlearn: 0.2901901\ttotal: 39.4s\tremaining: 23.7s\n",
      "625:\tlearn: 0.2901101\ttotal: 39.5s\tremaining: 23.6s\n",
      "626:\tlearn: 0.2900368\ttotal: 39.6s\tremaining: 23.5s\n",
      "627:\tlearn: 0.2899893\ttotal: 39.6s\tremaining: 23.5s\n",
      "628:\tlearn: 0.2899563\ttotal: 39.7s\tremaining: 23.4s\n",
      "629:\tlearn: 0.2899151\ttotal: 39.8s\tremaining: 23.4s\n",
      "630:\tlearn: 0.2898911\ttotal: 39.8s\tremaining: 23.3s\n",
      "631:\tlearn: 0.2898112\ttotal: 39.9s\tremaining: 23.2s\n",
      "632:\tlearn: 0.2897792\ttotal: 40s\tremaining: 23.2s\n",
      "633:\tlearn: 0.2897444\ttotal: 40s\tremaining: 23.1s\n",
      "634:\tlearn: 0.2897254\ttotal: 40.1s\tremaining: 23s\n",
      "635:\tlearn: 0.2897060\ttotal: 40.1s\tremaining: 23s\n",
      "636:\tlearn: 0.2895935\ttotal: 40.2s\tremaining: 22.9s\n",
      "637:\tlearn: 0.2894498\ttotal: 40.3s\tremaining: 22.8s\n",
      "638:\tlearn: 0.2894369\ttotal: 40.3s\tremaining: 22.8s\n",
      "639:\tlearn: 0.2894154\ttotal: 40.4s\tremaining: 22.7s\n",
      "640:\tlearn: 0.2893778\ttotal: 40.4s\tremaining: 22.6s\n",
      "641:\tlearn: 0.2893579\ttotal: 40.5s\tremaining: 22.6s\n",
      "642:\tlearn: 0.2893416\ttotal: 40.6s\tremaining: 22.5s\n",
      "643:\tlearn: 0.2893302\ttotal: 40.6s\tremaining: 22.5s\n",
      "644:\tlearn: 0.2892956\ttotal: 40.7s\tremaining: 22.4s\n",
      "645:\tlearn: 0.2892416\ttotal: 40.8s\tremaining: 22.3s\n",
      "646:\tlearn: 0.2892149\ttotal: 40.8s\tremaining: 22.3s\n",
      "647:\tlearn: 0.2891967\ttotal: 40.9s\tremaining: 22.2s\n",
      "648:\tlearn: 0.2891671\ttotal: 40.9s\tremaining: 22.1s\n",
      "649:\tlearn: 0.2890967\ttotal: 41s\tremaining: 22.1s\n",
      "650:\tlearn: 0.2890854\ttotal: 41.1s\tremaining: 22s\n",
      "651:\tlearn: 0.2890614\ttotal: 41.1s\tremaining: 22s\n",
      "652:\tlearn: 0.2889847\ttotal: 41.2s\tremaining: 21.9s\n",
      "653:\tlearn: 0.2889650\ttotal: 41.3s\tremaining: 21.8s\n",
      "654:\tlearn: 0.2888625\ttotal: 41.3s\tremaining: 21.8s\n",
      "655:\tlearn: 0.2887880\ttotal: 41.4s\tremaining: 21.7s\n",
      "656:\tlearn: 0.2887677\ttotal: 41.5s\tremaining: 21.6s\n",
      "657:\tlearn: 0.2887509\ttotal: 41.5s\tremaining: 21.6s\n",
      "658:\tlearn: 0.2887148\ttotal: 41.6s\tremaining: 21.5s\n",
      "659:\tlearn: 0.2886590\ttotal: 41.6s\tremaining: 21.5s\n",
      "660:\tlearn: 0.2886329\ttotal: 41.7s\tremaining: 21.4s\n",
      "661:\tlearn: 0.2885522\ttotal: 41.8s\tremaining: 21.3s\n",
      "662:\tlearn: 0.2885166\ttotal: 41.9s\tremaining: 21.3s\n",
      "663:\tlearn: 0.2884989\ttotal: 41.9s\tremaining: 21.2s\n",
      "664:\tlearn: 0.2884758\ttotal: 42s\tremaining: 21.2s\n",
      "665:\tlearn: 0.2884489\ttotal: 42.1s\tremaining: 21.1s\n",
      "666:\tlearn: 0.2884190\ttotal: 42.1s\tremaining: 21s\n",
      "667:\tlearn: 0.2883541\ttotal: 42.2s\tremaining: 21s\n",
      "668:\tlearn: 0.2883385\ttotal: 42.3s\tremaining: 20.9s\n",
      "669:\tlearn: 0.2882990\ttotal: 42.3s\tremaining: 20.8s\n",
      "670:\tlearn: 0.2882903\ttotal: 42.4s\tremaining: 20.8s\n",
      "671:\tlearn: 0.2882344\ttotal: 42.4s\tremaining: 20.7s\n",
      "672:\tlearn: 0.2882147\ttotal: 42.5s\tremaining: 20.7s\n",
      "673:\tlearn: 0.2881915\ttotal: 42.6s\tremaining: 20.6s\n",
      "674:\tlearn: 0.2881008\ttotal: 42.6s\tremaining: 20.5s\n",
      "675:\tlearn: 0.2880832\ttotal: 42.7s\tremaining: 20.5s\n",
      "676:\tlearn: 0.2880241\ttotal: 42.7s\tremaining: 20.4s\n",
      "677:\tlearn: 0.2880058\ttotal: 42.8s\tremaining: 20.3s\n",
      "678:\tlearn: 0.2879318\ttotal: 42.9s\tremaining: 20.3s\n",
      "679:\tlearn: 0.2879171\ttotal: 42.9s\tremaining: 20.2s\n",
      "680:\tlearn: 0.2878947\ttotal: 43s\tremaining: 20.1s\n",
      "681:\tlearn: 0.2878768\ttotal: 43s\tremaining: 20.1s\n",
      "682:\tlearn: 0.2878243\ttotal: 43.1s\tremaining: 20s\n",
      "683:\tlearn: 0.2878056\ttotal: 43.2s\tremaining: 20s\n",
      "684:\tlearn: 0.2877496\ttotal: 43.3s\tremaining: 19.9s\n",
      "685:\tlearn: 0.2877357\ttotal: 43.3s\tremaining: 19.8s\n",
      "686:\tlearn: 0.2877185\ttotal: 43.4s\tremaining: 19.8s\n",
      "687:\tlearn: 0.2876457\ttotal: 43.5s\tremaining: 19.7s\n",
      "688:\tlearn: 0.2876112\ttotal: 43.5s\tremaining: 19.6s\n",
      "689:\tlearn: 0.2876013\ttotal: 43.6s\tremaining: 19.6s\n",
      "690:\tlearn: 0.2875077\ttotal: 43.6s\tremaining: 19.5s\n",
      "691:\tlearn: 0.2874980\ttotal: 43.7s\tremaining: 19.5s\n",
      "692:\tlearn: 0.2874396\ttotal: 43.8s\tremaining: 19.4s\n",
      "693:\tlearn: 0.2874125\ttotal: 43.8s\tremaining: 19.3s\n",
      "694:\tlearn: 0.2874079\ttotal: 43.9s\tremaining: 19.3s\n",
      "695:\tlearn: 0.2873535\ttotal: 43.9s\tremaining: 19.2s\n",
      "696:\tlearn: 0.2873402\ttotal: 44s\tremaining: 19.1s\n",
      "697:\tlearn: 0.2872812\ttotal: 44.1s\tremaining: 19.1s\n",
      "698:\tlearn: 0.2872713\ttotal: 44.1s\tremaining: 19s\n",
      "699:\tlearn: 0.2871832\ttotal: 44.2s\tremaining: 18.9s\n",
      "700:\tlearn: 0.2871676\ttotal: 44.2s\tremaining: 18.9s\n",
      "701:\tlearn: 0.2870776\ttotal: 44.3s\tremaining: 18.8s\n",
      "702:\tlearn: 0.2870584\ttotal: 44.4s\tremaining: 18.7s\n",
      "703:\tlearn: 0.2870059\ttotal: 44.4s\tremaining: 18.7s\n",
      "704:\tlearn: 0.2869790\ttotal: 44.5s\tremaining: 18.6s\n",
      "705:\tlearn: 0.2869659\ttotal: 44.6s\tremaining: 18.6s\n",
      "706:\tlearn: 0.2869384\ttotal: 44.6s\tremaining: 18.5s\n",
      "707:\tlearn: 0.2868806\ttotal: 44.7s\tremaining: 18.4s\n",
      "708:\tlearn: 0.2868525\ttotal: 44.7s\tremaining: 18.4s\n",
      "709:\tlearn: 0.2868421\ttotal: 44.8s\tremaining: 18.3s\n",
      "710:\tlearn: 0.2868196\ttotal: 44.9s\tremaining: 18.2s\n",
      "711:\tlearn: 0.2867404\ttotal: 44.9s\tremaining: 18.2s\n",
      "712:\tlearn: 0.2866925\ttotal: 45s\tremaining: 18.1s\n",
      "713:\tlearn: 0.2866751\ttotal: 45s\tremaining: 18s\n",
      "714:\tlearn: 0.2866669\ttotal: 45.1s\tremaining: 18s\n",
      "715:\tlearn: 0.2866566\ttotal: 45.1s\tremaining: 17.9s\n",
      "716:\tlearn: 0.2866529\ttotal: 45.2s\tremaining: 17.8s\n",
      "717:\tlearn: 0.2866345\ttotal: 45.3s\tremaining: 17.8s\n",
      "718:\tlearn: 0.2866088\ttotal: 45.3s\tremaining: 17.7s\n",
      "719:\tlearn: 0.2865944\ttotal: 45.4s\tremaining: 17.6s\n",
      "720:\tlearn: 0.2865576\ttotal: 45.4s\tremaining: 17.6s\n",
      "721:\tlearn: 0.2865420\ttotal: 45.5s\tremaining: 17.5s\n",
      "722:\tlearn: 0.2864587\ttotal: 45.6s\tremaining: 17.5s\n",
      "723:\tlearn: 0.2864454\ttotal: 45.6s\tremaining: 17.4s\n",
      "724:\tlearn: 0.2864118\ttotal: 45.7s\tremaining: 17.3s\n",
      "725:\tlearn: 0.2863131\ttotal: 45.8s\tremaining: 17.3s\n",
      "726:\tlearn: 0.2862838\ttotal: 45.8s\tremaining: 17.2s\n",
      "727:\tlearn: 0.2862423\ttotal: 45.9s\tremaining: 17.1s\n",
      "728:\tlearn: 0.2862392\ttotal: 45.9s\tremaining: 17.1s\n",
      "729:\tlearn: 0.2862267\ttotal: 46s\tremaining: 17s\n",
      "730:\tlearn: 0.2862181\ttotal: 46s\tremaining: 16.9s\n",
      "731:\tlearn: 0.2861660\ttotal: 46.1s\tremaining: 16.9s\n",
      "732:\tlearn: 0.2861561\ttotal: 46.2s\tremaining: 16.8s\n",
      "733:\tlearn: 0.2861504\ttotal: 46.2s\tremaining: 16.8s\n",
      "734:\tlearn: 0.2861310\ttotal: 46.3s\tremaining: 16.7s\n",
      "735:\tlearn: 0.2861102\ttotal: 46.3s\tremaining: 16.6s\n",
      "736:\tlearn: 0.2860136\ttotal: 46.4s\tremaining: 16.6s\n",
      "737:\tlearn: 0.2859983\ttotal: 46.5s\tremaining: 16.5s\n",
      "738:\tlearn: 0.2859418\ttotal: 46.5s\tremaining: 16.4s\n",
      "739:\tlearn: 0.2858319\ttotal: 46.6s\tremaining: 16.4s\n",
      "740:\tlearn: 0.2858117\ttotal: 46.7s\tremaining: 16.3s\n",
      "741:\tlearn: 0.2858002\ttotal: 46.7s\tremaining: 16.2s\n",
      "742:\tlearn: 0.2857886\ttotal: 46.8s\tremaining: 16.2s\n",
      "743:\tlearn: 0.2857310\ttotal: 46.8s\tremaining: 16.1s\n",
      "744:\tlearn: 0.2857251\ttotal: 46.9s\tremaining: 16.1s\n",
      "745:\tlearn: 0.2856785\ttotal: 47s\tremaining: 16s\n",
      "746:\tlearn: 0.2856552\ttotal: 47s\tremaining: 15.9s\n",
      "747:\tlearn: 0.2855742\ttotal: 47.1s\tremaining: 15.9s\n",
      "748:\tlearn: 0.2855599\ttotal: 47.1s\tremaining: 15.8s\n",
      "749:\tlearn: 0.2855471\ttotal: 47.2s\tremaining: 15.7s\n",
      "750:\tlearn: 0.2854935\ttotal: 47.3s\tremaining: 15.7s\n",
      "751:\tlearn: 0.2853980\ttotal: 47.3s\tremaining: 15.6s\n",
      "752:\tlearn: 0.2853911\ttotal: 47.4s\tremaining: 15.5s\n",
      "753:\tlearn: 0.2853641\ttotal: 47.4s\tremaining: 15.5s\n",
      "754:\tlearn: 0.2853479\ttotal: 47.5s\tremaining: 15.4s\n",
      "755:\tlearn: 0.2852833\ttotal: 47.5s\tremaining: 15.3s\n",
      "756:\tlearn: 0.2852667\ttotal: 47.6s\tremaining: 15.3s\n",
      "757:\tlearn: 0.2851926\ttotal: 47.7s\tremaining: 15.2s\n",
      "758:\tlearn: 0.2851704\ttotal: 47.7s\tremaining: 15.2s\n",
      "759:\tlearn: 0.2851603\ttotal: 47.8s\tremaining: 15.1s\n",
      "760:\tlearn: 0.2850951\ttotal: 47.9s\tremaining: 15s\n",
      "761:\tlearn: 0.2850847\ttotal: 47.9s\tremaining: 15s\n",
      "762:\tlearn: 0.2849902\ttotal: 48s\tremaining: 14.9s\n",
      "763:\tlearn: 0.2849830\ttotal: 48s\tremaining: 14.8s\n",
      "764:\tlearn: 0.2849326\ttotal: 48.1s\tremaining: 14.8s\n",
      "765:\tlearn: 0.2849081\ttotal: 48.2s\tremaining: 14.7s\n",
      "766:\tlearn: 0.2848725\ttotal: 48.2s\tremaining: 14.7s\n",
      "767:\tlearn: 0.2848639\ttotal: 48.3s\tremaining: 14.6s\n",
      "768:\tlearn: 0.2847825\ttotal: 48.4s\tremaining: 14.5s\n",
      "769:\tlearn: 0.2847790\ttotal: 48.4s\tremaining: 14.5s\n",
      "770:\tlearn: 0.2847662\ttotal: 48.5s\tremaining: 14.4s\n",
      "771:\tlearn: 0.2847153\ttotal: 48.5s\tremaining: 14.3s\n",
      "772:\tlearn: 0.2846972\ttotal: 48.6s\tremaining: 14.3s\n",
      "773:\tlearn: 0.2846809\ttotal: 48.7s\tremaining: 14.2s\n",
      "774:\tlearn: 0.2846706\ttotal: 48.7s\tremaining: 14.1s\n",
      "775:\tlearn: 0.2845788\ttotal: 48.8s\tremaining: 14.1s\n",
      "776:\tlearn: 0.2845752\ttotal: 48.8s\tremaining: 14s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "777:\tlearn: 0.2845560\ttotal: 48.9s\tremaining: 14s\n",
      "778:\tlearn: 0.2845412\ttotal: 49s\tremaining: 13.9s\n",
      "779:\tlearn: 0.2845310\ttotal: 49s\tremaining: 13.8s\n",
      "780:\tlearn: 0.2844947\ttotal: 49.1s\tremaining: 13.8s\n",
      "781:\tlearn: 0.2844488\ttotal: 49.1s\tremaining: 13.7s\n",
      "782:\tlearn: 0.2844300\ttotal: 49.2s\tremaining: 13.6s\n",
      "783:\tlearn: 0.2844216\ttotal: 49.3s\tremaining: 13.6s\n",
      "784:\tlearn: 0.2843870\ttotal: 49.3s\tremaining: 13.5s\n",
      "785:\tlearn: 0.2843788\ttotal: 49.4s\tremaining: 13.4s\n",
      "786:\tlearn: 0.2843653\ttotal: 49.4s\tremaining: 13.4s\n",
      "787:\tlearn: 0.2843538\ttotal: 49.5s\tremaining: 13.3s\n",
      "788:\tlearn: 0.2843281\ttotal: 49.5s\tremaining: 13.2s\n",
      "789:\tlearn: 0.2843127\ttotal: 49.6s\tremaining: 13.2s\n",
      "790:\tlearn: 0.2843031\ttotal: 49.7s\tremaining: 13.1s\n",
      "791:\tlearn: 0.2842671\ttotal: 49.7s\tremaining: 13.1s\n",
      "792:\tlearn: 0.2842120\ttotal: 49.8s\tremaining: 13s\n",
      "793:\tlearn: 0.2842035\ttotal: 49.8s\tremaining: 12.9s\n",
      "794:\tlearn: 0.2841863\ttotal: 49.9s\tremaining: 12.9s\n",
      "795:\tlearn: 0.2841780\ttotal: 50s\tremaining: 12.8s\n",
      "796:\tlearn: 0.2841672\ttotal: 50s\tremaining: 12.7s\n",
      "797:\tlearn: 0.2841465\ttotal: 50.1s\tremaining: 12.7s\n",
      "798:\tlearn: 0.2841444\ttotal: 50.1s\tremaining: 12.6s\n",
      "799:\tlearn: 0.2841232\ttotal: 50.2s\tremaining: 12.5s\n",
      "800:\tlearn: 0.2840542\ttotal: 50.2s\tremaining: 12.5s\n",
      "801:\tlearn: 0.2839574\ttotal: 50.3s\tremaining: 12.4s\n",
      "802:\tlearn: 0.2839452\ttotal: 50.4s\tremaining: 12.4s\n",
      "803:\tlearn: 0.2839407\ttotal: 50.4s\tremaining: 12.3s\n",
      "804:\tlearn: 0.2839326\ttotal: 50.5s\tremaining: 12.2s\n",
      "805:\tlearn: 0.2839280\ttotal: 50.5s\tremaining: 12.2s\n",
      "806:\tlearn: 0.2839107\ttotal: 50.6s\tremaining: 12.1s\n",
      "807:\tlearn: 0.2838953\ttotal: 50.6s\tremaining: 12s\n",
      "808:\tlearn: 0.2837469\ttotal: 50.7s\tremaining: 12s\n",
      "809:\tlearn: 0.2837455\ttotal: 50.8s\tremaining: 11.9s\n",
      "810:\tlearn: 0.2837375\ttotal: 50.8s\tremaining: 11.8s\n",
      "811:\tlearn: 0.2837358\ttotal: 50.9s\tremaining: 11.8s\n",
      "812:\tlearn: 0.2836964\ttotal: 51s\tremaining: 11.7s\n",
      "813:\tlearn: 0.2836338\ttotal: 51s\tremaining: 11.7s\n",
      "814:\tlearn: 0.2836117\ttotal: 51.1s\tremaining: 11.6s\n",
      "815:\tlearn: 0.2835429\ttotal: 51.2s\tremaining: 11.5s\n",
      "816:\tlearn: 0.2835398\ttotal: 51.2s\tremaining: 11.5s\n",
      "817:\tlearn: 0.2834496\ttotal: 51.3s\tremaining: 11.4s\n",
      "818:\tlearn: 0.2834330\ttotal: 51.4s\tremaining: 11.3s\n",
      "819:\tlearn: 0.2834297\ttotal: 51.4s\tremaining: 11.3s\n",
      "820:\tlearn: 0.2834174\ttotal: 51.5s\tremaining: 11.2s\n",
      "821:\tlearn: 0.2833726\ttotal: 51.5s\tremaining: 11.2s\n",
      "822:\tlearn: 0.2833592\ttotal: 51.6s\tremaining: 11.1s\n",
      "823:\tlearn: 0.2833473\ttotal: 51.6s\tremaining: 11s\n",
      "824:\tlearn: 0.2833390\ttotal: 51.7s\tremaining: 11s\n",
      "825:\tlearn: 0.2833307\ttotal: 51.8s\tremaining: 10.9s\n",
      "826:\tlearn: 0.2833170\ttotal: 51.8s\tremaining: 10.8s\n",
      "827:\tlearn: 0.2832743\ttotal: 51.9s\tremaining: 10.8s\n",
      "828:\tlearn: 0.2831785\ttotal: 51.9s\tremaining: 10.7s\n",
      "829:\tlearn: 0.2831658\ttotal: 52s\tremaining: 10.6s\n",
      "830:\tlearn: 0.2831555\ttotal: 52.1s\tremaining: 10.6s\n",
      "831:\tlearn: 0.2831327\ttotal: 52.1s\tremaining: 10.5s\n",
      "832:\tlearn: 0.2831178\ttotal: 52.2s\tremaining: 10.5s\n",
      "833:\tlearn: 0.2830691\ttotal: 52.2s\tremaining: 10.4s\n",
      "834:\tlearn: 0.2830571\ttotal: 52.3s\tremaining: 10.3s\n",
      "835:\tlearn: 0.2830502\ttotal: 52.4s\tremaining: 10.3s\n",
      "836:\tlearn: 0.2830393\ttotal: 52.4s\tremaining: 10.2s\n",
      "837:\tlearn: 0.2830242\ttotal: 52.5s\tremaining: 10.1s\n",
      "838:\tlearn: 0.2829922\ttotal: 52.5s\tremaining: 10.1s\n",
      "839:\tlearn: 0.2829011\ttotal: 52.6s\tremaining: 10s\n",
      "840:\tlearn: 0.2828329\ttotal: 52.6s\tremaining: 9.95s\n",
      "841:\tlearn: 0.2828228\ttotal: 52.7s\tremaining: 9.89s\n",
      "842:\tlearn: 0.2828020\ttotal: 52.7s\tremaining: 9.82s\n",
      "843:\tlearn: 0.2827904\ttotal: 52.8s\tremaining: 9.76s\n",
      "844:\tlearn: 0.2827811\ttotal: 52.9s\tremaining: 9.7s\n",
      "845:\tlearn: 0.2827116\ttotal: 52.9s\tremaining: 9.63s\n",
      "846:\tlearn: 0.2826894\ttotal: 53s\tremaining: 9.57s\n",
      "847:\tlearn: 0.2826800\ttotal: 53s\tremaining: 9.51s\n",
      "848:\tlearn: 0.2826729\ttotal: 53.1s\tremaining: 9.44s\n",
      "849:\tlearn: 0.2826469\ttotal: 53.1s\tremaining: 9.38s\n",
      "850:\tlearn: 0.2826330\ttotal: 53.2s\tremaining: 9.31s\n",
      "851:\tlearn: 0.2825818\ttotal: 53.3s\tremaining: 9.25s\n",
      "852:\tlearn: 0.2825007\ttotal: 53.3s\tremaining: 9.19s\n",
      "853:\tlearn: 0.2824910\ttotal: 53.4s\tremaining: 9.12s\n",
      "854:\tlearn: 0.2824775\ttotal: 53.4s\tremaining: 9.06s\n",
      "855:\tlearn: 0.2823866\ttotal: 53.5s\tremaining: 9s\n",
      "856:\tlearn: 0.2823802\ttotal: 53.5s\tremaining: 8.93s\n",
      "857:\tlearn: 0.2823706\ttotal: 53.6s\tremaining: 8.87s\n",
      "858:\tlearn: 0.2823184\ttotal: 53.6s\tremaining: 8.8s\n",
      "859:\tlearn: 0.2822949\ttotal: 53.7s\tremaining: 8.74s\n",
      "860:\tlearn: 0.2822829\ttotal: 53.7s\tremaining: 8.68s\n",
      "861:\tlearn: 0.2822699\ttotal: 53.8s\tremaining: 8.61s\n",
      "862:\tlearn: 0.2822526\ttotal: 53.9s\tremaining: 8.55s\n",
      "863:\tlearn: 0.2822224\ttotal: 53.9s\tremaining: 8.48s\n",
      "864:\tlearn: 0.2822116\ttotal: 54s\tremaining: 8.42s\n",
      "865:\tlearn: 0.2821639\ttotal: 54s\tremaining: 8.36s\n",
      "866:\tlearn: 0.2821450\ttotal: 54.1s\tremaining: 8.3s\n",
      "867:\tlearn: 0.2820872\ttotal: 54.1s\tremaining: 8.23s\n",
      "868:\tlearn: 0.2820531\ttotal: 54.2s\tremaining: 8.17s\n",
      "869:\tlearn: 0.2820432\ttotal: 54.3s\tremaining: 8.11s\n",
      "870:\tlearn: 0.2820337\ttotal: 54.3s\tremaining: 8.04s\n",
      "871:\tlearn: 0.2819710\ttotal: 54.4s\tremaining: 7.98s\n",
      "872:\tlearn: 0.2819100\ttotal: 54.4s\tremaining: 7.92s\n",
      "873:\tlearn: 0.2818983\ttotal: 54.5s\tremaining: 7.86s\n",
      "874:\tlearn: 0.2818819\ttotal: 54.6s\tremaining: 7.79s\n",
      "875:\tlearn: 0.2818775\ttotal: 54.6s\tremaining: 7.73s\n",
      "876:\tlearn: 0.2818770\ttotal: 54.7s\tremaining: 7.67s\n",
      "877:\tlearn: 0.2818612\ttotal: 54.7s\tremaining: 7.6s\n",
      "878:\tlearn: 0.2818504\ttotal: 54.8s\tremaining: 7.54s\n",
      "879:\tlearn: 0.2818477\ttotal: 54.8s\tremaining: 7.48s\n",
      "880:\tlearn: 0.2817274\ttotal: 54.9s\tremaining: 7.42s\n",
      "881:\tlearn: 0.2817185\ttotal: 55s\tremaining: 7.35s\n",
      "882:\tlearn: 0.2817109\ttotal: 55s\tremaining: 7.29s\n",
      "883:\tlearn: 0.2817088\ttotal: 55.1s\tremaining: 7.23s\n",
      "884:\tlearn: 0.2817040\ttotal: 55.1s\tremaining: 7.16s\n",
      "885:\tlearn: 0.2816924\ttotal: 55.2s\tremaining: 7.1s\n",
      "886:\tlearn: 0.2816771\ttotal: 55.2s\tremaining: 7.04s\n",
      "887:\tlearn: 0.2816611\ttotal: 55.3s\tremaining: 6.97s\n",
      "888:\tlearn: 0.2816467\ttotal: 55.4s\tremaining: 6.91s\n",
      "889:\tlearn: 0.2816174\ttotal: 55.4s\tremaining: 6.85s\n",
      "890:\tlearn: 0.2816052\ttotal: 55.5s\tremaining: 6.79s\n",
      "891:\tlearn: 0.2815318\ttotal: 55.5s\tremaining: 6.72s\n",
      "892:\tlearn: 0.2815236\ttotal: 55.6s\tremaining: 6.66s\n",
      "893:\tlearn: 0.2815180\ttotal: 55.6s\tremaining: 6.6s\n",
      "894:\tlearn: 0.2815134\ttotal: 55.7s\tremaining: 6.54s\n",
      "895:\tlearn: 0.2815070\ttotal: 55.8s\tremaining: 6.47s\n",
      "896:\tlearn: 0.2814558\ttotal: 55.8s\tremaining: 6.41s\n",
      "897:\tlearn: 0.2814493\ttotal: 55.9s\tremaining: 6.35s\n",
      "898:\tlearn: 0.2814453\ttotal: 55.9s\tremaining: 6.28s\n",
      "899:\tlearn: 0.2814396\ttotal: 56s\tremaining: 6.22s\n",
      "900:\tlearn: 0.2814333\ttotal: 56s\tremaining: 6.16s\n",
      "901:\tlearn: 0.2814150\ttotal: 56.1s\tremaining: 6.09s\n",
      "902:\tlearn: 0.2814031\ttotal: 56.2s\tremaining: 6.03s\n",
      "903:\tlearn: 0.2813841\ttotal: 56.2s\tremaining: 5.97s\n",
      "904:\tlearn: 0.2813627\ttotal: 56.3s\tremaining: 5.91s\n",
      "905:\tlearn: 0.2813056\ttotal: 56.3s\tremaining: 5.84s\n",
      "906:\tlearn: 0.2812982\ttotal: 56.4s\tremaining: 5.78s\n",
      "907:\tlearn: 0.2812902\ttotal: 56.4s\tremaining: 5.72s\n",
      "908:\tlearn: 0.2812691\ttotal: 56.5s\tremaining: 5.66s\n",
      "909:\tlearn: 0.2812541\ttotal: 56.6s\tremaining: 5.59s\n",
      "910:\tlearn: 0.2812231\ttotal: 56.6s\tremaining: 5.53s\n",
      "911:\tlearn: 0.2812122\ttotal: 56.7s\tremaining: 5.47s\n",
      "912:\tlearn: 0.2811844\ttotal: 56.8s\tremaining: 5.41s\n",
      "913:\tlearn: 0.2810830\ttotal: 56.8s\tremaining: 5.35s\n",
      "914:\tlearn: 0.2810608\ttotal: 56.9s\tremaining: 5.29s\n",
      "915:\tlearn: 0.2810349\ttotal: 57s\tremaining: 5.22s\n",
      "916:\tlearn: 0.2810248\ttotal: 57s\tremaining: 5.16s\n",
      "917:\tlearn: 0.2810194\ttotal: 57.1s\tremaining: 5.1s\n",
      "918:\tlearn: 0.2810159\ttotal: 57.2s\tremaining: 5.04s\n",
      "919:\tlearn: 0.2810093\ttotal: 57.2s\tremaining: 4.98s\n",
      "920:\tlearn: 0.2809962\ttotal: 57.3s\tremaining: 4.91s\n",
      "921:\tlearn: 0.2809393\ttotal: 57.4s\tremaining: 4.85s\n",
      "922:\tlearn: 0.2809340\ttotal: 57.4s\tremaining: 4.79s\n",
      "923:\tlearn: 0.2809310\ttotal: 57.5s\tremaining: 4.73s\n",
      "924:\tlearn: 0.2808906\ttotal: 57.6s\tremaining: 4.67s\n",
      "925:\tlearn: 0.2808092\ttotal: 57.6s\tremaining: 4.61s\n",
      "926:\tlearn: 0.2808030\ttotal: 57.7s\tremaining: 4.54s\n",
      "927:\tlearn: 0.2807979\ttotal: 57.8s\tremaining: 4.48s\n",
      "928:\tlearn: 0.2807907\ttotal: 57.8s\tremaining: 4.42s\n",
      "929:\tlearn: 0.2807032\ttotal: 57.9s\tremaining: 4.36s\n",
      "930:\tlearn: 0.2807015\ttotal: 58s\tremaining: 4.3s\n",
      "931:\tlearn: 0.2806491\ttotal: 58.1s\tremaining: 4.24s\n",
      "932:\tlearn: 0.2806434\ttotal: 58.1s\tremaining: 4.17s\n",
      "933:\tlearn: 0.2806391\ttotal: 58.2s\tremaining: 4.11s\n",
      "934:\tlearn: 0.2806364\ttotal: 58.3s\tremaining: 4.05s\n",
      "935:\tlearn: 0.2805687\ttotal: 58.3s\tremaining: 3.99s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "936:\tlearn: 0.2805619\ttotal: 58.4s\tremaining: 3.93s\n",
      "937:\tlearn: 0.2805608\ttotal: 58.5s\tremaining: 3.86s\n",
      "938:\tlearn: 0.2805528\ttotal: 58.5s\tremaining: 3.8s\n",
      "939:\tlearn: 0.2805505\ttotal: 58.6s\tremaining: 3.74s\n",
      "940:\tlearn: 0.2805488\ttotal: 58.7s\tremaining: 3.68s\n",
      "941:\tlearn: 0.2805435\ttotal: 58.7s\tremaining: 3.62s\n",
      "942:\tlearn: 0.2804272\ttotal: 58.8s\tremaining: 3.55s\n",
      "943:\tlearn: 0.2804168\ttotal: 58.9s\tremaining: 3.49s\n",
      "944:\tlearn: 0.2804098\ttotal: 58.9s\tremaining: 3.43s\n",
      "945:\tlearn: 0.2804013\ttotal: 59s\tremaining: 3.37s\n",
      "946:\tlearn: 0.2803941\ttotal: 59.1s\tremaining: 3.31s\n",
      "947:\tlearn: 0.2803897\ttotal: 59.1s\tremaining: 3.24s\n",
      "948:\tlearn: 0.2803713\ttotal: 59.2s\tremaining: 3.18s\n",
      "949:\tlearn: 0.2803705\ttotal: 59.3s\tremaining: 3.12s\n",
      "950:\tlearn: 0.2803648\ttotal: 59.3s\tremaining: 3.06s\n",
      "951:\tlearn: 0.2803581\ttotal: 59.4s\tremaining: 2.99s\n",
      "952:\tlearn: 0.2803505\ttotal: 59.5s\tremaining: 2.93s\n",
      "953:\tlearn: 0.2803242\ttotal: 59.6s\tremaining: 2.87s\n",
      "954:\tlearn: 0.2802738\ttotal: 59.6s\tremaining: 2.81s\n",
      "955:\tlearn: 0.2802684\ttotal: 59.7s\tremaining: 2.75s\n",
      "956:\tlearn: 0.2802652\ttotal: 59.8s\tremaining: 2.69s\n",
      "957:\tlearn: 0.2801919\ttotal: 59.8s\tremaining: 2.62s\n",
      "958:\tlearn: 0.2801859\ttotal: 59.9s\tremaining: 2.56s\n",
      "959:\tlearn: 0.2801825\ttotal: 60s\tremaining: 2.5s\n",
      "960:\tlearn: 0.2801781\ttotal: 1m\tremaining: 2.44s\n",
      "961:\tlearn: 0.2801587\ttotal: 1m\tremaining: 2.37s\n",
      "962:\tlearn: 0.2800987\ttotal: 1m\tremaining: 2.31s\n",
      "963:\tlearn: 0.2800814\ttotal: 1m\tremaining: 2.25s\n",
      "964:\tlearn: 0.2800742\ttotal: 1m\tremaining: 2.19s\n",
      "965:\tlearn: 0.2800679\ttotal: 1m\tremaining: 2.12s\n",
      "966:\tlearn: 0.2800628\ttotal: 1m\tremaining: 2.06s\n",
      "967:\tlearn: 0.2800451\ttotal: 1m\tremaining: 2s\n",
      "968:\tlearn: 0.2800384\ttotal: 1m\tremaining: 1.94s\n",
      "969:\tlearn: 0.2800165\ttotal: 1m\tremaining: 1.88s\n",
      "970:\tlearn: 0.2800115\ttotal: 1m\tremaining: 1.81s\n",
      "971:\tlearn: 0.2800064\ttotal: 1m\tremaining: 1.75s\n",
      "972:\tlearn: 0.2799980\ttotal: 1m\tremaining: 1.69s\n",
      "973:\tlearn: 0.2799489\ttotal: 1m\tremaining: 1.63s\n",
      "974:\tlearn: 0.2799450\ttotal: 1m\tremaining: 1.56s\n",
      "975:\tlearn: 0.2798976\ttotal: 1m 1s\tremaining: 1.5s\n",
      "976:\tlearn: 0.2798935\ttotal: 1m 1s\tremaining: 1.44s\n",
      "977:\tlearn: 0.2798905\ttotal: 1m 1s\tremaining: 1.38s\n",
      "978:\tlearn: 0.2798824\ttotal: 1m 1s\tremaining: 1.31s\n",
      "979:\tlearn: 0.2798220\ttotal: 1m 1s\tremaining: 1.25s\n",
      "980:\tlearn: 0.2798012\ttotal: 1m 1s\tremaining: 1.19s\n",
      "981:\tlearn: 0.2797931\ttotal: 1m 1s\tremaining: 1.13s\n",
      "982:\tlearn: 0.2797887\ttotal: 1m 1s\tremaining: 1.06s\n",
      "983:\tlearn: 0.2797692\ttotal: 1m 1s\tremaining: 1s\n",
      "984:\tlearn: 0.2797666\ttotal: 1m 1s\tremaining: 939ms\n",
      "985:\tlearn: 0.2797579\ttotal: 1m 1s\tremaining: 876ms\n",
      "986:\tlearn: 0.2796813\ttotal: 1m 1s\tremaining: 814ms\n",
      "987:\tlearn: 0.2796705\ttotal: 1m 1s\tremaining: 751ms\n",
      "988:\tlearn: 0.2796637\ttotal: 1m 1s\tremaining: 688ms\n",
      "989:\tlearn: 0.2796597\ttotal: 1m 1s\tremaining: 626ms\n",
      "990:\tlearn: 0.2796535\ttotal: 1m 2s\tremaining: 563ms\n",
      "991:\tlearn: 0.2796385\ttotal: 1m 2s\tremaining: 501ms\n",
      "992:\tlearn: 0.2796312\ttotal: 1m 2s\tremaining: 438ms\n",
      "993:\tlearn: 0.2796179\ttotal: 1m 2s\tremaining: 376ms\n",
      "994:\tlearn: 0.2795654\ttotal: 1m 2s\tremaining: 313ms\n",
      "995:\tlearn: 0.2795435\ttotal: 1m 2s\tremaining: 250ms\n",
      "996:\tlearn: 0.2794702\ttotal: 1m 2s\tremaining: 188ms\n",
      "997:\tlearn: 0.2794642\ttotal: 1m 2s\tremaining: 125ms\n",
      "998:\tlearn: 0.2794388\ttotal: 1m 2s\tremaining: 62.6ms\n",
      "999:\tlearn: 0.2793509\ttotal: 1m 2s\tremaining: 0us\n",
      "CatBoostClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.90      0.88       763\n",
      "         1.0       0.89      0.85      0.87       728\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      1491\n",
      "   macro avg       0.87      0.87      0.87      1491\n",
      "weighted avg       0.87      0.87      0.87      1491\n",
      "\n",
      "[[684  79]\n",
      " [112 616]]\n",
      "Accuracy is  87.18980549966466\n",
      "Time on model's work: 65.259 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.89      0.87       763\n",
      "         1.0       0.88      0.85      0.86       728\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      1491\n",
      "   macro avg       0.87      0.87      0.87      1491\n",
      "weighted avg       0.87      0.87      0.87      1491\n",
      "\n",
      "[[676  87]\n",
      " [111 617]]\n",
      "Accuracy is  86.72032193158954\n",
      "Time on model's work: 0.131 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.58      0.70       763\n",
      "         1.0       0.68      0.92      0.78       728\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      1491\n",
      "   macro avg       0.78      0.75      0.74      1491\n",
      "weighted avg       0.78      0.75      0.74      1491\n",
      "\n",
      "[[442 321]\n",
      " [ 59 669]]\n",
      "Accuracy is  74.51374916163648\n",
      "Time on model's work: 0.159 s\n",
      "====================================================================================================\n",
      "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n",
      "TFFMClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.91      0.88       763\n",
      "         1.0       0.90      0.82      0.86       728\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      1491\n",
      "   macro avg       0.87      0.87      0.87      1491\n",
      "weighted avg       0.87      0.87      0.87      1491\n",
      "\n",
      "[[695  68]\n",
      " [130 598]]\n",
      "Accuracy is  86.72032193158954\n",
      "Time on model's work: 19.6 s\n",
      "====================================================================================================\n",
      "TOTAL TIME SPENT:  298.635 s\n"
     ]
    }
   ],
   "source": [
    "clfs = [\n",
    "        ['RandomForestClassifier', RandomForestClassifier()],\n",
    "        ['GradientBoostingClassifier', GradientBoostingClassifier()],\n",
    "        ['ExtraTreesClassifier', ExtraTreesClassifier()],\n",
    "        ['AdaBoostClassifier', AdaBoostClassifier()],\n",
    "        ['BaggingClassifier', BaggingClassifier()],\n",
    "        ['DecisionTreeClassifier', DecisionTreeClassifier()],\n",
    "        ['MLPClassifier', MLPClassifier()],\n",
    "        ['XGBClassifier', XGBClassifier()],\n",
    "        ['CatBoostClassifier', CatBoostClassifier()],\n",
    "        ['LogisticRegression', LogisticRegression()],\n",
    "        ['SGDClassifier', SGDClassifier()],\n",
    "        ['TFFMClassifier', TFFMClassifier()]\n",
    "       ]\n",
    "t = time()\n",
    "for name, clf in clfs:\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    prediction = clf.predict(X_test)\n",
    "    print(name)\n",
    "    print(classification_report(y_test, prediction))\n",
    "    print(confusion_matrix(y_test, prediction))\n",
    "    print('Accuracy is ', accuracy_score(y_test, prediction)*100)\n",
    "    print (\"Time on model's work:\", round(time()-t0, 3), \"s\")\n",
    "    print('='*100)\n",
    "print (\"TOTAL TIME SPENT: \", round(time()-t, 3), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:01<00:00, 17.37epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8443997317236754\n",
      "[[649 114]\n",
      " [118 610]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.85      0.85       763\n",
      "         1.0       0.84      0.84      0.84       728\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      1491\n",
      "   macro avg       0.84      0.84      0.84      1491\n",
      "weighted avg       0.84      0.84      0.84      1491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:01<00:00, 16.57epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7665995975855131\n",
      "[[458 305]\n",
      " [ 43 685]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.60      0.72       763\n",
      "         1.0       0.69      0.94      0.80       728\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      1491\n",
      "   macro avg       0.80      0.77      0.76      1491\n",
      "weighted avg       0.81      0.77      0.76      1491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:01<00:00, 16.73epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6955063715627096\n",
      "[[323 440]\n",
      " [ 14 714]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.42      0.59       763\n",
      "         1.0       0.62      0.98      0.76       728\n",
      "\n",
      "   micro avg       0.70      0.70      0.70      1491\n",
      "   macro avg       0.79      0.70      0.67      1491\n",
      "weighted avg       0.79      0.70      0.67      1491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:01<00:00, 15.72epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7015425888665325\n",
      "[[330 433]\n",
      " [ 12 716]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.43      0.60       763\n",
      "         1.0       0.62      0.98      0.76       728\n",
      "\n",
      "   micro avg       0.70      0.70      0.70      1491\n",
      "   macro avg       0.79      0.71      0.68      1491\n",
      "weighted avg       0.80      0.70      0.68      1491\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TFFM sparse - works worse with sparse\n",
    "# only CSR format supported\n",
    "X_train_sparse = sp.csr_matrix(X_train)\n",
    "X_test_sparse = sp.csr_matrix(X_test)\n",
    "# weight - optional / AdamOptimizer\n",
    "pos_class_weight = list(map(float,range(1, 5)))\n",
    "for weight in pos_class_weight:\n",
    "    model = TFFMClassifier(\n",
    "        order=2,\n",
    "        pos_class_weight=weight,\n",
    "        rank=10, \n",
    "        optimizer=tf.train.AdamOptimizer(learning_rate=0.001), \n",
    "        n_epochs=20, \n",
    "        batch_size=1024,\n",
    "        init_std=0.001,\n",
    "        reg=0.01,\n",
    "        input_type='sparse',\n",
    "        #log_dir='./tmp/logs',\n",
    "        #verbose=1,\n",
    "        seed=42\n",
    "    )\n",
    "    model.fit(X_train_sparse, y_train, show_progress=True)\n",
    "    predictions = model.predict(X_test_sparse)\n",
    "    print('accuracy: {}'.format(accuracy_score(y_test, predictions)))\n",
    "    # this will close tf.Session and free resources\n",
    "    print(confusion_matrix(y_test,predictions)) \n",
    "    print(classification_report(y_test, predictions))\n",
    "    model.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:01<00:00, 16.32epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7002012072434608\n",
      "[[384 379]\n",
      " [ 68 660]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.50      0.63       763\n",
      "         1.0       0.64      0.91      0.75       728\n",
      "\n",
      "   micro avg       0.70      0.70      0.70      1491\n",
      "   macro avg       0.74      0.70      0.69      1491\n",
      "weighted avg       0.74      0.70      0.69      1491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:01<00:00, 16.28epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7136150234741784\n",
      "[[366 397]\n",
      " [ 30 698]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.48      0.63       763\n",
      "         1.0       0.64      0.96      0.77       728\n",
      "\n",
      "   micro avg       0.71      0.71      0.71      1491\n",
      "   macro avg       0.78      0.72      0.70      1491\n",
      "weighted avg       0.78      0.71      0.70      1491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:01<00:00, 16.19epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6747149564050973\n",
      "[[292 471]\n",
      " [ 14 714]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.38      0.55       763\n",
      "         1.0       0.60      0.98      0.75       728\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      1491\n",
      "   macro avg       0.78      0.68      0.65      1491\n",
      "weighted avg       0.78      0.67      0.64      1491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:01<00:00, 16.94epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6753856472166331\n",
      "[[292 471]\n",
      " [ 13 715]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.38      0.55       763\n",
      "         1.0       0.60      0.98      0.75       728\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      1491\n",
      "   macro avg       0.78      0.68      0.65      1491\n",
      "weighted avg       0.78      0.68      0.64      1491\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# weight - optional / FtrlOptimizer\n",
    "pos_class_weight = list(map(float,range(1, 5)))\n",
    "for weight in pos_class_weight:\n",
    "    model = TFFMClassifier(\n",
    "        order=2,\n",
    "        pos_class_weight=weight,\n",
    "        rank=10, \n",
    "        optimizer=tf.train.FtrlOptimizer(0.01, l2_regularization_strength=0.001, l2_shrinkage_regularization_strength=0.001), \n",
    "        n_epochs=20, \n",
    "        batch_size=1024,\n",
    "        init_std=0.001,\n",
    "        reg=0.01,\n",
    "        input_type='sparse',\n",
    "        #log_dir='./tmp/logs',\n",
    "        #verbose=1,\n",
    "        seed=42\n",
    "    )\n",
    "    model.fit(X_train_sparse, y_train, show_progress=True)\n",
    "    predictions = model.predict(X_test_sparse)\n",
    "    print('accuracy: {}'.format(accuracy_score(y_test, predictions)))\n",
    "    # this will close tf.Session and free resources\n",
    "    print(confusion_matrix(y_test,predictions)) \n",
    "    print(classification_report(y_test, predictions))\n",
    "    model.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5961, 2226)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KERAS\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5961/5961 [==============================] - ETA: 6s - loss: 0.9735 - acc: 0.515 - ETA: 1s - loss: 0.8441 - acc: 0.496 - ETA: 0s - loss: 0.8211 - acc: 0.502 - ETA: 0s - loss: 0.8052 - acc: 0.495 - ETA: 0s - loss: 0.7865 - acc: 0.503 - ETA: 0s - loss: 0.7811 - acc: 0.502 - ETA: 0s - loss: 0.7739 - acc: 0.508 - ETA: 0s - loss: 0.7696 - acc: 0.510 - 1s 129us/step - loss: 0.7651 - acc: 0.5108\n",
      "Epoch 2/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.7403 - acc: 0.488 - ETA: 0s - loss: 0.7087 - acc: 0.541 - ETA: 0s - loss: 0.6961 - acc: 0.563 - ETA: 0s - loss: 0.6881 - acc: 0.575 - ETA: 0s - loss: 0.6828 - acc: 0.582 - ETA: 0s - loss: 0.6814 - acc: 0.583 - ETA: 0s - loss: 0.6772 - acc: 0.585 - ETA: 0s - loss: 0.6768 - acc: 0.586 - ETA: 0s - loss: 0.6720 - acc: 0.590 - 1s 87us/step - loss: 0.6726 - acc: 0.5895\n",
      "Epoch 3/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.6494 - acc: 0.621 - ETA: 0s - loss: 0.6422 - acc: 0.628 - ETA: 0s - loss: 0.6352 - acc: 0.641 - ETA: 0s - loss: 0.6315 - acc: 0.644 - ETA: 0s - loss: 0.6268 - acc: 0.641 - ETA: 0s - loss: 0.6258 - acc: 0.641 - ETA: 0s - loss: 0.6283 - acc: 0.642 - ETA: 0s - loss: 0.6274 - acc: 0.645 - ETA: 0s - loss: 0.6231 - acc: 0.648 - 0s 82us/step - loss: 0.6227 - acc: 0.6494\n",
      "Epoch 4/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.5936 - acc: 0.699 - ETA: 0s - loss: 0.5778 - acc: 0.698 - ETA: 0s - loss: 0.5834 - acc: 0.691 - ETA: 0s - loss: 0.5815 - acc: 0.692 - ETA: 0s - loss: 0.5824 - acc: 0.693 - ETA: 0s - loss: 0.5821 - acc: 0.694 - ETA: 0s - loss: 0.5774 - acc: 0.695 - ETA: 0s - loss: 0.5765 - acc: 0.695 - 0s 82us/step - loss: 0.5753 - acc: 0.6965\n",
      "Epoch 5/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.5525 - acc: 0.718 - ETA: 0s - loss: 0.5368 - acc: 0.736 - ETA: 0s - loss: 0.5422 - acc: 0.727 - ETA: 0s - loss: 0.5436 - acc: 0.721 - ETA: 0s - loss: 0.5484 - acc: 0.718 - ETA: 0s - loss: 0.5455 - acc: 0.719 - ETA: 0s - loss: 0.5454 - acc: 0.719 - ETA: 0s - loss: 0.5478 - acc: 0.715 - ETA: 0s - loss: 0.5478 - acc: 0.717 - 1s 89us/step - loss: 0.5459 - acc: 0.7195\n",
      "Epoch 6/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.5258 - acc: 0.734 - ETA: 0s - loss: 0.5564 - acc: 0.711 - ETA: 0s - loss: 0.5340 - acc: 0.735 - ETA: 0s - loss: 0.5315 - acc: 0.738 - ETA: 0s - loss: 0.5278 - acc: 0.741 - ETA: 0s - loss: 0.5274 - acc: 0.738 - ETA: 0s - loss: 0.5245 - acc: 0.738 - ETA: 0s - loss: 0.5220 - acc: 0.742 - ETA: 0s - loss: 0.5188 - acc: 0.743 - 1s 91us/step - loss: 0.5197 - acc: 0.7422\n",
      "Epoch 7/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.5276 - acc: 0.750 - ETA: 0s - loss: 0.5168 - acc: 0.763 - ETA: 0s - loss: 0.5038 - acc: 0.760 - ETA: 0s - loss: 0.4954 - acc: 0.766 - ETA: 0s - loss: 0.4999 - acc: 0.754 - ETA: 0s - loss: 0.4989 - acc: 0.753 - ETA: 0s - loss: 0.5021 - acc: 0.751 - ETA: 0s - loss: 0.5013 - acc: 0.750 - 0s 84us/step - loss: 0.4955 - acc: 0.7532\n",
      "Epoch 8/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.4529 - acc: 0.781 - ETA: 0s - loss: 0.4778 - acc: 0.761 - ETA: 0s - loss: 0.4905 - acc: 0.762 - ETA: 0s - loss: 0.4888 - acc: 0.762 - ETA: 0s - loss: 0.4919 - acc: 0.762 - ETA: 0s - loss: 0.4865 - acc: 0.765 - ETA: 0s - loss: 0.4879 - acc: 0.766 - ETA: 0s - loss: 0.4870 - acc: 0.766 - ETA: 0s - loss: 0.4861 - acc: 0.767 - ETA: 0s - loss: 0.4806 - acc: 0.769 - 1s 103us/step - loss: 0.4806 - acc: 0.7695\n",
      "Epoch 9/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.5019 - acc: 0.761 - ETA: 0s - loss: 0.4644 - acc: 0.777 - ETA: 0s - loss: 0.4610 - acc: 0.781 - ETA: 0s - loss: 0.4601 - acc: 0.787 - ETA: 0s - loss: 0.4552 - acc: 0.790 - ETA: 0s - loss: 0.4614 - acc: 0.785 - ETA: 0s - loss: 0.4584 - acc: 0.785 - ETA: 0s - loss: 0.4563 - acc: 0.785 - ETA: 0s - loss: 0.4587 - acc: 0.784 - 1s 91us/step - loss: 0.4577 - acc: 0.7868\n",
      "Epoch 10/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.4151 - acc: 0.812 - ETA: 0s - loss: 0.4252 - acc: 0.809 - ETA: 0s - loss: 0.4445 - acc: 0.799 - ETA: 0s - loss: 0.4493 - acc: 0.793 - ETA: 0s - loss: 0.4410 - acc: 0.798 - ETA: 0s - loss: 0.4402 - acc: 0.800 - ETA: 0s - loss: 0.4454 - acc: 0.795 - ETA: 0s - loss: 0.4474 - acc: 0.792 - ETA: 0s - loss: 0.4410 - acc: 0.795 - ETA: 0s - loss: 0.4436 - acc: 0.794 - 1s 108us/step - loss: 0.4451 - acc: 0.7935\n",
      "Epoch 11/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.4627 - acc: 0.777 - ETA: 0s - loss: 0.4489 - acc: 0.773 - ETA: 0s - loss: 0.4375 - acc: 0.790 - ETA: 0s - loss: 0.4398 - acc: 0.796 - ETA: 0s - loss: 0.4377 - acc: 0.799 - ETA: 0s - loss: 0.4315 - acc: 0.800 - ETA: 0s - loss: 0.4318 - acc: 0.800 - ETA: 0s - loss: 0.4331 - acc: 0.800 - ETA: 0s - loss: 0.4324 - acc: 0.801 - 1s 93us/step - loss: 0.4330 - acc: 0.8019\n",
      "Epoch 12/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.4582 - acc: 0.812 - ETA: 0s - loss: 0.4342 - acc: 0.803 - ETA: 0s - loss: 0.4515 - acc: 0.792 - ETA: 0s - loss: 0.4383 - acc: 0.799 - ETA: 0s - loss: 0.4364 - acc: 0.800 - ETA: 0s - loss: 0.4311 - acc: 0.800 - ETA: 0s - loss: 0.4298 - acc: 0.801 - ETA: 0s - loss: 0.4275 - acc: 0.800 - ETA: 0s - loss: 0.4249 - acc: 0.803 - ETA: 0s - loss: 0.4230 - acc: 0.805 - 1s 101us/step - loss: 0.4180 - acc: 0.8079\n",
      "Epoch 13/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.4628 - acc: 0.781 - ETA: 0s - loss: 0.4192 - acc: 0.803 - ETA: 0s - loss: 0.4171 - acc: 0.809 - ETA: 0s - loss: 0.4192 - acc: 0.805 - ETA: 0s - loss: 0.4194 - acc: 0.804 - ETA: 0s - loss: 0.4202 - acc: 0.806 - ETA: 0s - loss: 0.4212 - acc: 0.806 - ETA: 0s - loss: 0.4196 - acc: 0.808 - ETA: 0s - loss: 0.4157 - acc: 0.811 - ETA: 0s - loss: 0.4171 - acc: 0.810 - 1s 104us/step - loss: 0.4143 - acc: 0.8109\n",
      "Epoch 14/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.4200 - acc: 0.816 - ETA: 0s - loss: 0.4311 - acc: 0.799 - ETA: 0s - loss: 0.4253 - acc: 0.803 - ETA: 0s - loss: 0.4055 - acc: 0.814 - ETA: 0s - loss: 0.4070 - acc: 0.812 - ETA: 0s - loss: 0.4038 - acc: 0.813 - ETA: 0s - loss: 0.4025 - acc: 0.814 - ETA: 0s - loss: 0.4046 - acc: 0.814 - ETA: 0s - loss: 0.4048 - acc: 0.814 - 1s 103us/step - loss: 0.4064 - acc: 0.8145\n",
      "Epoch 15/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.3508 - acc: 0.843 - ETA: 0s - loss: 0.3933 - acc: 0.821 - ETA: 0s - loss: 0.4016 - acc: 0.812 - ETA: 0s - loss: 0.4045 - acc: 0.813 - ETA: 0s - loss: 0.3972 - acc: 0.817 - ETA: 0s - loss: 0.3991 - acc: 0.816 - ETA: 0s - loss: 0.4003 - acc: 0.815 - ETA: 0s - loss: 0.3996 - acc: 0.816 - ETA: 0s - loss: 0.3961 - acc: 0.817 - 1s 103us/step - loss: 0.3978 - acc: 0.8161\n",
      "Epoch 16/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.3772 - acc: 0.820 - ETA: 0s - loss: 0.3800 - acc: 0.830 - ETA: 0s - loss: 0.3768 - acc: 0.832 - ETA: 0s - loss: 0.3810 - acc: 0.828 - ETA: 0s - loss: 0.3930 - acc: 0.818 - ETA: 0s - loss: 0.3910 - acc: 0.821 - ETA: 0s - loss: 0.3925 - acc: 0.819 - ETA: 0s - loss: 0.3876 - acc: 0.822 - ETA: 0s - loss: 0.3866 - acc: 0.822 - 1s 97us/step - loss: 0.3858 - acc: 0.8228\n",
      "Epoch 17/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.4633 - acc: 0.753 - ETA: 0s - loss: 0.4101 - acc: 0.797 - ETA: 0s - loss: 0.4003 - acc: 0.808 - ETA: 0s - loss: 0.3920 - acc: 0.817 - ETA: 0s - loss: 0.3866 - acc: 0.822 - ETA: 0s - loss: 0.3889 - acc: 0.822 - ETA: 0s - loss: 0.3866 - acc: 0.823 - ETA: 0s - loss: 0.3828 - acc: 0.825 - ETA: 0s - loss: 0.3818 - acc: 0.825 - 1s 89us/step - loss: 0.3832 - acc: 0.8259\n",
      "Epoch 18/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.3393 - acc: 0.851 - ETA: 0s - loss: 0.3782 - acc: 0.833 - ETA: 0s - loss: 0.3858 - acc: 0.828 - ETA: 0s - loss: 0.3805 - acc: 0.832 - ETA: 0s - loss: 0.3812 - acc: 0.830 - ETA: 0s - loss: 0.3778 - acc: 0.830 - ETA: 0s - loss: 0.3801 - acc: 0.831 - ETA: 0s - loss: 0.3815 - acc: 0.831 - 0s 83us/step - loss: 0.3826 - acc: 0.8294\n",
      "Epoch 19/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.4286 - acc: 0.812 - ETA: 0s - loss: 0.4008 - acc: 0.823 - ETA: 0s - loss: 0.3940 - acc: 0.832 - ETA: 0s - loss: 0.3809 - acc: 0.836 - ETA: 0s - loss: 0.3817 - acc: 0.837 - ETA: 0s - loss: 0.3747 - acc: 0.840 - ETA: 0s - loss: 0.3731 - acc: 0.838 - ETA: 0s - loss: 0.3691 - acc: 0.839 - 0s 78us/step - loss: 0.3703 - acc: 0.8378\n",
      "Epoch 20/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.3460 - acc: 0.859 - ETA: 0s - loss: 0.3579 - acc: 0.847 - ETA: 0s - loss: 0.3572 - acc: 0.845 - ETA: 0s - loss: 0.3648 - acc: 0.839 - ETA: 0s - loss: 0.3717 - acc: 0.836 - ETA: 0s - loss: 0.3676 - acc: 0.840 - ETA: 0s - loss: 0.3676 - acc: 0.841 - ETA: 0s - loss: 0.3720 - acc: 0.839 - ETA: 0s - loss: 0.3674 - acc: 0.841 - 1s 92us/step - loss: 0.3667 - acc: 0.8416\n",
      "1491/1491 [==============================] - ETA:  - 0s 112us/step\n",
      "[0.3419680971774697, 0.8524480174245009]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=2226, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=256)\n",
    "score = model.evaluate(X_test, y_test, batch_size=256)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NearMiss (version = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 3726), (1.0, 3726)]\n"
     ]
    }
   ],
   "source": [
    "nm2 = NearMiss(version=2)\n",
    "X_resampled_nm2, y_resampled2 = nm2.fit_resample(features_list_array, labels_list_array)\n",
    "print(sorted(Counter(y_resampled2).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled_nm2, y_resampled2, random_state=35, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.82      0.81       763\n",
      "         1.0       0.81      0.78      0.79       728\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      1491\n",
      "   macro avg       0.80      0.80      0.80      1491\n",
      "weighted avg       0.80      0.80      0.80      1491\n",
      "\n",
      "[[625 138]\n",
      " [157 571]]\n",
      "Accuracy is  80.21462105969148\n",
      "Time on model's work: 0.722 s\n",
      "====================================================================================================\n",
      "GradientBoostingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.88      0.83       763\n",
      "         1.0       0.86      0.75      0.80       728\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      1491\n",
      "   macro avg       0.82      0.82      0.82      1491\n",
      "weighted avg       0.82      0.82      0.82      1491\n",
      "\n",
      "[[671  92]\n",
      " [181 547]]\n",
      "Accuracy is  81.69014084507043\n",
      "Time on model's work: 36.396 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.84      0.82       763\n",
      "         1.0       0.83      0.78      0.80       728\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      1491\n",
      "   macro avg       0.81      0.81      0.81      1491\n",
      "weighted avg       0.81      0.81      0.81      1491\n",
      "\n",
      "[[644 119]\n",
      " [160 568]]\n",
      "Accuracy is  81.28772635814889\n",
      "Time on model's work: 1.25 s\n",
      "====================================================================================================\n",
      "AdaBoostClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.72      0.79       763\n",
      "         1.0       0.75      0.88      0.81       728\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      1491\n",
      "   macro avg       0.80      0.80      0.80      1491\n",
      "weighted avg       0.81      0.80      0.80      1491\n",
      "\n",
      "[[552 211]\n",
      " [ 91 637]]\n",
      "Accuracy is  79.74513749161638\n",
      "Time on model's work: 9.626 s\n",
      "====================================================================================================\n",
      "BaggingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.87      0.83       763\n",
      "         1.0       0.84      0.76      0.80       728\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      1491\n",
      "   macro avg       0.82      0.81      0.81      1491\n",
      "weighted avg       0.82      0.81      0.81      1491\n",
      "\n",
      "[[661 102]\n",
      " [175 553]]\n",
      "Accuracy is  81.42186452045607\n",
      "Time on model's work: 4.463 s\n",
      "====================================================================================================\n",
      "DecisionTreeClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.85      0.83       763\n",
      "         1.0       0.84      0.78      0.81       728\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      1491\n",
      "   macro avg       0.82      0.82      0.82      1491\n",
      "weighted avg       0.82      0.82      0.82      1491\n",
      "\n",
      "[[651 112]\n",
      " [160 568]]\n",
      "Accuracy is  81.75720992622401\n",
      "Time on model's work: 0.853 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.80      0.81       763\n",
      "         1.0       0.80      0.80      0.80       728\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      1491\n",
      "   macro avg       0.80      0.80      0.80      1491\n",
      "weighted avg       0.80      0.80      0.80      1491\n",
      "\n",
      "[[613 150]\n",
      " [143 585]]\n",
      "Accuracy is  80.34875922199866\n",
      "Time on model's work: 129.26 s\n",
      "====================================================================================================\n",
      "XGBClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.89      0.85       763\n",
      "         1.0       0.87      0.77      0.82       728\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      1491\n",
      "   macro avg       0.84      0.83      0.83      1491\n",
      "weighted avg       0.84      0.83      0.83      1491\n",
      "\n",
      "[[680  83]\n",
      " [164 564]]\n",
      "Accuracy is  83.43393695506371\n",
      "Time on model's work: 42.938 s\n",
      "====================================================================================================\n",
      "0:\tlearn: 0.6516410\ttotal: 66.6ms\tremaining: 1m 6s\n",
      "1:\tlearn: 0.6170341\ttotal: 123ms\tremaining: 1m 1s\n",
      "2:\tlearn: 0.5852969\ttotal: 192ms\tremaining: 1m 3s\n",
      "3:\tlearn: 0.5586944\ttotal: 258ms\tremaining: 1m 4s\n",
      "4:\tlearn: 0.5342551\ttotal: 319ms\tremaining: 1m 3s\n",
      "5:\tlearn: 0.5234089\ttotal: 377ms\tremaining: 1m 2s\n",
      "6:\tlearn: 0.5135059\ttotal: 439ms\tremaining: 1m 2s\n",
      "7:\tlearn: 0.4939093\ttotal: 502ms\tremaining: 1m 2s\n",
      "8:\tlearn: 0.4755008\ttotal: 570ms\tremaining: 1m 2s\n",
      "9:\tlearn: 0.4685654\ttotal: 626ms\tremaining: 1m 2s\n",
      "10:\tlearn: 0.4540263\ttotal: 691ms\tremaining: 1m 2s\n",
      "11:\tlearn: 0.4418189\ttotal: 761ms\tremaining: 1m 2s\n",
      "12:\tlearn: 0.4363087\ttotal: 818ms\tremaining: 1m 2s\n",
      "13:\tlearn: 0.4309704\ttotal: 874ms\tremaining: 1m 1s\n",
      "14:\tlearn: 0.4263473\ttotal: 931ms\tremaining: 1m 1s\n",
      "15:\tlearn: 0.4221666\ttotal: 986ms\tremaining: 1m\n",
      "16:\tlearn: 0.4178063\ttotal: 1.04s\tremaining: 1m\n",
      "17:\tlearn: 0.4077746\ttotal: 1.11s\tremaining: 1m\n",
      "18:\tlearn: 0.4044917\ttotal: 1.16s\tremaining: 1m\n",
      "19:\tlearn: 0.4010644\ttotal: 1.23s\tremaining: 1m\n",
      "20:\tlearn: 0.3980717\ttotal: 1.28s\tremaining: 59.9s\n",
      "21:\tlearn: 0.3959231\ttotal: 1.34s\tremaining: 59.7s\n",
      "22:\tlearn: 0.3937969\ttotal: 1.4s\tremaining: 59.7s\n",
      "23:\tlearn: 0.3863487\ttotal: 1.47s\tremaining: 59.6s\n",
      "24:\tlearn: 0.3839700\ttotal: 1.52s\tremaining: 59.5s\n",
      "25:\tlearn: 0.3816119\ttotal: 1.59s\tremaining: 59.5s\n",
      "26:\tlearn: 0.3794844\ttotal: 1.64s\tremaining: 59.3s\n",
      "27:\tlearn: 0.3777133\ttotal: 1.7s\tremaining: 59s\n",
      "28:\tlearn: 0.3760234\ttotal: 1.75s\tremaining: 58.7s\n",
      "29:\tlearn: 0.3703047\ttotal: 1.83s\tremaining: 59.1s\n",
      "30:\tlearn: 0.3673634\ttotal: 1.9s\tremaining: 59.5s\n",
      "31:\tlearn: 0.3656072\ttotal: 1.97s\tremaining: 59.5s\n",
      "32:\tlearn: 0.3629556\ttotal: 2.03s\tremaining: 59.6s\n",
      "33:\tlearn: 0.3585511\ttotal: 2.1s\tremaining: 59.7s\n",
      "34:\tlearn: 0.3574549\ttotal: 2.16s\tremaining: 59.5s\n",
      "35:\tlearn: 0.3564579\ttotal: 2.22s\tremaining: 59.4s\n",
      "36:\tlearn: 0.3553445\ttotal: 2.27s\tremaining: 59.2s\n",
      "37:\tlearn: 0.3536697\ttotal: 2.33s\tremaining: 59.1s\n",
      "38:\tlearn: 0.3526198\ttotal: 2.39s\tremaining: 59s\n",
      "39:\tlearn: 0.3514842\ttotal: 2.45s\tremaining: 58.8s\n",
      "40:\tlearn: 0.3499778\ttotal: 2.51s\tremaining: 58.7s\n",
      "41:\tlearn: 0.3484743\ttotal: 2.58s\tremaining: 59s\n",
      "42:\tlearn: 0.3474779\ttotal: 2.65s\tremaining: 58.9s\n",
      "43:\tlearn: 0.3464842\ttotal: 2.71s\tremaining: 58.9s\n",
      "44:\tlearn: 0.3456135\ttotal: 2.77s\tremaining: 58.8s\n",
      "45:\tlearn: 0.3449039\ttotal: 2.84s\tremaining: 58.8s\n",
      "46:\tlearn: 0.3415345\ttotal: 2.9s\tremaining: 58.8s\n",
      "47:\tlearn: 0.3409037\ttotal: 2.96s\tremaining: 58.7s\n",
      "48:\tlearn: 0.3404937\ttotal: 3.02s\tremaining: 58.5s\n",
      "49:\tlearn: 0.3402896\ttotal: 3.07s\tremaining: 58.4s\n",
      "50:\tlearn: 0.3394803\ttotal: 3.13s\tremaining: 58.3s\n",
      "51:\tlearn: 0.3386841\ttotal: 3.2s\tremaining: 58.3s\n",
      "52:\tlearn: 0.3382590\ttotal: 3.25s\tremaining: 58.1s\n",
      "53:\tlearn: 0.3375026\ttotal: 3.32s\tremaining: 58.1s\n",
      "54:\tlearn: 0.3369580\ttotal: 3.38s\tremaining: 58s\n",
      "55:\tlearn: 0.3361039\ttotal: 3.43s\tremaining: 57.9s\n",
      "56:\tlearn: 0.3356362\ttotal: 3.49s\tremaining: 57.8s\n",
      "57:\tlearn: 0.3353778\ttotal: 3.57s\tremaining: 58s\n",
      "58:\tlearn: 0.3329335\ttotal: 3.66s\tremaining: 58.3s\n",
      "59:\tlearn: 0.3324810\ttotal: 3.73s\tremaining: 58.4s\n",
      "60:\tlearn: 0.3321989\ttotal: 3.78s\tremaining: 58.1s\n",
      "61:\tlearn: 0.3318779\ttotal: 3.83s\tremaining: 58s\n",
      "62:\tlearn: 0.3313706\ttotal: 3.88s\tremaining: 57.8s\n",
      "63:\tlearn: 0.3310117\ttotal: 3.94s\tremaining: 57.7s\n",
      "64:\tlearn: 0.3305238\ttotal: 4.01s\tremaining: 57.7s\n",
      "65:\tlearn: 0.3300378\ttotal: 4.07s\tremaining: 57.6s\n",
      "66:\tlearn: 0.3295423\ttotal: 4.13s\tremaining: 57.5s\n",
      "67:\tlearn: 0.3292320\ttotal: 4.2s\tremaining: 57.5s\n",
      "68:\tlearn: 0.3290001\ttotal: 4.25s\tremaining: 57.4s\n",
      "69:\tlearn: 0.3287607\ttotal: 4.31s\tremaining: 57.2s\n",
      "70:\tlearn: 0.3284905\ttotal: 4.36s\tremaining: 57s\n",
      "71:\tlearn: 0.3281887\ttotal: 4.41s\tremaining: 56.9s\n",
      "72:\tlearn: 0.3280384\ttotal: 4.47s\tremaining: 56.8s\n",
      "73:\tlearn: 0.3274949\ttotal: 4.53s\tremaining: 56.7s\n",
      "74:\tlearn: 0.3271686\ttotal: 4.6s\tremaining: 56.8s\n",
      "75:\tlearn: 0.3269851\ttotal: 4.68s\tremaining: 56.9s\n",
      "76:\tlearn: 0.3262890\ttotal: 4.78s\tremaining: 57.3s\n",
      "77:\tlearn: 0.3260149\ttotal: 4.84s\tremaining: 57.3s\n",
      "78:\tlearn: 0.3258005\ttotal: 4.9s\tremaining: 57.1s\n",
      "79:\tlearn: 0.3253947\ttotal: 4.96s\tremaining: 57s\n",
      "80:\tlearn: 0.3253273\ttotal: 5.02s\tremaining: 56.9s\n",
      "81:\tlearn: 0.3250758\ttotal: 5.07s\tremaining: 56.8s\n",
      "82:\tlearn: 0.3249218\ttotal: 5.13s\tremaining: 56.7s\n",
      "83:\tlearn: 0.3247981\ttotal: 5.18s\tremaining: 56.5s\n",
      "84:\tlearn: 0.3247143\ttotal: 5.23s\tremaining: 56.3s\n",
      "85:\tlearn: 0.3237092\ttotal: 5.29s\tremaining: 56.3s\n",
      "86:\tlearn: 0.3235241\ttotal: 5.36s\tremaining: 56.2s\n",
      "87:\tlearn: 0.3233437\ttotal: 5.43s\tremaining: 56.3s\n",
      "88:\tlearn: 0.3232053\ttotal: 5.52s\tremaining: 56.5s\n",
      "89:\tlearn: 0.3230708\ttotal: 5.59s\tremaining: 56.6s\n",
      "90:\tlearn: 0.3229103\ttotal: 5.66s\tremaining: 56.5s\n",
      "91:\tlearn: 0.3226775\ttotal: 5.71s\tremaining: 56.4s\n",
      "92:\tlearn: 0.3225285\ttotal: 5.77s\tremaining: 56.3s\n",
      "93:\tlearn: 0.3223619\ttotal: 5.82s\tremaining: 56.1s\n",
      "94:\tlearn: 0.3221513\ttotal: 5.88s\tremaining: 56s\n",
      "95:\tlearn: 0.3221182\ttotal: 5.93s\tremaining: 55.9s\n",
      "96:\tlearn: 0.3219898\ttotal: 5.99s\tremaining: 55.8s\n",
      "97:\tlearn: 0.3218190\ttotal: 6.05s\tremaining: 55.7s\n",
      "98:\tlearn: 0.3216914\ttotal: 6.12s\tremaining: 55.7s\n",
      "99:\tlearn: 0.3214548\ttotal: 6.19s\tremaining: 55.7s\n",
      "100:\tlearn: 0.3213137\ttotal: 6.25s\tremaining: 55.7s\n",
      "101:\tlearn: 0.3211285\ttotal: 6.32s\tremaining: 55.6s\n",
      "102:\tlearn: 0.3210438\ttotal: 6.38s\tremaining: 55.5s\n",
      "103:\tlearn: 0.3209384\ttotal: 6.44s\tremaining: 55.5s\n",
      "104:\tlearn: 0.3207750\ttotal: 6.5s\tremaining: 55.5s\n",
      "105:\tlearn: 0.3206771\ttotal: 6.56s\tremaining: 55.3s\n",
      "106:\tlearn: 0.3205729\ttotal: 6.62s\tremaining: 55.3s\n",
      "107:\tlearn: 0.3203116\ttotal: 6.7s\tremaining: 55.3s\n",
      "108:\tlearn: 0.3202131\ttotal: 6.76s\tremaining: 55.2s\n",
      "109:\tlearn: 0.3201186\ttotal: 6.82s\tremaining: 55.1s\n",
      "110:\tlearn: 0.3200451\ttotal: 6.87s\tremaining: 55s\n",
      "111:\tlearn: 0.3199397\ttotal: 6.93s\tremaining: 54.9s\n",
      "112:\tlearn: 0.3198292\ttotal: 6.99s\tremaining: 54.9s\n",
      "113:\tlearn: 0.3196397\ttotal: 7.06s\tremaining: 54.9s\n",
      "114:\tlearn: 0.3195920\ttotal: 7.12s\tremaining: 54.8s\n",
      "115:\tlearn: 0.3195257\ttotal: 7.18s\tremaining: 54.7s\n",
      "116:\tlearn: 0.3193967\ttotal: 7.24s\tremaining: 54.7s\n",
      "117:\tlearn: 0.3191971\ttotal: 7.31s\tremaining: 54.6s\n",
      "118:\tlearn: 0.3175867\ttotal: 7.38s\tremaining: 54.7s\n",
      "119:\tlearn: 0.3175081\ttotal: 7.45s\tremaining: 54.6s\n",
      "120:\tlearn: 0.3173622\ttotal: 7.52s\tremaining: 54.6s\n",
      "121:\tlearn: 0.3172781\ttotal: 7.63s\tremaining: 54.9s\n",
      "122:\tlearn: 0.3172047\ttotal: 7.71s\tremaining: 55s\n",
      "123:\tlearn: 0.3171062\ttotal: 7.79s\tremaining: 55s\n",
      "124:\tlearn: 0.3169978\ttotal: 7.85s\tremaining: 55s\n",
      "125:\tlearn: 0.3166463\ttotal: 7.91s\tremaining: 54.9s\n",
      "126:\tlearn: 0.3165642\ttotal: 7.96s\tremaining: 54.7s\n",
      "127:\tlearn: 0.3164863\ttotal: 8.01s\tremaining: 54.6s\n",
      "128:\tlearn: 0.3163961\ttotal: 8.07s\tremaining: 54.5s\n",
      "129:\tlearn: 0.3163565\ttotal: 8.13s\tremaining: 54.4s\n",
      "130:\tlearn: 0.3162380\ttotal: 8.18s\tremaining: 54.3s\n",
      "131:\tlearn: 0.3161901\ttotal: 8.24s\tremaining: 54.2s\n",
      "132:\tlearn: 0.3161173\ttotal: 8.29s\tremaining: 54s\n",
      "133:\tlearn: 0.3160510\ttotal: 8.34s\tremaining: 53.9s\n",
      "134:\tlearn: 0.3159877\ttotal: 8.4s\tremaining: 53.8s\n",
      "135:\tlearn: 0.3158723\ttotal: 8.46s\tremaining: 53.8s\n",
      "136:\tlearn: 0.3158134\ttotal: 8.52s\tremaining: 53.7s\n",
      "137:\tlearn: 0.3157633\ttotal: 8.63s\tremaining: 53.9s\n",
      "138:\tlearn: 0.3156714\ttotal: 8.7s\tremaining: 53.9s\n",
      "139:\tlearn: 0.3156192\ttotal: 8.76s\tremaining: 53.8s\n",
      "140:\tlearn: 0.3155346\ttotal: 8.82s\tremaining: 53.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141:\tlearn: 0.3154715\ttotal: 8.89s\tremaining: 53.7s\n",
      "142:\tlearn: 0.3154249\ttotal: 8.96s\tremaining: 53.7s\n",
      "143:\tlearn: 0.3153435\ttotal: 9.03s\tremaining: 53.7s\n",
      "144:\tlearn: 0.3152882\ttotal: 9.11s\tremaining: 53.7s\n",
      "145:\tlearn: 0.3152407\ttotal: 9.16s\tremaining: 53.6s\n",
      "146:\tlearn: 0.3151946\ttotal: 9.22s\tremaining: 53.5s\n",
      "147:\tlearn: 0.3151699\ttotal: 9.27s\tremaining: 53.4s\n",
      "148:\tlearn: 0.3150716\ttotal: 9.34s\tremaining: 53.4s\n",
      "149:\tlearn: 0.3150413\ttotal: 9.42s\tremaining: 53.4s\n",
      "150:\tlearn: 0.3150055\ttotal: 9.51s\tremaining: 53.5s\n",
      "151:\tlearn: 0.3149511\ttotal: 9.57s\tremaining: 53.4s\n",
      "152:\tlearn: 0.3148590\ttotal: 9.62s\tremaining: 53.3s\n",
      "153:\tlearn: 0.3148128\ttotal: 9.7s\tremaining: 53.3s\n",
      "154:\tlearn: 0.3147167\ttotal: 9.79s\tremaining: 53.4s\n",
      "155:\tlearn: 0.3146521\ttotal: 9.85s\tremaining: 53.3s\n",
      "156:\tlearn: 0.3145557\ttotal: 9.92s\tremaining: 53.3s\n",
      "157:\tlearn: 0.3144589\ttotal: 9.98s\tremaining: 53.2s\n",
      "158:\tlearn: 0.3143890\ttotal: 10s\tremaining: 53.1s\n",
      "159:\tlearn: 0.3143357\ttotal: 10.1s\tremaining: 53s\n",
      "160:\tlearn: 0.3142388\ttotal: 10.2s\tremaining: 52.9s\n",
      "161:\tlearn: 0.3141554\ttotal: 10.2s\tremaining: 52.8s\n",
      "162:\tlearn: 0.3141408\ttotal: 10.3s\tremaining: 52.7s\n",
      "163:\tlearn: 0.3141193\ttotal: 10.3s\tremaining: 52.6s\n",
      "164:\tlearn: 0.3140437\ttotal: 10.4s\tremaining: 52.6s\n",
      "165:\tlearn: 0.3139920\ttotal: 10.4s\tremaining: 52.4s\n",
      "166:\tlearn: 0.3137725\ttotal: 10.5s\tremaining: 52.4s\n",
      "167:\tlearn: 0.3137392\ttotal: 10.6s\tremaining: 52.3s\n",
      "168:\tlearn: 0.3136529\ttotal: 10.6s\tremaining: 52.1s\n",
      "169:\tlearn: 0.3135607\ttotal: 10.7s\tremaining: 52s\n",
      "170:\tlearn: 0.3135014\ttotal: 10.7s\tremaining: 51.9s\n",
      "171:\tlearn: 0.3134474\ttotal: 10.8s\tremaining: 51.8s\n",
      "172:\tlearn: 0.3133946\ttotal: 10.8s\tremaining: 51.7s\n",
      "173:\tlearn: 0.3133590\ttotal: 10.9s\tremaining: 51.5s\n",
      "174:\tlearn: 0.3133279\ttotal: 10.9s\tremaining: 51.4s\n",
      "175:\tlearn: 0.3132692\ttotal: 11s\tremaining: 51.3s\n",
      "176:\tlearn: 0.3132195\ttotal: 11s\tremaining: 51.2s\n",
      "177:\tlearn: 0.3131865\ttotal: 11.1s\tremaining: 51.1s\n",
      "178:\tlearn: 0.3131652\ttotal: 11.1s\tremaining: 51s\n",
      "179:\tlearn: 0.3131004\ttotal: 11.2s\tremaining: 50.9s\n",
      "180:\tlearn: 0.3130569\ttotal: 11.2s\tremaining: 50.8s\n",
      "181:\tlearn: 0.3129865\ttotal: 11.3s\tremaining: 50.7s\n",
      "182:\tlearn: 0.3129412\ttotal: 11.3s\tremaining: 50.5s\n",
      "183:\tlearn: 0.3129115\ttotal: 11.4s\tremaining: 50.5s\n",
      "184:\tlearn: 0.3128915\ttotal: 11.4s\tremaining: 50.3s\n",
      "185:\tlearn: 0.3128500\ttotal: 11.5s\tremaining: 50.3s\n",
      "186:\tlearn: 0.3128128\ttotal: 11.5s\tremaining: 50.2s\n",
      "187:\tlearn: 0.3127746\ttotal: 11.6s\tremaining: 50.1s\n",
      "188:\tlearn: 0.3127036\ttotal: 11.6s\tremaining: 50s\n",
      "189:\tlearn: 0.3126625\ttotal: 11.7s\tremaining: 49.8s\n",
      "190:\tlearn: 0.3126402\ttotal: 11.7s\tremaining: 49.8s\n",
      "191:\tlearn: 0.3126046\ttotal: 11.8s\tremaining: 49.7s\n",
      "192:\tlearn: 0.3125677\ttotal: 11.9s\tremaining: 49.6s\n",
      "193:\tlearn: 0.3124617\ttotal: 11.9s\tremaining: 49.5s\n",
      "194:\tlearn: 0.3124152\ttotal: 12s\tremaining: 49.4s\n",
      "195:\tlearn: 0.3123572\ttotal: 12s\tremaining: 49.3s\n",
      "196:\tlearn: 0.3123356\ttotal: 12.1s\tremaining: 49.2s\n",
      "197:\tlearn: 0.3123200\ttotal: 12.1s\tremaining: 49.1s\n",
      "198:\tlearn: 0.3122624\ttotal: 12.2s\tremaining: 49s\n",
      "199:\tlearn: 0.3122281\ttotal: 12.2s\tremaining: 48.9s\n",
      "200:\tlearn: 0.3122012\ttotal: 12.3s\tremaining: 48.9s\n",
      "201:\tlearn: 0.3121929\ttotal: 12.4s\tremaining: 48.8s\n",
      "202:\tlearn: 0.3121443\ttotal: 12.4s\tremaining: 48.7s\n",
      "203:\tlearn: 0.3120771\ttotal: 12.5s\tremaining: 48.7s\n",
      "204:\tlearn: 0.3119919\ttotal: 12.5s\tremaining: 48.6s\n",
      "205:\tlearn: 0.3119539\ttotal: 12.6s\tremaining: 48.6s\n",
      "206:\tlearn: 0.3119100\ttotal: 12.7s\tremaining: 48.5s\n",
      "207:\tlearn: 0.3118736\ttotal: 12.8s\tremaining: 48.6s\n",
      "208:\tlearn: 0.3118105\ttotal: 12.8s\tremaining: 48.6s\n",
      "209:\tlearn: 0.3117356\ttotal: 12.9s\tremaining: 48.5s\n",
      "210:\tlearn: 0.3116347\ttotal: 13s\tremaining: 48.5s\n",
      "211:\tlearn: 0.3115874\ttotal: 13s\tremaining: 48.5s\n",
      "212:\tlearn: 0.3115480\ttotal: 13.1s\tremaining: 48.4s\n",
      "213:\tlearn: 0.3114600\ttotal: 13.2s\tremaining: 48.4s\n",
      "214:\tlearn: 0.3114103\ttotal: 13.3s\tremaining: 48.4s\n",
      "215:\tlearn: 0.3113794\ttotal: 13.3s\tremaining: 48.4s\n",
      "216:\tlearn: 0.3113561\ttotal: 13.4s\tremaining: 48.3s\n",
      "217:\tlearn: 0.3113318\ttotal: 13.4s\tremaining: 48.2s\n",
      "218:\tlearn: 0.3112969\ttotal: 13.5s\tremaining: 48.2s\n",
      "219:\tlearn: 0.3112559\ttotal: 13.6s\tremaining: 48.1s\n",
      "220:\tlearn: 0.3112117\ttotal: 13.6s\tremaining: 48s\n",
      "221:\tlearn: 0.3111519\ttotal: 13.7s\tremaining: 48s\n",
      "222:\tlearn: 0.3110885\ttotal: 13.7s\tremaining: 47.9s\n",
      "223:\tlearn: 0.3110626\ttotal: 13.8s\tremaining: 47.8s\n",
      "224:\tlearn: 0.3109937\ttotal: 13.9s\tremaining: 47.7s\n",
      "225:\tlearn: 0.3109731\ttotal: 13.9s\tremaining: 47.7s\n",
      "226:\tlearn: 0.3109550\ttotal: 14s\tremaining: 47.6s\n",
      "227:\tlearn: 0.3109368\ttotal: 14s\tremaining: 47.5s\n",
      "228:\tlearn: 0.3108459\ttotal: 14.1s\tremaining: 47.5s\n",
      "229:\tlearn: 0.3108104\ttotal: 14.2s\tremaining: 47.4s\n",
      "230:\tlearn: 0.3106045\ttotal: 14.2s\tremaining: 47.4s\n",
      "231:\tlearn: 0.3105681\ttotal: 14.3s\tremaining: 47.3s\n",
      "232:\tlearn: 0.3105472\ttotal: 14.4s\tremaining: 47.2s\n",
      "233:\tlearn: 0.3105041\ttotal: 14.4s\tremaining: 47.2s\n",
      "234:\tlearn: 0.3104694\ttotal: 14.5s\tremaining: 47.1s\n",
      "235:\tlearn: 0.3104498\ttotal: 14.5s\tremaining: 47s\n",
      "236:\tlearn: 0.3104087\ttotal: 14.6s\tremaining: 46.9s\n",
      "237:\tlearn: 0.3103891\ttotal: 14.6s\tremaining: 46.9s\n",
      "238:\tlearn: 0.3103643\ttotal: 14.7s\tremaining: 46.8s\n",
      "239:\tlearn: 0.3103418\ttotal: 14.8s\tremaining: 46.7s\n",
      "240:\tlearn: 0.3102719\ttotal: 14.8s\tremaining: 46.7s\n",
      "241:\tlearn: 0.3102543\ttotal: 14.9s\tremaining: 46.6s\n",
      "242:\tlearn: 0.3101403\ttotal: 14.9s\tremaining: 46.6s\n",
      "243:\tlearn: 0.3100916\ttotal: 15s\tremaining: 46.5s\n",
      "244:\tlearn: 0.3100555\ttotal: 15.1s\tremaining: 46.5s\n",
      "245:\tlearn: 0.3100083\ttotal: 15.1s\tremaining: 46.4s\n",
      "246:\tlearn: 0.3099809\ttotal: 15.2s\tremaining: 46.3s\n",
      "247:\tlearn: 0.3099116\ttotal: 15.3s\tremaining: 46.3s\n",
      "248:\tlearn: 0.3098677\ttotal: 15.3s\tremaining: 46.2s\n",
      "249:\tlearn: 0.3098267\ttotal: 15.4s\tremaining: 46.2s\n",
      "250:\tlearn: 0.3097947\ttotal: 15.4s\tremaining: 46.1s\n",
      "251:\tlearn: 0.3097461\ttotal: 15.5s\tremaining: 46.1s\n",
      "252:\tlearn: 0.3096976\ttotal: 15.6s\tremaining: 46s\n",
      "253:\tlearn: 0.3096705\ttotal: 15.6s\tremaining: 45.9s\n",
      "254:\tlearn: 0.3096602\ttotal: 15.7s\tremaining: 45.8s\n",
      "255:\tlearn: 0.3096416\ttotal: 15.7s\tremaining: 45.8s\n",
      "256:\tlearn: 0.3095892\ttotal: 15.8s\tremaining: 45.7s\n",
      "257:\tlearn: 0.3095702\ttotal: 15.9s\tremaining: 45.6s\n",
      "258:\tlearn: 0.3095461\ttotal: 15.9s\tremaining: 45.5s\n",
      "259:\tlearn: 0.3095294\ttotal: 16s\tremaining: 45.5s\n",
      "260:\tlearn: 0.3094949\ttotal: 16s\tremaining: 45.4s\n",
      "261:\tlearn: 0.3094583\ttotal: 16.1s\tremaining: 45.4s\n",
      "262:\tlearn: 0.3094432\ttotal: 16.2s\tremaining: 45.3s\n",
      "263:\tlearn: 0.3094207\ttotal: 16.2s\tremaining: 45.2s\n",
      "264:\tlearn: 0.3093835\ttotal: 16.3s\tremaining: 45.1s\n",
      "265:\tlearn: 0.3093559\ttotal: 16.3s\tremaining: 45.1s\n",
      "266:\tlearn: 0.3093080\ttotal: 16.4s\tremaining: 45s\n",
      "267:\tlearn: 0.3092602\ttotal: 16.5s\tremaining: 45s\n",
      "268:\tlearn: 0.3092116\ttotal: 16.5s\tremaining: 44.9s\n",
      "269:\tlearn: 0.3091500\ttotal: 16.6s\tremaining: 44.8s\n",
      "270:\tlearn: 0.3091321\ttotal: 16.6s\tremaining: 44.8s\n",
      "271:\tlearn: 0.3090986\ttotal: 16.7s\tremaining: 44.7s\n",
      "272:\tlearn: 0.3090684\ttotal: 16.8s\tremaining: 44.6s\n",
      "273:\tlearn: 0.3090376\ttotal: 16.8s\tremaining: 44.6s\n",
      "274:\tlearn: 0.3090261\ttotal: 16.9s\tremaining: 44.5s\n",
      "275:\tlearn: 0.3090040\ttotal: 16.9s\tremaining: 44.4s\n",
      "276:\tlearn: 0.3089935\ttotal: 17s\tremaining: 44.4s\n",
      "277:\tlearn: 0.3088574\ttotal: 17.1s\tremaining: 44.4s\n",
      "278:\tlearn: 0.3088377\ttotal: 17.2s\tremaining: 44.4s\n",
      "279:\tlearn: 0.3087846\ttotal: 17.2s\tremaining: 44.3s\n",
      "280:\tlearn: 0.3087733\ttotal: 17.3s\tremaining: 44.2s\n",
      "281:\tlearn: 0.3087663\ttotal: 17.3s\tremaining: 44.2s\n",
      "282:\tlearn: 0.3086974\ttotal: 17.4s\tremaining: 44.1s\n",
      "283:\tlearn: 0.3086722\ttotal: 17.5s\tremaining: 44.1s\n",
      "284:\tlearn: 0.3085911\ttotal: 17.6s\tremaining: 44s\n",
      "285:\tlearn: 0.3085813\ttotal: 17.6s\tremaining: 44s\n",
      "286:\tlearn: 0.3085705\ttotal: 17.7s\tremaining: 43.9s\n",
      "287:\tlearn: 0.3085191\ttotal: 17.7s\tremaining: 43.8s\n",
      "288:\tlearn: 0.3084416\ttotal: 17.8s\tremaining: 43.8s\n",
      "289:\tlearn: 0.3083916\ttotal: 17.8s\tremaining: 43.7s\n",
      "290:\tlearn: 0.3083195\ttotal: 17.9s\tremaining: 43.6s\n",
      "291:\tlearn: 0.3083040\ttotal: 18s\tremaining: 43.6s\n",
      "292:\tlearn: 0.3082979\ttotal: 18s\tremaining: 43.5s\n",
      "293:\tlearn: 0.3082712\ttotal: 18.1s\tremaining: 43.4s\n",
      "294:\tlearn: 0.3082622\ttotal: 18.1s\tremaining: 43.4s\n",
      "295:\tlearn: 0.3082183\ttotal: 18.2s\tremaining: 43.3s\n",
      "296:\tlearn: 0.3081484\ttotal: 18.3s\tremaining: 43.2s\n",
      "297:\tlearn: 0.3081153\ttotal: 18.3s\tremaining: 43.2s\n",
      "298:\tlearn: 0.3080797\ttotal: 18.4s\tremaining: 43.1s\n",
      "299:\tlearn: 0.3080203\ttotal: 18.5s\tremaining: 43.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300:\tlearn: 0.3080180\ttotal: 18.5s\tremaining: 43s\n",
      "301:\tlearn: 0.3079932\ttotal: 18.6s\tremaining: 42.9s\n",
      "302:\tlearn: 0.3079661\ttotal: 18.6s\tremaining: 42.9s\n",
      "303:\tlearn: 0.3079475\ttotal: 18.7s\tremaining: 42.8s\n",
      "304:\tlearn: 0.3079127\ttotal: 18.8s\tremaining: 42.7s\n",
      "305:\tlearn: 0.3079079\ttotal: 18.8s\tremaining: 42.7s\n",
      "306:\tlearn: 0.3078988\ttotal: 18.9s\tremaining: 42.6s\n",
      "307:\tlearn: 0.3078522\ttotal: 18.9s\tremaining: 42.5s\n",
      "308:\tlearn: 0.3078481\ttotal: 19s\tremaining: 42.5s\n",
      "309:\tlearn: 0.3077678\ttotal: 19.1s\tremaining: 42.4s\n",
      "310:\tlearn: 0.3076946\ttotal: 19.1s\tremaining: 42.3s\n",
      "311:\tlearn: 0.3076809\ttotal: 19.2s\tremaining: 42.3s\n",
      "312:\tlearn: 0.3076618\ttotal: 19.2s\tremaining: 42.2s\n",
      "313:\tlearn: 0.3076567\ttotal: 19.3s\tremaining: 42.1s\n",
      "314:\tlearn: 0.3076552\ttotal: 19.3s\tremaining: 42s\n",
      "315:\tlearn: 0.3075058\ttotal: 19.4s\tremaining: 42s\n",
      "316:\tlearn: 0.3074937\ttotal: 19.5s\tremaining: 41.9s\n",
      "317:\tlearn: 0.3074489\ttotal: 19.5s\tremaining: 41.9s\n",
      "318:\tlearn: 0.3074092\ttotal: 19.6s\tremaining: 41.8s\n",
      "319:\tlearn: 0.3073826\ttotal: 19.6s\tremaining: 41.7s\n",
      "320:\tlearn: 0.3072630\ttotal: 19.7s\tremaining: 41.7s\n",
      "321:\tlearn: 0.3072593\ttotal: 19.8s\tremaining: 41.6s\n",
      "322:\tlearn: 0.3072284\ttotal: 19.8s\tremaining: 41.6s\n",
      "323:\tlearn: 0.3071688\ttotal: 19.9s\tremaining: 41.5s\n",
      "324:\tlearn: 0.3071590\ttotal: 19.9s\tremaining: 41.4s\n",
      "325:\tlearn: 0.3071377\ttotal: 20s\tremaining: 41.3s\n",
      "326:\tlearn: 0.3070884\ttotal: 20.1s\tremaining: 41.3s\n",
      "327:\tlearn: 0.3070457\ttotal: 20.1s\tremaining: 41.2s\n",
      "328:\tlearn: 0.3069295\ttotal: 20.2s\tremaining: 41.1s\n",
      "329:\tlearn: 0.3068999\ttotal: 20.2s\tremaining: 41.1s\n",
      "330:\tlearn: 0.3067665\ttotal: 20.3s\tremaining: 41s\n",
      "331:\tlearn: 0.3067454\ttotal: 20.4s\tremaining: 40.9s\n",
      "332:\tlearn: 0.3065472\ttotal: 20.4s\tremaining: 40.9s\n",
      "333:\tlearn: 0.3064962\ttotal: 20.5s\tremaining: 40.8s\n",
      "334:\tlearn: 0.3064481\ttotal: 20.5s\tremaining: 40.8s\n",
      "335:\tlearn: 0.3064361\ttotal: 20.6s\tremaining: 40.7s\n",
      "336:\tlearn: 0.3064022\ttotal: 20.7s\tremaining: 40.7s\n",
      "337:\tlearn: 0.3063598\ttotal: 20.7s\tremaining: 40.6s\n",
      "338:\tlearn: 0.3063372\ttotal: 20.8s\tremaining: 40.5s\n",
      "339:\tlearn: 0.3062353\ttotal: 20.8s\tremaining: 40.5s\n",
      "340:\tlearn: 0.3061632\ttotal: 20.9s\tremaining: 40.4s\n",
      "341:\tlearn: 0.3061285\ttotal: 21s\tremaining: 40.3s\n",
      "342:\tlearn: 0.3059647\ttotal: 21s\tremaining: 40.3s\n",
      "343:\tlearn: 0.3059013\ttotal: 21.1s\tremaining: 40.2s\n",
      "344:\tlearn: 0.3058635\ttotal: 21.2s\tremaining: 40.2s\n",
      "345:\tlearn: 0.3058271\ttotal: 21.2s\tremaining: 40.1s\n",
      "346:\tlearn: 0.3057765\ttotal: 21.3s\tremaining: 40.1s\n",
      "347:\tlearn: 0.3057160\ttotal: 21.4s\tremaining: 40s\n",
      "348:\tlearn: 0.3057061\ttotal: 21.4s\tremaining: 40s\n",
      "349:\tlearn: 0.3056848\ttotal: 21.5s\tremaining: 39.9s\n",
      "350:\tlearn: 0.3056644\ttotal: 21.6s\tremaining: 39.9s\n",
      "351:\tlearn: 0.3056348\ttotal: 21.6s\tremaining: 39.8s\n",
      "352:\tlearn: 0.3054501\ttotal: 21.7s\tremaining: 39.8s\n",
      "353:\tlearn: 0.3054244\ttotal: 21.8s\tremaining: 39.8s\n",
      "354:\tlearn: 0.3053841\ttotal: 21.8s\tremaining: 39.7s\n",
      "355:\tlearn: 0.3053600\ttotal: 21.9s\tremaining: 39.6s\n",
      "356:\tlearn: 0.3052885\ttotal: 22s\tremaining: 39.6s\n",
      "357:\tlearn: 0.3052231\ttotal: 22s\tremaining: 39.5s\n",
      "358:\tlearn: 0.3051796\ttotal: 22.1s\tremaining: 39.4s\n",
      "359:\tlearn: 0.3051409\ttotal: 22.2s\tremaining: 39.4s\n",
      "360:\tlearn: 0.3050458\ttotal: 22.2s\tremaining: 39.4s\n",
      "361:\tlearn: 0.3049656\ttotal: 22.3s\tremaining: 39.4s\n",
      "362:\tlearn: 0.3049380\ttotal: 22.4s\tremaining: 39.4s\n",
      "363:\tlearn: 0.3048137\ttotal: 22.5s\tremaining: 39.3s\n",
      "364:\tlearn: 0.3047636\ttotal: 22.6s\tremaining: 39.3s\n",
      "365:\tlearn: 0.3047046\ttotal: 22.6s\tremaining: 39.2s\n",
      "366:\tlearn: 0.3046263\ttotal: 22.7s\tremaining: 39.2s\n",
      "367:\tlearn: 0.3045900\ttotal: 22.8s\tremaining: 39.1s\n",
      "368:\tlearn: 0.3045494\ttotal: 22.8s\tremaining: 39s\n",
      "369:\tlearn: 0.3044913\ttotal: 22.9s\tremaining: 39s\n",
      "370:\tlearn: 0.3042652\ttotal: 23s\tremaining: 39s\n",
      "371:\tlearn: 0.3042263\ttotal: 23.1s\tremaining: 38.9s\n",
      "372:\tlearn: 0.3041247\ttotal: 23.1s\tremaining: 38.9s\n",
      "373:\tlearn: 0.3040781\ttotal: 23.2s\tremaining: 38.8s\n",
      "374:\tlearn: 0.3040387\ttotal: 23.2s\tremaining: 38.7s\n",
      "375:\tlearn: 0.3039897\ttotal: 23.3s\tremaining: 38.7s\n",
      "376:\tlearn: 0.3039538\ttotal: 23.3s\tremaining: 38.6s\n",
      "377:\tlearn: 0.3038456\ttotal: 23.4s\tremaining: 38.5s\n",
      "378:\tlearn: 0.3037615\ttotal: 23.5s\tremaining: 38.4s\n",
      "379:\tlearn: 0.3037612\ttotal: 23.5s\tremaining: 38.4s\n",
      "380:\tlearn: 0.3037351\ttotal: 23.6s\tremaining: 38.3s\n",
      "381:\tlearn: 0.3037060\ttotal: 23.6s\tremaining: 38.2s\n",
      "382:\tlearn: 0.3035787\ttotal: 23.7s\tremaining: 38.1s\n",
      "383:\tlearn: 0.3035556\ttotal: 23.7s\tremaining: 38.1s\n",
      "384:\tlearn: 0.3034995\ttotal: 23.8s\tremaining: 38s\n",
      "385:\tlearn: 0.3034283\ttotal: 23.8s\tremaining: 37.9s\n",
      "386:\tlearn: 0.3033065\ttotal: 23.9s\tremaining: 37.8s\n",
      "387:\tlearn: 0.3032196\ttotal: 23.9s\tremaining: 37.8s\n",
      "388:\tlearn: 0.3031568\ttotal: 24s\tremaining: 37.7s\n",
      "389:\tlearn: 0.3030844\ttotal: 24s\tremaining: 37.6s\n",
      "390:\tlearn: 0.3030447\ttotal: 24.1s\tremaining: 37.5s\n",
      "391:\tlearn: 0.3030267\ttotal: 24.1s\tremaining: 37.5s\n",
      "392:\tlearn: 0.3030088\ttotal: 24.2s\tremaining: 37.4s\n",
      "393:\tlearn: 0.3029447\ttotal: 24.3s\tremaining: 37.4s\n",
      "394:\tlearn: 0.3029259\ttotal: 24.4s\tremaining: 37.4s\n",
      "395:\tlearn: 0.3028931\ttotal: 24.5s\tremaining: 37.4s\n",
      "396:\tlearn: 0.3028750\ttotal: 24.6s\tremaining: 37.4s\n",
      "397:\tlearn: 0.3028324\ttotal: 24.7s\tremaining: 37.3s\n",
      "398:\tlearn: 0.3028124\ttotal: 24.7s\tremaining: 37.2s\n",
      "399:\tlearn: 0.3028120\ttotal: 24.8s\tremaining: 37.2s\n",
      "400:\tlearn: 0.3027920\ttotal: 24.8s\tremaining: 37.1s\n",
      "401:\tlearn: 0.3027468\ttotal: 24.9s\tremaining: 37.1s\n",
      "402:\tlearn: 0.3027131\ttotal: 25s\tremaining: 37s\n",
      "403:\tlearn: 0.3026719\ttotal: 25.1s\tremaining: 37s\n",
      "404:\tlearn: 0.3026504\ttotal: 25.1s\tremaining: 36.9s\n",
      "405:\tlearn: 0.3026331\ttotal: 25.2s\tremaining: 36.8s\n",
      "406:\tlearn: 0.3025425\ttotal: 25.2s\tremaining: 36.8s\n",
      "407:\tlearn: 0.3025168\ttotal: 25.3s\tremaining: 36.7s\n",
      "408:\tlearn: 0.3024679\ttotal: 25.4s\tremaining: 36.7s\n",
      "409:\tlearn: 0.3024401\ttotal: 25.4s\tremaining: 36.6s\n",
      "410:\tlearn: 0.3024136\ttotal: 25.5s\tremaining: 36.5s\n",
      "411:\tlearn: 0.3023967\ttotal: 25.6s\tremaining: 36.5s\n",
      "412:\tlearn: 0.3023491\ttotal: 25.7s\tremaining: 36.5s\n",
      "413:\tlearn: 0.3023260\ttotal: 25.7s\tremaining: 36.4s\n",
      "414:\tlearn: 0.3022924\ttotal: 25.8s\tremaining: 36.4s\n",
      "415:\tlearn: 0.3022354\ttotal: 25.9s\tremaining: 36.3s\n",
      "416:\tlearn: 0.3022247\ttotal: 25.9s\tremaining: 36.3s\n",
      "417:\tlearn: 0.3021633\ttotal: 26s\tremaining: 36.2s\n",
      "418:\tlearn: 0.3021403\ttotal: 26.1s\tremaining: 36.2s\n",
      "419:\tlearn: 0.3021029\ttotal: 26.2s\tremaining: 36.1s\n",
      "420:\tlearn: 0.3020831\ttotal: 26.2s\tremaining: 36.1s\n",
      "421:\tlearn: 0.3020477\ttotal: 26.3s\tremaining: 36s\n",
      "422:\tlearn: 0.3020306\ttotal: 26.4s\tremaining: 36s\n",
      "423:\tlearn: 0.3020210\ttotal: 26.4s\tremaining: 35.9s\n",
      "424:\tlearn: 0.3019784\ttotal: 26.5s\tremaining: 35.8s\n",
      "425:\tlearn: 0.3019584\ttotal: 26.5s\tremaining: 35.7s\n",
      "426:\tlearn: 0.3019295\ttotal: 26.6s\tremaining: 35.7s\n",
      "427:\tlearn: 0.3019079\ttotal: 26.6s\tremaining: 35.6s\n",
      "428:\tlearn: 0.3018465\ttotal: 26.8s\tremaining: 35.6s\n",
      "429:\tlearn: 0.3017850\ttotal: 26.8s\tremaining: 35.6s\n",
      "430:\tlearn: 0.3017392\ttotal: 26.9s\tremaining: 35.5s\n",
      "431:\tlearn: 0.3016929\ttotal: 27s\tremaining: 35.4s\n",
      "432:\tlearn: 0.3016794\ttotal: 27s\tremaining: 35.4s\n",
      "433:\tlearn: 0.3015543\ttotal: 27.1s\tremaining: 35.3s\n",
      "434:\tlearn: 0.3015142\ttotal: 27.2s\tremaining: 35.3s\n",
      "435:\tlearn: 0.3014772\ttotal: 27.2s\tremaining: 35.2s\n",
      "436:\tlearn: 0.3014360\ttotal: 27.3s\tremaining: 35.2s\n",
      "437:\tlearn: 0.3014132\ttotal: 27.4s\tremaining: 35.1s\n",
      "438:\tlearn: 0.3012789\ttotal: 27.4s\tremaining: 35.1s\n",
      "439:\tlearn: 0.3012597\ttotal: 27.5s\tremaining: 35s\n",
      "440:\tlearn: 0.3012524\ttotal: 27.6s\tremaining: 34.9s\n",
      "441:\tlearn: 0.3012308\ttotal: 27.6s\tremaining: 34.9s\n",
      "442:\tlearn: 0.3012158\ttotal: 27.7s\tremaining: 34.8s\n",
      "443:\tlearn: 0.3011325\ttotal: 27.8s\tremaining: 34.8s\n",
      "444:\tlearn: 0.3010735\ttotal: 27.8s\tremaining: 34.7s\n",
      "445:\tlearn: 0.3010363\ttotal: 27.9s\tremaining: 34.7s\n",
      "446:\tlearn: 0.3010157\ttotal: 28s\tremaining: 34.6s\n",
      "447:\tlearn: 0.3009491\ttotal: 28.1s\tremaining: 34.6s\n",
      "448:\tlearn: 0.3009323\ttotal: 28.1s\tremaining: 34.5s\n",
      "449:\tlearn: 0.3008292\ttotal: 28.2s\tremaining: 34.5s\n",
      "450:\tlearn: 0.3008126\ttotal: 28.3s\tremaining: 34.4s\n",
      "451:\tlearn: 0.3007964\ttotal: 28.4s\tremaining: 34.4s\n",
      "452:\tlearn: 0.3007789\ttotal: 28.4s\tremaining: 34.3s\n",
      "453:\tlearn: 0.3007396\ttotal: 28.5s\tremaining: 34.3s\n",
      "454:\tlearn: 0.3006733\ttotal: 28.6s\tremaining: 34.2s\n",
      "455:\tlearn: 0.3003697\ttotal: 28.7s\tremaining: 34.2s\n",
      "456:\tlearn: 0.3002984\ttotal: 28.8s\tremaining: 34.2s\n",
      "457:\tlearn: 0.3002894\ttotal: 28.8s\tremaining: 34.1s\n",
      "458:\tlearn: 0.3002428\ttotal: 28.9s\tremaining: 34s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459:\tlearn: 0.3001160\ttotal: 29s\tremaining: 34s\n",
      "460:\tlearn: 0.3000794\ttotal: 29s\tremaining: 33.9s\n",
      "461:\tlearn: 0.3000353\ttotal: 29.1s\tremaining: 33.9s\n",
      "462:\tlearn: 0.3000006\ttotal: 29.2s\tremaining: 33.8s\n",
      "463:\tlearn: 0.2999682\ttotal: 29.2s\tremaining: 33.8s\n",
      "464:\tlearn: 0.2999544\ttotal: 29.3s\tremaining: 33.7s\n",
      "465:\tlearn: 0.2999377\ttotal: 29.4s\tremaining: 33.6s\n",
      "466:\tlearn: 0.2999219\ttotal: 29.4s\tremaining: 33.6s\n",
      "467:\tlearn: 0.2998941\ttotal: 29.5s\tremaining: 33.5s\n",
      "468:\tlearn: 0.2998799\ttotal: 29.6s\tremaining: 33.5s\n",
      "469:\tlearn: 0.2996671\ttotal: 29.6s\tremaining: 33.4s\n",
      "470:\tlearn: 0.2996306\ttotal: 29.7s\tremaining: 33.4s\n",
      "471:\tlearn: 0.2996116\ttotal: 29.8s\tremaining: 33.3s\n",
      "472:\tlearn: 0.2996024\ttotal: 29.8s\tremaining: 33.3s\n",
      "473:\tlearn: 0.2995440\ttotal: 29.9s\tremaining: 33.2s\n",
      "474:\tlearn: 0.2995152\ttotal: 30s\tremaining: 33.1s\n",
      "475:\tlearn: 0.2994241\ttotal: 30s\tremaining: 33.1s\n",
      "476:\tlearn: 0.2993420\ttotal: 30.1s\tremaining: 33s\n",
      "477:\tlearn: 0.2992674\ttotal: 30.2s\tremaining: 32.9s\n",
      "478:\tlearn: 0.2992287\ttotal: 30.2s\tremaining: 32.9s\n",
      "479:\tlearn: 0.2991798\ttotal: 30.3s\tremaining: 32.8s\n",
      "480:\tlearn: 0.2991407\ttotal: 30.3s\tremaining: 32.7s\n",
      "481:\tlearn: 0.2991152\ttotal: 30.4s\tremaining: 32.7s\n",
      "482:\tlearn: 0.2990146\ttotal: 30.5s\tremaining: 32.6s\n",
      "483:\tlearn: 0.2989259\ttotal: 30.6s\tremaining: 32.6s\n",
      "484:\tlearn: 0.2988857\ttotal: 30.6s\tremaining: 32.5s\n",
      "485:\tlearn: 0.2988433\ttotal: 30.7s\tremaining: 32.5s\n",
      "486:\tlearn: 0.2987911\ttotal: 30.8s\tremaining: 32.4s\n",
      "487:\tlearn: 0.2987801\ttotal: 30.8s\tremaining: 32.3s\n",
      "488:\tlearn: 0.2987076\ttotal: 30.9s\tremaining: 32.3s\n",
      "489:\tlearn: 0.2986973\ttotal: 30.9s\tremaining: 32.2s\n",
      "490:\tlearn: 0.2985896\ttotal: 31s\tremaining: 32.2s\n",
      "491:\tlearn: 0.2985766\ttotal: 31.1s\tremaining: 32.1s\n",
      "492:\tlearn: 0.2985626\ttotal: 31.2s\tremaining: 32s\n",
      "493:\tlearn: 0.2985500\ttotal: 31.2s\tremaining: 32s\n",
      "494:\tlearn: 0.2985114\ttotal: 31.3s\tremaining: 31.9s\n",
      "495:\tlearn: 0.2984787\ttotal: 31.4s\tremaining: 31.9s\n",
      "496:\tlearn: 0.2983935\ttotal: 31.4s\tremaining: 31.8s\n",
      "497:\tlearn: 0.2983558\ttotal: 31.5s\tremaining: 31.7s\n",
      "498:\tlearn: 0.2983529\ttotal: 31.6s\tremaining: 31.7s\n",
      "499:\tlearn: 0.2983103\ttotal: 31.6s\tremaining: 31.6s\n",
      "500:\tlearn: 0.2982815\ttotal: 31.7s\tremaining: 31.6s\n",
      "501:\tlearn: 0.2982701\ttotal: 31.7s\tremaining: 31.5s\n",
      "502:\tlearn: 0.2981880\ttotal: 31.8s\tremaining: 31.4s\n",
      "503:\tlearn: 0.2981651\ttotal: 31.9s\tremaining: 31.4s\n",
      "504:\tlearn: 0.2981395\ttotal: 32s\tremaining: 31.3s\n",
      "505:\tlearn: 0.2981258\ttotal: 32s\tremaining: 31.3s\n",
      "506:\tlearn: 0.2980083\ttotal: 32.1s\tremaining: 31.2s\n",
      "507:\tlearn: 0.2979306\ttotal: 32.1s\tremaining: 31.1s\n",
      "508:\tlearn: 0.2968410\ttotal: 32.2s\tremaining: 31.1s\n",
      "509:\tlearn: 0.2967794\ttotal: 32.3s\tremaining: 31s\n",
      "510:\tlearn: 0.2967364\ttotal: 32.3s\tremaining: 31s\n",
      "511:\tlearn: 0.2966963\ttotal: 32.4s\tremaining: 30.9s\n",
      "512:\tlearn: 0.2966496\ttotal: 32.5s\tremaining: 30.8s\n",
      "513:\tlearn: 0.2966015\ttotal: 32.5s\tremaining: 30.8s\n",
      "514:\tlearn: 0.2965709\ttotal: 32.6s\tremaining: 30.7s\n",
      "515:\tlearn: 0.2965414\ttotal: 32.7s\tremaining: 30.6s\n",
      "516:\tlearn: 0.2964624\ttotal: 32.7s\tremaining: 30.6s\n",
      "517:\tlearn: 0.2964054\ttotal: 32.8s\tremaining: 30.5s\n",
      "518:\tlearn: 0.2963844\ttotal: 32.9s\tremaining: 30.5s\n",
      "519:\tlearn: 0.2963185\ttotal: 33s\tremaining: 30.4s\n",
      "520:\tlearn: 0.2962563\ttotal: 33s\tremaining: 30.4s\n",
      "521:\tlearn: 0.2956149\ttotal: 33.1s\tremaining: 30.3s\n",
      "522:\tlearn: 0.2955630\ttotal: 33.2s\tremaining: 30.3s\n",
      "523:\tlearn: 0.2954692\ttotal: 33.3s\tremaining: 30.2s\n",
      "524:\tlearn: 0.2953494\ttotal: 33.3s\tremaining: 30.2s\n",
      "525:\tlearn: 0.2952076\ttotal: 33.4s\tremaining: 30.1s\n",
      "526:\tlearn: 0.2950436\ttotal: 33.5s\tremaining: 30s\n",
      "527:\tlearn: 0.2950011\ttotal: 33.5s\tremaining: 30s\n",
      "528:\tlearn: 0.2949812\ttotal: 33.6s\tremaining: 29.9s\n",
      "529:\tlearn: 0.2949398\ttotal: 33.6s\tremaining: 29.8s\n",
      "530:\tlearn: 0.2949079\ttotal: 33.7s\tremaining: 29.8s\n",
      "531:\tlearn: 0.2948394\ttotal: 33.8s\tremaining: 29.7s\n",
      "532:\tlearn: 0.2948118\ttotal: 33.9s\tremaining: 29.7s\n",
      "533:\tlearn: 0.2947703\ttotal: 33.9s\tremaining: 29.6s\n",
      "534:\tlearn: 0.2947481\ttotal: 34s\tremaining: 29.5s\n",
      "535:\tlearn: 0.2947151\ttotal: 34s\tremaining: 29.5s\n",
      "536:\tlearn: 0.2946556\ttotal: 34.1s\tremaining: 29.4s\n",
      "537:\tlearn: 0.2946122\ttotal: 34.2s\tremaining: 29.3s\n",
      "538:\tlearn: 0.2945571\ttotal: 34.2s\tremaining: 29.3s\n",
      "539:\tlearn: 0.2945126\ttotal: 34.3s\tremaining: 29.2s\n",
      "540:\tlearn: 0.2944808\ttotal: 34.4s\tremaining: 29.2s\n",
      "541:\tlearn: 0.2944509\ttotal: 34.4s\tremaining: 29.1s\n",
      "542:\tlearn: 0.2935380\ttotal: 34.5s\tremaining: 29.1s\n",
      "543:\tlearn: 0.2935061\ttotal: 34.6s\tremaining: 29s\n",
      "544:\tlearn: 0.2934750\ttotal: 34.7s\tremaining: 28.9s\n",
      "545:\tlearn: 0.2934559\ttotal: 34.7s\tremaining: 28.9s\n",
      "546:\tlearn: 0.2932988\ttotal: 34.8s\tremaining: 28.8s\n",
      "547:\tlearn: 0.2932687\ttotal: 34.9s\tremaining: 28.8s\n",
      "548:\tlearn: 0.2932057\ttotal: 34.9s\tremaining: 28.7s\n",
      "549:\tlearn: 0.2931772\ttotal: 35s\tremaining: 28.6s\n",
      "550:\tlearn: 0.2931232\ttotal: 35s\tremaining: 28.6s\n",
      "551:\tlearn: 0.2930807\ttotal: 35.1s\tremaining: 28.5s\n",
      "552:\tlearn: 0.2930600\ttotal: 35.2s\tremaining: 28.4s\n",
      "553:\tlearn: 0.2930274\ttotal: 35.2s\tremaining: 28.4s\n",
      "554:\tlearn: 0.2928552\ttotal: 35.3s\tremaining: 28.3s\n",
      "555:\tlearn: 0.2928171\ttotal: 35.4s\tremaining: 28.2s\n",
      "556:\tlearn: 0.2927522\ttotal: 35.4s\tremaining: 28.2s\n",
      "557:\tlearn: 0.2926984\ttotal: 35.5s\tremaining: 28.1s\n",
      "558:\tlearn: 0.2926648\ttotal: 35.6s\tremaining: 28.1s\n",
      "559:\tlearn: 0.2925243\ttotal: 35.6s\tremaining: 28s\n",
      "560:\tlearn: 0.2924755\ttotal: 35.7s\tremaining: 27.9s\n",
      "561:\tlearn: 0.2924243\ttotal: 35.8s\tremaining: 27.9s\n",
      "562:\tlearn: 0.2922624\ttotal: 35.9s\tremaining: 27.8s\n",
      "563:\tlearn: 0.2922213\ttotal: 35.9s\tremaining: 27.8s\n",
      "564:\tlearn: 0.2921803\ttotal: 36s\tremaining: 27.7s\n",
      "565:\tlearn: 0.2921504\ttotal: 36.1s\tremaining: 27.7s\n",
      "566:\tlearn: 0.2920945\ttotal: 36.1s\tremaining: 27.6s\n",
      "567:\tlearn: 0.2919749\ttotal: 36.2s\tremaining: 27.5s\n",
      "568:\tlearn: 0.2918673\ttotal: 36.3s\tremaining: 27.5s\n",
      "569:\tlearn: 0.2918534\ttotal: 36.3s\tremaining: 27.4s\n",
      "570:\tlearn: 0.2914680\ttotal: 36.4s\tremaining: 27.4s\n",
      "571:\tlearn: 0.2914379\ttotal: 36.5s\tremaining: 27.3s\n",
      "572:\tlearn: 0.2913741\ttotal: 36.6s\tremaining: 27.2s\n",
      "573:\tlearn: 0.2913428\ttotal: 36.6s\tremaining: 27.2s\n",
      "574:\tlearn: 0.2912976\ttotal: 36.7s\tremaining: 27.1s\n",
      "575:\tlearn: 0.2912679\ttotal: 36.7s\tremaining: 27s\n",
      "576:\tlearn: 0.2912435\ttotal: 36.8s\tremaining: 27s\n",
      "577:\tlearn: 0.2911831\ttotal: 36.9s\tremaining: 26.9s\n",
      "578:\tlearn: 0.2910425\ttotal: 36.9s\tremaining: 26.9s\n",
      "579:\tlearn: 0.2910230\ttotal: 37s\tremaining: 26.8s\n",
      "580:\tlearn: 0.2909989\ttotal: 37.1s\tremaining: 26.7s\n",
      "581:\tlearn: 0.2906954\ttotal: 37.1s\tremaining: 26.7s\n",
      "582:\tlearn: 0.2906420\ttotal: 37.2s\tremaining: 26.6s\n",
      "583:\tlearn: 0.2906225\ttotal: 37.3s\tremaining: 26.5s\n",
      "584:\tlearn: 0.2905080\ttotal: 37.3s\tremaining: 26.5s\n",
      "585:\tlearn: 0.2904750\ttotal: 37.4s\tremaining: 26.4s\n",
      "586:\tlearn: 0.2904622\ttotal: 37.4s\tremaining: 26.3s\n",
      "587:\tlearn: 0.2904202\ttotal: 37.5s\tremaining: 26.3s\n",
      "588:\tlearn: 0.2903520\ttotal: 37.6s\tremaining: 26.2s\n",
      "589:\tlearn: 0.2903299\ttotal: 37.6s\tremaining: 26.1s\n",
      "590:\tlearn: 0.2903092\ttotal: 37.7s\tremaining: 26.1s\n",
      "591:\tlearn: 0.2902843\ttotal: 37.8s\tremaining: 26s\n",
      "592:\tlearn: 0.2902601\ttotal: 37.8s\tremaining: 26s\n",
      "593:\tlearn: 0.2902277\ttotal: 37.9s\tremaining: 25.9s\n",
      "594:\tlearn: 0.2902097\ttotal: 38s\tremaining: 25.8s\n",
      "595:\tlearn: 0.2901714\ttotal: 38s\tremaining: 25.8s\n",
      "596:\tlearn: 0.2901481\ttotal: 38.1s\tremaining: 25.7s\n",
      "597:\tlearn: 0.2900131\ttotal: 38.1s\tremaining: 25.6s\n",
      "598:\tlearn: 0.2899328\ttotal: 38.2s\tremaining: 25.6s\n",
      "599:\tlearn: 0.2898192\ttotal: 38.3s\tremaining: 25.5s\n",
      "600:\tlearn: 0.2897940\ttotal: 38.3s\tremaining: 25.5s\n",
      "601:\tlearn: 0.2896947\ttotal: 38.4s\tremaining: 25.4s\n",
      "602:\tlearn: 0.2896462\ttotal: 38.5s\tremaining: 25.3s\n",
      "603:\tlearn: 0.2896183\ttotal: 38.5s\tremaining: 25.3s\n",
      "604:\tlearn: 0.2895951\ttotal: 38.6s\tremaining: 25.2s\n",
      "605:\tlearn: 0.2895790\ttotal: 38.7s\tremaining: 25.1s\n",
      "606:\tlearn: 0.2895176\ttotal: 38.7s\tremaining: 25.1s\n",
      "607:\tlearn: 0.2894079\ttotal: 38.8s\tremaining: 25s\n",
      "608:\tlearn: 0.2893962\ttotal: 38.9s\tremaining: 24.9s\n",
      "609:\tlearn: 0.2893812\ttotal: 38.9s\tremaining: 24.9s\n",
      "610:\tlearn: 0.2893380\ttotal: 39s\tremaining: 24.8s\n",
      "611:\tlearn: 0.2893172\ttotal: 39.1s\tremaining: 24.8s\n",
      "612:\tlearn: 0.2892726\ttotal: 39.1s\tremaining: 24.7s\n",
      "613:\tlearn: 0.2892318\ttotal: 39.2s\tremaining: 24.6s\n",
      "614:\tlearn: 0.2892143\ttotal: 39.2s\tremaining: 24.6s\n",
      "615:\tlearn: 0.2891997\ttotal: 39.3s\tremaining: 24.5s\n",
      "616:\tlearn: 0.2891652\ttotal: 39.4s\tremaining: 24.4s\n",
      "617:\tlearn: 0.2891310\ttotal: 39.4s\tremaining: 24.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "618:\tlearn: 0.2891100\ttotal: 39.5s\tremaining: 24.3s\n",
      "619:\tlearn: 0.2890808\ttotal: 39.6s\tremaining: 24.2s\n",
      "620:\tlearn: 0.2890146\ttotal: 39.6s\tremaining: 24.2s\n",
      "621:\tlearn: 0.2890059\ttotal: 39.7s\tremaining: 24.1s\n",
      "622:\tlearn: 0.2889708\ttotal: 39.7s\tremaining: 24.1s\n",
      "623:\tlearn: 0.2889275\ttotal: 39.8s\tremaining: 24s\n",
      "624:\tlearn: 0.2888998\ttotal: 39.9s\tremaining: 23.9s\n",
      "625:\tlearn: 0.2887982\ttotal: 39.9s\tremaining: 23.9s\n",
      "626:\tlearn: 0.2887866\ttotal: 40s\tremaining: 23.8s\n",
      "627:\tlearn: 0.2887689\ttotal: 40.1s\tremaining: 23.7s\n",
      "628:\tlearn: 0.2887570\ttotal: 40.1s\tremaining: 23.7s\n",
      "629:\tlearn: 0.2887419\ttotal: 40.2s\tremaining: 23.6s\n",
      "630:\tlearn: 0.2887082\ttotal: 40.2s\tremaining: 23.5s\n",
      "631:\tlearn: 0.2886877\ttotal: 40.3s\tremaining: 23.5s\n",
      "632:\tlearn: 0.2886746\ttotal: 40.4s\tremaining: 23.4s\n",
      "633:\tlearn: 0.2886411\ttotal: 40.4s\tremaining: 23.3s\n",
      "634:\tlearn: 0.2886276\ttotal: 40.5s\tremaining: 23.3s\n",
      "635:\tlearn: 0.2885650\ttotal: 40.6s\tremaining: 23.2s\n",
      "636:\tlearn: 0.2885308\ttotal: 40.6s\tremaining: 23.2s\n",
      "637:\tlearn: 0.2884829\ttotal: 40.7s\tremaining: 23.1s\n",
      "638:\tlearn: 0.2884679\ttotal: 40.8s\tremaining: 23.1s\n",
      "639:\tlearn: 0.2884499\ttotal: 40.9s\tremaining: 23s\n",
      "640:\tlearn: 0.2884253\ttotal: 41s\tremaining: 23s\n",
      "641:\tlearn: 0.2884099\ttotal: 41.1s\tremaining: 22.9s\n",
      "642:\tlearn: 0.2883627\ttotal: 41.1s\tremaining: 22.8s\n",
      "643:\tlearn: 0.2883172\ttotal: 41.2s\tremaining: 22.8s\n",
      "644:\tlearn: 0.2882959\ttotal: 41.3s\tremaining: 22.7s\n",
      "645:\tlearn: 0.2881413\ttotal: 41.4s\tremaining: 22.7s\n",
      "646:\tlearn: 0.2881304\ttotal: 41.5s\tremaining: 22.6s\n",
      "647:\tlearn: 0.2881206\ttotal: 41.5s\tremaining: 22.6s\n",
      "648:\tlearn: 0.2881024\ttotal: 41.6s\tremaining: 22.5s\n",
      "649:\tlearn: 0.2880617\ttotal: 41.7s\tremaining: 22.5s\n",
      "650:\tlearn: 0.2880518\ttotal: 41.8s\tremaining: 22.4s\n",
      "651:\tlearn: 0.2880425\ttotal: 41.9s\tremaining: 22.4s\n",
      "652:\tlearn: 0.2880157\ttotal: 42s\tremaining: 22.3s\n",
      "653:\tlearn: 0.2880012\ttotal: 42.1s\tremaining: 22.3s\n",
      "654:\tlearn: 0.2879632\ttotal: 42.2s\tremaining: 22.2s\n",
      "655:\tlearn: 0.2878875\ttotal: 42.3s\tremaining: 22.2s\n",
      "656:\tlearn: 0.2878358\ttotal: 42.4s\tremaining: 22.1s\n",
      "657:\tlearn: 0.2877967\ttotal: 42.4s\tremaining: 22.1s\n",
      "658:\tlearn: 0.2877557\ttotal: 42.5s\tremaining: 22s\n",
      "659:\tlearn: 0.2876686\ttotal: 42.6s\tremaining: 21.9s\n",
      "660:\tlearn: 0.2876559\ttotal: 42.7s\tremaining: 21.9s\n",
      "661:\tlearn: 0.2876419\ttotal: 42.8s\tremaining: 21.8s\n",
      "662:\tlearn: 0.2876100\ttotal: 42.8s\tremaining: 21.8s\n",
      "663:\tlearn: 0.2875791\ttotal: 42.9s\tremaining: 21.7s\n",
      "664:\tlearn: 0.2875464\ttotal: 42.9s\tremaining: 21.6s\n",
      "665:\tlearn: 0.2874612\ttotal: 43s\tremaining: 21.6s\n",
      "666:\tlearn: 0.2874389\ttotal: 43.1s\tremaining: 21.5s\n",
      "667:\tlearn: 0.2874236\ttotal: 43.1s\tremaining: 21.4s\n",
      "668:\tlearn: 0.2873517\ttotal: 43.2s\tremaining: 21.4s\n",
      "669:\tlearn: 0.2873261\ttotal: 43.2s\tremaining: 21.3s\n",
      "670:\tlearn: 0.2873112\ttotal: 43.3s\tremaining: 21.2s\n",
      "671:\tlearn: 0.2872812\ttotal: 43.4s\tremaining: 21.2s\n",
      "672:\tlearn: 0.2872672\ttotal: 43.4s\tremaining: 21.1s\n",
      "673:\tlearn: 0.2872254\ttotal: 43.5s\tremaining: 21s\n",
      "674:\tlearn: 0.2872198\ttotal: 43.6s\tremaining: 21s\n",
      "675:\tlearn: 0.2872081\ttotal: 43.6s\tremaining: 20.9s\n",
      "676:\tlearn: 0.2871972\ttotal: 43.7s\tremaining: 20.9s\n",
      "677:\tlearn: 0.2871842\ttotal: 43.8s\tremaining: 20.8s\n",
      "678:\tlearn: 0.2871692\ttotal: 43.8s\tremaining: 20.7s\n",
      "679:\tlearn: 0.2871449\ttotal: 43.9s\tremaining: 20.7s\n",
      "680:\tlearn: 0.2871347\ttotal: 44s\tremaining: 20.6s\n",
      "681:\tlearn: 0.2870521\ttotal: 44s\tremaining: 20.5s\n",
      "682:\tlearn: 0.2869333\ttotal: 44.1s\tremaining: 20.5s\n",
      "683:\tlearn: 0.2868200\ttotal: 44.2s\tremaining: 20.4s\n",
      "684:\tlearn: 0.2868034\ttotal: 44.2s\tremaining: 20.3s\n",
      "685:\tlearn: 0.2867938\ttotal: 44.3s\tremaining: 20.3s\n",
      "686:\tlearn: 0.2867796\ttotal: 44.4s\tremaining: 20.2s\n",
      "687:\tlearn: 0.2867675\ttotal: 44.4s\tremaining: 20.1s\n",
      "688:\tlearn: 0.2867538\ttotal: 44.5s\tremaining: 20.1s\n",
      "689:\tlearn: 0.2867440\ttotal: 44.5s\tremaining: 20s\n",
      "690:\tlearn: 0.2867371\ttotal: 44.6s\tremaining: 19.9s\n",
      "691:\tlearn: 0.2867170\ttotal: 44.7s\tremaining: 19.9s\n",
      "692:\tlearn: 0.2867035\ttotal: 44.7s\tremaining: 19.8s\n",
      "693:\tlearn: 0.2866921\ttotal: 44.8s\tremaining: 19.7s\n",
      "694:\tlearn: 0.2866835\ttotal: 44.8s\tremaining: 19.7s\n",
      "695:\tlearn: 0.2866743\ttotal: 44.9s\tremaining: 19.6s\n",
      "696:\tlearn: 0.2866437\ttotal: 45s\tremaining: 19.5s\n",
      "697:\tlearn: 0.2866378\ttotal: 45s\tremaining: 19.5s\n",
      "698:\tlearn: 0.2866255\ttotal: 45.1s\tremaining: 19.4s\n",
      "699:\tlearn: 0.2866173\ttotal: 45.1s\tremaining: 19.3s\n",
      "700:\tlearn: 0.2865895\ttotal: 45.2s\tremaining: 19.3s\n",
      "701:\tlearn: 0.2865814\ttotal: 45.3s\tremaining: 19.2s\n",
      "702:\tlearn: 0.2865747\ttotal: 45.3s\tremaining: 19.2s\n",
      "703:\tlearn: 0.2865032\ttotal: 45.4s\tremaining: 19.1s\n",
      "704:\tlearn: 0.2864851\ttotal: 45.5s\tremaining: 19s\n",
      "705:\tlearn: 0.2864483\ttotal: 45.5s\tremaining: 19s\n",
      "706:\tlearn: 0.2863757\ttotal: 45.6s\tremaining: 18.9s\n",
      "707:\tlearn: 0.2863664\ttotal: 45.6s\tremaining: 18.8s\n",
      "708:\tlearn: 0.2862334\ttotal: 45.7s\tremaining: 18.8s\n",
      "709:\tlearn: 0.2862191\ttotal: 45.8s\tremaining: 18.7s\n",
      "710:\tlearn: 0.2862056\ttotal: 45.8s\tremaining: 18.6s\n",
      "711:\tlearn: 0.2861947\ttotal: 45.9s\tremaining: 18.6s\n",
      "712:\tlearn: 0.2861897\ttotal: 46s\tremaining: 18.5s\n",
      "713:\tlearn: 0.2861794\ttotal: 46s\tremaining: 18.4s\n",
      "714:\tlearn: 0.2861664\ttotal: 46.1s\tremaining: 18.4s\n",
      "715:\tlearn: 0.2861604\ttotal: 46.1s\tremaining: 18.3s\n",
      "716:\tlearn: 0.2861174\ttotal: 46.2s\tremaining: 18.2s\n",
      "717:\tlearn: 0.2861065\ttotal: 46.2s\tremaining: 18.2s\n",
      "718:\tlearn: 0.2860853\ttotal: 46.3s\tremaining: 18.1s\n",
      "719:\tlearn: 0.2860563\ttotal: 46.4s\tremaining: 18s\n",
      "720:\tlearn: 0.2860459\ttotal: 46.4s\tremaining: 18s\n",
      "721:\tlearn: 0.2860260\ttotal: 46.5s\tremaining: 17.9s\n",
      "722:\tlearn: 0.2860189\ttotal: 46.6s\tremaining: 17.8s\n",
      "723:\tlearn: 0.2860030\ttotal: 46.6s\tremaining: 17.8s\n",
      "724:\tlearn: 0.2859812\ttotal: 46.7s\tremaining: 17.7s\n",
      "725:\tlearn: 0.2859714\ttotal: 46.8s\tremaining: 17.7s\n",
      "726:\tlearn: 0.2858916\ttotal: 46.8s\tremaining: 17.6s\n",
      "727:\tlearn: 0.2858864\ttotal: 46.9s\tremaining: 17.5s\n",
      "728:\tlearn: 0.2858800\ttotal: 47s\tremaining: 17.5s\n",
      "729:\tlearn: 0.2858099\ttotal: 47s\tremaining: 17.4s\n",
      "730:\tlearn: 0.2857750\ttotal: 47.1s\tremaining: 17.3s\n",
      "731:\tlearn: 0.2857656\ttotal: 47.2s\tremaining: 17.3s\n",
      "732:\tlearn: 0.2857636\ttotal: 47.2s\tremaining: 17.2s\n",
      "733:\tlearn: 0.2857280\ttotal: 47.3s\tremaining: 17.1s\n",
      "734:\tlearn: 0.2857135\ttotal: 47.4s\tremaining: 17.1s\n",
      "735:\tlearn: 0.2856722\ttotal: 47.4s\tremaining: 17s\n",
      "736:\tlearn: 0.2856663\ttotal: 47.5s\tremaining: 16.9s\n",
      "737:\tlearn: 0.2856355\ttotal: 47.6s\tremaining: 16.9s\n",
      "738:\tlearn: 0.2856269\ttotal: 47.6s\tremaining: 16.8s\n",
      "739:\tlearn: 0.2856134\ttotal: 47.7s\tremaining: 16.8s\n",
      "740:\tlearn: 0.2855766\ttotal: 47.8s\tremaining: 16.7s\n",
      "741:\tlearn: 0.2855523\ttotal: 47.8s\tremaining: 16.6s\n",
      "742:\tlearn: 0.2855448\ttotal: 47.9s\tremaining: 16.6s\n",
      "743:\tlearn: 0.2854046\ttotal: 48s\tremaining: 16.5s\n",
      "744:\tlearn: 0.2853978\ttotal: 48s\tremaining: 16.4s\n",
      "745:\tlearn: 0.2853909\ttotal: 48.1s\tremaining: 16.4s\n",
      "746:\tlearn: 0.2853725\ttotal: 48.2s\tremaining: 16.3s\n",
      "747:\tlearn: 0.2853409\ttotal: 48.3s\tremaining: 16.3s\n",
      "748:\tlearn: 0.2853307\ttotal: 48.3s\tremaining: 16.2s\n",
      "749:\tlearn: 0.2846561\ttotal: 48.4s\tremaining: 16.1s\n",
      "750:\tlearn: 0.2846390\ttotal: 48.5s\tremaining: 16.1s\n",
      "751:\tlearn: 0.2846100\ttotal: 48.5s\tremaining: 16s\n",
      "752:\tlearn: 0.2845936\ttotal: 48.6s\tremaining: 15.9s\n",
      "753:\tlearn: 0.2845526\ttotal: 48.7s\tremaining: 15.9s\n",
      "754:\tlearn: 0.2845280\ttotal: 48.8s\tremaining: 15.8s\n",
      "755:\tlearn: 0.2845000\ttotal: 48.8s\tremaining: 15.8s\n",
      "756:\tlearn: 0.2844882\ttotal: 48.9s\tremaining: 15.7s\n",
      "757:\tlearn: 0.2844562\ttotal: 48.9s\tremaining: 15.6s\n",
      "758:\tlearn: 0.2844260\ttotal: 49s\tremaining: 15.6s\n",
      "759:\tlearn: 0.2843491\ttotal: 49.1s\tremaining: 15.5s\n",
      "760:\tlearn: 0.2843307\ttotal: 49.1s\tremaining: 15.4s\n",
      "761:\tlearn: 0.2843104\ttotal: 49.2s\tremaining: 15.4s\n",
      "762:\tlearn: 0.2842805\ttotal: 49.3s\tremaining: 15.3s\n",
      "763:\tlearn: 0.2842519\ttotal: 49.3s\tremaining: 15.2s\n",
      "764:\tlearn: 0.2842350\ttotal: 49.4s\tremaining: 15.2s\n",
      "765:\tlearn: 0.2842073\ttotal: 49.4s\tremaining: 15.1s\n",
      "766:\tlearn: 0.2841516\ttotal: 49.5s\tremaining: 15s\n",
      "767:\tlearn: 0.2841366\ttotal: 49.6s\tremaining: 15s\n",
      "768:\tlearn: 0.2841114\ttotal: 49.6s\tremaining: 14.9s\n",
      "769:\tlearn: 0.2840932\ttotal: 49.7s\tremaining: 14.8s\n",
      "770:\tlearn: 0.2839476\ttotal: 49.8s\tremaining: 14.8s\n",
      "771:\tlearn: 0.2838434\ttotal: 49.8s\tremaining: 14.7s\n",
      "772:\tlearn: 0.2838141\ttotal: 49.9s\tremaining: 14.7s\n",
      "773:\tlearn: 0.2837881\ttotal: 50s\tremaining: 14.6s\n",
      "774:\tlearn: 0.2836971\ttotal: 50s\tremaining: 14.5s\n",
      "775:\tlearn: 0.2836305\ttotal: 50.1s\tremaining: 14.5s\n",
      "776:\tlearn: 0.2836147\ttotal: 50.2s\tremaining: 14.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "777:\tlearn: 0.2836026\ttotal: 50.2s\tremaining: 14.3s\n",
      "778:\tlearn: 0.2835619\ttotal: 50.3s\tremaining: 14.3s\n",
      "779:\tlearn: 0.2835113\ttotal: 50.3s\tremaining: 14.2s\n",
      "780:\tlearn: 0.2834878\ttotal: 50.4s\tremaining: 14.1s\n",
      "781:\tlearn: 0.2834704\ttotal: 50.5s\tremaining: 14.1s\n",
      "782:\tlearn: 0.2834414\ttotal: 50.5s\tremaining: 14s\n",
      "783:\tlearn: 0.2834358\ttotal: 50.6s\tremaining: 13.9s\n",
      "784:\tlearn: 0.2834259\ttotal: 50.6s\tremaining: 13.9s\n",
      "785:\tlearn: 0.2834204\ttotal: 50.7s\tremaining: 13.8s\n",
      "786:\tlearn: 0.2833956\ttotal: 50.8s\tremaining: 13.7s\n",
      "787:\tlearn: 0.2833848\ttotal: 50.8s\tremaining: 13.7s\n",
      "788:\tlearn: 0.2833739\ttotal: 50.9s\tremaining: 13.6s\n",
      "789:\tlearn: 0.2832787\ttotal: 50.9s\tremaining: 13.5s\n",
      "790:\tlearn: 0.2832711\ttotal: 51s\tremaining: 13.5s\n",
      "791:\tlearn: 0.2832492\ttotal: 51.1s\tremaining: 13.4s\n",
      "792:\tlearn: 0.2832363\ttotal: 51.1s\tremaining: 13.3s\n",
      "793:\tlearn: 0.2832182\ttotal: 51.2s\tremaining: 13.3s\n",
      "794:\tlearn: 0.2832103\ttotal: 51.2s\tremaining: 13.2s\n",
      "795:\tlearn: 0.2831769\ttotal: 51.3s\tremaining: 13.1s\n",
      "796:\tlearn: 0.2831664\ttotal: 51.4s\tremaining: 13.1s\n",
      "797:\tlearn: 0.2831323\ttotal: 51.4s\tremaining: 13s\n",
      "798:\tlearn: 0.2831221\ttotal: 51.5s\tremaining: 12.9s\n",
      "799:\tlearn: 0.2831047\ttotal: 51.5s\tremaining: 12.9s\n",
      "800:\tlearn: 0.2830425\ttotal: 51.6s\tremaining: 12.8s\n",
      "801:\tlearn: 0.2830272\ttotal: 51.7s\tremaining: 12.8s\n",
      "802:\tlearn: 0.2830085\ttotal: 51.7s\tremaining: 12.7s\n",
      "803:\tlearn: 0.2829785\ttotal: 51.8s\tremaining: 12.6s\n",
      "804:\tlearn: 0.2829682\ttotal: 51.9s\tremaining: 12.6s\n",
      "805:\tlearn: 0.2829610\ttotal: 52s\tremaining: 12.5s\n",
      "806:\tlearn: 0.2828722\ttotal: 52.1s\tremaining: 12.5s\n",
      "807:\tlearn: 0.2828600\ttotal: 52.2s\tremaining: 12.4s\n",
      "808:\tlearn: 0.2828506\ttotal: 52.3s\tremaining: 12.3s\n",
      "809:\tlearn: 0.2828411\ttotal: 52.4s\tremaining: 12.3s\n",
      "810:\tlearn: 0.2827463\ttotal: 52.4s\tremaining: 12.2s\n",
      "811:\tlearn: 0.2826711\ttotal: 52.5s\tremaining: 12.2s\n",
      "812:\tlearn: 0.2826636\ttotal: 52.6s\tremaining: 12.1s\n",
      "813:\tlearn: 0.2826565\ttotal: 52.6s\tremaining: 12s\n",
      "814:\tlearn: 0.2826365\ttotal: 52.7s\tremaining: 12s\n",
      "815:\tlearn: 0.2826309\ttotal: 52.8s\tremaining: 11.9s\n",
      "816:\tlearn: 0.2825609\ttotal: 52.8s\tremaining: 11.8s\n",
      "817:\tlearn: 0.2825519\ttotal: 52.9s\tremaining: 11.8s\n",
      "818:\tlearn: 0.2825307\ttotal: 53s\tremaining: 11.7s\n",
      "819:\tlearn: 0.2825191\ttotal: 53s\tremaining: 11.6s\n",
      "820:\tlearn: 0.2825107\ttotal: 53.1s\tremaining: 11.6s\n",
      "821:\tlearn: 0.2824187\ttotal: 53.2s\tremaining: 11.5s\n",
      "822:\tlearn: 0.2824105\ttotal: 53.2s\tremaining: 11.4s\n",
      "823:\tlearn: 0.2824060\ttotal: 53.3s\tremaining: 11.4s\n",
      "824:\tlearn: 0.2823682\ttotal: 53.3s\tremaining: 11.3s\n",
      "825:\tlearn: 0.2823509\ttotal: 53.4s\tremaining: 11.2s\n",
      "826:\tlearn: 0.2823381\ttotal: 53.5s\tremaining: 11.2s\n",
      "827:\tlearn: 0.2822895\ttotal: 53.5s\tremaining: 11.1s\n",
      "828:\tlearn: 0.2822671\ttotal: 53.6s\tremaining: 11.1s\n",
      "829:\tlearn: 0.2822578\ttotal: 53.7s\tremaining: 11s\n",
      "830:\tlearn: 0.2822449\ttotal: 53.7s\tremaining: 10.9s\n",
      "831:\tlearn: 0.2822202\ttotal: 53.8s\tremaining: 10.9s\n",
      "832:\tlearn: 0.2822120\ttotal: 53.8s\tremaining: 10.8s\n",
      "833:\tlearn: 0.2821999\ttotal: 53.9s\tremaining: 10.7s\n",
      "834:\tlearn: 0.2821919\ttotal: 54s\tremaining: 10.7s\n",
      "835:\tlearn: 0.2821735\ttotal: 54s\tremaining: 10.6s\n",
      "836:\tlearn: 0.2821649\ttotal: 54.1s\tremaining: 10.5s\n",
      "837:\tlearn: 0.2821566\ttotal: 54.2s\tremaining: 10.5s\n",
      "838:\tlearn: 0.2821490\ttotal: 54.2s\tremaining: 10.4s\n",
      "839:\tlearn: 0.2821275\ttotal: 54.3s\tremaining: 10.3s\n",
      "840:\tlearn: 0.2821213\ttotal: 54.3s\tremaining: 10.3s\n",
      "841:\tlearn: 0.2821026\ttotal: 54.4s\tremaining: 10.2s\n",
      "842:\tlearn: 0.2820930\ttotal: 54.5s\tremaining: 10.1s\n",
      "843:\tlearn: 0.2820768\ttotal: 54.5s\tremaining: 10.1s\n",
      "844:\tlearn: 0.2820642\ttotal: 54.6s\tremaining: 10s\n",
      "845:\tlearn: 0.2820609\ttotal: 54.7s\tremaining: 9.95s\n",
      "846:\tlearn: 0.2820538\ttotal: 54.7s\tremaining: 9.88s\n",
      "847:\tlearn: 0.2820220\ttotal: 54.8s\tremaining: 9.82s\n",
      "848:\tlearn: 0.2820175\ttotal: 54.8s\tremaining: 9.75s\n",
      "849:\tlearn: 0.2819992\ttotal: 54.9s\tremaining: 9.69s\n",
      "850:\tlearn: 0.2819798\ttotal: 55s\tremaining: 9.63s\n",
      "851:\tlearn: 0.2819554\ttotal: 55s\tremaining: 9.56s\n",
      "852:\tlearn: 0.2819183\ttotal: 55.1s\tremaining: 9.49s\n",
      "853:\tlearn: 0.2819080\ttotal: 55.2s\tremaining: 9.43s\n",
      "854:\tlearn: 0.2818777\ttotal: 55.2s\tremaining: 9.37s\n",
      "855:\tlearn: 0.2818465\ttotal: 55.3s\tremaining: 9.3s\n",
      "856:\tlearn: 0.2818286\ttotal: 55.4s\tremaining: 9.24s\n",
      "857:\tlearn: 0.2818204\ttotal: 55.4s\tremaining: 9.17s\n",
      "858:\tlearn: 0.2818042\ttotal: 55.5s\tremaining: 9.11s\n",
      "859:\tlearn: 0.2818004\ttotal: 55.5s\tremaining: 9.04s\n",
      "860:\tlearn: 0.2817947\ttotal: 55.6s\tremaining: 8.98s\n",
      "861:\tlearn: 0.2817873\ttotal: 55.7s\tremaining: 8.91s\n",
      "862:\tlearn: 0.2817673\ttotal: 55.7s\tremaining: 8.85s\n",
      "863:\tlearn: 0.2817548\ttotal: 55.8s\tremaining: 8.78s\n",
      "864:\tlearn: 0.2817480\ttotal: 55.9s\tremaining: 8.72s\n",
      "865:\tlearn: 0.2817391\ttotal: 55.9s\tremaining: 8.65s\n",
      "866:\tlearn: 0.2817055\ttotal: 56s\tremaining: 8.59s\n",
      "867:\tlearn: 0.2816949\ttotal: 56.1s\tremaining: 8.52s\n",
      "868:\tlearn: 0.2816864\ttotal: 56.1s\tremaining: 8.46s\n",
      "869:\tlearn: 0.2816800\ttotal: 56.2s\tremaining: 8.39s\n",
      "870:\tlearn: 0.2816575\ttotal: 56.2s\tremaining: 8.33s\n",
      "871:\tlearn: 0.2816407\ttotal: 56.3s\tremaining: 8.26s\n",
      "872:\tlearn: 0.2816296\ttotal: 56.4s\tremaining: 8.2s\n",
      "873:\tlearn: 0.2815561\ttotal: 56.4s\tremaining: 8.14s\n",
      "874:\tlearn: 0.2815432\ttotal: 56.5s\tremaining: 8.07s\n",
      "875:\tlearn: 0.2815328\ttotal: 56.6s\tremaining: 8.01s\n",
      "876:\tlearn: 0.2815241\ttotal: 56.6s\tremaining: 7.94s\n",
      "877:\tlearn: 0.2815169\ttotal: 56.7s\tremaining: 7.88s\n",
      "878:\tlearn: 0.2814744\ttotal: 56.8s\tremaining: 7.81s\n",
      "879:\tlearn: 0.2814653\ttotal: 56.8s\tremaining: 7.75s\n",
      "880:\tlearn: 0.2814597\ttotal: 56.9s\tremaining: 7.68s\n",
      "881:\tlearn: 0.2814548\ttotal: 57s\tremaining: 7.62s\n",
      "882:\tlearn: 0.2814334\ttotal: 57s\tremaining: 7.55s\n",
      "883:\tlearn: 0.2814271\ttotal: 57.1s\tremaining: 7.49s\n",
      "884:\tlearn: 0.2814200\ttotal: 57.1s\tremaining: 7.42s\n",
      "885:\tlearn: 0.2813903\ttotal: 57.2s\tremaining: 7.36s\n",
      "886:\tlearn: 0.2813701\ttotal: 57.3s\tremaining: 7.3s\n",
      "887:\tlearn: 0.2813498\ttotal: 57.3s\tremaining: 7.23s\n",
      "888:\tlearn: 0.2813452\ttotal: 57.4s\tremaining: 7.17s\n",
      "889:\tlearn: 0.2813406\ttotal: 57.5s\tremaining: 7.1s\n",
      "890:\tlearn: 0.2813018\ttotal: 57.5s\tremaining: 7.04s\n",
      "891:\tlearn: 0.2806836\ttotal: 57.6s\tremaining: 6.97s\n",
      "892:\tlearn: 0.2806653\ttotal: 57.7s\tremaining: 6.91s\n",
      "893:\tlearn: 0.2806312\ttotal: 57.7s\tremaining: 6.85s\n",
      "894:\tlearn: 0.2806142\ttotal: 57.8s\tremaining: 6.78s\n",
      "895:\tlearn: 0.2806005\ttotal: 57.9s\tremaining: 6.72s\n",
      "896:\tlearn: 0.2805828\ttotal: 57.9s\tremaining: 6.65s\n",
      "897:\tlearn: 0.2805612\ttotal: 58s\tremaining: 6.59s\n",
      "898:\tlearn: 0.2805015\ttotal: 58.1s\tremaining: 6.52s\n",
      "899:\tlearn: 0.2804606\ttotal: 58.1s\tremaining: 6.46s\n",
      "900:\tlearn: 0.2804470\ttotal: 58.2s\tremaining: 6.39s\n",
      "901:\tlearn: 0.2804101\ttotal: 58.3s\tremaining: 6.33s\n",
      "902:\tlearn: 0.2803849\ttotal: 58.3s\tremaining: 6.26s\n",
      "903:\tlearn: 0.2803693\ttotal: 58.4s\tremaining: 6.2s\n",
      "904:\tlearn: 0.2803598\ttotal: 58.5s\tremaining: 6.14s\n",
      "905:\tlearn: 0.2803478\ttotal: 58.5s\tremaining: 6.07s\n",
      "906:\tlearn: 0.2803325\ttotal: 58.6s\tremaining: 6.01s\n",
      "907:\tlearn: 0.2803137\ttotal: 58.7s\tremaining: 5.94s\n",
      "908:\tlearn: 0.2803015\ttotal: 58.7s\tremaining: 5.88s\n",
      "909:\tlearn: 0.2802730\ttotal: 58.8s\tremaining: 5.82s\n",
      "910:\tlearn: 0.2802591\ttotal: 58.9s\tremaining: 5.75s\n",
      "911:\tlearn: 0.2802027\ttotal: 59s\tremaining: 5.69s\n",
      "912:\tlearn: 0.2801645\ttotal: 59s\tremaining: 5.63s\n",
      "913:\tlearn: 0.2800759\ttotal: 59.1s\tremaining: 5.56s\n",
      "914:\tlearn: 0.2800056\ttotal: 59.2s\tremaining: 5.5s\n",
      "915:\tlearn: 0.2799096\ttotal: 59.3s\tremaining: 5.43s\n",
      "916:\tlearn: 0.2798996\ttotal: 59.3s\tremaining: 5.37s\n",
      "917:\tlearn: 0.2798765\ttotal: 59.4s\tremaining: 5.31s\n",
      "918:\tlearn: 0.2798615\ttotal: 59.5s\tremaining: 5.24s\n",
      "919:\tlearn: 0.2798475\ttotal: 59.6s\tremaining: 5.18s\n",
      "920:\tlearn: 0.2798267\ttotal: 59.7s\tremaining: 5.12s\n",
      "921:\tlearn: 0.2798156\ttotal: 59.7s\tremaining: 5.05s\n",
      "922:\tlearn: 0.2797982\ttotal: 59.8s\tremaining: 4.99s\n",
      "923:\tlearn: 0.2796885\ttotal: 59.9s\tremaining: 4.92s\n",
      "924:\tlearn: 0.2796622\ttotal: 60s\tremaining: 4.86s\n",
      "925:\tlearn: 0.2796209\ttotal: 1m\tremaining: 4.8s\n",
      "926:\tlearn: 0.2795667\ttotal: 1m\tremaining: 4.73s\n",
      "927:\tlearn: 0.2795585\ttotal: 1m\tremaining: 4.67s\n",
      "928:\tlearn: 0.2795341\ttotal: 1m\tremaining: 4.61s\n",
      "929:\tlearn: 0.2795259\ttotal: 1m\tremaining: 4.54s\n",
      "930:\tlearn: 0.2795146\ttotal: 1m\tremaining: 4.48s\n",
      "931:\tlearn: 0.2794985\ttotal: 1m\tremaining: 4.41s\n",
      "932:\tlearn: 0.2794924\ttotal: 1m\tremaining: 4.35s\n",
      "933:\tlearn: 0.2794807\ttotal: 1m\tremaining: 4.29s\n",
      "934:\tlearn: 0.2794035\ttotal: 1m\tremaining: 4.22s\n",
      "935:\tlearn: 0.2793978\ttotal: 1m\tremaining: 4.16s\n",
      "936:\tlearn: 0.2793910\ttotal: 1m\tremaining: 4.09s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "937:\tlearn: 0.2793619\ttotal: 1m\tremaining: 4.03s\n",
      "938:\tlearn: 0.2792465\ttotal: 1m 1s\tremaining: 3.96s\n",
      "939:\tlearn: 0.2792380\ttotal: 1m 1s\tremaining: 3.9s\n",
      "940:\tlearn: 0.2792269\ttotal: 1m 1s\tremaining: 3.83s\n",
      "941:\tlearn: 0.2792213\ttotal: 1m 1s\tremaining: 3.77s\n",
      "942:\tlearn: 0.2792029\ttotal: 1m 1s\tremaining: 3.71s\n",
      "943:\tlearn: 0.2791362\ttotal: 1m 1s\tremaining: 3.64s\n",
      "944:\tlearn: 0.2791292\ttotal: 1m 1s\tremaining: 3.58s\n",
      "945:\tlearn: 0.2791112\ttotal: 1m 1s\tremaining: 3.51s\n",
      "946:\tlearn: 0.2791054\ttotal: 1m 1s\tremaining: 3.45s\n",
      "947:\tlearn: 0.2790948\ttotal: 1m 1s\tremaining: 3.38s\n",
      "948:\tlearn: 0.2790210\ttotal: 1m 1s\tremaining: 3.32s\n",
      "949:\tlearn: 0.2790054\ttotal: 1m 1s\tremaining: 3.25s\n",
      "950:\tlearn: 0.2789956\ttotal: 1m 1s\tremaining: 3.19s\n",
      "951:\tlearn: 0.2789875\ttotal: 1m 1s\tremaining: 3.12s\n",
      "952:\tlearn: 0.2789685\ttotal: 1m 2s\tremaining: 3.06s\n",
      "953:\tlearn: 0.2789524\ttotal: 1m 2s\tremaining: 2.99s\n",
      "954:\tlearn: 0.2789328\ttotal: 1m 2s\tremaining: 2.93s\n",
      "955:\tlearn: 0.2789261\ttotal: 1m 2s\tremaining: 2.86s\n",
      "956:\tlearn: 0.2789139\ttotal: 1m 2s\tremaining: 2.8s\n",
      "957:\tlearn: 0.2788974\ttotal: 1m 2s\tremaining: 2.73s\n",
      "958:\tlearn: 0.2788888\ttotal: 1m 2s\tremaining: 2.67s\n",
      "959:\tlearn: 0.2788719\ttotal: 1m 2s\tremaining: 2.6s\n",
      "960:\tlearn: 0.2788364\ttotal: 1m 2s\tremaining: 2.54s\n",
      "961:\tlearn: 0.2788245\ttotal: 1m 2s\tremaining: 2.47s\n",
      "962:\tlearn: 0.2788197\ttotal: 1m 2s\tremaining: 2.41s\n",
      "963:\tlearn: 0.2786845\ttotal: 1m 2s\tremaining: 2.34s\n",
      "964:\tlearn: 0.2786756\ttotal: 1m 2s\tremaining: 2.28s\n",
      "965:\tlearn: 0.2786584\ttotal: 1m 2s\tremaining: 2.21s\n",
      "966:\tlearn: 0.2786288\ttotal: 1m 2s\tremaining: 2.15s\n",
      "967:\tlearn: 0.2785978\ttotal: 1m 3s\tremaining: 2.08s\n",
      "968:\tlearn: 0.2785882\ttotal: 1m 3s\tremaining: 2.02s\n",
      "969:\tlearn: 0.2785775\ttotal: 1m 3s\tremaining: 1.95s\n",
      "970:\tlearn: 0.2785632\ttotal: 1m 3s\tremaining: 1.89s\n",
      "971:\tlearn: 0.2785596\ttotal: 1m 3s\tremaining: 1.82s\n",
      "972:\tlearn: 0.2785463\ttotal: 1m 3s\tremaining: 1.76s\n",
      "973:\tlearn: 0.2785383\ttotal: 1m 3s\tremaining: 1.69s\n",
      "974:\tlearn: 0.2785290\ttotal: 1m 3s\tremaining: 1.63s\n",
      "975:\tlearn: 0.2785221\ttotal: 1m 3s\tremaining: 1.56s\n",
      "976:\tlearn: 0.2785170\ttotal: 1m 3s\tremaining: 1.5s\n",
      "977:\tlearn: 0.2784336\ttotal: 1m 3s\tremaining: 1.43s\n",
      "978:\tlearn: 0.2784176\ttotal: 1m 3s\tremaining: 1.37s\n",
      "979:\tlearn: 0.2784006\ttotal: 1m 3s\tremaining: 1.3s\n",
      "980:\tlearn: 0.2783791\ttotal: 1m 3s\tremaining: 1.24s\n",
      "981:\tlearn: 0.2783658\ttotal: 1m 3s\tremaining: 1.17s\n",
      "982:\tlearn: 0.2783411\ttotal: 1m 3s\tremaining: 1.11s\n",
      "983:\tlearn: 0.2783357\ttotal: 1m 4s\tremaining: 1.04s\n",
      "984:\tlearn: 0.2782659\ttotal: 1m 4s\tremaining: 976ms\n",
      "985:\tlearn: 0.2782399\ttotal: 1m 4s\tremaining: 911ms\n",
      "986:\tlearn: 0.2782359\ttotal: 1m 4s\tremaining: 846ms\n",
      "987:\tlearn: 0.2782333\ttotal: 1m 4s\tremaining: 781ms\n",
      "988:\tlearn: 0.2782204\ttotal: 1m 4s\tremaining: 715ms\n",
      "989:\tlearn: 0.2782023\ttotal: 1m 4s\tremaining: 650ms\n",
      "990:\tlearn: 0.2781993\ttotal: 1m 4s\tremaining: 585ms\n",
      "991:\tlearn: 0.2781970\ttotal: 1m 4s\tremaining: 520ms\n",
      "992:\tlearn: 0.2781920\ttotal: 1m 4s\tremaining: 455ms\n",
      "993:\tlearn: 0.2781753\ttotal: 1m 4s\tremaining: 390ms\n",
      "994:\tlearn: 0.2781710\ttotal: 1m 4s\tremaining: 325ms\n",
      "995:\tlearn: 0.2781686\ttotal: 1m 4s\tremaining: 260ms\n",
      "996:\tlearn: 0.2781420\ttotal: 1m 4s\tremaining: 195ms\n",
      "997:\tlearn: 0.2781279\ttotal: 1m 4s\tremaining: 130ms\n",
      "998:\tlearn: 0.2781198\ttotal: 1m 4s\tremaining: 65ms\n",
      "999:\tlearn: 0.2780915\ttotal: 1m 4s\tremaining: 0us\n",
      "CatBoostClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.91      0.85       763\n",
      "         1.0       0.89      0.77      0.83       728\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      1491\n",
      "   macro avg       0.85      0.84      0.84      1491\n",
      "weighted avg       0.85      0.84      0.84      1491\n",
      "\n",
      "[[693  70]\n",
      " [167 561]]\n",
      "Accuracy is  84.10462776659959\n",
      "Time on model's work: 67.668 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.80      0.82       763\n",
      "         1.0       0.80      0.84      0.82       728\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      1491\n",
      "   macro avg       0.82      0.82      0.82      1491\n",
      "weighted avg       0.82      0.82      0.82      1491\n",
      "\n",
      "[[608 155]\n",
      " [117 611]]\n",
      "Accuracy is  81.75720992622401\n",
      "Time on model's work: 0.127 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.58      0.72       763\n",
      "         1.0       0.69      0.95      0.80       728\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      1491\n",
      "   macro avg       0.80      0.77      0.76      1491\n",
      "weighted avg       0.81      0.76      0.75      1491\n",
      "\n",
      "[[446 317]\n",
      " [ 38 690]]\n",
      "Accuracy is  76.19047619047619\n",
      "Time on model's work: 0.172 s\n",
      "====================================================================================================\n",
      "TFFMClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.85      0.84       763\n",
      "         1.0       0.83      0.81      0.82       728\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      1491\n",
      "   macro avg       0.83      0.83      0.83      1491\n",
      "weighted avg       0.83      0.83      0.83      1491\n",
      "\n",
      "[[645 118]\n",
      " [136 592]]\n",
      "Accuracy is  82.9644533869886\n",
      "Time on model's work: 20.84 s\n",
      "====================================================================================================\n",
      "TOTAL TIME SPENT:  314.363 s\n"
     ]
    }
   ],
   "source": [
    "clfs = [\n",
    "        ['RandomForestClassifier', RandomForestClassifier()],\n",
    "        ['GradientBoostingClassifier', GradientBoostingClassifier()],\n",
    "        ['ExtraTreesClassifier', ExtraTreesClassifier()],\n",
    "        ['AdaBoostClassifier', AdaBoostClassifier()],\n",
    "        ['BaggingClassifier', BaggingClassifier()],\n",
    "        ['DecisionTreeClassifier', DecisionTreeClassifier()],\n",
    "        ['MLPClassifier', MLPClassifier()],\n",
    "        ['XGBClassifier', XGBClassifier()],\n",
    "        ['CatBoostClassifier', CatBoostClassifier()],\n",
    "        ['LogisticRegression', LogisticRegression()],\n",
    "        ['SGDClassifier', SGDClassifier()],\n",
    "        ['TFFMClassifier', TFFMClassifier()]\n",
    "       ]\n",
    "t = time()\n",
    "for name, clf in clfs:\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    prediction = clf.predict(X_test)\n",
    "    print(name)\n",
    "    print(classification_report(y_test, prediction))\n",
    "    print(confusion_matrix(y_test, prediction))\n",
    "    print('Accuracy is ', accuracy_score(y_test, prediction)*100)\n",
    "    print (\"Time on model's work:\", round(time()-t0, 3), \"s\")\n",
    "    print('='*100)\n",
    "print (\"TOTAL TIME SPENT: \", round(time()-t, 3), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:02<00:00, 18.27epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8356807511737089\n",
      "[[658 105]\n",
      " [140 588]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.86      0.84       763\n",
      "         1.0       0.85      0.81      0.83       728\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      1491\n",
      "   macro avg       0.84      0.84      0.84      1491\n",
      "weighted avg       0.84      0.84      0.84      1491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:02<00:00, 17.32epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7994634473507712\n",
      "[[535 228]\n",
      " [ 71 657]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.70      0.78       763\n",
      "         1.0       0.74      0.90      0.81       728\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      1491\n",
      "   macro avg       0.81      0.80      0.80      1491\n",
      "weighted avg       0.81      0.80      0.80      1491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:02<00:00, 19.44epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.784037558685446\n",
      "[[491 272]\n",
      " [ 50 678]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.64      0.75       763\n",
      "         1.0       0.71      0.93      0.81       728\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      1491\n",
      "   macro avg       0.81      0.79      0.78      1491\n",
      "weighted avg       0.81      0.78      0.78      1491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:02<00:00, 20.03epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7739771965124078\n",
      "[[460 303]\n",
      " [ 34 694]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.60      0.73       763\n",
      "         1.0       0.70      0.95      0.80       728\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      1491\n",
      "   macro avg       0.81      0.78      0.77      1491\n",
      "weighted avg       0.82      0.77      0.77      1491\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TFFM sparse - works worse with sparse\n",
    "# only CSR format supported\n",
    "X_train_sparse = sp.csr_matrix(X_train)\n",
    "X_test_sparse = sp.csr_matrix(X_test)\n",
    "# weight - optional / AdamOptimizer\n",
    "pos_class_weight = list(map(float,range(1, 5)))\n",
    "for weight in pos_class_weight:\n",
    "    model = TFFMClassifier(\n",
    "        order=2,\n",
    "        pos_class_weight=weight,\n",
    "        rank=10, \n",
    "        optimizer=tf.train.AdamOptimizer(learning_rate=0.001), \n",
    "        n_epochs=50, \n",
    "        batch_size=1024,\n",
    "        init_std=0.001,\n",
    "        reg=0.01,\n",
    "        input_type='sparse',\n",
    "        #log_dir='./tmp/logs',\n",
    "        #verbose=1,\n",
    "        seed=42\n",
    "    )\n",
    "    model.fit(X_train_sparse, y_train, show_progress=True)\n",
    "    predictions = model.predict(X_test_sparse)\n",
    "    print('accuracy: {}'.format(accuracy_score(y_test, predictions)))\n",
    "    # this will close tf.Session and free resources\n",
    "    print(confusion_matrix(y_test,predictions)) \n",
    "    print(classification_report(y_test, predictions))\n",
    "    model.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:02<00:00, 17.57epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7323943661971831\n",
      "[[456 307]\n",
      " [ 92 636]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.60      0.70       763\n",
      "         1.0       0.67      0.87      0.76       728\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      1491\n",
      "   macro avg       0.75      0.74      0.73      1491\n",
      "weighted avg       0.76      0.73      0.73      1491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:02<00:00, 18.23epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7377598926894702\n",
      "[[445 318]\n",
      " [ 73 655]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.58      0.69       763\n",
      "         1.0       0.67      0.90      0.77       728\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      1491\n",
      "   macro avg       0.77      0.74      0.73      1491\n",
      "weighted avg       0.77      0.74      0.73      1491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:02<00:00, 18.37epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7431254191817572\n",
      "[[444 319]\n",
      " [ 64 664]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.58      0.70       763\n",
      "         1.0       0.68      0.91      0.78       728\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      1491\n",
      "   macro avg       0.77      0.75      0.74      1491\n",
      "weighted avg       0.78      0.74      0.74      1491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:02<00:00, 18.25epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7592219986586184\n",
      "[[436 327]\n",
      " [ 32 696]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.57      0.71       763\n",
      "         1.0       0.68      0.96      0.79       728\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      1491\n",
      "   macro avg       0.81      0.76      0.75      1491\n",
      "weighted avg       0.81      0.76      0.75      1491\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# weight - optional / FtrlOptimizer\n",
    "pos_class_weight = list(map(float,range(1, 5)))\n",
    "for weight in pos_class_weight:\n",
    "    model = TFFMClassifier(\n",
    "        order=2,\n",
    "        pos_class_weight=weight,\n",
    "        rank=10, \n",
    "        optimizer=tf.train.FtrlOptimizer(0.01, l2_regularization_strength=0.001, l2_shrinkage_regularization_strength=0.001), \n",
    "        n_epochs=50, \n",
    "        batch_size=1024,\n",
    "        init_std=0.001,\n",
    "        reg=0.01,\n",
    "        input_type='sparse',\n",
    "        #log_dir='./tmp/logs',\n",
    "        #verbose=1,\n",
    "        seed=42\n",
    "    )\n",
    "    model.fit(X_train_sparse, y_train, show_progress=True)\n",
    "    predictions = model.predict(X_test_sparse)\n",
    "    print('accuracy: {}'.format(accuracy_score(y_test, predictions)))\n",
    "    # this will close tf.Session and free resources\n",
    "    print(confusion_matrix(y_test,predictions)) \n",
    "    print(classification_report(y_test, predictions))\n",
    "    model.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2226"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KERAS\n",
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5961/5961 [==============================] - ETA: 7s - loss: 0.7537 - acc: 0.558 - ETA: 1s - loss: 0.7367 - acc: 0.515 - ETA: 1s - loss: 0.7329 - acc: 0.522 - ETA: 0s - loss: 0.7321 - acc: 0.519 - ETA: 0s - loss: 0.7309 - acc: 0.518 - ETA: 0s - loss: 0.7275 - acc: 0.518 - ETA: 0s - loss: 0.7217 - acc: 0.521 - ETA: 0s - loss: 0.7211 - acc: 0.522 - 1s 130us/step - loss: 0.7193 - acc: 0.5251\n",
      "Epoch 2/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.7055 - acc: 0.535 - ETA: 0s - loss: 0.6957 - acc: 0.539 - ETA: 0s - loss: 0.6938 - acc: 0.555 - ETA: 0s - loss: 0.6900 - acc: 0.557 - ETA: 0s - loss: 0.6884 - acc: 0.555 - ETA: 0s - loss: 0.6833 - acc: 0.564 - ETA: 0s - loss: 0.6818 - acc: 0.567 - ETA: 0s - loss: 0.6802 - acc: 0.570 - 0s 79us/step - loss: 0.6789 - acc: 0.5715\n",
      "Epoch 3/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.6451 - acc: 0.675 - ETA: 0s - loss: 0.6532 - acc: 0.642 - ETA: 0s - loss: 0.6517 - acc: 0.631 - ETA: 0s - loss: 0.6497 - acc: 0.630 - ETA: 0s - loss: 0.6488 - acc: 0.632 - ETA: 0s - loss: 0.6431 - acc: 0.640 - ETA: 0s - loss: 0.6378 - acc: 0.649 - 0s 74us/step - loss: 0.6366 - acc: 0.6502\n",
      "Epoch 4/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.6183 - acc: 0.671 - ETA: 0s - loss: 0.6089 - acc: 0.678 - ETA: 0s - loss: 0.6082 - acc: 0.679 - ETA: 0s - loss: 0.6058 - acc: 0.678 - ETA: 0s - loss: 0.6058 - acc: 0.681 - ETA: 0s - loss: 0.5984 - acc: 0.688 - ETA: 0s - loss: 0.5965 - acc: 0.691 - ETA: 0s - loss: 0.5959 - acc: 0.692 - 0s 77us/step - loss: 0.5949 - acc: 0.6940\n",
      "Epoch 5/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.5963 - acc: 0.675 - ETA: 0s - loss: 0.5818 - acc: 0.700 - ETA: 0s - loss: 0.5673 - acc: 0.717 - ETA: 0s - loss: 0.5674 - acc: 0.720 - ETA: 0s - loss: 0.5668 - acc: 0.718 - ETA: 0s - loss: 0.5637 - acc: 0.722 - ETA: 0s - loss: 0.5669 - acc: 0.718 - ETA: 0s - loss: 0.5685 - acc: 0.718 - 0s 79us/step - loss: 0.5692 - acc: 0.7177\n",
      "Epoch 6/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.5179 - acc: 0.761 - ETA: 0s - loss: 0.5534 - acc: 0.740 - ETA: 0s - loss: 0.5497 - acc: 0.744 - ETA: 0s - loss: 0.5501 - acc: 0.740 - ETA: 0s - loss: 0.5526 - acc: 0.735 - ETA: 0s - loss: 0.5537 - acc: 0.732 - ETA: 0s - loss: 0.5519 - acc: 0.734 - ETA: 0s - loss: 0.5533 - acc: 0.731 - 0s 80us/step - loss: 0.5529 - acc: 0.7309\n",
      "Epoch 7/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.5730 - acc: 0.714 - ETA: 0s - loss: 0.5503 - acc: 0.734 - ETA: 0s - loss: 0.5485 - acc: 0.733 - ETA: 0s - loss: 0.5483 - acc: 0.731 - ETA: 0s - loss: 0.5442 - acc: 0.734 - ETA: 0s - loss: 0.5372 - acc: 0.738 - ETA: 0s - loss: 0.5322 - acc: 0.742 - ETA: 0s - loss: 0.5321 - acc: 0.745 - 0s 74us/step - loss: 0.5326 - acc: 0.7447\n",
      "Epoch 8/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.5393 - acc: 0.742 - ETA: 0s - loss: 0.5558 - acc: 0.722 - ETA: 0s - loss: 0.5368 - acc: 0.740 - ETA: 0s - loss: 0.5359 - acc: 0.739 - ETA: 0s - loss: 0.5341 - acc: 0.739 - ETA: 0s - loss: 0.5318 - acc: 0.736 - ETA: 0s - loss: 0.5260 - acc: 0.741 - ETA: 0s - loss: 0.5230 - acc: 0.742 - ETA: 0s - loss: 0.5204 - acc: 0.745 - 1s 86us/step - loss: 0.5195 - acc: 0.7455\n",
      "Epoch 9/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.4485 - acc: 0.812 - ETA: 0s - loss: 0.4969 - acc: 0.757 - ETA: 0s - loss: 0.5070 - acc: 0.752 - ETA: 0s - loss: 0.5061 - acc: 0.753 - ETA: 0s - loss: 0.5140 - acc: 0.749 - ETA: 0s - loss: 0.5166 - acc: 0.748 - ETA: 0s - loss: 0.5135 - acc: 0.750 - 0s 69us/step - loss: 0.5148 - acc: 0.7504\n",
      "Epoch 10/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.4732 - acc: 0.785 - ETA: 0s - loss: 0.4786 - acc: 0.784 - ETA: 0s - loss: 0.4957 - acc: 0.767 - ETA: 0s - loss: 0.5027 - acc: 0.761 - ETA: 0s - loss: 0.5012 - acc: 0.762 - ETA: 0s - loss: 0.5045 - acc: 0.759 - ETA: 0s - loss: 0.5052 - acc: 0.757 - ETA: 0s - loss: 0.5051 - acc: 0.757 - 0s 82us/step - loss: 0.5070 - acc: 0.7542\n",
      "Epoch 11/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.5078 - acc: 0.734 - ETA: 0s - loss: 0.5133 - acc: 0.746 - ETA: 0s - loss: 0.5058 - acc: 0.748 - ETA: 0s - loss: 0.5018 - acc: 0.750 - ETA: 0s - loss: 0.4966 - acc: 0.755 - ETA: 0s - loss: 0.4975 - acc: 0.759 - ETA: 0s - loss: 0.4966 - acc: 0.758 - ETA: 0s - loss: 0.4943 - acc: 0.760 - 0s 76us/step - loss: 0.4949 - acc: 0.7601\n",
      "Epoch 12/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.4662 - acc: 0.781 - ETA: 0s - loss: 0.4780 - acc: 0.768 - ETA: 0s - loss: 0.4941 - acc: 0.764 - ETA: 0s - loss: 0.4949 - acc: 0.758 - ETA: 0s - loss: 0.4914 - acc: 0.759 - ETA: 0s - loss: 0.4876 - acc: 0.765 - ETA: 0s - loss: 0.4844 - acc: 0.768 - ETA: 0s - loss: 0.4860 - acc: 0.768 - ETA: 0s - loss: 0.4883 - acc: 0.767 - 1s 91us/step - loss: 0.4841 - acc: 0.7695\n",
      "Epoch 13/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.5075 - acc: 0.777 - ETA: 0s - loss: 0.4562 - acc: 0.795 - ETA: 0s - loss: 0.4723 - acc: 0.775 - ETA: 0s - loss: 0.4590 - acc: 0.780 - ETA: 0s - loss: 0.4602 - acc: 0.779 - ETA: 0s - loss: 0.4637 - acc: 0.771 - ETA: 0s - loss: 0.4675 - acc: 0.772 - ETA: 0s - loss: 0.4645 - acc: 0.774 - ETA: 0s - loss: 0.4694 - acc: 0.772 - ETA: 0s - loss: 0.4669 - acc: 0.774 - 1s 100us/step - loss: 0.4671 - acc: 0.7744\n",
      "Epoch 14/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.4951 - acc: 0.757 - ETA: 0s - loss: 0.4770 - acc: 0.773 - ETA: 0s - loss: 0.4695 - acc: 0.782 - ETA: 0s - loss: 0.4587 - acc: 0.786 - ETA: 0s - loss: 0.4657 - acc: 0.781 - ETA: 0s - loss: 0.4639 - acc: 0.781 - ETA: 0s - loss: 0.4654 - acc: 0.778 - ETA: 0s - loss: 0.4656 - acc: 0.779 - 1s 84us/step - loss: 0.4624 - acc: 0.7816\n",
      "Epoch 15/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.5063 - acc: 0.750 - ETA: 0s - loss: 0.4601 - acc: 0.777 - ETA: 0s - loss: 0.4589 - acc: 0.781 - ETA: 0s - loss: 0.4521 - acc: 0.785 - ETA: 0s - loss: 0.4562 - acc: 0.782 - ETA: 0s - loss: 0.4520 - acc: 0.784 - ETA: 0s - loss: 0.4541 - acc: 0.786 - ETA: 0s - loss: 0.4548 - acc: 0.787 - ETA: 0s - loss: 0.4495 - acc: 0.790 - 1s 94us/step - loss: 0.4502 - acc: 0.7901\n",
      "Epoch 16/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.4442 - acc: 0.769 - ETA: 0s - loss: 0.4388 - acc: 0.785 - ETA: 0s - loss: 0.4443 - acc: 0.787 - ETA: 0s - loss: 0.4522 - acc: 0.782 - ETA: 0s - loss: 0.4502 - acc: 0.782 - ETA: 0s - loss: 0.4443 - acc: 0.786 - ETA: 0s - loss: 0.4469 - acc: 0.786 - ETA: 0s - loss: 0.4503 - acc: 0.787 - 0s 80us/step - loss: 0.4516 - acc: 0.7863\n",
      "Epoch 17/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.4358 - acc: 0.804 - ETA: 0s - loss: 0.4469 - acc: 0.789 - ETA: 0s - loss: 0.4349 - acc: 0.792 - ETA: 0s - loss: 0.4330 - acc: 0.800 - ETA: 0s - loss: 0.4424 - acc: 0.791 - ETA: 0s - loss: 0.4395 - acc: 0.793 - ETA: 0s - loss: 0.4396 - acc: 0.793 - ETA: 0s - loss: 0.4393 - acc: 0.795 - 0s 79us/step - loss: 0.4428 - acc: 0.7930\n",
      "Epoch 18/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.4235 - acc: 0.785 - ETA: 0s - loss: 0.4132 - acc: 0.801 - ETA: 0s - loss: 0.4339 - acc: 0.794 - ETA: 0s - loss: 0.4362 - acc: 0.789 - ETA: 0s - loss: 0.4431 - acc: 0.783 - ETA: 0s - loss: 0.4389 - acc: 0.785 - ETA: 0s - loss: 0.4422 - acc: 0.783 - ETA: 0s - loss: 0.4401 - acc: 0.784 - 0s 77us/step - loss: 0.4368 - acc: 0.7875\n",
      "Epoch 19/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.4014 - acc: 0.796 - ETA: 0s - loss: 0.4373 - acc: 0.792 - ETA: 0s - loss: 0.4397 - acc: 0.798 - ETA: 0s - loss: 0.4345 - acc: 0.796 - ETA: 0s - loss: 0.4382 - acc: 0.793 - ETA: 0s - loss: 0.4310 - acc: 0.798 - ETA: 0s - loss: 0.4308 - acc: 0.796 - ETA: 0s - loss: 0.4308 - acc: 0.796 - 0s 76us/step - loss: 0.4289 - acc: 0.7980\n",
      "Epoch 20/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.3654 - acc: 0.804 - ETA: 0s - loss: 0.3951 - acc: 0.796 - ETA: 0s - loss: 0.4068 - acc: 0.795 - ETA: 0s - loss: 0.4112 - acc: 0.798 - ETA: 0s - loss: 0.4155 - acc: 0.796 - ETA: 0s - loss: 0.4135 - acc: 0.800 - ETA: 0s - loss: 0.4159 - acc: 0.800 - ETA: 0s - loss: 0.4171 - acc: 0.797 - 0s 79us/step - loss: 0.4161 - acc: 0.7987\n",
      "1491/1491 [==============================] - ETA:  - 0s 79us/step\n",
      "[0.3912385860239396, 0.8001341402010979]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=256)\n",
    "score = model.evaluate(X_test, y_test, batch_size=256)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NearMiss (version = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 3726), (1.0, 3726)]\n"
     ]
    }
   ],
   "source": [
    "nm3 = NearMiss(version=3)\n",
    "X_resampled_nm3, y_resampled3 = nm3.fit_resample(features_list_array, labels_list_array)\n",
    "print(sorted(Counter(y_resampled2).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled_nm3, y_resampled3, random_state=35, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.69      0.65       763\n",
      "         1.0       0.62      0.55      0.58       728\n",
      "\n",
      "   micro avg       0.62      0.62      0.62      1491\n",
      "   macro avg       0.62      0.62      0.62      1491\n",
      "weighted avg       0.62      0.62      0.62      1491\n",
      "\n",
      "[[524 239]\n",
      " [330 398]]\n",
      "Accuracy is  61.837692823608315\n",
      "Time on model's work: 0.907 s\n",
      "====================================================================================================\n",
      "GradientBoostingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.74      0.67       763\n",
      "         1.0       0.66      0.53      0.59       728\n",
      "\n",
      "   micro avg       0.64      0.64      0.64      1491\n",
      "   macro avg       0.64      0.63      0.63      1491\n",
      "weighted avg       0.64      0.64      0.63      1491\n",
      "\n",
      "[[562 201]\n",
      " [343 385]]\n",
      "Accuracy is  63.51441985244802\n",
      "Time on model's work: 35.264 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.66      0.62       763\n",
      "         1.0       0.59      0.51      0.55       728\n",
      "\n",
      "   micro avg       0.59      0.59      0.59      1491\n",
      "   macro avg       0.59      0.58      0.58      1491\n",
      "weighted avg       0.59      0.59      0.58      1491\n",
      "\n",
      "[[500 263]\n",
      " [354 374]]\n",
      "Accuracy is  58.61837692823608\n",
      "Time on model's work: 1.553 s\n",
      "====================================================================================================\n",
      "AdaBoostClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.70      0.66       763\n",
      "         1.0       0.64      0.56      0.59       728\n",
      "\n",
      "   micro avg       0.63      0.63      0.63      1491\n",
      "   macro avg       0.63      0.63      0.63      1491\n",
      "weighted avg       0.63      0.63      0.63      1491\n",
      "\n",
      "[[532 231]\n",
      " [322 406]]\n",
      "Accuracy is  62.91079812206573\n",
      "Time on model's work: 9.505 s\n",
      "====================================================================================================\n",
      "BaggingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.75      0.68       763\n",
      "         1.0       0.66      0.51      0.58       728\n",
      "\n",
      "   micro avg       0.63      0.63      0.63      1491\n",
      "   macro avg       0.64      0.63      0.63      1491\n",
      "weighted avg       0.64      0.63      0.63      1491\n",
      "\n",
      "[[575 188]\n",
      " [357 371]]\n",
      "Accuracy is  63.44735077129443\n",
      "Time on model's work: 5.484 s\n",
      "====================================================================================================\n",
      "DecisionTreeClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.66      0.64       763\n",
      "         1.0       0.61      0.58      0.59       728\n",
      "\n",
      "   micro avg       0.62      0.62      0.62      1491\n",
      "   macro avg       0.62      0.62      0.62      1491\n",
      "weighted avg       0.62      0.62      0.62      1491\n",
      "\n",
      "[[500 263]\n",
      " [309 419]]\n",
      "Accuracy is  61.63648558014755\n",
      "Time on model's work: 1.01 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.55      0.58       763\n",
      "         1.0       0.57      0.62      0.60       728\n",
      "\n",
      "   micro avg       0.59      0.59      0.59      1491\n",
      "   macro avg       0.59      0.59      0.59      1491\n",
      "weighted avg       0.59      0.59      0.59      1491\n",
      "\n",
      "[[421 342]\n",
      " [274 454]]\n",
      "Accuracy is  58.68544600938967\n",
      "Time on model's work: 118.156 s\n",
      "====================================================================================================\n",
      "XGBClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.74      0.67       763\n",
      "         1.0       0.66      0.52      0.58       728\n",
      "\n",
      "   micro avg       0.63      0.63      0.63      1491\n",
      "   macro avg       0.64      0.63      0.63      1491\n",
      "weighted avg       0.64      0.63      0.63      1491\n",
      "\n",
      "[[563 200]\n",
      " [347 381]]\n",
      "Accuracy is  63.31321260898726\n",
      "Time on model's work: 41.343 s\n",
      "====================================================================================================\n",
      "0:\tlearn: 0.6892697\ttotal: 69.8ms\tremaining: 1m 9s\n",
      "1:\tlearn: 0.6855122\ttotal: 153ms\tremaining: 1m 16s\n",
      "2:\tlearn: 0.6823025\ttotal: 224ms\tremaining: 1m 14s\n",
      "3:\tlearn: 0.6788473\ttotal: 296ms\tremaining: 1m 13s\n",
      "4:\tlearn: 0.6758956\ttotal: 367ms\tremaining: 1m 13s\n",
      "5:\tlearn: 0.6733132\ttotal: 446ms\tremaining: 1m 13s\n",
      "6:\tlearn: 0.6707453\ttotal: 535ms\tremaining: 1m 15s\n",
      "7:\tlearn: 0.6689539\ttotal: 613ms\tremaining: 1m 16s\n",
      "8:\tlearn: 0.6667042\ttotal: 695ms\tremaining: 1m 16s\n",
      "9:\tlearn: 0.6647311\ttotal: 774ms\tremaining: 1m 16s\n",
      "10:\tlearn: 0.6626775\ttotal: 849ms\tremaining: 1m 16s\n",
      "11:\tlearn: 0.6615174\ttotal: 922ms\tremaining: 1m 15s\n",
      "12:\tlearn: 0.6601132\ttotal: 990ms\tremaining: 1m 15s\n",
      "13:\tlearn: 0.6587081\ttotal: 1.06s\tremaining: 1m 14s\n",
      "14:\tlearn: 0.6572620\ttotal: 1.13s\tremaining: 1m 14s\n",
      "15:\tlearn: 0.6560949\ttotal: 1.21s\tremaining: 1m 14s\n",
      "16:\tlearn: 0.6548210\ttotal: 1.31s\tremaining: 1m 15s\n",
      "17:\tlearn: 0.6542042\ttotal: 1.39s\tremaining: 1m 15s\n",
      "18:\tlearn: 0.6535957\ttotal: 1.47s\tremaining: 1m 15s\n",
      "19:\tlearn: 0.6525773\ttotal: 1.55s\tremaining: 1m 15s\n",
      "20:\tlearn: 0.6520954\ttotal: 1.61s\tremaining: 1m 15s\n",
      "21:\tlearn: 0.6514220\ttotal: 1.69s\tremaining: 1m 14s\n",
      "22:\tlearn: 0.6505980\ttotal: 1.75s\tremaining: 1m 14s\n",
      "23:\tlearn: 0.6497566\ttotal: 1.82s\tremaining: 1m 13s\n",
      "24:\tlearn: 0.6489820\ttotal: 1.88s\tremaining: 1m 13s\n",
      "25:\tlearn: 0.6482030\ttotal: 1.95s\tremaining: 1m 13s\n",
      "26:\tlearn: 0.6477323\ttotal: 2.02s\tremaining: 1m 12s\n",
      "27:\tlearn: 0.6472400\ttotal: 2.08s\tremaining: 1m 12s\n",
      "28:\tlearn: 0.6466971\ttotal: 2.15s\tremaining: 1m 11s\n",
      "29:\tlearn: 0.6461056\ttotal: 2.23s\tremaining: 1m 12s\n",
      "30:\tlearn: 0.6456499\ttotal: 2.31s\tremaining: 1m 12s\n",
      "31:\tlearn: 0.6452669\ttotal: 2.39s\tremaining: 1m 12s\n",
      "32:\tlearn: 0.6445919\ttotal: 2.46s\tremaining: 1m 12s\n",
      "33:\tlearn: 0.6442451\ttotal: 2.54s\tremaining: 1m 12s\n",
      "34:\tlearn: 0.6437726\ttotal: 2.62s\tremaining: 1m 12s\n",
      "35:\tlearn: 0.6432504\ttotal: 2.71s\tremaining: 1m 12s\n",
      "36:\tlearn: 0.6426095\ttotal: 2.79s\tremaining: 1m 12s\n",
      "37:\tlearn: 0.6420252\ttotal: 2.87s\tremaining: 1m 12s\n",
      "38:\tlearn: 0.6411461\ttotal: 2.96s\tremaining: 1m 12s\n",
      "39:\tlearn: 0.6408078\ttotal: 3.03s\tremaining: 1m 12s\n",
      "40:\tlearn: 0.6404199\ttotal: 3.1s\tremaining: 1m 12s\n",
      "41:\tlearn: 0.6399928\ttotal: 3.18s\tremaining: 1m 12s\n",
      "42:\tlearn: 0.6393977\ttotal: 3.27s\tremaining: 1m 12s\n",
      "43:\tlearn: 0.6391744\ttotal: 3.35s\tremaining: 1m 12s\n",
      "44:\tlearn: 0.6383329\ttotal: 3.42s\tremaining: 1m 12s\n",
      "45:\tlearn: 0.6379198\ttotal: 3.5s\tremaining: 1m 12s\n",
      "46:\tlearn: 0.6377008\ttotal: 3.58s\tremaining: 1m 12s\n",
      "47:\tlearn: 0.6375624\ttotal: 3.67s\tremaining: 1m 12s\n",
      "48:\tlearn: 0.6372928\ttotal: 3.76s\tremaining: 1m 12s\n",
      "49:\tlearn: 0.6369974\ttotal: 3.84s\tremaining: 1m 12s\n",
      "50:\tlearn: 0.6365779\ttotal: 3.92s\tremaining: 1m 12s\n",
      "51:\tlearn: 0.6362124\ttotal: 4.02s\tremaining: 1m 13s\n",
      "52:\tlearn: 0.6358920\ttotal: 4.11s\tremaining: 1m 13s\n",
      "53:\tlearn: 0.6354298\ttotal: 4.18s\tremaining: 1m 13s\n",
      "54:\tlearn: 0.6348773\ttotal: 4.26s\tremaining: 1m 13s\n",
      "55:\tlearn: 0.6346842\ttotal: 4.32s\tremaining: 1m 12s\n",
      "56:\tlearn: 0.6343538\ttotal: 4.39s\tremaining: 1m 12s\n",
      "57:\tlearn: 0.6339371\ttotal: 4.46s\tremaining: 1m 12s\n",
      "58:\tlearn: 0.6335040\ttotal: 4.54s\tremaining: 1m 12s\n",
      "59:\tlearn: 0.6331910\ttotal: 4.65s\tremaining: 1m 12s\n",
      "60:\tlearn: 0.6327281\ttotal: 4.72s\tremaining: 1m 12s\n",
      "61:\tlearn: 0.6325018\ttotal: 4.79s\tremaining: 1m 12s\n",
      "62:\tlearn: 0.6320400\ttotal: 4.89s\tremaining: 1m 12s\n",
      "63:\tlearn: 0.6314066\ttotal: 4.98s\tremaining: 1m 12s\n",
      "64:\tlearn: 0.6307905\ttotal: 5.07s\tremaining: 1m 12s\n",
      "65:\tlearn: 0.6305521\ttotal: 5.14s\tremaining: 1m 12s\n",
      "66:\tlearn: 0.6303764\ttotal: 5.21s\tremaining: 1m 12s\n",
      "67:\tlearn: 0.6299507\ttotal: 5.29s\tremaining: 1m 12s\n",
      "68:\tlearn: 0.6298357\ttotal: 5.38s\tremaining: 1m 12s\n",
      "69:\tlearn: 0.6295632\ttotal: 5.46s\tremaining: 1m 12s\n",
      "70:\tlearn: 0.6293215\ttotal: 5.55s\tremaining: 1m 12s\n",
      "71:\tlearn: 0.6291582\ttotal: 5.64s\tremaining: 1m 12s\n",
      "72:\tlearn: 0.6289585\ttotal: 5.71s\tremaining: 1m 12s\n",
      "73:\tlearn: 0.6285342\ttotal: 5.79s\tremaining: 1m 12s\n",
      "74:\tlearn: 0.6283794\ttotal: 5.87s\tremaining: 1m 12s\n",
      "75:\tlearn: 0.6279781\ttotal: 5.97s\tremaining: 1m 12s\n",
      "76:\tlearn: 0.6278230\ttotal: 6.05s\tremaining: 1m 12s\n",
      "77:\tlearn: 0.6277150\ttotal: 6.13s\tremaining: 1m 12s\n",
      "78:\tlearn: 0.6276060\ttotal: 6.2s\tremaining: 1m 12s\n",
      "79:\tlearn: 0.6274201\ttotal: 6.31s\tremaining: 1m 12s\n",
      "80:\tlearn: 0.6271693\ttotal: 6.41s\tremaining: 1m 12s\n",
      "81:\tlearn: 0.6270372\ttotal: 6.48s\tremaining: 1m 12s\n",
      "82:\tlearn: 0.6268577\ttotal: 6.55s\tremaining: 1m 12s\n",
      "83:\tlearn: 0.6265666\ttotal: 6.62s\tremaining: 1m 12s\n",
      "84:\tlearn: 0.6263763\ttotal: 6.69s\tremaining: 1m 12s\n",
      "85:\tlearn: 0.6261614\ttotal: 6.76s\tremaining: 1m 11s\n",
      "86:\tlearn: 0.6259547\ttotal: 6.83s\tremaining: 1m 11s\n",
      "87:\tlearn: 0.6256825\ttotal: 6.91s\tremaining: 1m 11s\n",
      "88:\tlearn: 0.6254066\ttotal: 7s\tremaining: 1m 11s\n",
      "89:\tlearn: 0.6251304\ttotal: 7.09s\tremaining: 1m 11s\n",
      "90:\tlearn: 0.6250523\ttotal: 7.17s\tremaining: 1m 11s\n",
      "91:\tlearn: 0.6247817\ttotal: 7.25s\tremaining: 1m 11s\n",
      "92:\tlearn: 0.6245707\ttotal: 7.33s\tremaining: 1m 11s\n",
      "93:\tlearn: 0.6243689\ttotal: 7.42s\tremaining: 1m 11s\n",
      "94:\tlearn: 0.6242336\ttotal: 7.5s\tremaining: 1m 11s\n",
      "95:\tlearn: 0.6241304\ttotal: 7.58s\tremaining: 1m 11s\n",
      "96:\tlearn: 0.6238421\ttotal: 7.65s\tremaining: 1m 11s\n",
      "97:\tlearn: 0.6237334\ttotal: 7.72s\tremaining: 1m 11s\n",
      "98:\tlearn: 0.6236007\ttotal: 7.79s\tremaining: 1m 10s\n",
      "99:\tlearn: 0.6234317\ttotal: 7.86s\tremaining: 1m 10s\n",
      "100:\tlearn: 0.6232070\ttotal: 7.94s\tremaining: 1m 10s\n",
      "101:\tlearn: 0.6229658\ttotal: 8.05s\tremaining: 1m 10s\n",
      "102:\tlearn: 0.6228419\ttotal: 8.14s\tremaining: 1m 10s\n",
      "103:\tlearn: 0.6226243\ttotal: 8.23s\tremaining: 1m 10s\n",
      "104:\tlearn: 0.6224335\ttotal: 8.31s\tremaining: 1m 10s\n",
      "105:\tlearn: 0.6222533\ttotal: 8.38s\tremaining: 1m 10s\n",
      "106:\tlearn: 0.6221783\ttotal: 8.44s\tremaining: 1m 10s\n",
      "107:\tlearn: 0.6220442\ttotal: 8.52s\tremaining: 1m 10s\n",
      "108:\tlearn: 0.6218366\ttotal: 8.61s\tremaining: 1m 10s\n",
      "109:\tlearn: 0.6217154\ttotal: 8.7s\tremaining: 1m 10s\n",
      "110:\tlearn: 0.6216261\ttotal: 8.77s\tremaining: 1m 10s\n",
      "111:\tlearn: 0.6213586\ttotal: 8.86s\tremaining: 1m 10s\n",
      "112:\tlearn: 0.6212861\ttotal: 8.93s\tremaining: 1m 10s\n",
      "113:\tlearn: 0.6211825\ttotal: 9s\tremaining: 1m 9s\n",
      "114:\tlearn: 0.6211222\ttotal: 9.07s\tremaining: 1m 9s\n",
      "115:\tlearn: 0.6208958\ttotal: 9.14s\tremaining: 1m 9s\n",
      "116:\tlearn: 0.6208091\ttotal: 9.22s\tremaining: 1m 9s\n",
      "117:\tlearn: 0.6207087\ttotal: 9.33s\tremaining: 1m 9s\n",
      "118:\tlearn: 0.6205787\ttotal: 9.42s\tremaining: 1m 9s\n",
      "119:\tlearn: 0.6204600\ttotal: 9.5s\tremaining: 1m 9s\n",
      "120:\tlearn: 0.6201447\ttotal: 9.59s\tremaining: 1m 9s\n",
      "121:\tlearn: 0.6199877\ttotal: 9.67s\tremaining: 1m 9s\n",
      "122:\tlearn: 0.6199667\ttotal: 9.76s\tremaining: 1m 9s\n",
      "123:\tlearn: 0.6197293\ttotal: 9.84s\tremaining: 1m 9s\n",
      "124:\tlearn: 0.6194472\ttotal: 9.91s\tremaining: 1m 9s\n",
      "125:\tlearn: 0.6192720\ttotal: 9.98s\tremaining: 1m 9s\n",
      "126:\tlearn: 0.6191789\ttotal: 10.1s\tremaining: 1m 9s\n",
      "127:\tlearn: 0.6188496\ttotal: 10.2s\tremaining: 1m 9s\n",
      "128:\tlearn: 0.6187017\ttotal: 10.2s\tremaining: 1m 9s\n",
      "129:\tlearn: 0.6186551\ttotal: 10.3s\tremaining: 1m 9s\n",
      "130:\tlearn: 0.6185381\ttotal: 10.4s\tremaining: 1m 8s\n",
      "131:\tlearn: 0.6184098\ttotal: 10.5s\tremaining: 1m 8s\n",
      "132:\tlearn: 0.6182159\ttotal: 10.5s\tremaining: 1m 8s\n",
      "133:\tlearn: 0.6181461\ttotal: 10.7s\tremaining: 1m 8s\n",
      "134:\tlearn: 0.6180335\ttotal: 10.7s\tremaining: 1m 8s\n",
      "135:\tlearn: 0.6178830\ttotal: 10.8s\tremaining: 1m 8s\n",
      "136:\tlearn: 0.6176953\ttotal: 10.9s\tremaining: 1m 8s\n",
      "137:\tlearn: 0.6175441\ttotal: 11s\tremaining: 1m 8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138:\tlearn: 0.6174709\ttotal: 11.1s\tremaining: 1m 8s\n",
      "139:\tlearn: 0.6174350\ttotal: 11.1s\tremaining: 1m 8s\n",
      "140:\tlearn: 0.6172956\ttotal: 11.2s\tremaining: 1m 8s\n",
      "141:\tlearn: 0.6172124\ttotal: 11.2s\tremaining: 1m 7s\n",
      "142:\tlearn: 0.6170515\ttotal: 11.3s\tremaining: 1m 7s\n",
      "143:\tlearn: 0.6169332\ttotal: 11.4s\tremaining: 1m 7s\n",
      "144:\tlearn: 0.6167651\ttotal: 11.4s\tremaining: 1m 7s\n",
      "145:\tlearn: 0.6166714\ttotal: 11.5s\tremaining: 1m 7s\n",
      "146:\tlearn: 0.6166256\ttotal: 11.6s\tremaining: 1m 7s\n",
      "147:\tlearn: 0.6164988\ttotal: 11.6s\tremaining: 1m 7s\n",
      "148:\tlearn: 0.6163757\ttotal: 11.7s\tremaining: 1m 6s\n",
      "149:\tlearn: 0.6162858\ttotal: 11.8s\tremaining: 1m 6s\n",
      "150:\tlearn: 0.6162117\ttotal: 11.8s\tremaining: 1m 6s\n",
      "151:\tlearn: 0.6161325\ttotal: 11.9s\tremaining: 1m 6s\n",
      "152:\tlearn: 0.6159820\ttotal: 12s\tremaining: 1m 6s\n",
      "153:\tlearn: 0.6158477\ttotal: 12s\tremaining: 1m 6s\n",
      "154:\tlearn: 0.6156839\ttotal: 12.1s\tremaining: 1m 6s\n",
      "155:\tlearn: 0.6156033\ttotal: 12.2s\tremaining: 1m 5s\n",
      "156:\tlearn: 0.6155354\ttotal: 12.2s\tremaining: 1m 5s\n",
      "157:\tlearn: 0.6154731\ttotal: 12.3s\tremaining: 1m 5s\n",
      "158:\tlearn: 0.6153141\ttotal: 12.4s\tremaining: 1m 5s\n",
      "159:\tlearn: 0.6152287\ttotal: 12.5s\tremaining: 1m 5s\n",
      "160:\tlearn: 0.6151512\ttotal: 12.5s\tremaining: 1m 5s\n",
      "161:\tlearn: 0.6150723\ttotal: 12.6s\tremaining: 1m 5s\n",
      "162:\tlearn: 0.6149642\ttotal: 12.6s\tremaining: 1m 4s\n",
      "163:\tlearn: 0.6149013\ttotal: 12.7s\tremaining: 1m 4s\n",
      "164:\tlearn: 0.6148601\ttotal: 12.8s\tremaining: 1m 4s\n",
      "165:\tlearn: 0.6148273\ttotal: 12.8s\tremaining: 1m 4s\n",
      "166:\tlearn: 0.6147182\ttotal: 12.9s\tremaining: 1m 4s\n",
      "167:\tlearn: 0.6147108\ttotal: 13s\tremaining: 1m 4s\n",
      "168:\tlearn: 0.6146304\ttotal: 13s\tremaining: 1m 4s\n",
      "169:\tlearn: 0.6145540\ttotal: 13.1s\tremaining: 1m 3s\n",
      "170:\tlearn: 0.6144001\ttotal: 13.1s\tremaining: 1m 3s\n",
      "171:\tlearn: 0.6142055\ttotal: 13.2s\tremaining: 1m 3s\n",
      "172:\tlearn: 0.6141313\ttotal: 13.3s\tremaining: 1m 3s\n",
      "173:\tlearn: 0.6139303\ttotal: 13.3s\tremaining: 1m 3s\n",
      "174:\tlearn: 0.6138886\ttotal: 13.4s\tremaining: 1m 3s\n",
      "175:\tlearn: 0.6137870\ttotal: 13.5s\tremaining: 1m 3s\n",
      "176:\tlearn: 0.6136123\ttotal: 13.5s\tremaining: 1m 2s\n",
      "177:\tlearn: 0.6135021\ttotal: 13.6s\tremaining: 1m 2s\n",
      "178:\tlearn: 0.6133644\ttotal: 13.7s\tremaining: 1m 2s\n",
      "179:\tlearn: 0.6132713\ttotal: 13.7s\tremaining: 1m 2s\n",
      "180:\tlearn: 0.6132300\ttotal: 13.8s\tremaining: 1m 2s\n",
      "181:\tlearn: 0.6131171\ttotal: 13.9s\tremaining: 1m 2s\n",
      "182:\tlearn: 0.6127852\ttotal: 13.9s\tremaining: 1m 2s\n",
      "183:\tlearn: 0.6127102\ttotal: 14s\tremaining: 1m 2s\n",
      "184:\tlearn: 0.6126755\ttotal: 14.1s\tremaining: 1m 1s\n",
      "185:\tlearn: 0.6126071\ttotal: 14.1s\tremaining: 1m 1s\n",
      "186:\tlearn: 0.6125439\ttotal: 14.2s\tremaining: 1m 1s\n",
      "187:\tlearn: 0.6124531\ttotal: 14.3s\tremaining: 1m 1s\n",
      "188:\tlearn: 0.6123126\ttotal: 14.3s\tremaining: 1m 1s\n",
      "189:\tlearn: 0.6122364\ttotal: 14.4s\tremaining: 1m 1s\n",
      "190:\tlearn: 0.6121465\ttotal: 14.5s\tremaining: 1m 1s\n",
      "191:\tlearn: 0.6120659\ttotal: 14.5s\tremaining: 1m 1s\n",
      "192:\tlearn: 0.6120132\ttotal: 14.6s\tremaining: 1m\n",
      "193:\tlearn: 0.6118516\ttotal: 14.6s\tremaining: 1m\n",
      "194:\tlearn: 0.6117173\ttotal: 14.7s\tremaining: 1m\n",
      "195:\tlearn: 0.6116474\ttotal: 14.8s\tremaining: 1m\n",
      "196:\tlearn: 0.6114756\ttotal: 14.8s\tremaining: 1m\n",
      "197:\tlearn: 0.6113925\ttotal: 14.9s\tremaining: 1m\n",
      "198:\tlearn: 0.6113542\ttotal: 15s\tremaining: 1m\n",
      "199:\tlearn: 0.6113118\ttotal: 15s\tremaining: 1m\n",
      "200:\tlearn: 0.6112589\ttotal: 15.1s\tremaining: 1m\n",
      "201:\tlearn: 0.6112170\ttotal: 15.2s\tremaining: 59.9s\n",
      "202:\tlearn: 0.6111125\ttotal: 15.2s\tremaining: 59.8s\n",
      "203:\tlearn: 0.6110542\ttotal: 15.3s\tremaining: 59.7s\n",
      "204:\tlearn: 0.6109866\ttotal: 15.4s\tremaining: 59.6s\n",
      "205:\tlearn: 0.6108856\ttotal: 15.4s\tremaining: 59.5s\n",
      "206:\tlearn: 0.6108054\ttotal: 15.5s\tremaining: 59.4s\n",
      "207:\tlearn: 0.6107420\ttotal: 15.6s\tremaining: 59.3s\n",
      "208:\tlearn: 0.6106911\ttotal: 15.6s\tremaining: 59.2s\n",
      "209:\tlearn: 0.6106225\ttotal: 15.7s\tremaining: 59.1s\n",
      "210:\tlearn: 0.6105419\ttotal: 15.8s\tremaining: 59s\n",
      "211:\tlearn: 0.6105084\ttotal: 15.8s\tremaining: 58.9s\n",
      "212:\tlearn: 0.6104544\ttotal: 15.9s\tremaining: 58.8s\n",
      "213:\tlearn: 0.6104084\ttotal: 16s\tremaining: 58.7s\n",
      "214:\tlearn: 0.6103463\ttotal: 16s\tremaining: 58.5s\n",
      "215:\tlearn: 0.6102705\ttotal: 16.1s\tremaining: 58.4s\n",
      "216:\tlearn: 0.6102050\ttotal: 16.2s\tremaining: 58.3s\n",
      "217:\tlearn: 0.6100458\ttotal: 16.2s\tremaining: 58.2s\n",
      "218:\tlearn: 0.6099559\ttotal: 16.3s\tremaining: 58.1s\n",
      "219:\tlearn: 0.6098948\ttotal: 16.4s\tremaining: 58s\n",
      "220:\tlearn: 0.6097418\ttotal: 16.4s\tremaining: 57.9s\n",
      "221:\tlearn: 0.6095765\ttotal: 16.5s\tremaining: 57.8s\n",
      "222:\tlearn: 0.6094572\ttotal: 16.6s\tremaining: 57.7s\n",
      "223:\tlearn: 0.6094014\ttotal: 16.6s\tremaining: 57.6s\n",
      "224:\tlearn: 0.6093549\ttotal: 16.7s\tremaining: 57.5s\n",
      "225:\tlearn: 0.6092713\ttotal: 16.8s\tremaining: 57.4s\n",
      "226:\tlearn: 0.6092282\ttotal: 16.8s\tremaining: 57.3s\n",
      "227:\tlearn: 0.6091307\ttotal: 16.9s\tremaining: 57.2s\n",
      "228:\tlearn: 0.6090405\ttotal: 17s\tremaining: 57.1s\n",
      "229:\tlearn: 0.6089364\ttotal: 17s\tremaining: 57s\n",
      "230:\tlearn: 0.6088987\ttotal: 17.1s\tremaining: 56.9s\n",
      "231:\tlearn: 0.6088555\ttotal: 17.2s\tremaining: 56.8s\n",
      "232:\tlearn: 0.6088113\ttotal: 17.2s\tremaining: 56.7s\n",
      "233:\tlearn: 0.6086972\ttotal: 17.3s\tremaining: 56.6s\n",
      "234:\tlearn: 0.6086428\ttotal: 17.4s\tremaining: 56.5s\n",
      "235:\tlearn: 0.6086159\ttotal: 17.4s\tremaining: 56.4s\n",
      "236:\tlearn: 0.6084843\ttotal: 17.5s\tremaining: 56.3s\n",
      "237:\tlearn: 0.6083462\ttotal: 17.6s\tremaining: 56.3s\n",
      "238:\tlearn: 0.6082866\ttotal: 17.6s\tremaining: 56.2s\n",
      "239:\tlearn: 0.6081607\ttotal: 17.7s\tremaining: 56s\n",
      "240:\tlearn: 0.6081044\ttotal: 17.8s\tremaining: 56s\n",
      "241:\tlearn: 0.6080178\ttotal: 17.8s\tremaining: 55.9s\n",
      "242:\tlearn: 0.6079767\ttotal: 17.9s\tremaining: 55.8s\n",
      "243:\tlearn: 0.6079329\ttotal: 18s\tremaining: 55.7s\n",
      "244:\tlearn: 0.6078761\ttotal: 18s\tremaining: 55.6s\n",
      "245:\tlearn: 0.6078296\ttotal: 18.1s\tremaining: 55.5s\n",
      "246:\tlearn: 0.6077669\ttotal: 18.2s\tremaining: 55.4s\n",
      "247:\tlearn: 0.6076919\ttotal: 18.2s\tremaining: 55.3s\n",
      "248:\tlearn: 0.6076032\ttotal: 18.3s\tremaining: 55.2s\n",
      "249:\tlearn: 0.6075268\ttotal: 18.4s\tremaining: 55.1s\n",
      "250:\tlearn: 0.6074508\ttotal: 18.4s\tremaining: 55s\n",
      "251:\tlearn: 0.6074006\ttotal: 18.5s\tremaining: 54.9s\n",
      "252:\tlearn: 0.6073432\ttotal: 18.6s\tremaining: 54.8s\n",
      "253:\tlearn: 0.6072739\ttotal: 18.6s\tremaining: 54.7s\n",
      "254:\tlearn: 0.6072137\ttotal: 18.7s\tremaining: 54.6s\n",
      "255:\tlearn: 0.6071494\ttotal: 18.8s\tremaining: 54.5s\n",
      "256:\tlearn: 0.6069678\ttotal: 18.8s\tremaining: 54.5s\n",
      "257:\tlearn: 0.6069096\ttotal: 18.9s\tremaining: 54.4s\n",
      "258:\tlearn: 0.6068738\ttotal: 19s\tremaining: 54.3s\n",
      "259:\tlearn: 0.6068314\ttotal: 19s\tremaining: 54.2s\n",
      "260:\tlearn: 0.6067415\ttotal: 19.1s\tremaining: 54.1s\n",
      "261:\tlearn: 0.6066969\ttotal: 19.2s\tremaining: 54s\n",
      "262:\tlearn: 0.6066445\ttotal: 19.3s\tremaining: 53.9s\n",
      "263:\tlearn: 0.6065861\ttotal: 19.3s\tremaining: 53.9s\n",
      "264:\tlearn: 0.6064917\ttotal: 19.4s\tremaining: 53.8s\n",
      "265:\tlearn: 0.6064460\ttotal: 19.4s\tremaining: 53.7s\n",
      "266:\tlearn: 0.6064046\ttotal: 19.5s\tremaining: 53.6s\n",
      "267:\tlearn: 0.6061538\ttotal: 19.6s\tremaining: 53.5s\n",
      "268:\tlearn: 0.6061429\ttotal: 19.6s\tremaining: 53.4s\n",
      "269:\tlearn: 0.6061099\ttotal: 19.7s\tremaining: 53.3s\n",
      "270:\tlearn: 0.6059770\ttotal: 19.8s\tremaining: 53.2s\n",
      "271:\tlearn: 0.6058426\ttotal: 19.8s\tremaining: 53.1s\n",
      "272:\tlearn: 0.6058249\ttotal: 19.9s\tremaining: 53s\n",
      "273:\tlearn: 0.6057916\ttotal: 20s\tremaining: 53s\n",
      "274:\tlearn: 0.6056184\ttotal: 20.1s\tremaining: 52.9s\n",
      "275:\tlearn: 0.6054941\ttotal: 20.1s\tremaining: 52.8s\n",
      "276:\tlearn: 0.6052695\ttotal: 20.2s\tremaining: 52.7s\n",
      "277:\tlearn: 0.6051991\ttotal: 20.3s\tremaining: 52.6s\n",
      "278:\tlearn: 0.6051605\ttotal: 20.3s\tremaining: 52.5s\n",
      "279:\tlearn: 0.6051278\ttotal: 20.4s\tremaining: 52.5s\n",
      "280:\tlearn: 0.6050694\ttotal: 20.5s\tremaining: 52.4s\n",
      "281:\tlearn: 0.6049807\ttotal: 20.5s\tremaining: 52.3s\n",
      "282:\tlearn: 0.6049383\ttotal: 20.6s\tremaining: 52.2s\n",
      "283:\tlearn: 0.6049255\ttotal: 20.7s\tremaining: 52.1s\n",
      "284:\tlearn: 0.6047966\ttotal: 20.7s\tremaining: 52s\n",
      "285:\tlearn: 0.6047102\ttotal: 20.8s\tremaining: 51.9s\n",
      "286:\tlearn: 0.6046967\ttotal: 20.9s\tremaining: 51.9s\n",
      "287:\tlearn: 0.6046347\ttotal: 20.9s\tremaining: 51.8s\n",
      "288:\tlearn: 0.6045729\ttotal: 21s\tremaining: 51.7s\n",
      "289:\tlearn: 0.6044759\ttotal: 21.1s\tremaining: 51.6s\n",
      "290:\tlearn: 0.6043663\ttotal: 21.1s\tremaining: 51.5s\n",
      "291:\tlearn: 0.6042362\ttotal: 21.2s\tremaining: 51.4s\n",
      "292:\tlearn: 0.6041892\ttotal: 21.3s\tremaining: 51.3s\n",
      "293:\tlearn: 0.6040918\ttotal: 21.3s\tremaining: 51.3s\n",
      "294:\tlearn: 0.6039810\ttotal: 21.4s\tremaining: 51.2s\n",
      "295:\tlearn: 0.6038996\ttotal: 21.5s\tremaining: 51.1s\n",
      "296:\tlearn: 0.6038368\ttotal: 21.5s\tremaining: 51s\n",
      "297:\tlearn: 0.6037745\ttotal: 21.6s\tremaining: 50.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298:\tlearn: 0.6036119\ttotal: 21.7s\tremaining: 50.9s\n",
      "299:\tlearn: 0.6034671\ttotal: 21.8s\tremaining: 50.8s\n",
      "300:\tlearn: 0.6034338\ttotal: 21.8s\tremaining: 50.7s\n",
      "301:\tlearn: 0.6033851\ttotal: 21.9s\tremaining: 50.6s\n",
      "302:\tlearn: 0.6033305\ttotal: 22s\tremaining: 50.5s\n",
      "303:\tlearn: 0.6033122\ttotal: 22s\tremaining: 50.4s\n",
      "304:\tlearn: 0.6032613\ttotal: 22.1s\tremaining: 50.4s\n",
      "305:\tlearn: 0.6031080\ttotal: 22.2s\tremaining: 50.3s\n",
      "306:\tlearn: 0.6030356\ttotal: 22.2s\tremaining: 50.2s\n",
      "307:\tlearn: 0.6029953\ttotal: 22.3s\tremaining: 50.1s\n",
      "308:\tlearn: 0.6029229\ttotal: 22.4s\tremaining: 50s\n",
      "309:\tlearn: 0.6029106\ttotal: 22.4s\tremaining: 49.9s\n",
      "310:\tlearn: 0.6028973\ttotal: 22.5s\tremaining: 49.8s\n",
      "311:\tlearn: 0.6028416\ttotal: 22.6s\tremaining: 49.7s\n",
      "312:\tlearn: 0.6028074\ttotal: 22.6s\tremaining: 49.7s\n",
      "313:\tlearn: 0.6027421\ttotal: 22.7s\tremaining: 49.6s\n",
      "314:\tlearn: 0.6026392\ttotal: 22.8s\tremaining: 49.5s\n",
      "315:\tlearn: 0.6025229\ttotal: 22.8s\tremaining: 49.4s\n",
      "316:\tlearn: 0.6024768\ttotal: 22.9s\tremaining: 49.4s\n",
      "317:\tlearn: 0.6024156\ttotal: 23s\tremaining: 49.3s\n",
      "318:\tlearn: 0.6023099\ttotal: 23.1s\tremaining: 49.3s\n",
      "319:\tlearn: 0.6022372\ttotal: 23.2s\tremaining: 49.2s\n",
      "320:\tlearn: 0.6021649\ttotal: 23.2s\tremaining: 49.1s\n",
      "321:\tlearn: 0.6020767\ttotal: 23.3s\tremaining: 49.1s\n",
      "322:\tlearn: 0.6018959\ttotal: 23.4s\tremaining: 49s\n",
      "323:\tlearn: 0.6018264\ttotal: 23.5s\tremaining: 49s\n",
      "324:\tlearn: 0.6017862\ttotal: 23.5s\tremaining: 48.9s\n",
      "325:\tlearn: 0.6017582\ttotal: 23.6s\tremaining: 48.9s\n",
      "326:\tlearn: 0.6016766\ttotal: 23.7s\tremaining: 48.8s\n",
      "327:\tlearn: 0.6014999\ttotal: 23.8s\tremaining: 48.7s\n",
      "328:\tlearn: 0.6014448\ttotal: 23.9s\tremaining: 48.7s\n",
      "329:\tlearn: 0.6013914\ttotal: 23.9s\tremaining: 48.6s\n",
      "330:\tlearn: 0.6013820\ttotal: 24s\tremaining: 48.6s\n",
      "331:\tlearn: 0.6013353\ttotal: 24.1s\tremaining: 48.5s\n",
      "332:\tlearn: 0.6013182\ttotal: 24.2s\tremaining: 48.5s\n",
      "333:\tlearn: 0.6012707\ttotal: 24.3s\tremaining: 48.4s\n",
      "334:\tlearn: 0.6011571\ttotal: 24.3s\tremaining: 48.3s\n",
      "335:\tlearn: 0.6009989\ttotal: 24.4s\tremaining: 48.2s\n",
      "336:\tlearn: 0.6009839\ttotal: 24.5s\tremaining: 48.1s\n",
      "337:\tlearn: 0.6008725\ttotal: 24.5s\tremaining: 48.1s\n",
      "338:\tlearn: 0.6007630\ttotal: 24.6s\tremaining: 48s\n",
      "339:\tlearn: 0.6007028\ttotal: 24.7s\tremaining: 47.9s\n",
      "340:\tlearn: 0.6006212\ttotal: 24.7s\tremaining: 47.8s\n",
      "341:\tlearn: 0.6005621\ttotal: 24.8s\tremaining: 47.7s\n",
      "342:\tlearn: 0.6004381\ttotal: 24.9s\tremaining: 47.7s\n",
      "343:\tlearn: 0.6003528\ttotal: 24.9s\tremaining: 47.6s\n",
      "344:\tlearn: 0.6003123\ttotal: 25s\tremaining: 47.5s\n",
      "345:\tlearn: 0.6001394\ttotal: 25.1s\tremaining: 47.4s\n",
      "346:\tlearn: 0.6001254\ttotal: 25.2s\tremaining: 47.3s\n",
      "347:\tlearn: 0.6000417\ttotal: 25.2s\tremaining: 47.3s\n",
      "348:\tlearn: 0.5999118\ttotal: 25.3s\tremaining: 47.2s\n",
      "349:\tlearn: 0.5998440\ttotal: 25.4s\tremaining: 47.1s\n",
      "350:\tlearn: 0.5997624\ttotal: 25.4s\tremaining: 47s\n",
      "351:\tlearn: 0.5996245\ttotal: 25.5s\tremaining: 46.9s\n",
      "352:\tlearn: 0.5994314\ttotal: 25.6s\tremaining: 46.8s\n",
      "353:\tlearn: 0.5993601\ttotal: 25.6s\tremaining: 46.8s\n",
      "354:\tlearn: 0.5992954\ttotal: 25.7s\tremaining: 46.7s\n",
      "355:\tlearn: 0.5992125\ttotal: 25.8s\tremaining: 46.6s\n",
      "356:\tlearn: 0.5991057\ttotal: 25.8s\tremaining: 46.5s\n",
      "357:\tlearn: 0.5990954\ttotal: 25.9s\tremaining: 46.4s\n",
      "358:\tlearn: 0.5990131\ttotal: 26s\tremaining: 46.4s\n",
      "359:\tlearn: 0.5990044\ttotal: 26.1s\tremaining: 46.3s\n",
      "360:\tlearn: 0.5989518\ttotal: 26.1s\tremaining: 46.3s\n",
      "361:\tlearn: 0.5988566\ttotal: 26.2s\tremaining: 46.2s\n",
      "362:\tlearn: 0.5987480\ttotal: 26.4s\tremaining: 46.2s\n",
      "363:\tlearn: 0.5987132\ttotal: 26.4s\tremaining: 46.2s\n",
      "364:\tlearn: 0.5986503\ttotal: 26.5s\tremaining: 46.1s\n",
      "365:\tlearn: 0.5985573\ttotal: 26.6s\tremaining: 46s\n",
      "366:\tlearn: 0.5985165\ttotal: 26.6s\tremaining: 45.9s\n",
      "367:\tlearn: 0.5983873\ttotal: 26.7s\tremaining: 45.9s\n",
      "368:\tlearn: 0.5982854\ttotal: 26.8s\tremaining: 45.8s\n",
      "369:\tlearn: 0.5981652\ttotal: 26.9s\tremaining: 45.7s\n",
      "370:\tlearn: 0.5980660\ttotal: 26.9s\tremaining: 45.6s\n",
      "371:\tlearn: 0.5980513\ttotal: 27s\tremaining: 45.6s\n",
      "372:\tlearn: 0.5979214\ttotal: 27.1s\tremaining: 45.5s\n",
      "373:\tlearn: 0.5978508\ttotal: 27.1s\tremaining: 45.4s\n",
      "374:\tlearn: 0.5977769\ttotal: 27.2s\tremaining: 45.3s\n",
      "375:\tlearn: 0.5976694\ttotal: 27.3s\tremaining: 45.3s\n",
      "376:\tlearn: 0.5975913\ttotal: 27.3s\tremaining: 45.2s\n",
      "377:\tlearn: 0.5974138\ttotal: 27.4s\tremaining: 45.1s\n",
      "378:\tlearn: 0.5973639\ttotal: 27.5s\tremaining: 45s\n",
      "379:\tlearn: 0.5972178\ttotal: 27.6s\tremaining: 45s\n",
      "380:\tlearn: 0.5971347\ttotal: 27.6s\tremaining: 44.9s\n",
      "381:\tlearn: 0.5971160\ttotal: 27.7s\tremaining: 44.8s\n",
      "382:\tlearn: 0.5970034\ttotal: 27.8s\tremaining: 44.7s\n",
      "383:\tlearn: 0.5969515\ttotal: 27.8s\tremaining: 44.7s\n",
      "384:\tlearn: 0.5968608\ttotal: 27.9s\tremaining: 44.6s\n",
      "385:\tlearn: 0.5968463\ttotal: 28s\tremaining: 44.5s\n",
      "386:\tlearn: 0.5967209\ttotal: 28s\tremaining: 44.4s\n",
      "387:\tlearn: 0.5966692\ttotal: 28.1s\tremaining: 44.3s\n",
      "388:\tlearn: 0.5965391\ttotal: 28.2s\tremaining: 44.2s\n",
      "389:\tlearn: 0.5964562\ttotal: 28.2s\tremaining: 44.2s\n",
      "390:\tlearn: 0.5963857\ttotal: 28.3s\tremaining: 44.1s\n",
      "391:\tlearn: 0.5962498\ttotal: 28.4s\tremaining: 44s\n",
      "392:\tlearn: 0.5960624\ttotal: 28.4s\tremaining: 43.9s\n",
      "393:\tlearn: 0.5959279\ttotal: 28.5s\tremaining: 43.8s\n",
      "394:\tlearn: 0.5958307\ttotal: 28.6s\tremaining: 43.8s\n",
      "395:\tlearn: 0.5957533\ttotal: 28.6s\tremaining: 43.7s\n",
      "396:\tlearn: 0.5956666\ttotal: 28.7s\tremaining: 43.6s\n",
      "397:\tlearn: 0.5955564\ttotal: 28.8s\tremaining: 43.5s\n",
      "398:\tlearn: 0.5954535\ttotal: 28.8s\tremaining: 43.4s\n",
      "399:\tlearn: 0.5954030\ttotal: 28.9s\tremaining: 43.4s\n",
      "400:\tlearn: 0.5953105\ttotal: 29s\tremaining: 43.3s\n",
      "401:\tlearn: 0.5952425\ttotal: 29s\tremaining: 43.2s\n",
      "402:\tlearn: 0.5950417\ttotal: 29.1s\tremaining: 43.1s\n",
      "403:\tlearn: 0.5949367\ttotal: 29.2s\tremaining: 43.1s\n",
      "404:\tlearn: 0.5948591\ttotal: 29.3s\tremaining: 43s\n",
      "405:\tlearn: 0.5946077\ttotal: 29.3s\tremaining: 42.9s\n",
      "406:\tlearn: 0.5945593\ttotal: 29.4s\tremaining: 42.8s\n",
      "407:\tlearn: 0.5944957\ttotal: 29.5s\tremaining: 42.7s\n",
      "408:\tlearn: 0.5944542\ttotal: 29.5s\tremaining: 42.7s\n",
      "409:\tlearn: 0.5942756\ttotal: 29.6s\tremaining: 42.6s\n",
      "410:\tlearn: 0.5941735\ttotal: 29.7s\tremaining: 42.5s\n",
      "411:\tlearn: 0.5940842\ttotal: 29.7s\tremaining: 42.4s\n",
      "412:\tlearn: 0.5940316\ttotal: 29.8s\tremaining: 42.3s\n",
      "413:\tlearn: 0.5937777\ttotal: 29.9s\tremaining: 42.3s\n",
      "414:\tlearn: 0.5936992\ttotal: 29.9s\tremaining: 42.2s\n",
      "415:\tlearn: 0.5936080\ttotal: 30s\tremaining: 42.1s\n",
      "416:\tlearn: 0.5934458\ttotal: 30.1s\tremaining: 42.1s\n",
      "417:\tlearn: 0.5933354\ttotal: 30.1s\tremaining: 42s\n",
      "418:\tlearn: 0.5933205\ttotal: 30.2s\tremaining: 41.9s\n",
      "419:\tlearn: 0.5932179\ttotal: 30.3s\tremaining: 41.8s\n",
      "420:\tlearn: 0.5931759\ttotal: 30.4s\tremaining: 41.7s\n",
      "421:\tlearn: 0.5931513\ttotal: 30.4s\tremaining: 41.7s\n",
      "422:\tlearn: 0.5931332\ttotal: 30.5s\tremaining: 41.6s\n",
      "423:\tlearn: 0.5930778\ttotal: 30.5s\tremaining: 41.5s\n",
      "424:\tlearn: 0.5929203\ttotal: 30.6s\tremaining: 41.4s\n",
      "425:\tlearn: 0.5928524\ttotal: 30.7s\tremaining: 41.4s\n",
      "426:\tlearn: 0.5926387\ttotal: 30.8s\tremaining: 41.3s\n",
      "427:\tlearn: 0.5925790\ttotal: 30.8s\tremaining: 41.2s\n",
      "428:\tlearn: 0.5924996\ttotal: 30.9s\tremaining: 41.1s\n",
      "429:\tlearn: 0.5924394\ttotal: 31s\tremaining: 41s\n",
      "430:\tlearn: 0.5923865\ttotal: 31s\tremaining: 41s\n",
      "431:\tlearn: 0.5923123\ttotal: 31.1s\tremaining: 40.9s\n",
      "432:\tlearn: 0.5922473\ttotal: 31.2s\tremaining: 40.8s\n",
      "433:\tlearn: 0.5921884\ttotal: 31.2s\tremaining: 40.7s\n",
      "434:\tlearn: 0.5920712\ttotal: 31.3s\tremaining: 40.7s\n",
      "435:\tlearn: 0.5919823\ttotal: 31.4s\tremaining: 40.6s\n",
      "436:\tlearn: 0.5918739\ttotal: 31.4s\tremaining: 40.5s\n",
      "437:\tlearn: 0.5917733\ttotal: 31.5s\tremaining: 40.4s\n",
      "438:\tlearn: 0.5917119\ttotal: 31.6s\tremaining: 40.4s\n",
      "439:\tlearn: 0.5915144\ttotal: 31.7s\tremaining: 40.3s\n",
      "440:\tlearn: 0.5914104\ttotal: 31.7s\tremaining: 40.2s\n",
      "441:\tlearn: 0.5913555\ttotal: 31.8s\tremaining: 40.1s\n",
      "442:\tlearn: 0.5913422\ttotal: 31.9s\tremaining: 40s\n",
      "443:\tlearn: 0.5911695\ttotal: 31.9s\tremaining: 40s\n",
      "444:\tlearn: 0.5911187\ttotal: 32s\tremaining: 39.9s\n",
      "445:\tlearn: 0.5910635\ttotal: 32.1s\tremaining: 39.8s\n",
      "446:\tlearn: 0.5909584\ttotal: 32.1s\tremaining: 39.7s\n",
      "447:\tlearn: 0.5908777\ttotal: 32.2s\tremaining: 39.7s\n",
      "448:\tlearn: 0.5908567\ttotal: 32.2s\tremaining: 39.6s\n",
      "449:\tlearn: 0.5908431\ttotal: 32.3s\tremaining: 39.5s\n",
      "450:\tlearn: 0.5907959\ttotal: 32.4s\tremaining: 39.4s\n",
      "451:\tlearn: 0.5906991\ttotal: 32.4s\tremaining: 39.3s\n",
      "452:\tlearn: 0.5905405\ttotal: 32.5s\tremaining: 39.3s\n",
      "453:\tlearn: 0.5904812\ttotal: 32.6s\tremaining: 39.2s\n",
      "454:\tlearn: 0.5904147\ttotal: 32.7s\tremaining: 39.1s\n",
      "455:\tlearn: 0.5903591\ttotal: 32.7s\tremaining: 39s\n",
      "456:\tlearn: 0.5901844\ttotal: 32.8s\tremaining: 39s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457:\tlearn: 0.5901406\ttotal: 32.9s\tremaining: 38.9s\n",
      "458:\tlearn: 0.5901008\ttotal: 32.9s\tremaining: 38.8s\n",
      "459:\tlearn: 0.5900307\ttotal: 33s\tremaining: 38.7s\n",
      "460:\tlearn: 0.5899852\ttotal: 33.1s\tremaining: 38.7s\n",
      "461:\tlearn: 0.5898565\ttotal: 33.1s\tremaining: 38.6s\n",
      "462:\tlearn: 0.5898408\ttotal: 33.2s\tremaining: 38.5s\n",
      "463:\tlearn: 0.5898232\ttotal: 33.2s\tremaining: 38.4s\n",
      "464:\tlearn: 0.5897168\ttotal: 33.3s\tremaining: 38.3s\n",
      "465:\tlearn: 0.5895158\ttotal: 33.4s\tremaining: 38.3s\n",
      "466:\tlearn: 0.5894618\ttotal: 33.5s\tremaining: 38.2s\n",
      "467:\tlearn: 0.5894175\ttotal: 33.5s\tremaining: 38.1s\n",
      "468:\tlearn: 0.5893181\ttotal: 33.6s\tremaining: 38s\n",
      "469:\tlearn: 0.5892635\ttotal: 33.6s\tremaining: 37.9s\n",
      "470:\tlearn: 0.5892067\ttotal: 33.7s\tremaining: 37.9s\n",
      "471:\tlearn: 0.5891740\ttotal: 33.8s\tremaining: 37.8s\n",
      "472:\tlearn: 0.5891415\ttotal: 33.8s\tremaining: 37.7s\n",
      "473:\tlearn: 0.5890855\ttotal: 33.9s\tremaining: 37.6s\n",
      "474:\tlearn: 0.5890144\ttotal: 34s\tremaining: 37.6s\n",
      "475:\tlearn: 0.5889382\ttotal: 34s\tremaining: 37.5s\n",
      "476:\tlearn: 0.5888360\ttotal: 34.1s\tremaining: 37.4s\n",
      "477:\tlearn: 0.5887884\ttotal: 34.2s\tremaining: 37.3s\n",
      "478:\tlearn: 0.5886532\ttotal: 34.3s\tremaining: 37.3s\n",
      "479:\tlearn: 0.5886417\ttotal: 34.3s\tremaining: 37.2s\n",
      "480:\tlearn: 0.5886101\ttotal: 34.4s\tremaining: 37.1s\n",
      "481:\tlearn: 0.5885881\ttotal: 34.4s\tremaining: 37s\n",
      "482:\tlearn: 0.5885161\ttotal: 34.5s\tremaining: 36.9s\n",
      "483:\tlearn: 0.5884729\ttotal: 34.6s\tremaining: 36.9s\n",
      "484:\tlearn: 0.5883804\ttotal: 34.6s\tremaining: 36.8s\n",
      "485:\tlearn: 0.5883652\ttotal: 34.7s\tremaining: 36.7s\n",
      "486:\tlearn: 0.5882793\ttotal: 34.8s\tremaining: 36.6s\n",
      "487:\tlearn: 0.5882346\ttotal: 34.8s\tremaining: 36.5s\n",
      "488:\tlearn: 0.5881500\ttotal: 34.9s\tremaining: 36.5s\n",
      "489:\tlearn: 0.5881028\ttotal: 35s\tremaining: 36.4s\n",
      "490:\tlearn: 0.5880005\ttotal: 35s\tremaining: 36.3s\n",
      "491:\tlearn: 0.5879830\ttotal: 35.1s\tremaining: 36.2s\n",
      "492:\tlearn: 0.5879653\ttotal: 35.1s\tremaining: 36.1s\n",
      "493:\tlearn: 0.5879038\ttotal: 35.2s\tremaining: 36.1s\n",
      "494:\tlearn: 0.5878521\ttotal: 35.3s\tremaining: 36s\n",
      "495:\tlearn: 0.5878251\ttotal: 35.3s\tremaining: 35.9s\n",
      "496:\tlearn: 0.5877241\ttotal: 35.4s\tremaining: 35.8s\n",
      "497:\tlearn: 0.5876436\ttotal: 35.5s\tremaining: 35.8s\n",
      "498:\tlearn: 0.5875469\ttotal: 35.5s\tremaining: 35.7s\n",
      "499:\tlearn: 0.5874615\ttotal: 35.6s\tremaining: 35.6s\n",
      "500:\tlearn: 0.5874051\ttotal: 35.7s\tremaining: 35.5s\n",
      "501:\tlearn: 0.5873662\ttotal: 35.7s\tremaining: 35.5s\n",
      "502:\tlearn: 0.5873520\ttotal: 35.8s\tremaining: 35.4s\n",
      "503:\tlearn: 0.5873141\ttotal: 35.9s\tremaining: 35.3s\n",
      "504:\tlearn: 0.5872633\ttotal: 35.9s\tremaining: 35.2s\n",
      "505:\tlearn: 0.5871797\ttotal: 36s\tremaining: 35.1s\n",
      "506:\tlearn: 0.5871612\ttotal: 36.1s\tremaining: 35.1s\n",
      "507:\tlearn: 0.5870484\ttotal: 36.1s\tremaining: 35s\n",
      "508:\tlearn: 0.5869894\ttotal: 36.2s\tremaining: 34.9s\n",
      "509:\tlearn: 0.5869461\ttotal: 36.3s\tremaining: 34.8s\n",
      "510:\tlearn: 0.5868929\ttotal: 36.3s\tremaining: 34.8s\n",
      "511:\tlearn: 0.5868409\ttotal: 36.4s\tremaining: 34.7s\n",
      "512:\tlearn: 0.5867766\ttotal: 36.5s\tremaining: 34.6s\n",
      "513:\tlearn: 0.5867332\ttotal: 36.5s\tremaining: 34.5s\n",
      "514:\tlearn: 0.5866737\ttotal: 36.6s\tremaining: 34.5s\n",
      "515:\tlearn: 0.5866312\ttotal: 36.7s\tremaining: 34.4s\n",
      "516:\tlearn: 0.5865743\ttotal: 36.7s\tremaining: 34.3s\n",
      "517:\tlearn: 0.5865107\ttotal: 36.8s\tremaining: 34.2s\n",
      "518:\tlearn: 0.5864985\ttotal: 36.9s\tremaining: 34.2s\n",
      "519:\tlearn: 0.5863406\ttotal: 36.9s\tremaining: 34.1s\n",
      "520:\tlearn: 0.5863312\ttotal: 37s\tremaining: 34s\n",
      "521:\tlearn: 0.5862816\ttotal: 37.1s\tremaining: 33.9s\n",
      "522:\tlearn: 0.5862230\ttotal: 37.1s\tremaining: 33.9s\n",
      "523:\tlearn: 0.5861698\ttotal: 37.2s\tremaining: 33.8s\n",
      "524:\tlearn: 0.5861105\ttotal: 37.3s\tremaining: 33.7s\n",
      "525:\tlearn: 0.5860675\ttotal: 37.3s\tremaining: 33.6s\n",
      "526:\tlearn: 0.5860155\ttotal: 37.4s\tremaining: 33.6s\n",
      "527:\tlearn: 0.5859611\ttotal: 37.4s\tremaining: 33.5s\n",
      "528:\tlearn: 0.5858953\ttotal: 37.5s\tremaining: 33.4s\n",
      "529:\tlearn: 0.5857421\ttotal: 37.6s\tremaining: 33.3s\n",
      "530:\tlearn: 0.5856982\ttotal: 37.6s\tremaining: 33.3s\n",
      "531:\tlearn: 0.5855967\ttotal: 37.7s\tremaining: 33.2s\n",
      "532:\tlearn: 0.5855687\ttotal: 37.8s\tremaining: 33.1s\n",
      "533:\tlearn: 0.5855256\ttotal: 37.8s\tremaining: 33s\n",
      "534:\tlearn: 0.5854719\ttotal: 37.9s\tremaining: 33s\n",
      "535:\tlearn: 0.5854432\ttotal: 38s\tremaining: 32.9s\n",
      "536:\tlearn: 0.5853878\ttotal: 38s\tremaining: 32.8s\n",
      "537:\tlearn: 0.5852196\ttotal: 38.1s\tremaining: 32.7s\n",
      "538:\tlearn: 0.5852031\ttotal: 38.2s\tremaining: 32.6s\n",
      "539:\tlearn: 0.5851515\ttotal: 38.2s\tremaining: 32.6s\n",
      "540:\tlearn: 0.5850740\ttotal: 38.3s\tremaining: 32.5s\n",
      "541:\tlearn: 0.5850380\ttotal: 38.4s\tremaining: 32.4s\n",
      "542:\tlearn: 0.5849322\ttotal: 38.4s\tremaining: 32.4s\n",
      "543:\tlearn: 0.5848775\ttotal: 38.5s\tremaining: 32.3s\n",
      "544:\tlearn: 0.5848378\ttotal: 38.6s\tremaining: 32.2s\n",
      "545:\tlearn: 0.5848013\ttotal: 38.7s\tremaining: 32.1s\n",
      "546:\tlearn: 0.5847389\ttotal: 38.7s\tremaining: 32.1s\n",
      "547:\tlearn: 0.5847223\ttotal: 38.8s\tremaining: 32s\n",
      "548:\tlearn: 0.5846854\ttotal: 38.8s\tremaining: 31.9s\n",
      "549:\tlearn: 0.5846696\ttotal: 38.9s\tremaining: 31.8s\n",
      "550:\tlearn: 0.5846277\ttotal: 39s\tremaining: 31.8s\n",
      "551:\tlearn: 0.5845899\ttotal: 39s\tremaining: 31.7s\n",
      "552:\tlearn: 0.5845602\ttotal: 39.1s\tremaining: 31.6s\n",
      "553:\tlearn: 0.5844599\ttotal: 39.2s\tremaining: 31.5s\n",
      "554:\tlearn: 0.5844090\ttotal: 39.2s\tremaining: 31.5s\n",
      "555:\tlearn: 0.5843713\ttotal: 39.3s\tremaining: 31.4s\n",
      "556:\tlearn: 0.5843305\ttotal: 39.4s\tremaining: 31.3s\n",
      "557:\tlearn: 0.5843078\ttotal: 39.4s\tremaining: 31.2s\n",
      "558:\tlearn: 0.5842564\ttotal: 39.5s\tremaining: 31.2s\n",
      "559:\tlearn: 0.5842193\ttotal: 39.6s\tremaining: 31.1s\n",
      "560:\tlearn: 0.5841866\ttotal: 39.6s\tremaining: 31s\n",
      "561:\tlearn: 0.5841311\ttotal: 39.7s\tremaining: 30.9s\n",
      "562:\tlearn: 0.5840306\ttotal: 39.8s\tremaining: 30.9s\n",
      "563:\tlearn: 0.5839968\ttotal: 39.9s\tremaining: 30.8s\n",
      "564:\tlearn: 0.5839642\ttotal: 39.9s\tremaining: 30.7s\n",
      "565:\tlearn: 0.5839174\ttotal: 40s\tremaining: 30.7s\n",
      "566:\tlearn: 0.5837580\ttotal: 40.1s\tremaining: 30.6s\n",
      "567:\tlearn: 0.5837190\ttotal: 40.1s\tremaining: 30.5s\n",
      "568:\tlearn: 0.5836201\ttotal: 40.2s\tremaining: 30.4s\n",
      "569:\tlearn: 0.5835986\ttotal: 40.3s\tremaining: 30.4s\n",
      "570:\tlearn: 0.5835513\ttotal: 40.3s\tremaining: 30.3s\n",
      "571:\tlearn: 0.5834935\ttotal: 40.4s\tremaining: 30.2s\n",
      "572:\tlearn: 0.5834658\ttotal: 40.5s\tremaining: 30.1s\n",
      "573:\tlearn: 0.5834195\ttotal: 40.5s\tremaining: 30.1s\n",
      "574:\tlearn: 0.5833889\ttotal: 40.6s\tremaining: 30s\n",
      "575:\tlearn: 0.5833602\ttotal: 40.7s\tremaining: 29.9s\n",
      "576:\tlearn: 0.5832186\ttotal: 40.7s\tremaining: 29.9s\n",
      "577:\tlearn: 0.5831108\ttotal: 40.8s\tremaining: 29.8s\n",
      "578:\tlearn: 0.5830767\ttotal: 40.9s\tremaining: 29.7s\n",
      "579:\tlearn: 0.5830283\ttotal: 40.9s\tremaining: 29.6s\n",
      "580:\tlearn: 0.5829877\ttotal: 41s\tremaining: 29.6s\n",
      "581:\tlearn: 0.5829632\ttotal: 41.1s\tremaining: 29.5s\n",
      "582:\tlearn: 0.5829327\ttotal: 41.1s\tremaining: 29.4s\n",
      "583:\tlearn: 0.5829077\ttotal: 41.2s\tremaining: 29.3s\n",
      "584:\tlearn: 0.5828818\ttotal: 41.3s\tremaining: 29.3s\n",
      "585:\tlearn: 0.5828456\ttotal: 41.3s\tremaining: 29.2s\n",
      "586:\tlearn: 0.5828344\ttotal: 41.4s\tremaining: 29.1s\n",
      "587:\tlearn: 0.5826903\ttotal: 41.5s\tremaining: 29s\n",
      "588:\tlearn: 0.5825883\ttotal: 41.5s\tremaining: 29s\n",
      "589:\tlearn: 0.5825665\ttotal: 41.6s\tremaining: 28.9s\n",
      "590:\tlearn: 0.5825222\ttotal: 41.7s\tremaining: 28.8s\n",
      "591:\tlearn: 0.5824553\ttotal: 41.7s\tremaining: 28.8s\n",
      "592:\tlearn: 0.5824157\ttotal: 41.8s\tremaining: 28.7s\n",
      "593:\tlearn: 0.5823915\ttotal: 41.9s\tremaining: 28.6s\n",
      "594:\tlearn: 0.5823700\ttotal: 41.9s\tremaining: 28.5s\n",
      "595:\tlearn: 0.5823604\ttotal: 42s\tremaining: 28.5s\n",
      "596:\tlearn: 0.5823210\ttotal: 42.1s\tremaining: 28.4s\n",
      "597:\tlearn: 0.5822925\ttotal: 42.1s\tremaining: 28.3s\n",
      "598:\tlearn: 0.5822412\ttotal: 42.2s\tremaining: 28.3s\n",
      "599:\tlearn: 0.5821581\ttotal: 42.3s\tremaining: 28.2s\n",
      "600:\tlearn: 0.5820717\ttotal: 42.3s\tremaining: 28.1s\n",
      "601:\tlearn: 0.5820300\ttotal: 42.4s\tremaining: 28s\n",
      "602:\tlearn: 0.5819489\ttotal: 42.5s\tremaining: 28s\n",
      "603:\tlearn: 0.5818968\ttotal: 42.5s\tremaining: 27.9s\n",
      "604:\tlearn: 0.5818783\ttotal: 42.6s\tremaining: 27.8s\n",
      "605:\tlearn: 0.5818355\ttotal: 42.7s\tremaining: 27.7s\n",
      "606:\tlearn: 0.5817896\ttotal: 42.7s\tremaining: 27.7s\n",
      "607:\tlearn: 0.5817657\ttotal: 42.8s\tremaining: 27.6s\n",
      "608:\tlearn: 0.5817440\ttotal: 42.9s\tremaining: 27.5s\n",
      "609:\tlearn: 0.5816992\ttotal: 42.9s\tremaining: 27.5s\n",
      "610:\tlearn: 0.5816618\ttotal: 43s\tremaining: 27.4s\n",
      "611:\tlearn: 0.5815806\ttotal: 43.1s\tremaining: 27.3s\n",
      "612:\tlearn: 0.5815538\ttotal: 43.1s\tremaining: 27.2s\n",
      "613:\tlearn: 0.5814233\ttotal: 43.2s\tremaining: 27.2s\n",
      "614:\tlearn: 0.5814135\ttotal: 43.3s\tremaining: 27.1s\n",
      "615:\tlearn: 0.5813750\ttotal: 43.3s\tremaining: 27s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616:\tlearn: 0.5813544\ttotal: 43.4s\tremaining: 26.9s\n",
      "617:\tlearn: 0.5812389\ttotal: 43.5s\tremaining: 26.9s\n",
      "618:\tlearn: 0.5812043\ttotal: 43.5s\tremaining: 26.8s\n",
      "619:\tlearn: 0.5810824\ttotal: 43.6s\tremaining: 26.7s\n",
      "620:\tlearn: 0.5810119\ttotal: 43.7s\tremaining: 26.7s\n",
      "621:\tlearn: 0.5808386\ttotal: 43.7s\tremaining: 26.6s\n",
      "622:\tlearn: 0.5806233\ttotal: 43.8s\tremaining: 26.5s\n",
      "623:\tlearn: 0.5806050\ttotal: 43.9s\tremaining: 26.4s\n",
      "624:\tlearn: 0.5805144\ttotal: 43.9s\tremaining: 26.4s\n",
      "625:\tlearn: 0.5804725\ttotal: 44s\tremaining: 26.3s\n",
      "626:\tlearn: 0.5804541\ttotal: 44.1s\tremaining: 26.2s\n",
      "627:\tlearn: 0.5804153\ttotal: 44.1s\tremaining: 26.1s\n",
      "628:\tlearn: 0.5803419\ttotal: 44.2s\tremaining: 26.1s\n",
      "629:\tlearn: 0.5803042\ttotal: 44.3s\tremaining: 26s\n",
      "630:\tlearn: 0.5802678\ttotal: 44.3s\tremaining: 25.9s\n",
      "631:\tlearn: 0.5802208\ttotal: 44.4s\tremaining: 25.8s\n",
      "632:\tlearn: 0.5801493\ttotal: 44.4s\tremaining: 25.8s\n",
      "633:\tlearn: 0.5799163\ttotal: 44.5s\tremaining: 25.7s\n",
      "634:\tlearn: 0.5798280\ttotal: 44.6s\tremaining: 25.6s\n",
      "635:\tlearn: 0.5798003\ttotal: 44.7s\tremaining: 25.6s\n",
      "636:\tlearn: 0.5797515\ttotal: 44.7s\tremaining: 25.5s\n",
      "637:\tlearn: 0.5797125\ttotal: 44.8s\tremaining: 25.4s\n",
      "638:\tlearn: 0.5795352\ttotal: 44.9s\tremaining: 25.3s\n",
      "639:\tlearn: 0.5794765\ttotal: 44.9s\tremaining: 25.3s\n",
      "640:\tlearn: 0.5794136\ttotal: 45s\tremaining: 25.2s\n",
      "641:\tlearn: 0.5793891\ttotal: 45s\tremaining: 25.1s\n",
      "642:\tlearn: 0.5793274\ttotal: 45.1s\tremaining: 25s\n",
      "643:\tlearn: 0.5792747\ttotal: 45.2s\tremaining: 25s\n",
      "644:\tlearn: 0.5792480\ttotal: 45.2s\tremaining: 24.9s\n",
      "645:\tlearn: 0.5792189\ttotal: 45.3s\tremaining: 24.8s\n",
      "646:\tlearn: 0.5791549\ttotal: 45.4s\tremaining: 24.8s\n",
      "647:\tlearn: 0.5791165\ttotal: 45.4s\tremaining: 24.7s\n",
      "648:\tlearn: 0.5790813\ttotal: 45.5s\tremaining: 24.6s\n",
      "649:\tlearn: 0.5790111\ttotal: 45.6s\tremaining: 24.5s\n",
      "650:\tlearn: 0.5789838\ttotal: 45.6s\tremaining: 24.5s\n",
      "651:\tlearn: 0.5789490\ttotal: 45.7s\tremaining: 24.4s\n",
      "652:\tlearn: 0.5789134\ttotal: 45.8s\tremaining: 24.3s\n",
      "653:\tlearn: 0.5788017\ttotal: 45.8s\tremaining: 24.2s\n",
      "654:\tlearn: 0.5787341\ttotal: 45.9s\tremaining: 24.2s\n",
      "655:\tlearn: 0.5787147\ttotal: 46s\tremaining: 24.1s\n",
      "656:\tlearn: 0.5787054\ttotal: 46s\tremaining: 24s\n",
      "657:\tlearn: 0.5786921\ttotal: 46.1s\tremaining: 24s\n",
      "658:\tlearn: 0.5786427\ttotal: 46.2s\tremaining: 23.9s\n",
      "659:\tlearn: 0.5785847\ttotal: 46.2s\tremaining: 23.8s\n",
      "660:\tlearn: 0.5784104\ttotal: 46.3s\tremaining: 23.8s\n",
      "661:\tlearn: 0.5783762\ttotal: 46.4s\tremaining: 23.7s\n",
      "662:\tlearn: 0.5783394\ttotal: 46.4s\tremaining: 23.6s\n",
      "663:\tlearn: 0.5782703\ttotal: 46.5s\tremaining: 23.5s\n",
      "664:\tlearn: 0.5782404\ttotal: 46.6s\tremaining: 23.5s\n",
      "665:\tlearn: 0.5782128\ttotal: 46.6s\tremaining: 23.4s\n",
      "666:\tlearn: 0.5781546\ttotal: 46.7s\tremaining: 23.3s\n",
      "667:\tlearn: 0.5781342\ttotal: 46.8s\tremaining: 23.2s\n",
      "668:\tlearn: 0.5780990\ttotal: 46.8s\tremaining: 23.2s\n",
      "669:\tlearn: 0.5780939\ttotal: 46.9s\tremaining: 23.1s\n",
      "670:\tlearn: 0.5780628\ttotal: 46.9s\tremaining: 23s\n",
      "671:\tlearn: 0.5780231\ttotal: 47s\tremaining: 22.9s\n",
      "672:\tlearn: 0.5779550\ttotal: 47.1s\tremaining: 22.9s\n",
      "673:\tlearn: 0.5778389\ttotal: 47.1s\tremaining: 22.8s\n",
      "674:\tlearn: 0.5778063\ttotal: 47.2s\tremaining: 22.7s\n",
      "675:\tlearn: 0.5777739\ttotal: 47.3s\tremaining: 22.7s\n",
      "676:\tlearn: 0.5777590\ttotal: 47.3s\tremaining: 22.6s\n",
      "677:\tlearn: 0.5776610\ttotal: 47.4s\tremaining: 22.5s\n",
      "678:\tlearn: 0.5775870\ttotal: 47.5s\tremaining: 22.4s\n",
      "679:\tlearn: 0.5774763\ttotal: 47.5s\tremaining: 22.4s\n",
      "680:\tlearn: 0.5773722\ttotal: 47.6s\tremaining: 22.3s\n",
      "681:\tlearn: 0.5773560\ttotal: 47.7s\tremaining: 22.2s\n",
      "682:\tlearn: 0.5772839\ttotal: 47.7s\tremaining: 22.2s\n",
      "683:\tlearn: 0.5772527\ttotal: 47.8s\tremaining: 22.1s\n",
      "684:\tlearn: 0.5772323\ttotal: 47.9s\tremaining: 22s\n",
      "685:\tlearn: 0.5772257\ttotal: 47.9s\tremaining: 21.9s\n",
      "686:\tlearn: 0.5772170\ttotal: 48s\tremaining: 21.9s\n",
      "687:\tlearn: 0.5771675\ttotal: 48.1s\tremaining: 21.8s\n",
      "688:\tlearn: 0.5770039\ttotal: 48.1s\tremaining: 21.7s\n",
      "689:\tlearn: 0.5769651\ttotal: 48.2s\tremaining: 21.7s\n",
      "690:\tlearn: 0.5768787\ttotal: 48.3s\tremaining: 21.6s\n",
      "691:\tlearn: 0.5768516\ttotal: 48.3s\tremaining: 21.5s\n",
      "692:\tlearn: 0.5768409\ttotal: 48.4s\tremaining: 21.4s\n",
      "693:\tlearn: 0.5767421\ttotal: 48.5s\tremaining: 21.4s\n",
      "694:\tlearn: 0.5767134\ttotal: 48.5s\tremaining: 21.3s\n",
      "695:\tlearn: 0.5766904\ttotal: 48.6s\tremaining: 21.2s\n",
      "696:\tlearn: 0.5765758\ttotal: 48.7s\tremaining: 21.2s\n",
      "697:\tlearn: 0.5765383\ttotal: 48.7s\tremaining: 21.1s\n",
      "698:\tlearn: 0.5765270\ttotal: 48.8s\tremaining: 21s\n",
      "699:\tlearn: 0.5764912\ttotal: 48.9s\tremaining: 20.9s\n",
      "700:\tlearn: 0.5763624\ttotal: 48.9s\tremaining: 20.9s\n",
      "701:\tlearn: 0.5763111\ttotal: 49s\tremaining: 20.8s\n",
      "702:\tlearn: 0.5762809\ttotal: 49.1s\tremaining: 20.7s\n",
      "703:\tlearn: 0.5761666\ttotal: 49.1s\tremaining: 20.7s\n",
      "704:\tlearn: 0.5761010\ttotal: 49.2s\tremaining: 20.6s\n",
      "705:\tlearn: 0.5760921\ttotal: 49.3s\tremaining: 20.5s\n",
      "706:\tlearn: 0.5760682\ttotal: 49.3s\tremaining: 20.4s\n",
      "707:\tlearn: 0.5760324\ttotal: 49.4s\tremaining: 20.4s\n",
      "708:\tlearn: 0.5760189\ttotal: 49.5s\tremaining: 20.3s\n",
      "709:\tlearn: 0.5759824\ttotal: 49.5s\tremaining: 20.2s\n",
      "710:\tlearn: 0.5759486\ttotal: 49.6s\tremaining: 20.2s\n",
      "711:\tlearn: 0.5759337\ttotal: 49.7s\tremaining: 20.1s\n",
      "712:\tlearn: 0.5758161\ttotal: 49.7s\tremaining: 20s\n",
      "713:\tlearn: 0.5757856\ttotal: 49.8s\tremaining: 19.9s\n",
      "714:\tlearn: 0.5757523\ttotal: 49.9s\tremaining: 19.9s\n",
      "715:\tlearn: 0.5757196\ttotal: 49.9s\tremaining: 19.8s\n",
      "716:\tlearn: 0.5757068\ttotal: 50s\tremaining: 19.7s\n",
      "717:\tlearn: 0.5756558\ttotal: 50.1s\tremaining: 19.7s\n",
      "718:\tlearn: 0.5756392\ttotal: 50.1s\tremaining: 19.6s\n",
      "719:\tlearn: 0.5756034\ttotal: 50.2s\tremaining: 19.5s\n",
      "720:\tlearn: 0.5755765\ttotal: 50.3s\tremaining: 19.5s\n",
      "721:\tlearn: 0.5755633\ttotal: 50.3s\tremaining: 19.4s\n",
      "722:\tlearn: 0.5755531\ttotal: 50.4s\tremaining: 19.3s\n",
      "723:\tlearn: 0.5755350\ttotal: 50.5s\tremaining: 19.2s\n",
      "724:\tlearn: 0.5754394\ttotal: 50.5s\tremaining: 19.2s\n",
      "725:\tlearn: 0.5753669\ttotal: 50.6s\tremaining: 19.1s\n",
      "726:\tlearn: 0.5752577\ttotal: 50.7s\tremaining: 19s\n",
      "727:\tlearn: 0.5752314\ttotal: 50.7s\tremaining: 19s\n",
      "728:\tlearn: 0.5752146\ttotal: 50.8s\tremaining: 18.9s\n",
      "729:\tlearn: 0.5750680\ttotal: 50.9s\tremaining: 18.8s\n",
      "730:\tlearn: 0.5750236\ttotal: 50.9s\tremaining: 18.7s\n",
      "731:\tlearn: 0.5749593\ttotal: 51s\tremaining: 18.7s\n",
      "732:\tlearn: 0.5749241\ttotal: 51.1s\tremaining: 18.6s\n",
      "733:\tlearn: 0.5748528\ttotal: 51.1s\tremaining: 18.5s\n",
      "734:\tlearn: 0.5748271\ttotal: 51.2s\tremaining: 18.5s\n",
      "735:\tlearn: 0.5748078\ttotal: 51.3s\tremaining: 18.4s\n",
      "736:\tlearn: 0.5747782\ttotal: 51.3s\tremaining: 18.3s\n",
      "737:\tlearn: 0.5745854\ttotal: 51.4s\tremaining: 18.2s\n",
      "738:\tlearn: 0.5745431\ttotal: 51.5s\tremaining: 18.2s\n",
      "739:\tlearn: 0.5745290\ttotal: 51.5s\tremaining: 18.1s\n",
      "740:\tlearn: 0.5744337\ttotal: 51.6s\tremaining: 18s\n",
      "741:\tlearn: 0.5743887\ttotal: 51.7s\tremaining: 18s\n",
      "742:\tlearn: 0.5743513\ttotal: 51.8s\tremaining: 17.9s\n",
      "743:\tlearn: 0.5742677\ttotal: 51.8s\tremaining: 17.8s\n",
      "744:\tlearn: 0.5741744\ttotal: 51.9s\tremaining: 17.8s\n",
      "745:\tlearn: 0.5741490\ttotal: 52s\tremaining: 17.7s\n",
      "746:\tlearn: 0.5741202\ttotal: 52s\tremaining: 17.6s\n",
      "747:\tlearn: 0.5740995\ttotal: 52.1s\tremaining: 17.6s\n",
      "748:\tlearn: 0.5740528\ttotal: 52.2s\tremaining: 17.5s\n",
      "749:\tlearn: 0.5740366\ttotal: 52.2s\tremaining: 17.4s\n",
      "750:\tlearn: 0.5740153\ttotal: 52.3s\tremaining: 17.3s\n",
      "751:\tlearn: 0.5740022\ttotal: 52.4s\tremaining: 17.3s\n",
      "752:\tlearn: 0.5739698\ttotal: 52.4s\tremaining: 17.2s\n",
      "753:\tlearn: 0.5739501\ttotal: 52.5s\tremaining: 17.1s\n",
      "754:\tlearn: 0.5739403\ttotal: 52.6s\tremaining: 17.1s\n",
      "755:\tlearn: 0.5739229\ttotal: 52.6s\tremaining: 17s\n",
      "756:\tlearn: 0.5739187\ttotal: 52.7s\tremaining: 16.9s\n",
      "757:\tlearn: 0.5738934\ttotal: 52.8s\tremaining: 16.8s\n",
      "758:\tlearn: 0.5738782\ttotal: 52.8s\tremaining: 16.8s\n",
      "759:\tlearn: 0.5738067\ttotal: 52.9s\tremaining: 16.7s\n",
      "760:\tlearn: 0.5737992\ttotal: 53s\tremaining: 16.6s\n",
      "761:\tlearn: 0.5737670\ttotal: 53s\tremaining: 16.6s\n",
      "762:\tlearn: 0.5737605\ttotal: 53.1s\tremaining: 16.5s\n",
      "763:\tlearn: 0.5737532\ttotal: 53.2s\tremaining: 16.4s\n",
      "764:\tlearn: 0.5737143\ttotal: 53.2s\tremaining: 16.4s\n",
      "765:\tlearn: 0.5737015\ttotal: 53.3s\tremaining: 16.3s\n",
      "766:\tlearn: 0.5736982\ttotal: 53.4s\tremaining: 16.2s\n",
      "767:\tlearn: 0.5736134\ttotal: 53.4s\tremaining: 16.1s\n",
      "768:\tlearn: 0.5735950\ttotal: 53.5s\tremaining: 16.1s\n",
      "769:\tlearn: 0.5735775\ttotal: 53.6s\tremaining: 16s\n",
      "770:\tlearn: 0.5735483\ttotal: 53.6s\tremaining: 15.9s\n",
      "771:\tlearn: 0.5735058\ttotal: 53.7s\tremaining: 15.9s\n",
      "772:\tlearn: 0.5734909\ttotal: 53.8s\tremaining: 15.8s\n",
      "773:\tlearn: 0.5734685\ttotal: 53.8s\tremaining: 15.7s\n",
      "774:\tlearn: 0.5734291\ttotal: 53.9s\tremaining: 15.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "775:\tlearn: 0.5733868\ttotal: 54s\tremaining: 15.6s\n",
      "776:\tlearn: 0.5733314\ttotal: 54s\tremaining: 15.5s\n",
      "777:\tlearn: 0.5733053\ttotal: 54.1s\tremaining: 15.4s\n",
      "778:\tlearn: 0.5732751\ttotal: 54.2s\tremaining: 15.4s\n",
      "779:\tlearn: 0.5732277\ttotal: 54.2s\tremaining: 15.3s\n",
      "780:\tlearn: 0.5732134\ttotal: 54.3s\tremaining: 15.2s\n",
      "781:\tlearn: 0.5729589\ttotal: 54.4s\tremaining: 15.2s\n",
      "782:\tlearn: 0.5729465\ttotal: 54.4s\tremaining: 15.1s\n",
      "783:\tlearn: 0.5729295\ttotal: 54.5s\tremaining: 15s\n",
      "784:\tlearn: 0.5729161\ttotal: 54.6s\tremaining: 14.9s\n",
      "785:\tlearn: 0.5728844\ttotal: 54.7s\tremaining: 14.9s\n",
      "786:\tlearn: 0.5728701\ttotal: 54.7s\tremaining: 14.8s\n",
      "787:\tlearn: 0.5728493\ttotal: 54.8s\tremaining: 14.7s\n",
      "788:\tlearn: 0.5728373\ttotal: 54.9s\tremaining: 14.7s\n",
      "789:\tlearn: 0.5728157\ttotal: 54.9s\tremaining: 14.6s\n",
      "790:\tlearn: 0.5727557\ttotal: 55s\tremaining: 14.5s\n",
      "791:\tlearn: 0.5727464\ttotal: 55.1s\tremaining: 14.5s\n",
      "792:\tlearn: 0.5727180\ttotal: 55.1s\tremaining: 14.4s\n",
      "793:\tlearn: 0.5726942\ttotal: 55.2s\tremaining: 14.3s\n",
      "794:\tlearn: 0.5726689\ttotal: 55.3s\tremaining: 14.3s\n",
      "795:\tlearn: 0.5726250\ttotal: 55.4s\tremaining: 14.2s\n",
      "796:\tlearn: 0.5726050\ttotal: 55.5s\tremaining: 14.1s\n",
      "797:\tlearn: 0.5725832\ttotal: 55.5s\tremaining: 14.1s\n",
      "798:\tlearn: 0.5725747\ttotal: 55.6s\tremaining: 14s\n",
      "799:\tlearn: 0.5725527\ttotal: 55.7s\tremaining: 13.9s\n",
      "800:\tlearn: 0.5725224\ttotal: 55.8s\tremaining: 13.9s\n",
      "801:\tlearn: 0.5724816\ttotal: 55.8s\tremaining: 13.8s\n",
      "802:\tlearn: 0.5723962\ttotal: 55.9s\tremaining: 13.7s\n",
      "803:\tlearn: 0.5723881\ttotal: 56s\tremaining: 13.6s\n",
      "804:\tlearn: 0.5723692\ttotal: 56s\tremaining: 13.6s\n",
      "805:\tlearn: 0.5722602\ttotal: 56.1s\tremaining: 13.5s\n",
      "806:\tlearn: 0.5722138\ttotal: 56.2s\tremaining: 13.4s\n",
      "807:\tlearn: 0.5721863\ttotal: 56.2s\tremaining: 13.4s\n",
      "808:\tlearn: 0.5720434\ttotal: 56.3s\tremaining: 13.3s\n",
      "809:\tlearn: 0.5720230\ttotal: 56.4s\tremaining: 13.2s\n",
      "810:\tlearn: 0.5720096\ttotal: 56.5s\tremaining: 13.2s\n",
      "811:\tlearn: 0.5719956\ttotal: 56.5s\tremaining: 13.1s\n",
      "812:\tlearn: 0.5719350\ttotal: 56.6s\tremaining: 13s\n",
      "813:\tlearn: 0.5718644\ttotal: 56.7s\tremaining: 12.9s\n",
      "814:\tlearn: 0.5717819\ttotal: 56.7s\tremaining: 12.9s\n",
      "815:\tlearn: 0.5717591\ttotal: 56.8s\tremaining: 12.8s\n",
      "816:\tlearn: 0.5717375\ttotal: 56.9s\tremaining: 12.7s\n",
      "817:\tlearn: 0.5716149\ttotal: 57s\tremaining: 12.7s\n",
      "818:\tlearn: 0.5716084\ttotal: 57s\tremaining: 12.6s\n",
      "819:\tlearn: 0.5715683\ttotal: 57.1s\tremaining: 12.5s\n",
      "820:\tlearn: 0.5715388\ttotal: 57.2s\tremaining: 12.5s\n",
      "821:\tlearn: 0.5715147\ttotal: 57.2s\tremaining: 12.4s\n",
      "822:\tlearn: 0.5715094\ttotal: 57.3s\tremaining: 12.3s\n",
      "823:\tlearn: 0.5714784\ttotal: 57.4s\tremaining: 12.2s\n",
      "824:\tlearn: 0.5714695\ttotal: 57.4s\tremaining: 12.2s\n",
      "825:\tlearn: 0.5714320\ttotal: 57.5s\tremaining: 12.1s\n",
      "826:\tlearn: 0.5713983\ttotal: 57.5s\tremaining: 12s\n",
      "827:\tlearn: 0.5713699\ttotal: 57.6s\tremaining: 12s\n",
      "828:\tlearn: 0.5713400\ttotal: 57.7s\tremaining: 11.9s\n",
      "829:\tlearn: 0.5713298\ttotal: 57.7s\tremaining: 11.8s\n",
      "830:\tlearn: 0.5713159\ttotal: 57.8s\tremaining: 11.8s\n",
      "831:\tlearn: 0.5713047\ttotal: 57.9s\tremaining: 11.7s\n",
      "832:\tlearn: 0.5712632\ttotal: 57.9s\tremaining: 11.6s\n",
      "833:\tlearn: 0.5712488\ttotal: 58s\tremaining: 11.5s\n",
      "834:\tlearn: 0.5712030\ttotal: 58.1s\tremaining: 11.5s\n",
      "835:\tlearn: 0.5711497\ttotal: 58.1s\tremaining: 11.4s\n",
      "836:\tlearn: 0.5711316\ttotal: 58.2s\tremaining: 11.3s\n",
      "837:\tlearn: 0.5711014\ttotal: 58.3s\tremaining: 11.3s\n",
      "838:\tlearn: 0.5710881\ttotal: 58.4s\tremaining: 11.2s\n",
      "839:\tlearn: 0.5710725\ttotal: 58.4s\tremaining: 11.1s\n",
      "840:\tlearn: 0.5710500\ttotal: 58.5s\tremaining: 11.1s\n",
      "841:\tlearn: 0.5710116\ttotal: 58.6s\tremaining: 11s\n",
      "842:\tlearn: 0.5710022\ttotal: 58.7s\tremaining: 10.9s\n",
      "843:\tlearn: 0.5709458\ttotal: 58.7s\tremaining: 10.9s\n",
      "844:\tlearn: 0.5709249\ttotal: 58.8s\tremaining: 10.8s\n",
      "845:\tlearn: 0.5707830\ttotal: 58.9s\tremaining: 10.7s\n",
      "846:\tlearn: 0.5707606\ttotal: 58.9s\tremaining: 10.6s\n",
      "847:\tlearn: 0.5707257\ttotal: 59s\tremaining: 10.6s\n",
      "848:\tlearn: 0.5706988\ttotal: 59.1s\tremaining: 10.5s\n",
      "849:\tlearn: 0.5706906\ttotal: 59.1s\tremaining: 10.4s\n",
      "850:\tlearn: 0.5706644\ttotal: 59.2s\tremaining: 10.4s\n",
      "851:\tlearn: 0.5706634\ttotal: 59.3s\tremaining: 10.3s\n",
      "852:\tlearn: 0.5706457\ttotal: 59.4s\tremaining: 10.2s\n",
      "853:\tlearn: 0.5705805\ttotal: 59.4s\tremaining: 10.2s\n",
      "854:\tlearn: 0.5705487\ttotal: 59.5s\tremaining: 10.1s\n",
      "855:\tlearn: 0.5705232\ttotal: 59.6s\tremaining: 10s\n",
      "856:\tlearn: 0.5704944\ttotal: 59.7s\tremaining: 9.96s\n",
      "857:\tlearn: 0.5704785\ttotal: 59.8s\tremaining: 9.89s\n",
      "858:\tlearn: 0.5704719\ttotal: 59.8s\tremaining: 9.82s\n",
      "859:\tlearn: 0.5704321\ttotal: 59.9s\tremaining: 9.76s\n",
      "860:\tlearn: 0.5703748\ttotal: 1m\tremaining: 9.69s\n",
      "861:\tlearn: 0.5703581\ttotal: 1m\tremaining: 9.62s\n",
      "862:\tlearn: 0.5703410\ttotal: 1m\tremaining: 9.55s\n",
      "863:\tlearn: 0.5703323\ttotal: 1m\tremaining: 9.49s\n",
      "864:\tlearn: 0.5702346\ttotal: 1m\tremaining: 9.42s\n",
      "865:\tlearn: 0.5702161\ttotal: 1m\tremaining: 9.35s\n",
      "866:\tlearn: 0.5701882\ttotal: 1m\tremaining: 9.29s\n",
      "867:\tlearn: 0.5701469\ttotal: 1m\tremaining: 9.22s\n",
      "868:\tlearn: 0.5701046\ttotal: 1m\tremaining: 9.16s\n",
      "869:\tlearn: 0.5700856\ttotal: 1m\tremaining: 9.09s\n",
      "870:\tlearn: 0.5700168\ttotal: 1m\tremaining: 9.02s\n",
      "871:\tlearn: 0.5699994\ttotal: 1m 1s\tremaining: 8.95s\n",
      "872:\tlearn: 0.5699891\ttotal: 1m 1s\tremaining: 8.89s\n",
      "873:\tlearn: 0.5699664\ttotal: 1m 1s\tremaining: 8.82s\n",
      "874:\tlearn: 0.5699303\ttotal: 1m 1s\tremaining: 8.75s\n",
      "875:\tlearn: 0.5699229\ttotal: 1m 1s\tremaining: 8.69s\n",
      "876:\tlearn: 0.5698727\ttotal: 1m 1s\tremaining: 8.62s\n",
      "877:\tlearn: 0.5698526\ttotal: 1m 1s\tremaining: 8.55s\n",
      "878:\tlearn: 0.5698168\ttotal: 1m 1s\tremaining: 8.48s\n",
      "879:\tlearn: 0.5697723\ttotal: 1m 1s\tremaining: 8.41s\n",
      "880:\tlearn: 0.5697094\ttotal: 1m 1s\tremaining: 8.34s\n",
      "881:\tlearn: 0.5697001\ttotal: 1m 1s\tremaining: 8.28s\n",
      "882:\tlearn: 0.5696890\ttotal: 1m 1s\tremaining: 8.21s\n",
      "883:\tlearn: 0.5696727\ttotal: 1m 2s\tremaining: 8.14s\n",
      "884:\tlearn: 0.5696517\ttotal: 1m 2s\tremaining: 8.08s\n",
      "885:\tlearn: 0.5695775\ttotal: 1m 2s\tremaining: 8.01s\n",
      "886:\tlearn: 0.5695509\ttotal: 1m 2s\tremaining: 7.94s\n",
      "887:\tlearn: 0.5695331\ttotal: 1m 2s\tremaining: 7.88s\n",
      "888:\tlearn: 0.5694607\ttotal: 1m 2s\tremaining: 7.81s\n",
      "889:\tlearn: 0.5694477\ttotal: 1m 2s\tremaining: 7.74s\n",
      "890:\tlearn: 0.5694219\ttotal: 1m 2s\tremaining: 7.67s\n",
      "891:\tlearn: 0.5693093\ttotal: 1m 2s\tremaining: 7.6s\n",
      "892:\tlearn: 0.5692750\ttotal: 1m 2s\tremaining: 7.53s\n",
      "893:\tlearn: 0.5692686\ttotal: 1m 2s\tremaining: 7.46s\n",
      "894:\tlearn: 0.5692397\ttotal: 1m 3s\tremaining: 7.39s\n",
      "895:\tlearn: 0.5691826\ttotal: 1m 3s\tremaining: 7.33s\n",
      "896:\tlearn: 0.5691699\ttotal: 1m 3s\tremaining: 7.26s\n",
      "897:\tlearn: 0.5691367\ttotal: 1m 3s\tremaining: 7.19s\n",
      "898:\tlearn: 0.5691107\ttotal: 1m 3s\tremaining: 7.12s\n",
      "899:\tlearn: 0.5690987\ttotal: 1m 3s\tremaining: 7.05s\n",
      "900:\tlearn: 0.5690710\ttotal: 1m 3s\tremaining: 6.98s\n",
      "901:\tlearn: 0.5690137\ttotal: 1m 3s\tremaining: 6.91s\n",
      "902:\tlearn: 0.5689977\ttotal: 1m 3s\tremaining: 6.84s\n",
      "903:\tlearn: 0.5689814\ttotal: 1m 3s\tremaining: 6.78s\n",
      "904:\tlearn: 0.5689718\ttotal: 1m 3s\tremaining: 6.71s\n",
      "905:\tlearn: 0.5689261\ttotal: 1m 3s\tremaining: 6.64s\n",
      "906:\tlearn: 0.5688798\ttotal: 1m 4s\tremaining: 6.57s\n",
      "907:\tlearn: 0.5688119\ttotal: 1m 4s\tremaining: 6.5s\n",
      "908:\tlearn: 0.5687264\ttotal: 1m 4s\tremaining: 6.43s\n",
      "909:\tlearn: 0.5687200\ttotal: 1m 4s\tremaining: 6.37s\n",
      "910:\tlearn: 0.5687044\ttotal: 1m 4s\tremaining: 6.3s\n",
      "911:\tlearn: 0.5686282\ttotal: 1m 4s\tremaining: 6.23s\n",
      "912:\tlearn: 0.5685981\ttotal: 1m 4s\tremaining: 6.16s\n",
      "913:\tlearn: 0.5685753\ttotal: 1m 4s\tremaining: 6.09s\n",
      "914:\tlearn: 0.5685262\ttotal: 1m 4s\tremaining: 6.02s\n",
      "915:\tlearn: 0.5685071\ttotal: 1m 4s\tremaining: 5.95s\n",
      "916:\tlearn: 0.5684763\ttotal: 1m 5s\tremaining: 5.88s\n",
      "917:\tlearn: 0.5684504\ttotal: 1m 5s\tremaining: 5.82s\n",
      "918:\tlearn: 0.5684076\ttotal: 1m 5s\tremaining: 5.75s\n",
      "919:\tlearn: 0.5682806\ttotal: 1m 5s\tremaining: 5.68s\n",
      "920:\tlearn: 0.5681976\ttotal: 1m 5s\tremaining: 5.61s\n",
      "921:\tlearn: 0.5681876\ttotal: 1m 5s\tremaining: 5.54s\n",
      "922:\tlearn: 0.5681743\ttotal: 1m 5s\tremaining: 5.47s\n",
      "923:\tlearn: 0.5681672\ttotal: 1m 5s\tremaining: 5.4s\n",
      "924:\tlearn: 0.5680697\ttotal: 1m 5s\tremaining: 5.33s\n",
      "925:\tlearn: 0.5680652\ttotal: 1m 5s\tremaining: 5.26s\n",
      "926:\tlearn: 0.5680426\ttotal: 1m 5s\tremaining: 5.19s\n",
      "927:\tlearn: 0.5680323\ttotal: 1m 6s\tremaining: 5.12s\n",
      "928:\tlearn: 0.5680227\ttotal: 1m 6s\tremaining: 5.05s\n",
      "929:\tlearn: 0.5678856\ttotal: 1m 6s\tremaining: 4.98s\n",
      "930:\tlearn: 0.5678745\ttotal: 1m 6s\tremaining: 4.91s\n",
      "931:\tlearn: 0.5678667\ttotal: 1m 6s\tremaining: 4.84s\n",
      "932:\tlearn: 0.5678630\ttotal: 1m 6s\tremaining: 4.77s\n",
      "933:\tlearn: 0.5678345\ttotal: 1m 6s\tremaining: 4.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934:\tlearn: 0.5678095\ttotal: 1m 6s\tremaining: 4.63s\n",
      "935:\tlearn: 0.5677453\ttotal: 1m 6s\tremaining: 4.56s\n",
      "936:\tlearn: 0.5676420\ttotal: 1m 6s\tremaining: 4.49s\n",
      "937:\tlearn: 0.5676173\ttotal: 1m 6s\tremaining: 4.42s\n",
      "938:\tlearn: 0.5676035\ttotal: 1m 6s\tremaining: 4.35s\n",
      "939:\tlearn: 0.5675919\ttotal: 1m 6s\tremaining: 4.27s\n",
      "940:\tlearn: 0.5675682\ttotal: 1m 7s\tremaining: 4.2s\n",
      "941:\tlearn: 0.5675423\ttotal: 1m 7s\tremaining: 4.13s\n",
      "942:\tlearn: 0.5674986\ttotal: 1m 7s\tremaining: 4.06s\n",
      "943:\tlearn: 0.5674316\ttotal: 1m 7s\tremaining: 3.99s\n",
      "944:\tlearn: 0.5674041\ttotal: 1m 7s\tremaining: 3.92s\n",
      "945:\tlearn: 0.5673762\ttotal: 1m 7s\tremaining: 3.85s\n",
      "946:\tlearn: 0.5673639\ttotal: 1m 7s\tremaining: 3.77s\n",
      "947:\tlearn: 0.5673618\ttotal: 1m 7s\tremaining: 3.7s\n",
      "948:\tlearn: 0.5672606\ttotal: 1m 7s\tremaining: 3.63s\n",
      "949:\tlearn: 0.5672333\ttotal: 1m 7s\tremaining: 3.56s\n",
      "950:\tlearn: 0.5672122\ttotal: 1m 7s\tremaining: 3.49s\n",
      "951:\tlearn: 0.5672005\ttotal: 1m 7s\tremaining: 3.42s\n",
      "952:\tlearn: 0.5671820\ttotal: 1m 7s\tremaining: 3.35s\n",
      "953:\tlearn: 0.5671628\ttotal: 1m 7s\tremaining: 3.27s\n",
      "954:\tlearn: 0.5671460\ttotal: 1m 7s\tremaining: 3.2s\n",
      "955:\tlearn: 0.5671215\ttotal: 1m 8s\tremaining: 3.13s\n",
      "956:\tlearn: 0.5670989\ttotal: 1m 8s\tremaining: 3.06s\n",
      "957:\tlearn: 0.5670739\ttotal: 1m 8s\tremaining: 2.99s\n",
      "958:\tlearn: 0.5670645\ttotal: 1m 8s\tremaining: 2.92s\n",
      "959:\tlearn: 0.5669132\ttotal: 1m 8s\tremaining: 2.85s\n",
      "960:\tlearn: 0.5668912\ttotal: 1m 8s\tremaining: 2.77s\n",
      "961:\tlearn: 0.5668825\ttotal: 1m 8s\tremaining: 2.7s\n",
      "962:\tlearn: 0.5668658\ttotal: 1m 8s\tremaining: 2.63s\n",
      "963:\tlearn: 0.5668543\ttotal: 1m 8s\tremaining: 2.56s\n",
      "964:\tlearn: 0.5668415\ttotal: 1m 8s\tremaining: 2.49s\n",
      "965:\tlearn: 0.5668057\ttotal: 1m 8s\tremaining: 2.42s\n",
      "966:\tlearn: 0.5667942\ttotal: 1m 8s\tremaining: 2.35s\n",
      "967:\tlearn: 0.5667846\ttotal: 1m 8s\tremaining: 2.27s\n",
      "968:\tlearn: 0.5667661\ttotal: 1m 8s\tremaining: 2.2s\n",
      "969:\tlearn: 0.5667549\ttotal: 1m 8s\tremaining: 2.13s\n",
      "970:\tlearn: 0.5667245\ttotal: 1m 9s\tremaining: 2.06s\n",
      "971:\tlearn: 0.5667166\ttotal: 1m 9s\tremaining: 1.99s\n",
      "972:\tlearn: 0.5666976\ttotal: 1m 9s\tremaining: 1.92s\n",
      "973:\tlearn: 0.5665746\ttotal: 1m 9s\tremaining: 1.85s\n",
      "974:\tlearn: 0.5665626\ttotal: 1m 9s\tremaining: 1.78s\n",
      "975:\tlearn: 0.5665462\ttotal: 1m 9s\tremaining: 1.7s\n",
      "976:\tlearn: 0.5665348\ttotal: 1m 9s\tremaining: 1.63s\n",
      "977:\tlearn: 0.5665171\ttotal: 1m 9s\tremaining: 1.56s\n",
      "978:\tlearn: 0.5665066\ttotal: 1m 9s\tremaining: 1.49s\n",
      "979:\tlearn: 0.5664987\ttotal: 1m 9s\tremaining: 1.42s\n",
      "980:\tlearn: 0.5664730\ttotal: 1m 9s\tremaining: 1.35s\n",
      "981:\tlearn: 0.5664411\ttotal: 1m 9s\tremaining: 1.28s\n",
      "982:\tlearn: 0.5664339\ttotal: 1m 9s\tremaining: 1.21s\n",
      "983:\tlearn: 0.5664039\ttotal: 1m 9s\tremaining: 1.14s\n",
      "984:\tlearn: 0.5663733\ttotal: 1m 10s\tremaining: 1.07s\n",
      "985:\tlearn: 0.5663115\ttotal: 1m 10s\tremaining: 996ms\n",
      "986:\tlearn: 0.5663084\ttotal: 1m 10s\tremaining: 925ms\n",
      "987:\tlearn: 0.5662488\ttotal: 1m 10s\tremaining: 854ms\n",
      "988:\tlearn: 0.5662394\ttotal: 1m 10s\tremaining: 783ms\n",
      "989:\tlearn: 0.5662186\ttotal: 1m 10s\tremaining: 711ms\n",
      "990:\tlearn: 0.5662096\ttotal: 1m 10s\tremaining: 640ms\n",
      "991:\tlearn: 0.5661692\ttotal: 1m 10s\tremaining: 569ms\n",
      "992:\tlearn: 0.5661536\ttotal: 1m 10s\tremaining: 498ms\n",
      "993:\tlearn: 0.5660794\ttotal: 1m 10s\tremaining: 427ms\n",
      "994:\tlearn: 0.5660538\ttotal: 1m 10s\tremaining: 356ms\n",
      "995:\tlearn: 0.5660285\ttotal: 1m 10s\tremaining: 285ms\n",
      "996:\tlearn: 0.5660157\ttotal: 1m 10s\tremaining: 214ms\n",
      "997:\tlearn: 0.5660086\ttotal: 1m 11s\tremaining: 142ms\n",
      "998:\tlearn: 0.5659825\ttotal: 1m 11s\tremaining: 71.2ms\n",
      "999:\tlearn: 0.5659498\ttotal: 1m 11s\tremaining: 0us\n",
      "CatBoostClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.71      0.66       763\n",
      "         1.0       0.64      0.54      0.59       728\n",
      "\n",
      "   micro avg       0.63      0.63      0.63      1491\n",
      "   macro avg       0.63      0.62      0.62      1491\n",
      "weighted avg       0.63      0.63      0.62      1491\n",
      "\n",
      "[[538 225]\n",
      " [334 394]]\n",
      "Accuracy is  62.50838363514419\n",
      "Time on model's work: 73.947 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.67      0.65       763\n",
      "         1.0       0.63      0.57      0.60       728\n",
      "\n",
      "   micro avg       0.62      0.62      0.62      1491\n",
      "   macro avg       0.62      0.62      0.62      1491\n",
      "weighted avg       0.62      0.62      0.62      1491\n",
      "\n",
      "[[515 248]\n",
      " [313 415]]\n",
      "Accuracy is  62.37424547283702\n",
      "Time on model's work: 0.184 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.93      0.70       763\n",
      "         1.0       0.76      0.22      0.35       728\n",
      "\n",
      "   micro avg       0.59      0.59      0.59      1491\n",
      "   macro avg       0.66      0.58      0.52      1491\n",
      "weighted avg       0.66      0.59      0.53      1491\n",
      "\n",
      "[[712  51]\n",
      " [565 163]]\n",
      "Accuracy is  58.68544600938967\n",
      "Time on model's work: 0.163 s\n",
      "====================================================================================================\n",
      "TFFMClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.62      0.61       763\n",
      "         1.0       0.59      0.58      0.58       728\n",
      "\n",
      "   micro avg       0.60      0.60      0.60      1491\n",
      "   macro avg       0.60      0.60      0.60      1491\n",
      "weighted avg       0.60      0.60      0.60      1491\n",
      "\n",
      "[[475 288]\n",
      " [308 420]]\n",
      "Accuracy is  60.02682763246143\n",
      "Time on model's work: 19.537 s\n",
      "====================================================================================================\n",
      "TOTAL TIME SPENT:  307.114 s\n"
     ]
    }
   ],
   "source": [
    "clfs = [\n",
    "        ['RandomForestClassifier', RandomForestClassifier()],\n",
    "        ['GradientBoostingClassifier', GradientBoostingClassifier()],\n",
    "        ['ExtraTreesClassifier', ExtraTreesClassifier()],\n",
    "        ['AdaBoostClassifier', AdaBoostClassifier()],\n",
    "        ['BaggingClassifier', BaggingClassifier()],\n",
    "        ['DecisionTreeClassifier', DecisionTreeClassifier()],\n",
    "        ['MLPClassifier', MLPClassifier()],\n",
    "        ['XGBClassifier', XGBClassifier()],\n",
    "        ['CatBoostClassifier', CatBoostClassifier()],\n",
    "        ['LogisticRegression', LogisticRegression()],\n",
    "        ['SGDClassifier', SGDClassifier()],\n",
    "        ['TFFMClassifier', TFFMClassifier()]\n",
    "       ]\n",
    "t = time()\n",
    "for name, clf in clfs:\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    prediction = clf.predict(X_test)\n",
    "    print(name)\n",
    "    print(classification_report(y_test, prediction))\n",
    "    print(confusion_matrix(y_test, prediction))\n",
    "    print('Accuracy is ', accuracy_score(y_test, prediction)*100)\n",
    "    print (\"Time on model's work:\", round(time()-t0, 3), \"s\")\n",
    "    print('='*100)\n",
    "print (\"TOTAL TIME SPENT: \", round(time()-t, 3), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:02<00:00, 17.89epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6170355466130114\n",
      "[[512 251]\n",
      " [320 408]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.67      0.64       763\n",
      "         1.0       0.62      0.56      0.59       728\n",
      "\n",
      "   micro avg       0.62      0.62      0.62      1491\n",
      "   macro avg       0.62      0.62      0.62      1491\n",
      "weighted avg       0.62      0.62      0.62      1491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:02<00:00, 18.10epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.574111334674715\n",
      "[[261 502]\n",
      " [133 595]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.34      0.45       763\n",
      "         1.0       0.54      0.82      0.65       728\n",
      "\n",
      "   micro avg       0.57      0.57      0.57      1491\n",
      "   macro avg       0.60      0.58      0.55      1491\n",
      "weighted avg       0.60      0.57      0.55      1491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:02<00:00, 17.87epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5197853789403085\n",
      "[[117 646]\n",
      " [ 70 658]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.15      0.25       763\n",
      "         1.0       0.50      0.90      0.65       728\n",
      "\n",
      "   micro avg       0.52      0.52      0.52      1491\n",
      "   macro avg       0.57      0.53      0.45      1491\n",
      "weighted avg       0.57      0.52      0.44      1491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:02<00:00, 18.27epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5050301810865191\n",
      "[[ 54 709]\n",
      " [ 29 699]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.07      0.13       763\n",
      "         1.0       0.50      0.96      0.65       728\n",
      "\n",
      "   micro avg       0.51      0.51      0.51      1491\n",
      "   macro avg       0.57      0.52      0.39      1491\n",
      "weighted avg       0.58      0.51      0.38      1491\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TFFM sparse - works worse with sparse\n",
    "# only CSR format supported\n",
    "X_train_sparse = sp.csr_matrix(X_train)\n",
    "X_test_sparse = sp.csr_matrix(X_test)\n",
    "# weight - optional / AdamOptimizer\n",
    "pos_class_weight = list(map(float,range(1, 5)))\n",
    "for weight in pos_class_weight:\n",
    "    model = TFFMClassifier(\n",
    "        order=2,\n",
    "        pos_class_weight=weight,\n",
    "        rank=10, \n",
    "        optimizer=tf.train.AdamOptimizer(learning_rate=0.001), \n",
    "        n_epochs=50, \n",
    "        batch_size=1024,\n",
    "        init_std=0.001,\n",
    "        reg=0.01,\n",
    "        input_type='sparse',\n",
    "        #log_dir='./tmp/logs',\n",
    "        #verbose=1,\n",
    "        seed=42\n",
    "    )\n",
    "    model.fit(X_train_sparse, y_train, show_progress=True)\n",
    "    predictions = model.predict(X_test_sparse)\n",
    "    print('accuracy: {}'.format(accuracy_score(y_test, predictions)))\n",
    "    # this will close tf.Session and free resources\n",
    "    print(confusion_matrix(y_test,predictions)) \n",
    "    print(classification_report(y_test, predictions))\n",
    "    model.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:02<00:00, 17.59epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5761234071093226\n",
      "[[412 351]\n",
      " [281 447]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.54      0.57       763\n",
      "         1.0       0.56      0.61      0.59       728\n",
      "\n",
      "   micro avg       0.58      0.58      0.58      1491\n",
      "   macro avg       0.58      0.58      0.58      1491\n",
      "weighted avg       0.58      0.58      0.58      1491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:02<00:00, 17.53epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.48826291079812206\n",
      "[[  0 763]\n",
      " [  0 728]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       763\n",
      "         1.0       0.49      1.00      0.66       728\n",
      "\n",
      "   micro avg       0.49      0.49      0.49      1491\n",
      "   macro avg       0.24      0.50      0.33      1491\n",
      "weighted avg       0.24      0.49      0.32      1491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:02<00:00, 18.10epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.48826291079812206\n",
      "[[  0 763]\n",
      " [  0 728]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       763\n",
      "         1.0       0.49      1.00      0.66       728\n",
      "\n",
      "   micro avg       0.49      0.49      0.49      1491\n",
      "   macro avg       0.24      0.50      0.33      1491\n",
      "weighted avg       0.24      0.49      0.32      1491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:02<00:00, 18.28epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.48826291079812206\n",
      "[[  0 763]\n",
      " [  0 728]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       763\n",
      "         1.0       0.49      1.00      0.66       728\n",
      "\n",
      "   micro avg       0.49      0.49      0.49      1491\n",
      "   macro avg       0.24      0.50      0.33      1491\n",
      "weighted avg       0.24      0.49      0.32      1491\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# weight - optional / FtrlOptimizer\n",
    "pos_class_weight = list(map(float,range(1, 5)))\n",
    "for weight in pos_class_weight:\n",
    "    model = TFFMClassifier(\n",
    "        order=2,\n",
    "        pos_class_weight=weight,\n",
    "        rank=10, \n",
    "        optimizer=tf.train.FtrlOptimizer(0.01, l2_regularization_strength=0.001, l2_shrinkage_regularization_strength=0.001), \n",
    "        n_epochs=50, \n",
    "        batch_size=1024,\n",
    "        init_std=0.001,\n",
    "        reg=0.01,\n",
    "        input_type='sparse',\n",
    "        #log_dir='./tmp/logs',\n",
    "        #verbose=1,\n",
    "        seed=42\n",
    "    )\n",
    "    model.fit(X_train_sparse, y_train, show_progress=True)\n",
    "    predictions = model.predict(X_test_sparse)\n",
    "    print('accuracy: {}'.format(accuracy_score(y_test, predictions)))\n",
    "    # this will close tf.Session and free resources\n",
    "    print(confusion_matrix(y_test,predictions)) \n",
    "    print(classification_report(y_test, predictions))\n",
    "    model.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5961/5961 [==============================] - ETA: 7s - loss: 0.7270 - acc: 0.550 - ETA: 1s - loss: 0.7410 - acc: 0.529 - ETA: 1s - loss: 0.7400 - acc: 0.524 - ETA: 0s - loss: 0.7391 - acc: 0.518 - ETA: 0s - loss: 0.7422 - acc: 0.510 - ETA: 0s - loss: 0.7414 - acc: 0.507 - ETA: 0s - loss: 0.7421 - acc: 0.505 - ETA: 0s - loss: 0.7400 - acc: 0.507 - 1s 132us/step - loss: 0.7398 - acc: 0.5063\n",
      "Epoch 2/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.6979 - acc: 0.535 - ETA: 0s - loss: 0.7250 - acc: 0.496 - ETA: 0s - loss: 0.7181 - acc: 0.506 - ETA: 0s - loss: 0.7198 - acc: 0.506 - ETA: 0s - loss: 0.7218 - acc: 0.503 - ETA: 0s - loss: 0.7206 - acc: 0.503 - ETA: 0s - loss: 0.7204 - acc: 0.503 - 0s 67us/step - loss: 0.7203 - acc: 0.5036\n",
      "Epoch 3/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.7210 - acc: 0.511 - ETA: 0s - loss: 0.7145 - acc: 0.503 - ETA: 0s - loss: 0.7161 - acc: 0.503 - ETA: 0s - loss: 0.7165 - acc: 0.497 - ETA: 0s - loss: 0.7162 - acc: 0.495 - ETA: 0s - loss: 0.7148 - acc: 0.500 - ETA: 0s - loss: 0.7128 - acc: 0.500 - 0s 72us/step - loss: 0.7112 - acc: 0.5056\n",
      "Epoch 4/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.6908 - acc: 0.554 - ETA: 0s - loss: 0.6967 - acc: 0.527 - ETA: 0s - loss: 0.7018 - acc: 0.512 - ETA: 0s - loss: 0.7014 - acc: 0.512 - ETA: 0s - loss: 0.6990 - acc: 0.517 - ETA: 0s - loss: 0.6980 - acc: 0.518 - ETA: 0s - loss: 0.6972 - acc: 0.517 - 0s 68us/step - loss: 0.6972 - acc: 0.5179\n",
      "Epoch 5/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.7008 - acc: 0.527 - ETA: 0s - loss: 0.6914 - acc: 0.546 - ETA: 0s - loss: 0.6940 - acc: 0.533 - ETA: 0s - loss: 0.6962 - acc: 0.521 - ETA: 0s - loss: 0.6955 - acc: 0.523 - ETA: 0s - loss: 0.6945 - acc: 0.525 - ETA: 0s - loss: 0.6946 - acc: 0.523 - 0s 68us/step - loss: 0.6948 - acc: 0.5222\n",
      "Epoch 6/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.7014 - acc: 0.535 - ETA: 0s - loss: 0.6910 - acc: 0.554 - ETA: 0s - loss: 0.6901 - acc: 0.546 - ETA: 0s - loss: 0.6902 - acc: 0.545 - ETA: 0s - loss: 0.6908 - acc: 0.540 - ETA: 0s - loss: 0.6900 - acc: 0.540 - ETA: 0s - loss: 0.6912 - acc: 0.538 - 0s 72us/step - loss: 0.6913 - acc: 0.5375\n",
      "Epoch 7/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.7046 - acc: 0.480 - ETA: 0s - loss: 0.6851 - acc: 0.541 - ETA: 0s - loss: 0.6849 - acc: 0.550 - ETA: 0s - loss: 0.6855 - acc: 0.550 - ETA: 0s - loss: 0.6859 - acc: 0.552 - ETA: 0s - loss: 0.6851 - acc: 0.554 - ETA: 0s - loss: 0.6867 - acc: 0.547 - 0s 74us/step - loss: 0.6863 - acc: 0.5487\n",
      "Epoch 8/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.6921 - acc: 0.554 - ETA: 0s - loss: 0.6879 - acc: 0.542 - ETA: 0s - loss: 0.6873 - acc: 0.543 - ETA: 0s - loss: 0.6878 - acc: 0.539 - ETA: 0s - loss: 0.6871 - acc: 0.541 - ETA: 0s - loss: 0.6859 - acc: 0.547 - ETA: 0s - loss: 0.6855 - acc: 0.546 - 0s 68us/step - loss: 0.6850 - acc: 0.5479\n",
      "Epoch 9/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.6851 - acc: 0.554 - ETA: 0s - loss: 0.6922 - acc: 0.538 - ETA: 0s - loss: 0.6872 - acc: 0.545 - ETA: 0s - loss: 0.6871 - acc: 0.545 - ETA: 0s - loss: 0.6858 - acc: 0.549 - ETA: 0s - loss: 0.6861 - acc: 0.548 - ETA: 0s - loss: 0.6851 - acc: 0.549 - 0s 69us/step - loss: 0.6851 - acc: 0.5499\n",
      "Epoch 10/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.6721 - acc: 0.636 - ETA: 0s - loss: 0.6791 - acc: 0.578 - ETA: 0s - loss: 0.6758 - acc: 0.583 - ETA: 0s - loss: 0.6777 - acc: 0.575 - ETA: 0s - loss: 0.6786 - acc: 0.573 - ETA: 0s - loss: 0.6777 - acc: 0.570 - ETA: 0s - loss: 0.6787 - acc: 0.567 - 0s 68us/step - loss: 0.6789 - acc: 0.5674\n",
      "Epoch 11/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.6703 - acc: 0.605 - ETA: 0s - loss: 0.6804 - acc: 0.565 - ETA: 0s - loss: 0.6781 - acc: 0.560 - ETA: 0s - loss: 0.6766 - acc: 0.563 - ETA: 0s - loss: 0.6774 - acc: 0.563 - ETA: 0s - loss: 0.6783 - acc: 0.559 - ETA: 0s - loss: 0.6763 - acc: 0.566 - 0s 71us/step - loss: 0.6769 - acc: 0.5670\n",
      "Epoch 12/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.6696 - acc: 0.570 - ETA: 0s - loss: 0.6757 - acc: 0.565 - ETA: 0s - loss: 0.6744 - acc: 0.571 - ETA: 0s - loss: 0.6720 - acc: 0.575 - ETA: 0s - loss: 0.6729 - acc: 0.576 - ETA: 0s - loss: 0.6746 - acc: 0.575 - ETA: 0s - loss: 0.6742 - acc: 0.576 - 0s 68us/step - loss: 0.6741 - acc: 0.5766\n",
      "Epoch 13/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.6943 - acc: 0.511 - ETA: 0s - loss: 0.6691 - acc: 0.574 - ETA: 0s - loss: 0.6691 - acc: 0.576 - ETA: 0s - loss: 0.6692 - acc: 0.579 - ETA: 0s - loss: 0.6708 - acc: 0.579 - ETA: 0s - loss: 0.6703 - acc: 0.583 - ETA: 0s - loss: 0.6715 - acc: 0.580 - 0s 71us/step - loss: 0.6713 - acc: 0.5811\n",
      "Epoch 14/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.6952 - acc: 0.519 - ETA: 0s - loss: 0.6669 - acc: 0.591 - ETA: 0s - loss: 0.6685 - acc: 0.592 - ETA: 0s - loss: 0.6668 - acc: 0.592 - ETA: 0s - loss: 0.6673 - acc: 0.593 - ETA: 0s - loss: 0.6689 - acc: 0.592 - ETA: 0s - loss: 0.6681 - acc: 0.593 - 0s 66us/step - loss: 0.6680 - acc: 0.5934\n",
      "Epoch 15/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.6626 - acc: 0.585 - ETA: 0s - loss: 0.6616 - acc: 0.588 - ETA: 0s - loss: 0.6673 - acc: 0.573 - ETA: 0s - loss: 0.6644 - acc: 0.584 - ETA: 0s - loss: 0.6637 - acc: 0.584 - ETA: 0s - loss: 0.6642 - acc: 0.584 - ETA: 0s - loss: 0.6650 - acc: 0.582 - 0s 71us/step - loss: 0.6644 - acc: 0.5846\n",
      "Epoch 16/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.6683 - acc: 0.601 - ETA: 0s - loss: 0.6641 - acc: 0.603 - ETA: 0s - loss: 0.6624 - acc: 0.593 - ETA: 0s - loss: 0.6596 - acc: 0.600 - ETA: 0s - loss: 0.6629 - acc: 0.598 - ETA: 0s - loss: 0.6604 - acc: 0.604 - ETA: 0s - loss: 0.6603 - acc: 0.605 - ETA: 0s - loss: 0.6612 - acc: 0.600 - 0s 75us/step - loss: 0.6612 - acc: 0.6006\n",
      "Epoch 17/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.6710 - acc: 0.605 - ETA: 0s - loss: 0.6653 - acc: 0.600 - ETA: 0s - loss: 0.6620 - acc: 0.598 - ETA: 0s - loss: 0.6575 - acc: 0.605 - ETA: 0s - loss: 0.6564 - acc: 0.607 - ETA: 0s - loss: 0.6579 - acc: 0.604 - ETA: 0s - loss: 0.6576 - acc: 0.604 - 0s 71us/step - loss: 0.6577 - acc: 0.6028\n",
      "Epoch 18/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.6301 - acc: 0.648 - ETA: 0s - loss: 0.6469 - acc: 0.614 - ETA: 0s - loss: 0.6535 - acc: 0.608 - ETA: 0s - loss: 0.6558 - acc: 0.603 - ETA: 0s - loss: 0.6558 - acc: 0.603 - ETA: 0s - loss: 0.6556 - acc: 0.601 - ETA: 0s - loss: 0.6547 - acc: 0.604 - ETA: 0s - loss: 0.6530 - acc: 0.607 - 0s 74us/step - loss: 0.6528 - acc: 0.6074\n",
      "Epoch 19/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.6233 - acc: 0.648 - ETA: 0s - loss: 0.6431 - acc: 0.626 - ETA: 0s - loss: 0.6453 - acc: 0.623 - ETA: 0s - loss: 0.6469 - acc: 0.618 - ETA: 0s - loss: 0.6475 - acc: 0.616 - ETA: 0s - loss: 0.6488 - acc: 0.615 - ETA: 0s - loss: 0.6498 - acc: 0.610 - 0s 75us/step - loss: 0.6499 - acc: 0.6120\n",
      "Epoch 20/20\n",
      "5961/5961 [==============================] - ETA: 0s - loss: 0.6541 - acc: 0.601 - ETA: 0s - loss: 0.6553 - acc: 0.615 - ETA: 0s - loss: 0.6423 - acc: 0.628 - ETA: 0s - loss: 0.6429 - acc: 0.624 - ETA: 0s - loss: 0.6438 - acc: 0.617 - ETA: 0s - loss: 0.6458 - acc: 0.615 - ETA: 0s - loss: 0.6448 - acc: 0.616 - ETA: 0s - loss: 0.6451 - acc: 0.617 - 0s 79us/step - loss: 0.6463 - acc: 0.6160\n",
      "1491/1491 [==============================] - ETA:  - 0s 104us/step\n",
      "[0.6452999041034102, 0.6351441986843853]\n"
     ]
    }
   ],
   "source": [
    "# KERAS\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=256)\n",
    "score = model.evaluate(X_test, y_test, batch_size=256)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearmiss (version = 1) shows the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 38652), (1.0, 3726)]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "enn = EditedNearestNeighbours()\n",
    "X_resampled_enn, y_resampled_enn = enn.fit_resample(features_list_array, labels_list_array)\n",
    "print(sorted(Counter(y_resampled_enn).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled_enn, y_resampled_enn, random_state=35, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.99      0.96      7748\n",
      "         1.0       0.75      0.32      0.45       728\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      8476\n",
      "   macro avg       0.84      0.65      0.71      8476\n",
      "weighted avg       0.92      0.93      0.92      8476\n",
      "\n",
      "[[7669   79]\n",
      " [ 496  232]]\n",
      "Accuracy is  93.21613968853232\n",
      "Time on model's work: 5.333 s\n",
      "====================================================================================================\n",
      "GradientBoostingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      1.00      0.96      7748\n",
      "         1.0       0.92      0.07      0.13       728\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      8476\n",
      "   macro avg       0.92      0.53      0.54      8476\n",
      "weighted avg       0.92      0.92      0.89      8476\n",
      "\n",
      "[[7744    4]\n",
      " [ 679   49]]\n",
      "Accuracy is  91.9419537517697\n",
      "Time on model's work: 308.262 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.99      0.96      7748\n",
      "         1.0       0.75      0.32      0.45       728\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      8476\n",
      "   macro avg       0.85      0.66      0.71      8476\n",
      "weighted avg       0.92      0.93      0.92      8476\n",
      "\n",
      "[[7672   76]\n",
      " [ 495  233]]\n",
      "Accuracy is  93.26333176026428\n",
      "Time on model's work: 9.704 s\n",
      "====================================================================================================\n",
      "AdaBoostClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      1.00      0.96      7748\n",
      "         1.0       0.83      0.07      0.13       728\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      8476\n",
      "   macro avg       0.88      0.53      0.54      8476\n",
      "weighted avg       0.91      0.92      0.89      8476\n",
      "\n",
      "[[7738   10]\n",
      " [ 678   50]]\n",
      "Accuracy is  91.88296366210477\n",
      "Time on model's work: 72.002 s\n",
      "====================================================================================================\n",
      "BaggingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.98      0.96      7748\n",
      "         1.0       0.66      0.31      0.42       728\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      8476\n",
      "   macro avg       0.80      0.65      0.69      8476\n",
      "weighted avg       0.91      0.93      0.91      8476\n",
      "\n",
      "[[7630  118]\n",
      " [ 503  225]]\n",
      "Accuracy is  92.67343086361491\n",
      "Time on model's work: 63.776 s\n",
      "====================================================================================================\n",
      "DecisionTreeClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.97      0.96      7748\n",
      "         1.0       0.60      0.41      0.49       728\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      8476\n",
      "   macro avg       0.77      0.69      0.72      8476\n",
      "weighted avg       0.92      0.93      0.92      8476\n",
      "\n",
      "[[7554  194]\n",
      " [ 432  296]]\n",
      "Accuracy is  92.61444077394998\n",
      "Time on model's work: 27.449 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.96      0.96      7748\n",
      "         1.0       0.54      0.44      0.49       728\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      8476\n",
      "   macro avg       0.74      0.70      0.72      8476\n",
      "weighted avg       0.91      0.92      0.92      8476\n",
      "\n",
      "[[7474  274]\n",
      " [ 405  323]]\n",
      "Accuracy is  91.98914582350164\n",
      "Time on model's work: 1019.896 s\n",
      "====================================================================================================\n",
      "XGBClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      1.00      0.96      7748\n",
      "         1.0       0.96      0.06      0.12       728\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      8476\n",
      "   macro avg       0.94      0.53      0.54      8476\n",
      "weighted avg       0.92      0.92      0.89      8476\n",
      "\n",
      "[[7746    2]\n",
      " [ 681   47]]\n",
      "Accuracy is  91.9419537517697\n",
      "Time on model's work: 265.292 s\n",
      "====================================================================================================\n",
      "0:\tlearn: 0.6249963\ttotal: 444ms\tremaining: 7m 23s\n",
      "1:\tlearn: 0.5666980\ttotal: 798ms\tremaining: 6m 38s\n",
      "2:\tlearn: 0.5169890\ttotal: 1.15s\tremaining: 6m 23s\n",
      "3:\tlearn: 0.4773383\ttotal: 1.5s\tremaining: 6m 14s\n",
      "4:\tlearn: 0.4552634\ttotal: 1.78s\tremaining: 5m 53s\n",
      "5:\tlearn: 0.4348966\ttotal: 2.06s\tremaining: 5m 41s\n",
      "6:\tlearn: 0.4138876\ttotal: 2.44s\tremaining: 5m 46s\n",
      "7:\tlearn: 0.3865177\ttotal: 2.78s\tremaining: 5m 44s\n",
      "8:\tlearn: 0.3726732\ttotal: 3.05s\tremaining: 5m 36s\n",
      "9:\tlearn: 0.3604587\ttotal: 3.32s\tremaining: 5m 29s\n",
      "10:\tlearn: 0.3434864\ttotal: 3.72s\tremaining: 5m 34s\n",
      "11:\tlearn: 0.3345937\ttotal: 3.99s\tremaining: 5m 28s\n",
      "12:\tlearn: 0.3219748\ttotal: 4.33s\tremaining: 5m 29s\n",
      "13:\tlearn: 0.3145799\ttotal: 4.6s\tremaining: 5m 23s\n",
      "14:\tlearn: 0.3079835\ttotal: 4.88s\tremaining: 5m 20s\n",
      "15:\tlearn: 0.3014014\ttotal: 5.19s\tremaining: 5m 19s\n",
      "16:\tlearn: 0.2948982\ttotal: 5.49s\tremaining: 5m 17s\n",
      "17:\tlearn: 0.2861756\ttotal: 5.92s\tremaining: 5m 22s\n",
      "18:\tlearn: 0.2773562\ttotal: 6.27s\tremaining: 5m 23s\n",
      "19:\tlearn: 0.2741666\ttotal: 6.55s\tremaining: 5m 21s\n",
      "20:\tlearn: 0.2706421\ttotal: 6.84s\tremaining: 5m 18s\n",
      "21:\tlearn: 0.2639163\ttotal: 7.17s\tremaining: 5m 18s\n",
      "22:\tlearn: 0.2591148\ttotal: 7.52s\tremaining: 5m 19s\n",
      "23:\tlearn: 0.2570653\ttotal: 7.8s\tremaining: 5m 17s\n",
      "24:\tlearn: 0.2530107\ttotal: 8.12s\tremaining: 5m 16s\n",
      "25:\tlearn: 0.2507696\ttotal: 8.46s\tremaining: 5m 16s\n",
      "26:\tlearn: 0.2489614\ttotal: 8.77s\tremaining: 5m 15s\n",
      "27:\tlearn: 0.2471749\ttotal: 9.08s\tremaining: 5m 15s\n",
      "28:\tlearn: 0.2442090\ttotal: 9.45s\tremaining: 5m 16s\n",
      "29:\tlearn: 0.2418720\ttotal: 9.82s\tremaining: 5m 17s\n",
      "30:\tlearn: 0.2410608\ttotal: 10.1s\tremaining: 5m 15s\n",
      "31:\tlearn: 0.2383693\ttotal: 10.5s\tremaining: 5m 16s\n",
      "32:\tlearn: 0.2360415\ttotal: 10.8s\tremaining: 5m 16s\n",
      "33:\tlearn: 0.2353745\ttotal: 11.1s\tremaining: 5m 15s\n",
      "34:\tlearn: 0.2336080\ttotal: 11.5s\tremaining: 5m 16s\n",
      "35:\tlearn: 0.2331142\ttotal: 11.8s\tremaining: 5m 15s\n",
      "36:\tlearn: 0.2325570\ttotal: 12.1s\tremaining: 5m 13s\n",
      "37:\tlearn: 0.2318456\ttotal: 12.4s\tremaining: 5m 12s\n",
      "38:\tlearn: 0.2308041\ttotal: 12.8s\tremaining: 5m 16s\n",
      "39:\tlearn: 0.2294076\ttotal: 13.3s\tremaining: 5m 18s\n",
      "40:\tlearn: 0.2282676\ttotal: 13.7s\tremaining: 5m 19s\n",
      "41:\tlearn: 0.2278084\ttotal: 14s\tremaining: 5m 19s\n",
      "42:\tlearn: 0.2268807\ttotal: 14.5s\tremaining: 5m 21s\n",
      "43:\tlearn: 0.2264690\ttotal: 14.9s\tremaining: 5m 24s\n",
      "44:\tlearn: 0.2254518\ttotal: 15.4s\tremaining: 5m 27s\n",
      "45:\tlearn: 0.2250212\ttotal: 15.8s\tremaining: 5m 28s\n",
      "46:\tlearn: 0.2247213\ttotal: 16.2s\tremaining: 5m 28s\n",
      "47:\tlearn: 0.2237809\ttotal: 16.7s\tremaining: 5m 31s\n",
      "48:\tlearn: 0.2235505\ttotal: 17s\tremaining: 5m 30s\n",
      "49:\tlearn: 0.2230648\ttotal: 17.6s\tremaining: 5m 33s\n",
      "50:\tlearn: 0.2229221\ttotal: 17.9s\tremaining: 5m 33s\n",
      "51:\tlearn: 0.2222464\ttotal: 18.4s\tremaining: 5m 35s\n",
      "52:\tlearn: 0.2218767\ttotal: 18.9s\tremaining: 5m 36s\n",
      "53:\tlearn: 0.2213087\ttotal: 19.3s\tremaining: 5m 37s\n",
      "54:\tlearn: 0.2212357\ttotal: 19.6s\tremaining: 5m 36s\n",
      "55:\tlearn: 0.2210957\ttotal: 19.9s\tremaining: 5m 35s\n",
      "56:\tlearn: 0.2209081\ttotal: 20.2s\tremaining: 5m 34s\n",
      "57:\tlearn: 0.2203551\ttotal: 20.7s\tremaining: 5m 35s\n",
      "58:\tlearn: 0.2199486\ttotal: 21.1s\tremaining: 5m 36s\n",
      "59:\tlearn: 0.2198889\ttotal: 21.3s\tremaining: 5m 34s\n",
      "60:\tlearn: 0.2197625\ttotal: 21.6s\tremaining: 5m 32s\n",
      "61:\tlearn: 0.2196786\ttotal: 21.9s\tremaining: 5m 31s\n",
      "62:\tlearn: 0.2194618\ttotal: 22.3s\tremaining: 5m 31s\n",
      "63:\tlearn: 0.2190598\ttotal: 22.6s\tremaining: 5m 30s\n",
      "64:\tlearn: 0.2187287\ttotal: 23s\tremaining: 5m 30s\n",
      "65:\tlearn: 0.2186840\ttotal: 23.3s\tremaining: 5m 29s\n",
      "66:\tlearn: 0.2186203\ttotal: 23.5s\tremaining: 5m 27s\n",
      "67:\tlearn: 0.2185650\ttotal: 23.8s\tremaining: 5m 26s\n",
      "68:\tlearn: 0.2182355\ttotal: 24.2s\tremaining: 5m 26s\n",
      "69:\tlearn: 0.2179665\ttotal: 24.6s\tremaining: 5m 26s\n",
      "70:\tlearn: 0.2176890\ttotal: 25s\tremaining: 5m 26s\n",
      "71:\tlearn: 0.2176375\ttotal: 25.3s\tremaining: 5m 25s\n",
      "72:\tlearn: 0.2175703\ttotal: 25.6s\tremaining: 5m 25s\n",
      "73:\tlearn: 0.2175176\ttotal: 26s\tremaining: 5m 25s\n",
      "74:\tlearn: 0.2173238\ttotal: 26.5s\tremaining: 5m 26s\n",
      "75:\tlearn: 0.2170465\ttotal: 26.9s\tremaining: 5m 26s\n",
      "76:\tlearn: 0.2168774\ttotal: 27.4s\tremaining: 5m 28s\n",
      "77:\tlearn: 0.2167551\ttotal: 27.7s\tremaining: 5m 27s\n",
      "78:\tlearn: 0.2167157\ttotal: 28s\tremaining: 5m 26s\n",
      "79:\tlearn: 0.2165973\ttotal: 28.5s\tremaining: 5m 27s\n",
      "80:\tlearn: 0.2165312\ttotal: 28.9s\tremaining: 5m 27s\n",
      "81:\tlearn: 0.2165015\ttotal: 29.2s\tremaining: 5m 27s\n",
      "82:\tlearn: 0.2164475\ttotal: 29.6s\tremaining: 5m 27s\n",
      "83:\tlearn: 0.2164211\ttotal: 29.9s\tremaining: 5m 26s\n",
      "84:\tlearn: 0.2162825\ttotal: 30.3s\tremaining: 5m 26s\n",
      "85:\tlearn: 0.2162584\ttotal: 30.6s\tremaining: 5m 25s\n",
      "86:\tlearn: 0.2161972\ttotal: 30.9s\tremaining: 5m 23s\n",
      "87:\tlearn: 0.2161098\ttotal: 31.2s\tremaining: 5m 23s\n",
      "88:\tlearn: 0.2159507\ttotal: 31.6s\tremaining: 5m 23s\n",
      "89:\tlearn: 0.2159278\ttotal: 31.9s\tremaining: 5m 22s\n",
      "90:\tlearn: 0.2159060\ttotal: 32.2s\tremaining: 5m 21s\n",
      "91:\tlearn: 0.2158822\ttotal: 32.5s\tremaining: 5m 20s\n",
      "92:\tlearn: 0.2158510\ttotal: 32.8s\tremaining: 5m 19s\n",
      "93:\tlearn: 0.2158123\ttotal: 33.1s\tremaining: 5m 19s\n",
      "94:\tlearn: 0.2156698\ttotal: 33.5s\tremaining: 5m 19s\n",
      "95:\tlearn: 0.2156536\ttotal: 33.8s\tremaining: 5m 18s\n",
      "96:\tlearn: 0.2156388\ttotal: 34.1s\tremaining: 5m 17s\n",
      "97:\tlearn: 0.2156144\ttotal: 34.4s\tremaining: 5m 16s\n",
      "98:\tlearn: 0.2155038\ttotal: 34.9s\tremaining: 5m 17s\n",
      "99:\tlearn: 0.2153505\ttotal: 35.3s\tremaining: 5m 18s\n",
      "100:\tlearn: 0.2153145\ttotal: 35.8s\tremaining: 5m 18s\n",
      "101:\tlearn: 0.2151883\ttotal: 36.3s\tremaining: 5m 19s\n",
      "102:\tlearn: 0.2151527\ttotal: 36.7s\tremaining: 5m 19s\n",
      "103:\tlearn: 0.2151318\ttotal: 37s\tremaining: 5m 18s\n",
      "104:\tlearn: 0.2151066\ttotal: 37.3s\tremaining: 5m 17s\n",
      "105:\tlearn: 0.2150943\ttotal: 37.6s\tremaining: 5m 17s\n",
      "106:\tlearn: 0.2150380\ttotal: 38s\tremaining: 5m 17s\n",
      "107:\tlearn: 0.2149997\ttotal: 38.3s\tremaining: 5m 16s\n",
      "108:\tlearn: 0.2148912\ttotal: 38.7s\tremaining: 5m 15s\n",
      "109:\tlearn: 0.2148277\ttotal: 38.9s\tremaining: 5m 15s\n",
      "110:\tlearn: 0.2147938\ttotal: 39.3s\tremaining: 5m 14s\n",
      "111:\tlearn: 0.2146717\ttotal: 39.7s\tremaining: 5m 14s\n",
      "112:\tlearn: 0.2146061\ttotal: 40s\tremaining: 5m 13s\n",
      "113:\tlearn: 0.2145877\ttotal: 40.2s\tremaining: 5m 12s\n",
      "114:\tlearn: 0.2145205\ttotal: 40.6s\tremaining: 5m 12s\n",
      "115:\tlearn: 0.2143993\ttotal: 41s\tremaining: 5m 12s\n",
      "116:\tlearn: 0.2143642\ttotal: 41.3s\tremaining: 5m 11s\n",
      "117:\tlearn: 0.2142622\ttotal: 41.6s\tremaining: 5m 11s\n",
      "118:\tlearn: 0.2142367\ttotal: 42s\tremaining: 5m 10s\n",
      "119:\tlearn: 0.2142127\ttotal: 42.3s\tremaining: 5m 10s\n",
      "120:\tlearn: 0.2141851\ttotal: 42.7s\tremaining: 5m 10s\n",
      "121:\tlearn: 0.2141209\ttotal: 43.1s\tremaining: 5m 10s\n",
      "122:\tlearn: 0.2140970\ttotal: 43.5s\tremaining: 5m 10s\n",
      "123:\tlearn: 0.2140623\ttotal: 43.8s\tremaining: 5m 9s\n",
      "124:\tlearn: 0.2140524\ttotal: 44.1s\tremaining: 5m 8s\n",
      "125:\tlearn: 0.2140056\ttotal: 44.6s\tremaining: 5m 9s\n",
      "126:\tlearn: 0.2139546\ttotal: 45.1s\tremaining: 5m 9s\n",
      "127:\tlearn: 0.2139230\ttotal: 45.5s\tremaining: 5m 9s\n",
      "128:\tlearn: 0.2139105\ttotal: 45.9s\tremaining: 5m 9s\n",
      "129:\tlearn: 0.2139014\ttotal: 46.2s\tremaining: 5m 9s\n",
      "130:\tlearn: 0.2138779\ttotal: 46.6s\tremaining: 5m 9s\n",
      "131:\tlearn: 0.2138599\ttotal: 47s\tremaining: 5m 8s\n",
      "132:\tlearn: 0.2138245\ttotal: 47.4s\tremaining: 5m 9s\n",
      "133:\tlearn: 0.2137523\ttotal: 47.9s\tremaining: 5m 9s\n",
      "134:\tlearn: 0.2137237\ttotal: 48.3s\tremaining: 5m 9s\n",
      "135:\tlearn: 0.2137001\ttotal: 48.6s\tremaining: 5m 8s\n",
      "136:\tlearn: 0.2136744\ttotal: 48.9s\tremaining: 5m 7s\n",
      "137:\tlearn: 0.2136603\ttotal: 49.1s\tremaining: 5m 6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138:\tlearn: 0.2136336\ttotal: 49.4s\tremaining: 5m 6s\n",
      "139:\tlearn: 0.2136093\ttotal: 49.7s\tremaining: 5m 5s\n",
      "140:\tlearn: 0.2136069\ttotal: 50s\tremaining: 5m 4s\n",
      "141:\tlearn: 0.2135793\ttotal: 50.3s\tremaining: 5m 4s\n",
      "142:\tlearn: 0.2135630\ttotal: 50.6s\tremaining: 5m 3s\n",
      "143:\tlearn: 0.2135402\ttotal: 50.9s\tremaining: 5m 2s\n",
      "144:\tlearn: 0.2134616\ttotal: 51.2s\tremaining: 5m 1s\n",
      "145:\tlearn: 0.2134420\ttotal: 51.5s\tremaining: 5m\n",
      "146:\tlearn: 0.2134211\ttotal: 51.8s\tremaining: 5m\n",
      "147:\tlearn: 0.2134002\ttotal: 52.1s\tremaining: 4m 59s\n",
      "148:\tlearn: 0.2133066\ttotal: 52.6s\tremaining: 5m\n",
      "149:\tlearn: 0.2132858\ttotal: 53.1s\tremaining: 5m\n",
      "150:\tlearn: 0.2132438\ttotal: 53.6s\tremaining: 5m 1s\n",
      "151:\tlearn: 0.2131767\ttotal: 54s\tremaining: 5m 1s\n",
      "152:\tlearn: 0.2131261\ttotal: 54.5s\tremaining: 5m 1s\n",
      "153:\tlearn: 0.2131106\ttotal: 54.9s\tremaining: 5m 1s\n",
      "154:\tlearn: 0.2130904\ttotal: 55.3s\tremaining: 5m 1s\n",
      "155:\tlearn: 0.2130708\ttotal: 55.6s\tremaining: 5m 1s\n",
      "156:\tlearn: 0.2130586\ttotal: 56s\tremaining: 5m\n",
      "157:\tlearn: 0.2130442\ttotal: 56.3s\tremaining: 4m 59s\n",
      "158:\tlearn: 0.2130186\ttotal: 56.6s\tremaining: 4m 59s\n",
      "159:\tlearn: 0.2130009\ttotal: 56.9s\tremaining: 4m 58s\n",
      "160:\tlearn: 0.2129765\ttotal: 57.2s\tremaining: 4m 58s\n",
      "161:\tlearn: 0.2129618\ttotal: 57.6s\tremaining: 4m 57s\n",
      "162:\tlearn: 0.2129432\ttotal: 57.9s\tremaining: 4m 57s\n",
      "163:\tlearn: 0.2129339\ttotal: 58.2s\tremaining: 4m 56s\n",
      "164:\tlearn: 0.2129082\ttotal: 58.5s\tremaining: 4m 56s\n",
      "165:\tlearn: 0.2128358\ttotal: 59s\tremaining: 4m 56s\n",
      "166:\tlearn: 0.2128165\ttotal: 59.3s\tremaining: 4m 55s\n",
      "167:\tlearn: 0.2128028\ttotal: 59.6s\tremaining: 4m 55s\n",
      "168:\tlearn: 0.2127783\ttotal: 60s\tremaining: 4m 54s\n",
      "169:\tlearn: 0.2127521\ttotal: 1m\tremaining: 4m 54s\n",
      "170:\tlearn: 0.2127307\ttotal: 1m\tremaining: 4m 53s\n",
      "171:\tlearn: 0.2127236\ttotal: 1m 1s\tremaining: 4m 54s\n",
      "172:\tlearn: 0.2126993\ttotal: 1m 1s\tremaining: 4m 53s\n",
      "173:\tlearn: 0.2126877\ttotal: 1m 1s\tremaining: 4m 53s\n",
      "174:\tlearn: 0.2126320\ttotal: 1m 2s\tremaining: 4m 54s\n",
      "175:\tlearn: 0.2126148\ttotal: 1m 2s\tremaining: 4m 54s\n",
      "176:\tlearn: 0.2125899\ttotal: 1m 3s\tremaining: 4m 54s\n",
      "177:\tlearn: 0.2125416\ttotal: 1m 3s\tremaining: 4m 54s\n",
      "178:\tlearn: 0.2125161\ttotal: 1m 4s\tremaining: 4m 54s\n",
      "179:\tlearn: 0.2125121\ttotal: 1m 4s\tremaining: 4m 53s\n",
      "180:\tlearn: 0.2124908\ttotal: 1m 4s\tremaining: 4m 53s\n",
      "181:\tlearn: 0.2124774\ttotal: 1m 5s\tremaining: 4m 53s\n",
      "182:\tlearn: 0.2124110\ttotal: 1m 5s\tremaining: 4m 53s\n",
      "183:\tlearn: 0.2123817\ttotal: 1m 6s\tremaining: 4m 53s\n",
      "184:\tlearn: 0.2123678\ttotal: 1m 6s\tremaining: 4m 52s\n",
      "185:\tlearn: 0.2123387\ttotal: 1m 6s\tremaining: 4m 52s\n",
      "186:\tlearn: 0.2123255\ttotal: 1m 7s\tremaining: 4m 52s\n",
      "187:\tlearn: 0.2123125\ttotal: 1m 7s\tremaining: 4m 51s\n",
      "188:\tlearn: 0.2122992\ttotal: 1m 7s\tremaining: 4m 51s\n",
      "189:\tlearn: 0.2122797\ttotal: 1m 8s\tremaining: 4m 51s\n",
      "190:\tlearn: 0.2122691\ttotal: 1m 8s\tremaining: 4m 50s\n",
      "191:\tlearn: 0.2121636\ttotal: 1m 9s\tremaining: 4m 50s\n",
      "192:\tlearn: 0.2121506\ttotal: 1m 9s\tremaining: 4m 50s\n",
      "193:\tlearn: 0.2121379\ttotal: 1m 9s\tremaining: 4m 49s\n",
      "194:\tlearn: 0.2121045\ttotal: 1m 10s\tremaining: 4m 49s\n",
      "195:\tlearn: 0.2120474\ttotal: 1m 10s\tremaining: 4m 49s\n",
      "196:\tlearn: 0.2120275\ttotal: 1m 10s\tremaining: 4m 49s\n",
      "197:\tlearn: 0.2120129\ttotal: 1m 11s\tremaining: 4m 49s\n",
      "198:\tlearn: 0.2119680\ttotal: 1m 11s\tremaining: 4m 48s\n",
      "199:\tlearn: 0.2119557\ttotal: 1m 12s\tremaining: 4m 48s\n",
      "200:\tlearn: 0.2119213\ttotal: 1m 12s\tremaining: 4m 48s\n",
      "201:\tlearn: 0.2118618\ttotal: 1m 12s\tremaining: 4m 48s\n",
      "202:\tlearn: 0.2118440\ttotal: 1m 13s\tremaining: 4m 47s\n",
      "203:\tlearn: 0.2118325\ttotal: 1m 13s\tremaining: 4m 47s\n",
      "204:\tlearn: 0.2118224\ttotal: 1m 14s\tremaining: 4m 47s\n",
      "205:\tlearn: 0.2118052\ttotal: 1m 14s\tremaining: 4m 46s\n",
      "206:\tlearn: 0.2117919\ttotal: 1m 14s\tremaining: 4m 46s\n",
      "207:\tlearn: 0.2117766\ttotal: 1m 15s\tremaining: 4m 45s\n",
      "208:\tlearn: 0.2117651\ttotal: 1m 15s\tremaining: 4m 44s\n",
      "209:\tlearn: 0.2117383\ttotal: 1m 15s\tremaining: 4m 44s\n",
      "210:\tlearn: 0.2117299\ttotal: 1m 15s\tremaining: 4m 44s\n",
      "211:\tlearn: 0.2116967\ttotal: 1m 16s\tremaining: 4m 43s\n",
      "212:\tlearn: 0.2116813\ttotal: 1m 16s\tremaining: 4m 42s\n",
      "213:\tlearn: 0.2116665\ttotal: 1m 16s\tremaining: 4m 42s\n",
      "214:\tlearn: 0.2116338\ttotal: 1m 17s\tremaining: 4m 42s\n",
      "215:\tlearn: 0.2116004\ttotal: 1m 17s\tremaining: 4m 41s\n",
      "216:\tlearn: 0.2115846\ttotal: 1m 17s\tremaining: 4m 41s\n",
      "217:\tlearn: 0.2115486\ttotal: 1m 18s\tremaining: 4m 40s\n",
      "218:\tlearn: 0.2115287\ttotal: 1m 18s\tremaining: 4m 40s\n",
      "219:\tlearn: 0.2115101\ttotal: 1m 18s\tremaining: 4m 39s\n",
      "220:\tlearn: 0.2114605\ttotal: 1m 19s\tremaining: 4m 39s\n",
      "221:\tlearn: 0.2113873\ttotal: 1m 19s\tremaining: 4m 39s\n",
      "222:\tlearn: 0.2113785\ttotal: 1m 20s\tremaining: 4m 39s\n",
      "223:\tlearn: 0.2113663\ttotal: 1m 20s\tremaining: 4m 38s\n",
      "224:\tlearn: 0.2113539\ttotal: 1m 20s\tremaining: 4m 37s\n",
      "225:\tlearn: 0.2113362\ttotal: 1m 20s\tremaining: 4m 37s\n",
      "226:\tlearn: 0.2113128\ttotal: 1m 21s\tremaining: 4m 36s\n",
      "227:\tlearn: 0.2112145\ttotal: 1m 21s\tremaining: 4m 36s\n",
      "228:\tlearn: 0.2112047\ttotal: 1m 21s\tremaining: 4m 35s\n",
      "229:\tlearn: 0.2111662\ttotal: 1m 22s\tremaining: 4m 35s\n",
      "230:\tlearn: 0.2111487\ttotal: 1m 22s\tremaining: 4m 35s\n",
      "231:\tlearn: 0.2111293\ttotal: 1m 22s\tremaining: 4m 34s\n",
      "232:\tlearn: 0.2111017\ttotal: 1m 23s\tremaining: 4m 34s\n",
      "233:\tlearn: 0.2110714\ttotal: 1m 23s\tremaining: 4m 33s\n",
      "234:\tlearn: 0.2110472\ttotal: 1m 23s\tremaining: 4m 32s\n",
      "235:\tlearn: 0.2109801\ttotal: 1m 24s\tremaining: 4m 32s\n",
      "236:\tlearn: 0.2109597\ttotal: 1m 24s\tremaining: 4m 31s\n",
      "237:\tlearn: 0.2108354\ttotal: 1m 24s\tremaining: 4m 31s\n",
      "238:\tlearn: 0.2108205\ttotal: 1m 25s\tremaining: 4m 31s\n",
      "239:\tlearn: 0.2107829\ttotal: 1m 25s\tremaining: 4m 30s\n",
      "240:\tlearn: 0.2107384\ttotal: 1m 25s\tremaining: 4m 30s\n",
      "241:\tlearn: 0.2107202\ttotal: 1m 26s\tremaining: 4m 29s\n",
      "242:\tlearn: 0.2106971\ttotal: 1m 26s\tremaining: 4m 29s\n",
      "243:\tlearn: 0.2106784\ttotal: 1m 26s\tremaining: 4m 28s\n",
      "244:\tlearn: 0.2105941\ttotal: 1m 26s\tremaining: 4m 28s\n",
      "245:\tlearn: 0.2105646\ttotal: 1m 27s\tremaining: 4m 27s\n",
      "246:\tlearn: 0.2105178\ttotal: 1m 27s\tremaining: 4m 27s\n",
      "247:\tlearn: 0.2104959\ttotal: 1m 27s\tremaining: 4m 26s\n",
      "248:\tlearn: 0.2104776\ttotal: 1m 28s\tremaining: 4m 26s\n",
      "249:\tlearn: 0.2104544\ttotal: 1m 28s\tremaining: 4m 25s\n",
      "250:\tlearn: 0.2104348\ttotal: 1m 28s\tremaining: 4m 25s\n",
      "251:\tlearn: 0.2103570\ttotal: 1m 29s\tremaining: 4m 24s\n",
      "252:\tlearn: 0.2102840\ttotal: 1m 29s\tremaining: 4m 24s\n",
      "253:\tlearn: 0.2101993\ttotal: 1m 29s\tremaining: 4m 24s\n",
      "254:\tlearn: 0.2101488\ttotal: 1m 30s\tremaining: 4m 23s\n",
      "255:\tlearn: 0.2101333\ttotal: 1m 30s\tremaining: 4m 23s\n",
      "256:\tlearn: 0.2100940\ttotal: 1m 30s\tremaining: 4m 23s\n",
      "257:\tlearn: 0.2100107\ttotal: 1m 31s\tremaining: 4m 22s\n",
      "258:\tlearn: 0.2099709\ttotal: 1m 31s\tremaining: 4m 22s\n",
      "259:\tlearn: 0.2099318\ttotal: 1m 32s\tremaining: 4m 21s\n",
      "260:\tlearn: 0.2099144\ttotal: 1m 32s\tremaining: 4m 21s\n",
      "261:\tlearn: 0.2098926\ttotal: 1m 32s\tremaining: 4m 20s\n",
      "262:\tlearn: 0.2098180\ttotal: 1m 33s\tremaining: 4m 20s\n",
      "263:\tlearn: 0.2097669\ttotal: 1m 33s\tremaining: 4m 20s\n",
      "264:\tlearn: 0.2097379\ttotal: 1m 33s\tremaining: 4m 20s\n",
      "265:\tlearn: 0.2097085\ttotal: 1m 34s\tremaining: 4m 19s\n",
      "266:\tlearn: 0.2096548\ttotal: 1m 34s\tremaining: 4m 19s\n",
      "267:\tlearn: 0.2096377\ttotal: 1m 34s\tremaining: 4m 19s\n",
      "268:\tlearn: 0.2096009\ttotal: 1m 35s\tremaining: 4m 18s\n",
      "269:\tlearn: 0.2095876\ttotal: 1m 35s\tremaining: 4m 18s\n",
      "270:\tlearn: 0.2095363\ttotal: 1m 36s\tremaining: 4m 18s\n",
      "271:\tlearn: 0.2094677\ttotal: 1m 36s\tremaining: 4m 18s\n",
      "272:\tlearn: 0.2094452\ttotal: 1m 36s\tremaining: 4m 17s\n",
      "273:\tlearn: 0.2093896\ttotal: 1m 37s\tremaining: 4m 17s\n",
      "274:\tlearn: 0.2093639\ttotal: 1m 37s\tremaining: 4m 17s\n",
      "275:\tlearn: 0.2093403\ttotal: 1m 37s\tremaining: 4m 16s\n",
      "276:\tlearn: 0.2093223\ttotal: 1m 38s\tremaining: 4m 16s\n",
      "277:\tlearn: 0.2093048\ttotal: 1m 38s\tremaining: 4m 16s\n",
      "278:\tlearn: 0.2092877\ttotal: 1m 38s\tremaining: 4m 15s\n",
      "279:\tlearn: 0.2092723\ttotal: 1m 39s\tremaining: 4m 15s\n",
      "280:\tlearn: 0.2092302\ttotal: 1m 39s\tremaining: 4m 14s\n",
      "281:\tlearn: 0.2092098\ttotal: 1m 39s\tremaining: 4m 14s\n",
      "282:\tlearn: 0.2091877\ttotal: 1m 40s\tremaining: 4m 13s\n",
      "283:\tlearn: 0.2091117\ttotal: 1m 40s\tremaining: 4m 13s\n",
      "284:\tlearn: 0.2090660\ttotal: 1m 40s\tremaining: 4m 12s\n",
      "285:\tlearn: 0.2090450\ttotal: 1m 41s\tremaining: 4m 12s\n",
      "286:\tlearn: 0.2090246\ttotal: 1m 41s\tremaining: 4m 11s\n",
      "287:\tlearn: 0.2089957\ttotal: 1m 41s\tremaining: 4m 11s\n",
      "288:\tlearn: 0.2089785\ttotal: 1m 41s\tremaining: 4m 10s\n",
      "289:\tlearn: 0.2089516\ttotal: 1m 42s\tremaining: 4m 10s\n",
      "290:\tlearn: 0.2089344\ttotal: 1m 42s\tremaining: 4m 9s\n",
      "291:\tlearn: 0.2089183\ttotal: 1m 42s\tremaining: 4m 9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292:\tlearn: 0.2088979\ttotal: 1m 43s\tremaining: 4m 8s\n",
      "293:\tlearn: 0.2088604\ttotal: 1m 43s\tremaining: 4m 8s\n",
      "294:\tlearn: 0.2088434\ttotal: 1m 43s\tremaining: 4m 8s\n",
      "295:\tlearn: 0.2088261\ttotal: 1m 44s\tremaining: 4m 7s\n",
      "296:\tlearn: 0.2088053\ttotal: 1m 44s\tremaining: 4m 7s\n",
      "297:\tlearn: 0.2087905\ttotal: 1m 44s\tremaining: 4m 6s\n",
      "298:\tlearn: 0.2087767\ttotal: 1m 44s\tremaining: 4m 6s\n",
      "299:\tlearn: 0.2087596\ttotal: 1m 45s\tremaining: 4m 5s\n",
      "300:\tlearn: 0.2087392\ttotal: 1m 45s\tremaining: 4m 5s\n",
      "301:\tlearn: 0.2087079\ttotal: 1m 45s\tremaining: 4m 4s\n",
      "302:\tlearn: 0.2086315\ttotal: 1m 46s\tremaining: 4m 4s\n",
      "303:\tlearn: 0.2086177\ttotal: 1m 46s\tremaining: 4m 3s\n",
      "304:\tlearn: 0.2085762\ttotal: 1m 46s\tremaining: 4m 3s\n",
      "305:\tlearn: 0.2085580\ttotal: 1m 47s\tremaining: 4m 3s\n",
      "306:\tlearn: 0.2085364\ttotal: 1m 47s\tremaining: 4m 2s\n",
      "307:\tlearn: 0.2084976\ttotal: 1m 47s\tremaining: 4m 2s\n",
      "308:\tlearn: 0.2084762\ttotal: 1m 48s\tremaining: 4m 1s\n",
      "309:\tlearn: 0.2084507\ttotal: 1m 48s\tremaining: 4m 1s\n",
      "310:\tlearn: 0.2084312\ttotal: 1m 48s\tremaining: 4m\n",
      "311:\tlearn: 0.2083962\ttotal: 1m 49s\tremaining: 4m\n",
      "312:\tlearn: 0.2083839\ttotal: 1m 49s\tremaining: 4m\n",
      "313:\tlearn: 0.2083209\ttotal: 1m 49s\tremaining: 4m\n",
      "314:\tlearn: 0.2082887\ttotal: 1m 50s\tremaining: 3m 59s\n",
      "315:\tlearn: 0.2082379\ttotal: 1m 50s\tremaining: 3m 59s\n",
      "316:\tlearn: 0.2082224\ttotal: 1m 51s\tremaining: 3m 59s\n",
      "317:\tlearn: 0.2082137\ttotal: 1m 51s\tremaining: 3m 58s\n",
      "318:\tlearn: 0.2081671\ttotal: 1m 51s\tremaining: 3m 58s\n",
      "319:\tlearn: 0.2080724\ttotal: 1m 52s\tremaining: 3m 58s\n",
      "320:\tlearn: 0.2080424\ttotal: 1m 52s\tremaining: 3m 58s\n",
      "321:\tlearn: 0.2079843\ttotal: 1m 52s\tremaining: 3m 57s\n",
      "322:\tlearn: 0.2079243\ttotal: 1m 53s\tremaining: 3m 57s\n",
      "323:\tlearn: 0.2079073\ttotal: 1m 53s\tremaining: 3m 56s\n",
      "324:\tlearn: 0.2078937\ttotal: 1m 53s\tremaining: 3m 56s\n",
      "325:\tlearn: 0.2078783\ttotal: 1m 54s\tremaining: 3m 55s\n",
      "326:\tlearn: 0.2078309\ttotal: 1m 54s\tremaining: 3m 55s\n",
      "327:\tlearn: 0.2078190\ttotal: 1m 54s\tremaining: 3m 54s\n",
      "328:\tlearn: 0.2076392\ttotal: 1m 55s\tremaining: 3m 54s\n",
      "329:\tlearn: 0.2076178\ttotal: 1m 55s\tremaining: 3m 54s\n",
      "330:\tlearn: 0.2075537\ttotal: 1m 55s\tremaining: 3m 53s\n",
      "331:\tlearn: 0.2075350\ttotal: 1m 56s\tremaining: 3m 53s\n",
      "332:\tlearn: 0.2075213\ttotal: 1m 56s\tremaining: 3m 52s\n",
      "333:\tlearn: 0.2074869\ttotal: 1m 56s\tremaining: 3m 52s\n",
      "334:\tlearn: 0.2074492\ttotal: 1m 56s\tremaining: 3m 52s\n",
      "335:\tlearn: 0.2074361\ttotal: 1m 57s\tremaining: 3m 51s\n",
      "336:\tlearn: 0.2074175\ttotal: 1m 57s\tremaining: 3m 51s\n",
      "337:\tlearn: 0.2074046\ttotal: 1m 57s\tremaining: 3m 50s\n",
      "338:\tlearn: 0.2073923\ttotal: 1m 58s\tremaining: 3m 50s\n",
      "339:\tlearn: 0.2073513\ttotal: 1m 58s\tremaining: 3m 49s\n",
      "340:\tlearn: 0.2073394\ttotal: 1m 58s\tremaining: 3m 49s\n",
      "341:\tlearn: 0.2072936\ttotal: 1m 58s\tremaining: 3m 48s\n",
      "342:\tlearn: 0.2072735\ttotal: 1m 59s\tremaining: 3m 48s\n",
      "343:\tlearn: 0.2072585\ttotal: 1m 59s\tremaining: 3m 48s\n",
      "344:\tlearn: 0.2072439\ttotal: 1m 59s\tremaining: 3m 47s\n",
      "345:\tlearn: 0.2072312\ttotal: 2m\tremaining: 3m 47s\n",
      "346:\tlearn: 0.2071519\ttotal: 2m\tremaining: 3m 47s\n",
      "347:\tlearn: 0.2071111\ttotal: 2m 1s\tremaining: 3m 47s\n",
      "348:\tlearn: 0.2070971\ttotal: 2m 1s\tremaining: 3m 46s\n",
      "349:\tlearn: 0.2070841\ttotal: 2m 2s\tremaining: 3m 46s\n",
      "350:\tlearn: 0.2070727\ttotal: 2m 2s\tremaining: 3m 46s\n",
      "351:\tlearn: 0.2070595\ttotal: 2m 2s\tremaining: 3m 46s\n",
      "352:\tlearn: 0.2070327\ttotal: 2m 3s\tremaining: 3m 45s\n",
      "353:\tlearn: 0.2070174\ttotal: 2m 3s\tremaining: 3m 45s\n",
      "354:\tlearn: 0.2069994\ttotal: 2m 3s\tremaining: 3m 45s\n",
      "355:\tlearn: 0.2069334\ttotal: 2m 4s\tremaining: 3m 44s\n",
      "356:\tlearn: 0.2069192\ttotal: 2m 4s\tremaining: 3m 44s\n",
      "357:\tlearn: 0.2069079\ttotal: 2m 4s\tremaining: 3m 43s\n",
      "358:\tlearn: 0.2068909\ttotal: 2m 5s\tremaining: 3m 43s\n",
      "359:\tlearn: 0.2068684\ttotal: 2m 5s\tremaining: 3m 42s\n",
      "360:\tlearn: 0.2068529\ttotal: 2m 5s\tremaining: 3m 42s\n",
      "361:\tlearn: 0.2068049\ttotal: 2m 5s\tremaining: 3m 41s\n",
      "362:\tlearn: 0.2067623\ttotal: 2m 6s\tremaining: 3m 41s\n",
      "363:\tlearn: 0.2067452\ttotal: 2m 6s\tremaining: 3m 41s\n",
      "364:\tlearn: 0.2067008\ttotal: 2m 6s\tremaining: 3m 40s\n",
      "365:\tlearn: 0.2066718\ttotal: 2m 7s\tremaining: 3m 40s\n",
      "366:\tlearn: 0.2066594\ttotal: 2m 7s\tremaining: 3m 39s\n",
      "367:\tlearn: 0.2066382\ttotal: 2m 7s\tremaining: 3m 39s\n",
      "368:\tlearn: 0.2066192\ttotal: 2m 8s\tremaining: 3m 38s\n",
      "369:\tlearn: 0.2066082\ttotal: 2m 8s\tremaining: 3m 38s\n",
      "370:\tlearn: 0.2065986\ttotal: 2m 8s\tremaining: 3m 38s\n",
      "371:\tlearn: 0.2065826\ttotal: 2m 9s\tremaining: 3m 37s\n",
      "372:\tlearn: 0.2065656\ttotal: 2m 9s\tremaining: 3m 37s\n",
      "373:\tlearn: 0.2065281\ttotal: 2m 9s\tremaining: 3m 36s\n",
      "374:\tlearn: 0.2065191\ttotal: 2m 9s\tremaining: 3m 36s\n",
      "375:\tlearn: 0.2064743\ttotal: 2m 10s\tremaining: 3m 36s\n",
      "376:\tlearn: 0.2064622\ttotal: 2m 10s\tremaining: 3m 35s\n",
      "377:\tlearn: 0.2064534\ttotal: 2m 10s\tremaining: 3m 35s\n",
      "378:\tlearn: 0.2064406\ttotal: 2m 11s\tremaining: 3m 35s\n",
      "379:\tlearn: 0.2064230\ttotal: 2m 11s\tremaining: 3m 34s\n",
      "380:\tlearn: 0.2064130\ttotal: 2m 12s\tremaining: 3m 34s\n",
      "381:\tlearn: 0.2063620\ttotal: 2m 12s\tremaining: 3m 34s\n",
      "382:\tlearn: 0.2063228\ttotal: 2m 12s\tremaining: 3m 33s\n",
      "383:\tlearn: 0.2062989\ttotal: 2m 13s\tremaining: 3m 33s\n",
      "384:\tlearn: 0.2062848\ttotal: 2m 13s\tremaining: 3m 33s\n",
      "385:\tlearn: 0.2062444\ttotal: 2m 13s\tremaining: 3m 33s\n",
      "386:\tlearn: 0.2062237\ttotal: 2m 14s\tremaining: 3m 32s\n",
      "387:\tlearn: 0.2062152\ttotal: 2m 14s\tremaining: 3m 32s\n",
      "388:\tlearn: 0.2062029\ttotal: 2m 14s\tremaining: 3m 31s\n",
      "389:\tlearn: 0.2061751\ttotal: 2m 15s\tremaining: 3m 31s\n",
      "390:\tlearn: 0.2061611\ttotal: 2m 15s\tremaining: 3m 31s\n",
      "391:\tlearn: 0.2061515\ttotal: 2m 15s\tremaining: 3m 30s\n",
      "392:\tlearn: 0.2061165\ttotal: 2m 16s\tremaining: 3m 30s\n",
      "393:\tlearn: 0.2061032\ttotal: 2m 16s\tremaining: 3m 30s\n",
      "394:\tlearn: 0.2060833\ttotal: 2m 17s\tremaining: 3m 29s\n",
      "395:\tlearn: 0.2060701\ttotal: 2m 17s\tremaining: 3m 29s\n",
      "396:\tlearn: 0.2060327\ttotal: 2m 17s\tremaining: 3m 29s\n",
      "397:\tlearn: 0.2060017\ttotal: 2m 18s\tremaining: 3m 28s\n",
      "398:\tlearn: 0.2059871\ttotal: 2m 18s\tremaining: 3m 28s\n",
      "399:\tlearn: 0.2059798\ttotal: 2m 18s\tremaining: 3m 28s\n",
      "400:\tlearn: 0.2059241\ttotal: 2m 19s\tremaining: 3m 28s\n",
      "401:\tlearn: 0.2059153\ttotal: 2m 19s\tremaining: 3m 27s\n",
      "402:\tlearn: 0.2058558\ttotal: 2m 19s\tremaining: 3m 27s\n",
      "403:\tlearn: 0.2058319\ttotal: 2m 20s\tremaining: 3m 26s\n",
      "404:\tlearn: 0.2058234\ttotal: 2m 20s\tremaining: 3m 26s\n",
      "405:\tlearn: 0.2057912\ttotal: 2m 21s\tremaining: 3m 26s\n",
      "406:\tlearn: 0.2057838\ttotal: 2m 21s\tremaining: 3m 25s\n",
      "407:\tlearn: 0.2057747\ttotal: 2m 21s\tremaining: 3m 25s\n",
      "408:\tlearn: 0.2057634\ttotal: 2m 21s\tremaining: 3m 25s\n",
      "409:\tlearn: 0.2057540\ttotal: 2m 22s\tremaining: 3m 24s\n",
      "410:\tlearn: 0.2057382\ttotal: 2m 22s\tremaining: 3m 24s\n",
      "411:\tlearn: 0.2056847\ttotal: 2m 22s\tremaining: 3m 23s\n",
      "412:\tlearn: 0.2056159\ttotal: 2m 23s\tremaining: 3m 23s\n",
      "413:\tlearn: 0.2056079\ttotal: 2m 23s\tremaining: 3m 22s\n",
      "414:\tlearn: 0.2055934\ttotal: 2m 23s\tremaining: 3m 22s\n",
      "415:\tlearn: 0.2055558\ttotal: 2m 24s\tremaining: 3m 22s\n",
      "416:\tlearn: 0.2055446\ttotal: 2m 24s\tremaining: 3m 21s\n",
      "417:\tlearn: 0.2055349\ttotal: 2m 24s\tremaining: 3m 21s\n",
      "418:\tlearn: 0.2055267\ttotal: 2m 24s\tremaining: 3m 20s\n",
      "419:\tlearn: 0.2055170\ttotal: 2m 25s\tremaining: 3m 20s\n",
      "420:\tlearn: 0.2055082\ttotal: 2m 25s\tremaining: 3m 19s\n",
      "421:\tlearn: 0.2054985\ttotal: 2m 25s\tremaining: 3m 19s\n",
      "422:\tlearn: 0.2054880\ttotal: 2m 25s\tremaining: 3m 18s\n",
      "423:\tlearn: 0.2054795\ttotal: 2m 26s\tremaining: 3m 18s\n",
      "424:\tlearn: 0.2054711\ttotal: 2m 26s\tremaining: 3m 18s\n",
      "425:\tlearn: 0.2054283\ttotal: 2m 26s\tremaining: 3m 17s\n",
      "426:\tlearn: 0.2054195\ttotal: 2m 26s\tremaining: 3m 17s\n",
      "427:\tlearn: 0.2053638\ttotal: 2m 27s\tremaining: 3m 16s\n",
      "428:\tlearn: 0.2053361\ttotal: 2m 27s\tremaining: 3m 16s\n",
      "429:\tlearn: 0.2053243\ttotal: 2m 27s\tremaining: 3m 15s\n",
      "430:\tlearn: 0.2052893\ttotal: 2m 28s\tremaining: 3m 15s\n",
      "431:\tlearn: 0.2052400\ttotal: 2m 28s\tremaining: 3m 15s\n",
      "432:\tlearn: 0.2052293\ttotal: 2m 28s\tremaining: 3m 14s\n",
      "433:\tlearn: 0.2052120\ttotal: 2m 28s\tremaining: 3m 14s\n",
      "434:\tlearn: 0.2051719\ttotal: 2m 29s\tremaining: 3m 13s\n",
      "435:\tlearn: 0.2051443\ttotal: 2m 29s\tremaining: 3m 13s\n",
      "436:\tlearn: 0.2051219\ttotal: 2m 29s\tremaining: 3m 12s\n",
      "437:\tlearn: 0.2051155\ttotal: 2m 30s\tremaining: 3m 12s\n",
      "438:\tlearn: 0.2050622\ttotal: 2m 30s\tremaining: 3m 12s\n",
      "439:\tlearn: 0.2050313\ttotal: 2m 30s\tremaining: 3m 11s\n",
      "440:\tlearn: 0.2049742\ttotal: 2m 30s\tremaining: 3m 11s\n",
      "441:\tlearn: 0.2049660\ttotal: 2m 31s\tremaining: 3m 10s\n",
      "442:\tlearn: 0.2049625\ttotal: 2m 31s\tremaining: 3m 10s\n",
      "443:\tlearn: 0.2049553\ttotal: 2m 31s\tremaining: 3m 9s\n",
      "444:\tlearn: 0.2049329\ttotal: 2m 31s\tremaining: 3m 9s\n",
      "445:\tlearn: 0.2049258\ttotal: 2m 32s\tremaining: 3m 9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446:\tlearn: 0.2049190\ttotal: 2m 32s\tremaining: 3m 8s\n",
      "447:\tlearn: 0.2048935\ttotal: 2m 32s\tremaining: 3m 8s\n",
      "448:\tlearn: 0.2048238\ttotal: 2m 33s\tremaining: 3m 7s\n",
      "449:\tlearn: 0.2047820\ttotal: 2m 33s\tremaining: 3m 7s\n",
      "450:\tlearn: 0.2047718\ttotal: 2m 33s\tremaining: 3m 7s\n",
      "451:\tlearn: 0.2047586\ttotal: 2m 33s\tremaining: 3m 6s\n",
      "452:\tlearn: 0.2047482\ttotal: 2m 34s\tremaining: 3m 6s\n",
      "453:\tlearn: 0.2047395\ttotal: 2m 34s\tremaining: 3m 5s\n",
      "454:\tlearn: 0.2047312\ttotal: 2m 34s\tremaining: 3m 5s\n",
      "455:\tlearn: 0.2047146\ttotal: 2m 34s\tremaining: 3m 4s\n",
      "456:\tlearn: 0.2046941\ttotal: 2m 35s\tremaining: 3m 4s\n",
      "457:\tlearn: 0.2046719\ttotal: 2m 35s\tremaining: 3m 4s\n",
      "458:\tlearn: 0.2046612\ttotal: 2m 35s\tremaining: 3m 3s\n",
      "459:\tlearn: 0.2046427\ttotal: 2m 36s\tremaining: 3m 3s\n",
      "460:\tlearn: 0.2046257\ttotal: 2m 36s\tremaining: 3m 2s\n",
      "461:\tlearn: 0.2046162\ttotal: 2m 36s\tremaining: 3m 2s\n",
      "462:\tlearn: 0.2045882\ttotal: 2m 36s\tremaining: 3m 1s\n",
      "463:\tlearn: 0.2045407\ttotal: 2m 37s\tremaining: 3m 1s\n",
      "464:\tlearn: 0.2045349\ttotal: 2m 37s\tremaining: 3m 1s\n",
      "465:\tlearn: 0.2045268\ttotal: 2m 37s\tremaining: 3m\n",
      "466:\tlearn: 0.2045115\ttotal: 2m 38s\tremaining: 3m\n",
      "467:\tlearn: 0.2045061\ttotal: 2m 38s\tremaining: 2m 59s\n",
      "468:\tlearn: 0.2044802\ttotal: 2m 38s\tremaining: 2m 59s\n",
      "469:\tlearn: 0.2044694\ttotal: 2m 38s\tremaining: 2m 59s\n",
      "470:\tlearn: 0.2044597\ttotal: 2m 39s\tremaining: 2m 58s\n",
      "471:\tlearn: 0.2044408\ttotal: 2m 39s\tremaining: 2m 58s\n",
      "472:\tlearn: 0.2044280\ttotal: 2m 39s\tremaining: 2m 57s\n",
      "473:\tlearn: 0.2043827\ttotal: 2m 39s\tremaining: 2m 57s\n",
      "474:\tlearn: 0.2043701\ttotal: 2m 40s\tremaining: 2m 57s\n",
      "475:\tlearn: 0.2043621\ttotal: 2m 40s\tremaining: 2m 56s\n",
      "476:\tlearn: 0.2043242\ttotal: 2m 40s\tremaining: 2m 56s\n",
      "477:\tlearn: 0.2042679\ttotal: 2m 41s\tremaining: 2m 55s\n",
      "478:\tlearn: 0.2042620\ttotal: 2m 41s\tremaining: 2m 55s\n",
      "479:\tlearn: 0.2042100\ttotal: 2m 41s\tremaining: 2m 55s\n",
      "480:\tlearn: 0.2041674\ttotal: 2m 42s\tremaining: 2m 54s\n",
      "481:\tlearn: 0.2041626\ttotal: 2m 42s\tremaining: 2m 54s\n",
      "482:\tlearn: 0.2041547\ttotal: 2m 42s\tremaining: 2m 54s\n",
      "483:\tlearn: 0.2041495\ttotal: 2m 42s\tremaining: 2m 53s\n",
      "484:\tlearn: 0.2041434\ttotal: 2m 43s\tremaining: 2m 53s\n",
      "485:\tlearn: 0.2040989\ttotal: 2m 43s\tremaining: 2m 52s\n",
      "486:\tlearn: 0.2040910\ttotal: 2m 43s\tremaining: 2m 52s\n",
      "487:\tlearn: 0.2040246\ttotal: 2m 43s\tremaining: 2m 51s\n",
      "488:\tlearn: 0.2039980\ttotal: 2m 44s\tremaining: 2m 51s\n",
      "489:\tlearn: 0.2039636\ttotal: 2m 44s\tremaining: 2m 51s\n",
      "490:\tlearn: 0.2039583\ttotal: 2m 44s\tremaining: 2m 50s\n",
      "491:\tlearn: 0.2039537\ttotal: 2m 45s\tremaining: 2m 50s\n",
      "492:\tlearn: 0.2039381\ttotal: 2m 45s\tremaining: 2m 50s\n",
      "493:\tlearn: 0.2039028\ttotal: 2m 45s\tremaining: 2m 49s\n",
      "494:\tlearn: 0.2038820\ttotal: 2m 46s\tremaining: 2m 49s\n",
      "495:\tlearn: 0.2038746\ttotal: 2m 46s\tremaining: 2m 49s\n",
      "496:\tlearn: 0.2038373\ttotal: 2m 46s\tremaining: 2m 48s\n",
      "497:\tlearn: 0.2038277\ttotal: 2m 46s\tremaining: 2m 48s\n",
      "498:\tlearn: 0.2037981\ttotal: 2m 47s\tremaining: 2m 47s\n",
      "499:\tlearn: 0.2037733\ttotal: 2m 47s\tremaining: 2m 47s\n",
      "500:\tlearn: 0.2037633\ttotal: 2m 47s\tremaining: 2m 47s\n",
      "501:\tlearn: 0.2037392\ttotal: 2m 48s\tremaining: 2m 46s\n",
      "502:\tlearn: 0.2037204\ttotal: 2m 48s\tremaining: 2m 46s\n",
      "503:\tlearn: 0.2037115\ttotal: 2m 48s\tremaining: 2m 45s\n",
      "504:\tlearn: 0.2037041\ttotal: 2m 48s\tremaining: 2m 45s\n",
      "505:\tlearn: 0.2036723\ttotal: 2m 49s\tremaining: 2m 45s\n",
      "506:\tlearn: 0.2036248\ttotal: 2m 49s\tremaining: 2m 44s\n",
      "507:\tlearn: 0.2034996\ttotal: 2m 49s\tremaining: 2m 44s\n",
      "508:\tlearn: 0.2034536\ttotal: 2m 50s\tremaining: 2m 44s\n",
      "509:\tlearn: 0.2034385\ttotal: 2m 50s\tremaining: 2m 43s\n",
      "510:\tlearn: 0.2034066\ttotal: 2m 50s\tremaining: 2m 43s\n",
      "511:\tlearn: 0.2033697\ttotal: 2m 51s\tremaining: 2m 42s\n",
      "512:\tlearn: 0.2033094\ttotal: 2m 51s\tremaining: 2m 42s\n",
      "513:\tlearn: 0.2033050\ttotal: 2m 51s\tremaining: 2m 42s\n",
      "514:\tlearn: 0.2032954\ttotal: 2m 51s\tremaining: 2m 41s\n",
      "515:\tlearn: 0.2032869\ttotal: 2m 52s\tremaining: 2m 41s\n",
      "516:\tlearn: 0.2032411\ttotal: 2m 52s\tremaining: 2m 41s\n",
      "517:\tlearn: 0.2032316\ttotal: 2m 52s\tremaining: 2m 40s\n",
      "518:\tlearn: 0.2032269\ttotal: 2m 52s\tremaining: 2m 40s\n",
      "519:\tlearn: 0.2032105\ttotal: 2m 53s\tremaining: 2m 39s\n",
      "520:\tlearn: 0.2031898\ttotal: 2m 53s\tremaining: 2m 39s\n",
      "521:\tlearn: 0.2031781\ttotal: 2m 53s\tremaining: 2m 39s\n",
      "522:\tlearn: 0.2031561\ttotal: 2m 53s\tremaining: 2m 38s\n",
      "523:\tlearn: 0.2031164\ttotal: 2m 54s\tremaining: 2m 38s\n",
      "524:\tlearn: 0.2031039\ttotal: 2m 54s\tremaining: 2m 37s\n",
      "525:\tlearn: 0.2030950\ttotal: 2m 54s\tremaining: 2m 37s\n",
      "526:\tlearn: 0.2030591\ttotal: 2m 55s\tremaining: 2m 37s\n",
      "527:\tlearn: 0.2030370\ttotal: 2m 55s\tremaining: 2m 36s\n",
      "528:\tlearn: 0.2030326\ttotal: 2m 55s\tremaining: 2m 36s\n",
      "529:\tlearn: 0.2029866\ttotal: 2m 55s\tremaining: 2m 35s\n",
      "530:\tlearn: 0.2029782\ttotal: 2m 56s\tremaining: 2m 35s\n",
      "531:\tlearn: 0.2029723\ttotal: 2m 56s\tremaining: 2m 35s\n",
      "532:\tlearn: 0.2029642\ttotal: 2m 56s\tremaining: 2m 34s\n",
      "533:\tlearn: 0.2029542\ttotal: 2m 56s\tremaining: 2m 34s\n",
      "534:\tlearn: 0.2029170\ttotal: 2m 57s\tremaining: 2m 34s\n",
      "535:\tlearn: 0.2029117\ttotal: 2m 57s\tremaining: 2m 33s\n",
      "536:\tlearn: 0.2028900\ttotal: 2m 57s\tremaining: 2m 33s\n",
      "537:\tlearn: 0.2028826\ttotal: 2m 57s\tremaining: 2m 32s\n",
      "538:\tlearn: 0.2028714\ttotal: 2m 58s\tremaining: 2m 32s\n",
      "539:\tlearn: 0.2026955\ttotal: 2m 58s\tremaining: 2m 32s\n",
      "540:\tlearn: 0.2026766\ttotal: 2m 58s\tremaining: 2m 31s\n",
      "541:\tlearn: 0.2026440\ttotal: 2m 59s\tremaining: 2m 31s\n",
      "542:\tlearn: 0.2026386\ttotal: 2m 59s\tremaining: 2m 30s\n",
      "543:\tlearn: 0.2026301\ttotal: 2m 59s\tremaining: 2m 30s\n",
      "544:\tlearn: 0.2026111\ttotal: 3m\tremaining: 2m 30s\n",
      "545:\tlearn: 0.2026048\ttotal: 3m\tremaining: 2m 29s\n",
      "546:\tlearn: 0.2025770\ttotal: 3m\tremaining: 2m 29s\n",
      "547:\tlearn: 0.2025744\ttotal: 3m\tremaining: 2m 29s\n",
      "548:\tlearn: 0.2025686\ttotal: 3m 1s\tremaining: 2m 28s\n",
      "549:\tlearn: 0.2025590\ttotal: 3m 1s\tremaining: 2m 28s\n",
      "550:\tlearn: 0.2025371\ttotal: 3m 1s\tremaining: 2m 28s\n",
      "551:\tlearn: 0.2025313\ttotal: 3m 2s\tremaining: 2m 27s\n",
      "552:\tlearn: 0.2025226\ttotal: 3m 2s\tremaining: 2m 27s\n",
      "553:\tlearn: 0.2025089\ttotal: 3m 2s\tremaining: 2m 27s\n",
      "554:\tlearn: 0.2024547\ttotal: 3m 3s\tremaining: 2m 26s\n",
      "555:\tlearn: 0.2024208\ttotal: 3m 3s\tremaining: 2m 26s\n",
      "556:\tlearn: 0.2023708\ttotal: 3m 4s\tremaining: 2m 26s\n",
      "557:\tlearn: 0.2023467\ttotal: 3m 4s\tremaining: 2m 26s\n",
      "558:\tlearn: 0.2023145\ttotal: 3m 4s\tremaining: 2m 25s\n",
      "559:\tlearn: 0.2022874\ttotal: 3m 5s\tremaining: 2m 25s\n",
      "560:\tlearn: 0.2022737\ttotal: 3m 5s\tremaining: 2m 25s\n",
      "561:\tlearn: 0.2022606\ttotal: 3m 5s\tremaining: 2m 24s\n",
      "562:\tlearn: 0.2022315\ttotal: 3m 6s\tremaining: 2m 24s\n",
      "563:\tlearn: 0.2022236\ttotal: 3m 6s\tremaining: 2m 24s\n",
      "564:\tlearn: 0.2021511\ttotal: 3m 7s\tremaining: 2m 24s\n",
      "565:\tlearn: 0.2021460\ttotal: 3m 7s\tremaining: 2m 23s\n",
      "566:\tlearn: 0.2021130\ttotal: 3m 8s\tremaining: 2m 23s\n",
      "567:\tlearn: 0.2020732\ttotal: 3m 8s\tremaining: 2m 23s\n",
      "568:\tlearn: 0.2020583\ttotal: 3m 8s\tremaining: 2m 23s\n",
      "569:\tlearn: 0.2020352\ttotal: 3m 9s\tremaining: 2m 22s\n",
      "570:\tlearn: 0.2020018\ttotal: 3m 9s\tremaining: 2m 22s\n",
      "571:\tlearn: 0.2019903\ttotal: 3m 10s\tremaining: 2m 22s\n",
      "572:\tlearn: 0.2019871\ttotal: 3m 10s\tremaining: 2m 21s\n",
      "573:\tlearn: 0.2019207\ttotal: 3m 10s\tremaining: 2m 21s\n",
      "574:\tlearn: 0.2018820\ttotal: 3m 11s\tremaining: 2m 21s\n",
      "575:\tlearn: 0.2018731\ttotal: 3m 11s\tremaining: 2m 20s\n",
      "576:\tlearn: 0.2018630\ttotal: 3m 11s\tremaining: 2m 20s\n",
      "577:\tlearn: 0.2018569\ttotal: 3m 12s\tremaining: 2m 20s\n",
      "578:\tlearn: 0.2018144\ttotal: 3m 12s\tremaining: 2m 19s\n",
      "579:\tlearn: 0.2017890\ttotal: 3m 12s\tremaining: 2m 19s\n",
      "580:\tlearn: 0.2017725\ttotal: 3m 12s\tremaining: 2m 19s\n",
      "581:\tlearn: 0.2017430\ttotal: 3m 13s\tremaining: 2m 18s\n",
      "582:\tlearn: 0.2017377\ttotal: 3m 13s\tremaining: 2m 18s\n",
      "583:\tlearn: 0.2017166\ttotal: 3m 13s\tremaining: 2m 18s\n",
      "584:\tlearn: 0.2016760\ttotal: 3m 14s\tremaining: 2m 17s\n",
      "585:\tlearn: 0.2016721\ttotal: 3m 14s\tremaining: 2m 17s\n",
      "586:\tlearn: 0.2015919\ttotal: 3m 14s\tremaining: 2m 17s\n",
      "587:\tlearn: 0.2015829\ttotal: 3m 15s\tremaining: 2m 16s\n",
      "588:\tlearn: 0.2015682\ttotal: 3m 15s\tremaining: 2m 16s\n",
      "589:\tlearn: 0.2015634\ttotal: 3m 15s\tremaining: 2m 16s\n",
      "590:\tlearn: 0.2015591\ttotal: 3m 16s\tremaining: 2m 15s\n",
      "591:\tlearn: 0.2015461\ttotal: 3m 16s\tremaining: 2m 15s\n",
      "592:\tlearn: 0.2015203\ttotal: 3m 16s\tremaining: 2m 15s\n",
      "593:\tlearn: 0.2014935\ttotal: 3m 17s\tremaining: 2m 14s\n",
      "594:\tlearn: 0.2014508\ttotal: 3m 17s\tremaining: 2m 14s\n",
      "595:\tlearn: 0.2014379\ttotal: 3m 17s\tremaining: 2m 14s\n",
      "596:\tlearn: 0.2014293\ttotal: 3m 18s\tremaining: 2m 13s\n",
      "597:\tlearn: 0.2014029\ttotal: 3m 18s\tremaining: 2m 13s\n",
      "598:\tlearn: 0.2013961\ttotal: 3m 18s\tremaining: 2m 13s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599:\tlearn: 0.2013885\ttotal: 3m 19s\tremaining: 2m 12s\n",
      "600:\tlearn: 0.2013240\ttotal: 3m 19s\tremaining: 2m 12s\n",
      "601:\tlearn: 0.2013213\ttotal: 3m 19s\tremaining: 2m 12s\n",
      "602:\tlearn: 0.2013114\ttotal: 3m 20s\tremaining: 2m 11s\n",
      "603:\tlearn: 0.2012907\ttotal: 3m 20s\tremaining: 2m 11s\n",
      "604:\tlearn: 0.2012868\ttotal: 3m 20s\tremaining: 2m 11s\n",
      "605:\tlearn: 0.2012611\ttotal: 3m 21s\tremaining: 2m 10s\n",
      "606:\tlearn: 0.2012522\ttotal: 3m 21s\tremaining: 2m 10s\n",
      "607:\tlearn: 0.2011921\ttotal: 3m 22s\tremaining: 2m 10s\n",
      "608:\tlearn: 0.2011443\ttotal: 3m 22s\tremaining: 2m 9s\n",
      "609:\tlearn: 0.2011261\ttotal: 3m 22s\tremaining: 2m 9s\n",
      "610:\tlearn: 0.2011193\ttotal: 3m 23s\tremaining: 2m 9s\n",
      "611:\tlearn: 0.2011015\ttotal: 3m 23s\tremaining: 2m 8s\n",
      "612:\tlearn: 0.2010964\ttotal: 3m 23s\tremaining: 2m 8s\n",
      "613:\tlearn: 0.2010255\ttotal: 3m 24s\tremaining: 2m 8s\n",
      "614:\tlearn: 0.2010145\ttotal: 3m 24s\tremaining: 2m 7s\n",
      "615:\tlearn: 0.2009908\ttotal: 3m 24s\tremaining: 2m 7s\n",
      "616:\tlearn: 0.2009662\ttotal: 3m 24s\tremaining: 2m 7s\n",
      "617:\tlearn: 0.2009468\ttotal: 3m 25s\tremaining: 2m 6s\n",
      "618:\tlearn: 0.2009131\ttotal: 3m 25s\tremaining: 2m 6s\n",
      "619:\tlearn: 0.2008953\ttotal: 3m 26s\tremaining: 2m 6s\n",
      "620:\tlearn: 0.2008635\ttotal: 3m 26s\tremaining: 2m 5s\n",
      "621:\tlearn: 0.2008445\ttotal: 3m 26s\tremaining: 2m 5s\n",
      "622:\tlearn: 0.2008208\ttotal: 3m 27s\tremaining: 2m 5s\n",
      "623:\tlearn: 0.2007807\ttotal: 3m 27s\tremaining: 2m 4s\n",
      "624:\tlearn: 0.2007690\ttotal: 3m 27s\tremaining: 2m 4s\n",
      "625:\tlearn: 0.2007611\ttotal: 3m 27s\tremaining: 2m 4s\n",
      "626:\tlearn: 0.2007476\ttotal: 3m 28s\tremaining: 2m 3s\n",
      "627:\tlearn: 0.2007328\ttotal: 3m 28s\tremaining: 2m 3s\n",
      "628:\tlearn: 0.2007053\ttotal: 3m 29s\tremaining: 2m 3s\n",
      "629:\tlearn: 0.2006992\ttotal: 3m 29s\tremaining: 2m 2s\n",
      "630:\tlearn: 0.2006898\ttotal: 3m 29s\tremaining: 2m 2s\n",
      "631:\tlearn: 0.2006574\ttotal: 3m 30s\tremaining: 2m 2s\n",
      "632:\tlearn: 0.2006332\ttotal: 3m 30s\tremaining: 2m 2s\n",
      "633:\tlearn: 0.2006299\ttotal: 3m 30s\tremaining: 2m 1s\n",
      "634:\tlearn: 0.2006240\ttotal: 3m 31s\tremaining: 2m 1s\n",
      "635:\tlearn: 0.2006145\ttotal: 3m 31s\tremaining: 2m\n",
      "636:\tlearn: 0.2005886\ttotal: 3m 31s\tremaining: 2m\n",
      "637:\tlearn: 0.2005755\ttotal: 3m 31s\tremaining: 2m\n",
      "638:\tlearn: 0.2005714\ttotal: 3m 32s\tremaining: 1m 59s\n",
      "639:\tlearn: 0.2005643\ttotal: 3m 32s\tremaining: 1m 59s\n",
      "640:\tlearn: 0.2005318\ttotal: 3m 32s\tremaining: 1m 59s\n",
      "641:\tlearn: 0.2005047\ttotal: 3m 33s\tremaining: 1m 58s\n",
      "642:\tlearn: 0.2004822\ttotal: 3m 33s\tremaining: 1m 58s\n",
      "643:\tlearn: 0.2004771\ttotal: 3m 33s\tremaining: 1m 58s\n",
      "644:\tlearn: 0.2004140\ttotal: 3m 34s\tremaining: 1m 57s\n",
      "645:\tlearn: 0.2004106\ttotal: 3m 34s\tremaining: 1m 57s\n",
      "646:\tlearn: 0.2004079\ttotal: 3m 34s\tremaining: 1m 57s\n",
      "647:\tlearn: 0.2003983\ttotal: 3m 34s\tremaining: 1m 56s\n",
      "648:\tlearn: 0.2003953\ttotal: 3m 35s\tremaining: 1m 56s\n",
      "649:\tlearn: 0.2003795\ttotal: 3m 35s\tremaining: 1m 55s\n",
      "650:\tlearn: 0.2003246\ttotal: 3m 35s\tremaining: 1m 55s\n",
      "651:\tlearn: 0.2002604\ttotal: 3m 36s\tremaining: 1m 55s\n",
      "652:\tlearn: 0.2001551\ttotal: 3m 36s\tremaining: 1m 55s\n",
      "653:\tlearn: 0.2001455\ttotal: 3m 36s\tremaining: 1m 54s\n",
      "654:\tlearn: 0.2001421\ttotal: 3m 37s\tremaining: 1m 54s\n",
      "655:\tlearn: 0.2001388\ttotal: 3m 37s\tremaining: 1m 54s\n",
      "656:\tlearn: 0.2000858\ttotal: 3m 37s\tremaining: 1m 53s\n",
      "657:\tlearn: 0.2000825\ttotal: 3m 38s\tremaining: 1m 53s\n",
      "658:\tlearn: 0.2000592\ttotal: 3m 38s\tremaining: 1m 53s\n",
      "659:\tlearn: 0.2000033\ttotal: 3m 39s\tremaining: 1m 52s\n",
      "660:\tlearn: 0.1999983\ttotal: 3m 39s\tremaining: 1m 52s\n",
      "661:\tlearn: 0.1999959\ttotal: 3m 39s\tremaining: 1m 52s\n",
      "662:\tlearn: 0.1999909\ttotal: 3m 40s\tremaining: 1m 51s\n",
      "663:\tlearn: 0.1999677\ttotal: 3m 40s\tremaining: 1m 51s\n",
      "664:\tlearn: 0.1999592\ttotal: 3m 40s\tremaining: 1m 51s\n",
      "665:\tlearn: 0.1999529\ttotal: 3m 40s\tremaining: 1m 50s\n",
      "666:\tlearn: 0.1999455\ttotal: 3m 41s\tremaining: 1m 50s\n",
      "667:\tlearn: 0.1999410\ttotal: 3m 41s\tremaining: 1m 50s\n",
      "668:\tlearn: 0.1999095\ttotal: 3m 41s\tremaining: 1m 49s\n",
      "669:\tlearn: 0.1999041\ttotal: 3m 42s\tremaining: 1m 49s\n",
      "670:\tlearn: 0.1999004\ttotal: 3m 42s\tremaining: 1m 48s\n",
      "671:\tlearn: 0.1998940\ttotal: 3m 42s\tremaining: 1m 48s\n",
      "672:\tlearn: 0.1998905\ttotal: 3m 42s\tremaining: 1m 48s\n",
      "673:\tlearn: 0.1998847\ttotal: 3m 43s\tremaining: 1m 47s\n",
      "674:\tlearn: 0.1998421\ttotal: 3m 43s\tremaining: 1m 47s\n",
      "675:\tlearn: 0.1998320\ttotal: 3m 43s\tremaining: 1m 47s\n",
      "676:\tlearn: 0.1997828\ttotal: 3m 43s\tremaining: 1m 46s\n",
      "677:\tlearn: 0.1997624\ttotal: 3m 44s\tremaining: 1m 46s\n",
      "678:\tlearn: 0.1997450\ttotal: 3m 44s\tremaining: 1m 46s\n",
      "679:\tlearn: 0.1997410\ttotal: 3m 44s\tremaining: 1m 45s\n",
      "680:\tlearn: 0.1997356\ttotal: 3m 45s\tremaining: 1m 45s\n",
      "681:\tlearn: 0.1997244\ttotal: 3m 45s\tremaining: 1m 45s\n",
      "682:\tlearn: 0.1997197\ttotal: 3m 45s\tremaining: 1m 44s\n",
      "683:\tlearn: 0.1997081\ttotal: 3m 45s\tremaining: 1m 44s\n",
      "684:\tlearn: 0.1996947\ttotal: 3m 46s\tremaining: 1m 44s\n",
      "685:\tlearn: 0.1996889\ttotal: 3m 46s\tremaining: 1m 43s\n",
      "686:\tlearn: 0.1996625\ttotal: 3m 46s\tremaining: 1m 43s\n",
      "687:\tlearn: 0.1996209\ttotal: 3m 47s\tremaining: 1m 42s\n",
      "688:\tlearn: 0.1996115\ttotal: 3m 47s\tremaining: 1m 42s\n",
      "689:\tlearn: 0.1996084\ttotal: 3m 47s\tremaining: 1m 42s\n",
      "690:\tlearn: 0.1995988\ttotal: 3m 47s\tremaining: 1m 41s\n",
      "691:\tlearn: 0.1995885\ttotal: 3m 48s\tremaining: 1m 41s\n",
      "692:\tlearn: 0.1995804\ttotal: 3m 48s\tremaining: 1m 41s\n",
      "693:\tlearn: 0.1995620\ttotal: 3m 48s\tremaining: 1m 40s\n",
      "694:\tlearn: 0.1995524\ttotal: 3m 49s\tremaining: 1m 40s\n",
      "695:\tlearn: 0.1995493\ttotal: 3m 49s\tremaining: 1m 40s\n",
      "696:\tlearn: 0.1995461\ttotal: 3m 49s\tremaining: 1m 39s\n",
      "697:\tlearn: 0.1995407\ttotal: 3m 49s\tremaining: 1m 39s\n",
      "698:\tlearn: 0.1995384\ttotal: 3m 50s\tremaining: 1m 39s\n",
      "699:\tlearn: 0.1995065\ttotal: 3m 50s\tremaining: 1m 38s\n",
      "700:\tlearn: 0.1994988\ttotal: 3m 50s\tremaining: 1m 38s\n",
      "701:\tlearn: 0.1994936\ttotal: 3m 50s\tremaining: 1m 38s\n",
      "702:\tlearn: 0.1994705\ttotal: 3m 51s\tremaining: 1m 37s\n",
      "703:\tlearn: 0.1994629\ttotal: 3m 51s\tremaining: 1m 37s\n",
      "704:\tlearn: 0.1994227\ttotal: 3m 51s\tremaining: 1m 37s\n",
      "705:\tlearn: 0.1994172\ttotal: 3m 52s\tremaining: 1m 36s\n",
      "706:\tlearn: 0.1994142\ttotal: 3m 52s\tremaining: 1m 36s\n",
      "707:\tlearn: 0.1994117\ttotal: 3m 52s\tremaining: 1m 35s\n",
      "708:\tlearn: 0.1994054\ttotal: 3m 53s\tremaining: 1m 35s\n",
      "709:\tlearn: 0.1993659\ttotal: 3m 53s\tremaining: 1m 35s\n",
      "710:\tlearn: 0.1993306\ttotal: 3m 53s\tremaining: 1m 34s\n",
      "711:\tlearn: 0.1993274\ttotal: 3m 53s\tremaining: 1m 34s\n",
      "712:\tlearn: 0.1992810\ttotal: 3m 54s\tremaining: 1m 34s\n",
      "713:\tlearn: 0.1992487\ttotal: 3m 54s\tremaining: 1m 33s\n",
      "714:\tlearn: 0.1992455\ttotal: 3m 54s\tremaining: 1m 33s\n",
      "715:\tlearn: 0.1992259\ttotal: 3m 54s\tremaining: 1m 33s\n",
      "716:\tlearn: 0.1991900\ttotal: 3m 55s\tremaining: 1m 32s\n",
      "717:\tlearn: 0.1991829\ttotal: 3m 55s\tremaining: 1m 32s\n",
      "718:\tlearn: 0.1991728\ttotal: 3m 55s\tremaining: 1m 32s\n",
      "719:\tlearn: 0.1991699\ttotal: 3m 56s\tremaining: 1m 31s\n",
      "720:\tlearn: 0.1991345\ttotal: 3m 56s\tremaining: 1m 31s\n",
      "721:\tlearn: 0.1991175\ttotal: 3m 56s\tremaining: 1m 31s\n",
      "722:\tlearn: 0.1991039\ttotal: 3m 56s\tremaining: 1m 30s\n",
      "723:\tlearn: 0.1990774\ttotal: 3m 57s\tremaining: 1m 30s\n",
      "724:\tlearn: 0.1990727\ttotal: 3m 57s\tremaining: 1m 30s\n",
      "725:\tlearn: 0.1990704\ttotal: 3m 57s\tremaining: 1m 29s\n",
      "726:\tlearn: 0.1990594\ttotal: 3m 58s\tremaining: 1m 29s\n",
      "727:\tlearn: 0.1990438\ttotal: 3m 58s\tremaining: 1m 29s\n",
      "728:\tlearn: 0.1990325\ttotal: 3m 59s\tremaining: 1m 28s\n",
      "729:\tlearn: 0.1990124\ttotal: 3m 59s\tremaining: 1m 28s\n",
      "730:\tlearn: 0.1989913\ttotal: 3m 59s\tremaining: 1m 28s\n",
      "731:\tlearn: 0.1989872\ttotal: 4m\tremaining: 1m 27s\n",
      "732:\tlearn: 0.1989821\ttotal: 4m\tremaining: 1m 27s\n",
      "733:\tlearn: 0.1989623\ttotal: 4m\tremaining: 1m 27s\n",
      "734:\tlearn: 0.1989605\ttotal: 4m 1s\tremaining: 1m 27s\n",
      "735:\tlearn: 0.1989223\ttotal: 4m 1s\tremaining: 1m 26s\n",
      "736:\tlearn: 0.1989044\ttotal: 4m 2s\tremaining: 1m 26s\n",
      "737:\tlearn: 0.1988717\ttotal: 4m 2s\tremaining: 1m 26s\n",
      "738:\tlearn: 0.1988610\ttotal: 4m 2s\tremaining: 1m 25s\n",
      "739:\tlearn: 0.1988260\ttotal: 4m 3s\tremaining: 1m 25s\n",
      "740:\tlearn: 0.1988237\ttotal: 4m 3s\tremaining: 1m 25s\n",
      "741:\tlearn: 0.1988201\ttotal: 4m 4s\tremaining: 1m 24s\n",
      "742:\tlearn: 0.1987790\ttotal: 4m 4s\tremaining: 1m 24s\n",
      "743:\tlearn: 0.1987650\ttotal: 4m 5s\tremaining: 1m 24s\n",
      "744:\tlearn: 0.1987561\ttotal: 4m 5s\tremaining: 1m 23s\n",
      "745:\tlearn: 0.1987531\ttotal: 4m 5s\tremaining: 1m 23s\n",
      "746:\tlearn: 0.1987399\ttotal: 4m 6s\tremaining: 1m 23s\n",
      "747:\tlearn: 0.1987267\ttotal: 4m 6s\tremaining: 1m 23s\n",
      "748:\tlearn: 0.1986949\ttotal: 4m 6s\tremaining: 1m 22s\n",
      "749:\tlearn: 0.1986425\ttotal: 4m 7s\tremaining: 1m 22s\n",
      "750:\tlearn: 0.1986385\ttotal: 4m 7s\tremaining: 1m 22s\n",
      "751:\tlearn: 0.1986210\ttotal: 4m 7s\tremaining: 1m 21s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "752:\tlearn: 0.1986105\ttotal: 4m 8s\tremaining: 1m 21s\n",
      "753:\tlearn: 0.1986065\ttotal: 4m 8s\tremaining: 1m 21s\n",
      "754:\tlearn: 0.1985993\ttotal: 4m 8s\tremaining: 1m 20s\n",
      "755:\tlearn: 0.1985771\ttotal: 4m 9s\tremaining: 1m 20s\n",
      "756:\tlearn: 0.1985678\ttotal: 4m 9s\tremaining: 1m 20s\n",
      "757:\tlearn: 0.1985649\ttotal: 4m 9s\tremaining: 1m 19s\n",
      "758:\tlearn: 0.1985585\ttotal: 4m 10s\tremaining: 1m 19s\n",
      "759:\tlearn: 0.1985244\ttotal: 4m 10s\tremaining: 1m 19s\n",
      "760:\tlearn: 0.1984878\ttotal: 4m 10s\tremaining: 1m 18s\n",
      "761:\tlearn: 0.1984673\ttotal: 4m 11s\tremaining: 1m 18s\n",
      "762:\tlearn: 0.1984482\ttotal: 4m 11s\tremaining: 1m 18s\n",
      "763:\tlearn: 0.1984426\ttotal: 4m 11s\tremaining: 1m 17s\n",
      "764:\tlearn: 0.1984381\ttotal: 4m 12s\tremaining: 1m 17s\n",
      "765:\tlearn: 0.1984337\ttotal: 4m 12s\tremaining: 1m 17s\n",
      "766:\tlearn: 0.1984031\ttotal: 4m 13s\tremaining: 1m 16s\n",
      "767:\tlearn: 0.1983508\ttotal: 4m 13s\tremaining: 1m 16s\n",
      "768:\tlearn: 0.1982999\ttotal: 4m 13s\tremaining: 1m 16s\n",
      "769:\tlearn: 0.1982867\ttotal: 4m 14s\tremaining: 1m 15s\n",
      "770:\tlearn: 0.1982763\ttotal: 4m 14s\tremaining: 1m 15s\n",
      "771:\tlearn: 0.1982540\ttotal: 4m 14s\tremaining: 1m 15s\n",
      "772:\tlearn: 0.1982296\ttotal: 4m 15s\tremaining: 1m 14s\n",
      "773:\tlearn: 0.1982058\ttotal: 4m 15s\tremaining: 1m 14s\n",
      "774:\tlearn: 0.1981918\ttotal: 4m 15s\tremaining: 1m 14s\n",
      "775:\tlearn: 0.1981512\ttotal: 4m 16s\tremaining: 1m 13s\n",
      "776:\tlearn: 0.1981176\ttotal: 4m 16s\tremaining: 1m 13s\n",
      "777:\tlearn: 0.1981095\ttotal: 4m 16s\tremaining: 1m 13s\n",
      "778:\tlearn: 0.1981071\ttotal: 4m 16s\tremaining: 1m 12s\n",
      "779:\tlearn: 0.1981019\ttotal: 4m 17s\tremaining: 1m 12s\n",
      "780:\tlearn: 0.1980988\ttotal: 4m 17s\tremaining: 1m 12s\n",
      "781:\tlearn: 0.1980815\ttotal: 4m 17s\tremaining: 1m 11s\n",
      "782:\tlearn: 0.1980575\ttotal: 4m 18s\tremaining: 1m 11s\n",
      "783:\tlearn: 0.1980392\ttotal: 4m 18s\tremaining: 1m 11s\n",
      "784:\tlearn: 0.1980342\ttotal: 4m 18s\tremaining: 1m 10s\n",
      "785:\tlearn: 0.1980295\ttotal: 4m 18s\tremaining: 1m 10s\n",
      "786:\tlearn: 0.1980240\ttotal: 4m 19s\tremaining: 1m 10s\n",
      "787:\tlearn: 0.1980103\ttotal: 4m 19s\tremaining: 1m 9s\n",
      "788:\tlearn: 0.1979954\ttotal: 4m 19s\tremaining: 1m 9s\n",
      "789:\tlearn: 0.1979856\ttotal: 4m 20s\tremaining: 1m 9s\n",
      "790:\tlearn: 0.1979828\ttotal: 4m 20s\tremaining: 1m 8s\n",
      "791:\tlearn: 0.1979803\ttotal: 4m 20s\tremaining: 1m 8s\n",
      "792:\tlearn: 0.1979710\ttotal: 4m 20s\tremaining: 1m 8s\n",
      "793:\tlearn: 0.1979600\ttotal: 4m 21s\tremaining: 1m 7s\n",
      "794:\tlearn: 0.1979347\ttotal: 4m 21s\tremaining: 1m 7s\n",
      "795:\tlearn: 0.1979278\ttotal: 4m 21s\tremaining: 1m 7s\n",
      "796:\tlearn: 0.1978978\ttotal: 4m 21s\tremaining: 1m 6s\n",
      "797:\tlearn: 0.1978941\ttotal: 4m 22s\tremaining: 1m 6s\n",
      "798:\tlearn: 0.1978884\ttotal: 4m 22s\tremaining: 1m 6s\n",
      "799:\tlearn: 0.1978646\ttotal: 4m 22s\tremaining: 1m 5s\n",
      "800:\tlearn: 0.1978310\ttotal: 4m 23s\tremaining: 1m 5s\n",
      "801:\tlearn: 0.1978259\ttotal: 4m 23s\tremaining: 1m 5s\n",
      "802:\tlearn: 0.1978237\ttotal: 4m 23s\tremaining: 1m 4s\n",
      "803:\tlearn: 0.1978138\ttotal: 4m 23s\tremaining: 1m 4s\n",
      "804:\tlearn: 0.1977699\ttotal: 4m 24s\tremaining: 1m 4s\n",
      "805:\tlearn: 0.1977500\ttotal: 4m 24s\tremaining: 1m 3s\n",
      "806:\tlearn: 0.1977456\ttotal: 4m 24s\tremaining: 1m 3s\n",
      "807:\tlearn: 0.1977432\ttotal: 4m 25s\tremaining: 1m 3s\n",
      "808:\tlearn: 0.1977289\ttotal: 4m 25s\tremaining: 1m 2s\n",
      "809:\tlearn: 0.1977267\ttotal: 4m 25s\tremaining: 1m 2s\n",
      "810:\tlearn: 0.1976927\ttotal: 4m 26s\tremaining: 1m 2s\n",
      "811:\tlearn: 0.1976790\ttotal: 4m 26s\tremaining: 1m 1s\n",
      "812:\tlearn: 0.1976566\ttotal: 4m 26s\tremaining: 1m 1s\n",
      "813:\tlearn: 0.1976455\ttotal: 4m 26s\tremaining: 1m 1s\n",
      "814:\tlearn: 0.1976239\ttotal: 4m 27s\tremaining: 1m\n",
      "815:\tlearn: 0.1976202\ttotal: 4m 27s\tremaining: 1m\n",
      "816:\tlearn: 0.1976027\ttotal: 4m 27s\tremaining: 1m\n",
      "817:\tlearn: 0.1975888\ttotal: 4m 28s\tremaining: 59.7s\n",
      "818:\tlearn: 0.1975868\ttotal: 4m 28s\tremaining: 59.4s\n",
      "819:\tlearn: 0.1975741\ttotal: 4m 29s\tremaining: 59.1s\n",
      "820:\tlearn: 0.1975668\ttotal: 4m 29s\tremaining: 58.8s\n",
      "821:\tlearn: 0.1975644\ttotal: 4m 29s\tremaining: 58.4s\n",
      "822:\tlearn: 0.1975615\ttotal: 4m 30s\tremaining: 58.1s\n",
      "823:\tlearn: 0.1975160\ttotal: 4m 30s\tremaining: 57.8s\n",
      "824:\tlearn: 0.1975112\ttotal: 4m 30s\tremaining: 57.5s\n",
      "825:\tlearn: 0.1974856\ttotal: 4m 31s\tremaining: 57.1s\n",
      "826:\tlearn: 0.1974724\ttotal: 4m 31s\tremaining: 56.8s\n",
      "827:\tlearn: 0.1974678\ttotal: 4m 31s\tremaining: 56.5s\n",
      "828:\tlearn: 0.1974478\ttotal: 4m 32s\tremaining: 56.1s\n",
      "829:\tlearn: 0.1974226\ttotal: 4m 32s\tremaining: 55.8s\n",
      "830:\tlearn: 0.1974095\ttotal: 4m 32s\tremaining: 55.5s\n",
      "831:\tlearn: 0.1974036\ttotal: 4m 32s\tremaining: 55.1s\n",
      "832:\tlearn: 0.1973688\ttotal: 4m 33s\tremaining: 54.8s\n",
      "833:\tlearn: 0.1973445\ttotal: 4m 33s\tremaining: 54.4s\n",
      "834:\tlearn: 0.1973269\ttotal: 4m 33s\tremaining: 54.1s\n",
      "835:\tlearn: 0.1973230\ttotal: 4m 34s\tremaining: 53.8s\n",
      "836:\tlearn: 0.1973172\ttotal: 4m 34s\tremaining: 53.4s\n",
      "837:\tlearn: 0.1973078\ttotal: 4m 34s\tremaining: 53.1s\n",
      "838:\tlearn: 0.1972925\ttotal: 4m 34s\tremaining: 52.7s\n",
      "839:\tlearn: 0.1972860\ttotal: 4m 35s\tremaining: 52.4s\n",
      "840:\tlearn: 0.1972849\ttotal: 4m 35s\tremaining: 52.1s\n",
      "841:\tlearn: 0.1972828\ttotal: 4m 35s\tremaining: 51.7s\n",
      "842:\tlearn: 0.1972762\ttotal: 4m 36s\tremaining: 51.4s\n",
      "843:\tlearn: 0.1972735\ttotal: 4m 36s\tremaining: 51.1s\n",
      "844:\tlearn: 0.1972665\ttotal: 4m 36s\tremaining: 50.7s\n",
      "845:\tlearn: 0.1972620\ttotal: 4m 36s\tremaining: 50.4s\n",
      "846:\tlearn: 0.1972360\ttotal: 4m 37s\tremaining: 50.1s\n",
      "847:\tlearn: 0.1972348\ttotal: 4m 37s\tremaining: 49.7s\n",
      "848:\tlearn: 0.1972301\ttotal: 4m 37s\tremaining: 49.4s\n",
      "849:\tlearn: 0.1972270\ttotal: 4m 38s\tremaining: 49.1s\n",
      "850:\tlearn: 0.1972205\ttotal: 4m 38s\tremaining: 48.8s\n",
      "851:\tlearn: 0.1972185\ttotal: 4m 38s\tremaining: 48.4s\n",
      "852:\tlearn: 0.1972173\ttotal: 4m 39s\tremaining: 48.1s\n",
      "853:\tlearn: 0.1971941\ttotal: 4m 39s\tremaining: 47.8s\n",
      "854:\tlearn: 0.1971722\ttotal: 4m 39s\tremaining: 47.4s\n",
      "855:\tlearn: 0.1971694\ttotal: 4m 40s\tremaining: 47.1s\n",
      "856:\tlearn: 0.1971662\ttotal: 4m 40s\tremaining: 46.8s\n",
      "857:\tlearn: 0.1971431\ttotal: 4m 40s\tremaining: 46.5s\n",
      "858:\tlearn: 0.1971023\ttotal: 4m 41s\tremaining: 46.2s\n",
      "859:\tlearn: 0.1970921\ttotal: 4m 41s\tremaining: 45.8s\n",
      "860:\tlearn: 0.1970849\ttotal: 4m 41s\tremaining: 45.5s\n",
      "861:\tlearn: 0.1970759\ttotal: 4m 42s\tremaining: 45.2s\n",
      "862:\tlearn: 0.1970556\ttotal: 4m 42s\tremaining: 44.8s\n",
      "863:\tlearn: 0.1970323\ttotal: 4m 42s\tremaining: 44.5s\n",
      "864:\tlearn: 0.1970219\ttotal: 4m 43s\tremaining: 44.2s\n",
      "865:\tlearn: 0.1970199\ttotal: 4m 43s\tremaining: 43.8s\n",
      "866:\tlearn: 0.1970118\ttotal: 4m 43s\tremaining: 43.5s\n",
      "867:\tlearn: 0.1970049\ttotal: 4m 43s\tremaining: 43.2s\n",
      "868:\tlearn: 0.1970018\ttotal: 4m 44s\tremaining: 42.8s\n",
      "869:\tlearn: 0.1969911\ttotal: 4m 44s\tremaining: 42.5s\n",
      "870:\tlearn: 0.1969604\ttotal: 4m 44s\tremaining: 42.2s\n",
      "871:\tlearn: 0.1969508\ttotal: 4m 45s\tremaining: 41.9s\n",
      "872:\tlearn: 0.1969463\ttotal: 4m 45s\tremaining: 41.5s\n",
      "873:\tlearn: 0.1969192\ttotal: 4m 45s\tremaining: 41.2s\n",
      "874:\tlearn: 0.1968945\ttotal: 4m 46s\tremaining: 40.9s\n",
      "875:\tlearn: 0.1968893\ttotal: 4m 46s\tremaining: 40.6s\n",
      "876:\tlearn: 0.1968746\ttotal: 4m 47s\tremaining: 40.3s\n",
      "877:\tlearn: 0.1968688\ttotal: 4m 47s\tremaining: 39.9s\n",
      "878:\tlearn: 0.1968670\ttotal: 4m 47s\tremaining: 39.6s\n",
      "879:\tlearn: 0.1968486\ttotal: 4m 48s\tremaining: 39.3s\n",
      "880:\tlearn: 0.1968439\ttotal: 4m 48s\tremaining: 39s\n",
      "881:\tlearn: 0.1968329\ttotal: 4m 48s\tremaining: 38.6s\n",
      "882:\tlearn: 0.1968312\ttotal: 4m 49s\tremaining: 38.3s\n",
      "883:\tlearn: 0.1968274\ttotal: 4m 49s\tremaining: 38s\n",
      "884:\tlearn: 0.1968186\ttotal: 4m 49s\tremaining: 37.7s\n",
      "885:\tlearn: 0.1968157\ttotal: 4m 50s\tremaining: 37.3s\n",
      "886:\tlearn: 0.1967902\ttotal: 4m 50s\tremaining: 37s\n",
      "887:\tlearn: 0.1967830\ttotal: 4m 50s\tremaining: 36.7s\n",
      "888:\tlearn: 0.1967799\ttotal: 4m 51s\tremaining: 36.4s\n",
      "889:\tlearn: 0.1967170\ttotal: 4m 51s\tremaining: 36.1s\n",
      "890:\tlearn: 0.1967153\ttotal: 4m 52s\tremaining: 35.7s\n",
      "891:\tlearn: 0.1967139\ttotal: 4m 52s\tremaining: 35.4s\n",
      "892:\tlearn: 0.1967082\ttotal: 4m 52s\tremaining: 35.1s\n",
      "893:\tlearn: 0.1966944\ttotal: 4m 53s\tremaining: 34.8s\n",
      "894:\tlearn: 0.1966909\ttotal: 4m 53s\tremaining: 34.4s\n",
      "895:\tlearn: 0.1966895\ttotal: 4m 53s\tremaining: 34.1s\n",
      "896:\tlearn: 0.1966430\ttotal: 4m 54s\tremaining: 33.8s\n",
      "897:\tlearn: 0.1966235\ttotal: 4m 54s\tremaining: 33.5s\n",
      "898:\tlearn: 0.1966136\ttotal: 4m 54s\tremaining: 33.1s\n",
      "899:\tlearn: 0.1965888\ttotal: 4m 55s\tremaining: 32.8s\n",
      "900:\tlearn: 0.1965449\ttotal: 4m 55s\tremaining: 32.5s\n",
      "901:\tlearn: 0.1965426\ttotal: 4m 55s\tremaining: 32.2s\n",
      "902:\tlearn: 0.1965042\ttotal: 4m 56s\tremaining: 31.8s\n",
      "903:\tlearn: 0.1964816\ttotal: 4m 56s\tremaining: 31.5s\n",
      "904:\tlearn: 0.1964708\ttotal: 4m 57s\tremaining: 31.2s\n",
      "905:\tlearn: 0.1964675\ttotal: 4m 57s\tremaining: 30.9s\n",
      "906:\tlearn: 0.1964409\ttotal: 4m 57s\tremaining: 30.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "907:\tlearn: 0.1964215\ttotal: 4m 58s\tremaining: 30.2s\n",
      "908:\tlearn: 0.1964021\ttotal: 4m 58s\tremaining: 29.9s\n",
      "909:\tlearn: 0.1963979\ttotal: 4m 58s\tremaining: 29.5s\n",
      "910:\tlearn: 0.1963696\ttotal: 4m 58s\tremaining: 29.2s\n",
      "911:\tlearn: 0.1963628\ttotal: 4m 59s\tremaining: 28.9s\n",
      "912:\tlearn: 0.1963617\ttotal: 4m 59s\tremaining: 28.5s\n",
      "913:\tlearn: 0.1963543\ttotal: 4m 59s\tremaining: 28.2s\n",
      "914:\tlearn: 0.1963526\ttotal: 5m\tremaining: 27.9s\n",
      "915:\tlearn: 0.1963347\ttotal: 5m\tremaining: 27.6s\n",
      "916:\tlearn: 0.1963109\ttotal: 5m\tremaining: 27.2s\n",
      "917:\tlearn: 0.1963037\ttotal: 5m 1s\tremaining: 26.9s\n",
      "918:\tlearn: 0.1962748\ttotal: 5m 1s\tremaining: 26.6s\n",
      "919:\tlearn: 0.1962534\ttotal: 5m 1s\tremaining: 26.2s\n",
      "920:\tlearn: 0.1962260\ttotal: 5m 2s\tremaining: 25.9s\n",
      "921:\tlearn: 0.1962223\ttotal: 5m 2s\tremaining: 25.6s\n",
      "922:\tlearn: 0.1962216\ttotal: 5m 2s\tremaining: 25.2s\n",
      "923:\tlearn: 0.1962153\ttotal: 5m 2s\tremaining: 24.9s\n",
      "924:\tlearn: 0.1962096\ttotal: 5m 3s\tremaining: 24.6s\n",
      "925:\tlearn: 0.1962078\ttotal: 5m 3s\tremaining: 24.2s\n",
      "926:\tlearn: 0.1961593\ttotal: 5m 3s\tremaining: 23.9s\n",
      "927:\tlearn: 0.1961482\ttotal: 5m 4s\tremaining: 23.6s\n",
      "928:\tlearn: 0.1961469\ttotal: 5m 4s\tremaining: 23.3s\n",
      "929:\tlearn: 0.1961390\ttotal: 5m 4s\tremaining: 22.9s\n",
      "930:\tlearn: 0.1961306\ttotal: 5m 4s\tremaining: 22.6s\n",
      "931:\tlearn: 0.1961022\ttotal: 5m 5s\tremaining: 22.3s\n",
      "932:\tlearn: 0.1960938\ttotal: 5m 5s\tremaining: 21.9s\n",
      "933:\tlearn: 0.1960930\ttotal: 5m 5s\tremaining: 21.6s\n",
      "934:\tlearn: 0.1960593\ttotal: 5m 6s\tremaining: 21.3s\n",
      "935:\tlearn: 0.1960506\ttotal: 5m 6s\tremaining: 21s\n",
      "936:\tlearn: 0.1960494\ttotal: 5m 6s\tremaining: 20.6s\n",
      "937:\tlearn: 0.1960379\ttotal: 5m 7s\tremaining: 20.3s\n",
      "938:\tlearn: 0.1960362\ttotal: 5m 7s\tremaining: 20s\n",
      "939:\tlearn: 0.1960136\ttotal: 5m 7s\tremaining: 19.7s\n",
      "940:\tlearn: 0.1959912\ttotal: 5m 8s\tremaining: 19.3s\n",
      "941:\tlearn: 0.1959869\ttotal: 5m 8s\tremaining: 19s\n",
      "942:\tlearn: 0.1959601\ttotal: 5m 8s\tremaining: 18.7s\n",
      "943:\tlearn: 0.1959592\ttotal: 5m 9s\tremaining: 18.3s\n",
      "944:\tlearn: 0.1958838\ttotal: 5m 9s\tremaining: 18s\n",
      "945:\tlearn: 0.1958705\ttotal: 5m 9s\tremaining: 17.7s\n",
      "946:\tlearn: 0.1958694\ttotal: 5m 10s\tremaining: 17.4s\n",
      "947:\tlearn: 0.1958247\ttotal: 5m 10s\tremaining: 17s\n",
      "948:\tlearn: 0.1958240\ttotal: 5m 10s\tremaining: 16.7s\n",
      "949:\tlearn: 0.1958152\ttotal: 5m 11s\tremaining: 16.4s\n",
      "950:\tlearn: 0.1957822\ttotal: 5m 11s\tremaining: 16s\n",
      "951:\tlearn: 0.1957809\ttotal: 5m 11s\tremaining: 15.7s\n",
      "952:\tlearn: 0.1957540\ttotal: 5m 12s\tremaining: 15.4s\n",
      "953:\tlearn: 0.1957306\ttotal: 5m 12s\tremaining: 15.1s\n",
      "954:\tlearn: 0.1956986\ttotal: 5m 12s\tremaining: 14.7s\n",
      "955:\tlearn: 0.1956770\ttotal: 5m 12s\tremaining: 14.4s\n",
      "956:\tlearn: 0.1956754\ttotal: 5m 13s\tremaining: 14.1s\n",
      "957:\tlearn: 0.1956742\ttotal: 5m 13s\tremaining: 13.7s\n",
      "958:\tlearn: 0.1956656\ttotal: 5m 13s\tremaining: 13.4s\n",
      "959:\tlearn: 0.1956526\ttotal: 5m 14s\tremaining: 13.1s\n",
      "960:\tlearn: 0.1956186\ttotal: 5m 14s\tremaining: 12.8s\n",
      "961:\tlearn: 0.1956053\ttotal: 5m 14s\tremaining: 12.4s\n",
      "962:\tlearn: 0.1956044\ttotal: 5m 15s\tremaining: 12.1s\n",
      "963:\tlearn: 0.1955855\ttotal: 5m 15s\tremaining: 11.8s\n",
      "964:\tlearn: 0.1955732\ttotal: 5m 15s\tremaining: 11.5s\n",
      "965:\tlearn: 0.1955525\ttotal: 5m 16s\tremaining: 11.1s\n",
      "966:\tlearn: 0.1955443\ttotal: 5m 16s\tremaining: 10.8s\n",
      "967:\tlearn: 0.1955215\ttotal: 5m 16s\tremaining: 10.5s\n",
      "968:\tlearn: 0.1955163\ttotal: 5m 17s\tremaining: 10.1s\n",
      "969:\tlearn: 0.1955046\ttotal: 5m 17s\tremaining: 9.81s\n",
      "970:\tlearn: 0.1954813\ttotal: 5m 17s\tremaining: 9.49s\n",
      "971:\tlearn: 0.1954732\ttotal: 5m 17s\tremaining: 9.16s\n",
      "972:\tlearn: 0.1954642\ttotal: 5m 18s\tremaining: 8.83s\n",
      "973:\tlearn: 0.1954618\ttotal: 5m 18s\tremaining: 8.5s\n",
      "974:\tlearn: 0.1954600\ttotal: 5m 18s\tremaining: 8.18s\n",
      "975:\tlearn: 0.1954591\ttotal: 5m 19s\tremaining: 7.85s\n",
      "976:\tlearn: 0.1954486\ttotal: 5m 19s\tremaining: 7.52s\n",
      "977:\tlearn: 0.1954362\ttotal: 5m 19s\tremaining: 7.19s\n",
      "978:\tlearn: 0.1954209\ttotal: 5m 20s\tremaining: 6.86s\n",
      "979:\tlearn: 0.1954165\ttotal: 5m 20s\tremaining: 6.54s\n",
      "980:\tlearn: 0.1953997\ttotal: 5m 20s\tremaining: 6.21s\n",
      "981:\tlearn: 0.1953988\ttotal: 5m 20s\tremaining: 5.88s\n",
      "982:\tlearn: 0.1953888\ttotal: 5m 21s\tremaining: 5.55s\n",
      "983:\tlearn: 0.1953805\ttotal: 5m 21s\tremaining: 5.23s\n",
      "984:\tlearn: 0.1953790\ttotal: 5m 21s\tremaining: 4.9s\n",
      "985:\tlearn: 0.1953584\ttotal: 5m 22s\tremaining: 4.57s\n",
      "986:\tlearn: 0.1953520\ttotal: 5m 22s\tremaining: 4.25s\n",
      "987:\tlearn: 0.1953501\ttotal: 5m 22s\tremaining: 3.92s\n",
      "988:\tlearn: 0.1953491\ttotal: 5m 22s\tremaining: 3.59s\n",
      "989:\tlearn: 0.1953395\ttotal: 5m 23s\tremaining: 3.27s\n",
      "990:\tlearn: 0.1953312\ttotal: 5m 23s\tremaining: 2.94s\n",
      "991:\tlearn: 0.1953231\ttotal: 5m 23s\tremaining: 2.61s\n",
      "992:\tlearn: 0.1953151\ttotal: 5m 24s\tremaining: 2.28s\n",
      "993:\tlearn: 0.1952444\ttotal: 5m 24s\tremaining: 1.96s\n",
      "994:\tlearn: 0.1952435\ttotal: 5m 24s\tremaining: 1.63s\n",
      "995:\tlearn: 0.1952359\ttotal: 5m 24s\tremaining: 1.3s\n",
      "996:\tlearn: 0.1952198\ttotal: 5m 25s\tremaining: 979ms\n",
      "997:\tlearn: 0.1952177\ttotal: 5m 25s\tremaining: 652ms\n",
      "998:\tlearn: 0.1952121\ttotal: 5m 25s\tremaining: 326ms\n",
      "999:\tlearn: 0.1952080\ttotal: 5m 26s\tremaining: 0us\n",
      "CatBoostClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      1.00      0.96      7748\n",
      "         1.0       0.77      0.12      0.21       728\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      8476\n",
      "   macro avg       0.85      0.56      0.59      8476\n",
      "weighted avg       0.91      0.92      0.89      8476\n",
      "\n",
      "[[7721   27]\n",
      " [ 638   90]]\n",
      "Accuracy is  92.15431807456346\n",
      "Time on model's work: 337.443 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      1.00      0.96      7748\n",
      "         1.0       0.73      0.09      0.17       728\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      8476\n",
      "   macro avg       0.83      0.55      0.56      8476\n",
      "weighted avg       0.91      0.92      0.89      8476\n",
      "\n",
      "[[7723   25]\n",
      " [ 659   69]]\n",
      "Accuracy is  91.93015573383671\n",
      "Time on model's work: 1.057 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.97      0.95      7748\n",
      "         1.0       0.42      0.25      0.32       728\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      8476\n",
      "   macro avg       0.68      0.61      0.63      8476\n",
      "weighted avg       0.89      0.91      0.90      8476\n",
      "\n",
      "[[7495  253]\n",
      " [ 544  184]]\n",
      "Accuracy is  90.59697970740915\n",
      "Time on model's work: 0.804 s\n",
      "====================================================================================================\n",
      "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n",
      "TFFMClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.99      0.95      7748\n",
      "         1.0       0.51      0.13      0.21       728\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      8476\n",
      "   macro avg       0.72      0.56      0.58      8476\n",
      "weighted avg       0.89      0.91      0.89      8476\n",
      "\n",
      "[[7659   89]\n",
      " [ 634   94]]\n",
      "Accuracy is  91.47003303445021\n",
      "Time on model's work: 119.171 s\n",
      "====================================================================================================\n",
      "TOTAL TIME SPENT:  2230.274 s\n"
     ]
    }
   ],
   "source": [
    "clfs = [\n",
    "        ['RandomForestClassifier', RandomForestClassifier()],\n",
    "        ['GradientBoostingClassifier', GradientBoostingClassifier()],\n",
    "        ['ExtraTreesClassifier', ExtraTreesClassifier()],\n",
    "        ['AdaBoostClassifier', AdaBoostClassifier()],\n",
    "        ['BaggingClassifier', BaggingClassifier()],\n",
    "        ['DecisionTreeClassifier', DecisionTreeClassifier()],\n",
    "        ['MLPClassifier', MLPClassifier()],\n",
    "        ['XGBClassifier', XGBClassifier()],\n",
    "        ['CatBoostClassifier', CatBoostClassifier()],\n",
    "        ['LogisticRegression', LogisticRegression()],\n",
    "        ['SGDClassifier', SGDClassifier()],\n",
    "        ['TFFMClassifier', TFFMClassifier()]\n",
    "       ]\n",
    "t = time()\n",
    "for name, clf in clfs:\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    prediction = clf.predict(X_test)\n",
    "    print(name)\n",
    "    print(classification_report(y_test, prediction))\n",
    "    print(confusion_matrix(y_test, prediction))\n",
    "    print('Accuracy is ', accuracy_score(y_test, prediction)*100)\n",
    "    print (\"Time on model's work:\", round(time()-t0, 3), \"s\")\n",
    "    print('='*100)\n",
    "print (\"TOTAL TIME SPENT: \", round(time()-t, 3), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:16<00:00,  3.36epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9177678150070788\n",
      "[[7662   86]\n",
      " [ 611  117]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.99      0.96      7748\n",
      "         1.0       0.58      0.16      0.25       728\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      8476\n",
      "   macro avg       0.75      0.57      0.60      8476\n",
      "weighted avg       0.90      0.92      0.90      8476\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:15<00:00,  3.44epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9084473808400189\n",
      "[[7495  253]\n",
      " [ 523  205]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.97      0.95      7748\n",
      "         1.0       0.45      0.28      0.35       728\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      8476\n",
      "   macro avg       0.69      0.62      0.65      8476\n",
      "weighted avg       0.89      0.91      0.90      8476\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:14<00:00,  3.44epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8900424728645587\n",
      "[[7231  517]\n",
      " [ 415  313]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.93      0.94      7748\n",
      "         1.0       0.38      0.43      0.40       728\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      8476\n",
      "   macro avg       0.66      0.68      0.67      8476\n",
      "weighted avg       0.90      0.89      0.89      8476\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:14<00:00,  3.53epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8671543180745634\n",
      "[[6947  801]\n",
      " [ 325  403]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.90      0.93      7748\n",
      "         1.0       0.33      0.55      0.42       728\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      8476\n",
      "   macro avg       0.65      0.73      0.67      8476\n",
      "weighted avg       0.90      0.87      0.88      8476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TFFM sparse - works worse with sparse\n",
    "# only CSR format supported\n",
    "X_train_sparse = sp.csr_matrix(X_train)\n",
    "X_test_sparse = sp.csr_matrix(X_test)\n",
    "# weight - optional / AdamOptimizer\n",
    "pos_class_weight = list(map(float,range(1, 5)))\n",
    "for weight in pos_class_weight:\n",
    "    model = TFFMClassifier(\n",
    "        order=2,\n",
    "        pos_class_weight=weight,\n",
    "        rank=10, \n",
    "        optimizer=tf.train.AdamOptimizer(learning_rate=0.001), \n",
    "        n_epochs=50, \n",
    "        batch_size=1024,\n",
    "        init_std=0.001,\n",
    "        reg=0.01,\n",
    "        input_type='sparse',\n",
    "        #log_dir='./tmp/logs',\n",
    "        #verbose=1,\n",
    "        seed=42\n",
    "    )\n",
    "    model.fit(X_train_sparse, y_train, show_progress=True)\n",
    "    predictions = model.predict(X_test_sparse)\n",
    "    print('accuracy: {}'.format(accuracy_score(y_test, predictions)))\n",
    "    # this will close tf.Session and free resources\n",
    "    print(confusion_matrix(y_test,predictions)) \n",
    "    print(classification_report(y_test, predictions))\n",
    "    model.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:14<00:00,  3.47epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9141104294478528\n",
      "[[7748    0]\n",
      " [ 728    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      1.00      0.96      7748\n",
      "         1.0       0.00      0.00      0.00       728\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      8476\n",
      "   macro avg       0.46      0.50      0.48      8476\n",
      "weighted avg       0.84      0.91      0.87      8476\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:14<00:00,  3.44epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9137564889098632\n",
      "[[7745    3]\n",
      " [ 728    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      1.00      0.95      7748\n",
      "         1.0       0.00      0.00      0.00       728\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      8476\n",
      "   macro avg       0.46      0.50      0.48      8476\n",
      "weighted avg       0.84      0.91      0.87      8476\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:14<00:00,  3.49epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8856772062293534\n",
      "[[7313  435]\n",
      " [ 534  194]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.94      0.94      7748\n",
      "         1.0       0.31      0.27      0.29       728\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      8476\n",
      "   macro avg       0.62      0.61      0.61      8476\n",
      "weighted avg       0.88      0.89      0.88      8476\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:14<00:00,  3.41epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8141812175554507\n",
      "[[6442 1306]\n",
      " [ 269  459]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.83      0.89      7748\n",
      "         1.0       0.26      0.63      0.37       728\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      8476\n",
      "   macro avg       0.61      0.73      0.63      8476\n",
      "weighted avg       0.90      0.81      0.85      8476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# weight - optional / FtrlOptimizer\n",
    "pos_class_weight = list(map(float,range(1, 5)))\n",
    "for weight in pos_class_weight:\n",
    "    model = TFFMClassifier(\n",
    "        order=2,\n",
    "        pos_class_weight=weight,\n",
    "        rank=10, \n",
    "        optimizer=tf.train.FtrlOptimizer(0.01, l2_regularization_strength=0.001, l2_shrinkage_regularization_strength=0.001), \n",
    "        n_epochs=50, \n",
    "        batch_size=1024,\n",
    "        init_std=0.001,\n",
    "        reg=0.01,\n",
    "        input_type='sparse',\n",
    "        #log_dir='./tmp/logs',\n",
    "        #verbose=1,\n",
    "        seed=42\n",
    "    )\n",
    "    model.fit(X_train_sparse, y_train, show_progress=True)\n",
    "    predictions = model.predict(X_test_sparse)\n",
    "    print('accuracy: {}'.format(accuracy_score(y_test, predictions)))\n",
    "    # this will close tf.Session and free resources\n",
    "    print(confusion_matrix(y_test,predictions)) \n",
    "    print(classification_report(y_test, predictions))\n",
    "    model.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "33902/33902 [==============================] - ETA: 38s - loss: 0.8785 - acc: 0.40 - ETA: 9s - loss: 0.5812 - acc: 0.7008 - ETA: 7s - loss: 0.5273 - acc: 0.753 - ETA: 7s - loss: 0.5113 - acc: 0.769 - ETA: 6s - loss: 0.4750 - acc: 0.801 - ETA: 5s - loss: 0.4467 - acc: 0.823 - ETA: 4s - loss: 0.4322 - acc: 0.834 - ETA: 4s - loss: 0.4231 - acc: 0.841 - ETA: 4s - loss: 0.4115 - acc: 0.850 - ETA: 4s - loss: 0.4007 - acc: 0.857 - ETA: 4s - loss: 0.3965 - acc: 0.860 - ETA: 4s - loss: 0.3898 - acc: 0.864 - ETA: 3s - loss: 0.3832 - acc: 0.868 - ETA: 3s - loss: 0.3757 - acc: 0.872 - ETA: 3s - loss: 0.3692 - acc: 0.875 - ETA: 3s - loss: 0.3662 - acc: 0.877 - ETA: 3s - loss: 0.3623 - acc: 0.879 - ETA: 3s - loss: 0.3613 - acc: 0.879 - ETA: 3s - loss: 0.3591 - acc: 0.880 - ETA: 2s - loss: 0.3574 - acc: 0.882 - ETA: 2s - loss: 0.3532 - acc: 0.885 - ETA: 2s - loss: 0.3515 - acc: 0.886 - ETA: 2s - loss: 0.3477 - acc: 0.887 - ETA: 2s - loss: 0.3450 - acc: 0.889 - ETA: 2s - loss: 0.3426 - acc: 0.890 - ETA: 2s - loss: 0.3396 - acc: 0.891 - ETA: 2s - loss: 0.3386 - acc: 0.892 - ETA: 2s - loss: 0.3374 - acc: 0.892 - ETA: 1s - loss: 0.3358 - acc: 0.893 - ETA: 1s - loss: 0.3334 - acc: 0.894 - ETA: 1s - loss: 0.3306 - acc: 0.895 - ETA: 1s - loss: 0.3289 - acc: 0.896 - ETA: 1s - loss: 0.3289 - acc: 0.896 - ETA: 1s - loss: 0.3282 - acc: 0.896 - ETA: 1s - loss: 0.3281 - acc: 0.896 - ETA: 1s - loss: 0.3270 - acc: 0.896 - ETA: 1s - loss: 0.3261 - acc: 0.897 - ETA: 1s - loss: 0.3253 - acc: 0.897 - ETA: 1s - loss: 0.3242 - acc: 0.897 - ETA: 1s - loss: 0.3226 - acc: 0.898 - ETA: 0s - loss: 0.3224 - acc: 0.898 - ETA: 0s - loss: 0.3210 - acc: 0.898 - ETA: 0s - loss: 0.3200 - acc: 0.899 - ETA: 0s - loss: 0.3193 - acc: 0.899 - ETA: 0s - loss: 0.3184 - acc: 0.899 - ETA: 0s - loss: 0.3177 - acc: 0.899 - ETA: 0s - loss: 0.3169 - acc: 0.899 - ETA: 0s - loss: 0.3166 - acc: 0.899 - ETA: 0s - loss: 0.3160 - acc: 0.899 - ETA: 0s - loss: 0.3144 - acc: 0.900 - ETA: 0s - loss: 0.3133 - acc: 0.900 - ETA: 0s - loss: 0.3121 - acc: 0.900 - ETA: 0s - loss: 0.3121 - acc: 0.900 - ETA: 0s - loss: 0.3111 - acc: 0.900 - ETA: 0s - loss: 0.3101 - acc: 0.901 - 4s 112us/step - loss: 0.3091 - acc: 0.9015\n",
      "Epoch 2/20\n",
      "33902/33902 [==============================] - ETA: 5s - loss: 0.2400 - acc: 0.933 - ETA: 3s - loss: 0.2291 - acc: 0.928 - ETA: 2s - loss: 0.2431 - acc: 0.919 - ETA: 2s - loss: 0.2506 - acc: 0.916 - ETA: 2s - loss: 0.2553 - acc: 0.912 - ETA: 2s - loss: 0.2553 - acc: 0.911 - ETA: 2s - loss: 0.2570 - acc: 0.911 - ETA: 2s - loss: 0.2545 - acc: 0.912 - ETA: 2s - loss: 0.2568 - acc: 0.910 - ETA: 2s - loss: 0.2602 - acc: 0.909 - ETA: 2s - loss: 0.2602 - acc: 0.908 - ETA: 2s - loss: 0.2613 - acc: 0.907 - ETA: 2s - loss: 0.2614 - acc: 0.908 - ETA: 2s - loss: 0.2642 - acc: 0.907 - ETA: 2s - loss: 0.2641 - acc: 0.907 - ETA: 1s - loss: 0.2619 - acc: 0.908 - ETA: 1s - loss: 0.2628 - acc: 0.907 - ETA: 1s - loss: 0.2625 - acc: 0.907 - ETA: 1s - loss: 0.2639 - acc: 0.907 - ETA: 1s - loss: 0.2638 - acc: 0.908 - ETA: 1s - loss: 0.2641 - acc: 0.908 - ETA: 1s - loss: 0.2638 - acc: 0.908 - ETA: 1s - loss: 0.2640 - acc: 0.908 - ETA: 1s - loss: 0.2626 - acc: 0.908 - ETA: 1s - loss: 0.2622 - acc: 0.908 - ETA: 1s - loss: 0.2623 - acc: 0.908 - ETA: 1s - loss: 0.2629 - acc: 0.908 - ETA: 1s - loss: 0.2640 - acc: 0.907 - ETA: 1s - loss: 0.2651 - acc: 0.907 - ETA: 1s - loss: 0.2639 - acc: 0.907 - ETA: 1s - loss: 0.2642 - acc: 0.907 - ETA: 1s - loss: 0.2634 - acc: 0.908 - ETA: 0s - loss: 0.2631 - acc: 0.908 - ETA: 0s - loss: 0.2628 - acc: 0.908 - ETA: 0s - loss: 0.2620 - acc: 0.908 - ETA: 0s - loss: 0.2616 - acc: 0.908 - ETA: 0s - loss: 0.2606 - acc: 0.908 - ETA: 0s - loss: 0.2590 - acc: 0.909 - ETA: 0s - loss: 0.2579 - acc: 0.910 - ETA: 0s - loss: 0.2575 - acc: 0.910 - ETA: 0s - loss: 0.2574 - acc: 0.910 - ETA: 0s - loss: 0.2575 - acc: 0.910 - ETA: 0s - loss: 0.2578 - acc: 0.910 - ETA: 0s - loss: 0.2588 - acc: 0.909 - ETA: 0s - loss: 0.2582 - acc: 0.910 - ETA: 0s - loss: 0.2574 - acc: 0.910 - ETA: 0s - loss: 0.2572 - acc: 0.910 - ETA: 0s - loss: 0.2579 - acc: 0.910 - ETA: 0s - loss: 0.2577 - acc: 0.910 - ETA: 0s - loss: 0.2580 - acc: 0.910 - ETA: 0s - loss: 0.2580 - acc: 0.910 - 3s 93us/step - loss: 0.2580 - acc: 0.9102\n",
      "Epoch 3/20\n",
      "33902/33902 [==============================] - ETA: 2s - loss: 0.2239 - acc: 0.921 - ETA: 2s - loss: 0.2478 - acc: 0.908 - ETA: 2s - loss: 0.2636 - acc: 0.902 - ETA: 2s - loss: 0.2565 - acc: 0.908 - ETA: 3s - loss: 0.2608 - acc: 0.907 - ETA: 3s - loss: 0.2565 - acc: 0.909 - ETA: 2s - loss: 0.2497 - acc: 0.912 - ETA: 2s - loss: 0.2469 - acc: 0.912 - ETA: 2s - loss: 0.2461 - acc: 0.913 - ETA: 2s - loss: 0.2446 - acc: 0.913 - ETA: 2s - loss: 0.2442 - acc: 0.913 - ETA: 2s - loss: 0.2452 - acc: 0.912 - ETA: 2s - loss: 0.2447 - acc: 0.912 - ETA: 2s - loss: 0.2451 - acc: 0.912 - ETA: 2s - loss: 0.2487 - acc: 0.911 - ETA: 2s - loss: 0.2484 - acc: 0.911 - ETA: 2s - loss: 0.2496 - acc: 0.911 - ETA: 2s - loss: 0.2491 - acc: 0.911 - ETA: 2s - loss: 0.2497 - acc: 0.911 - ETA: 1s - loss: 0.2515 - acc: 0.909 - ETA: 1s - loss: 0.2543 - acc: 0.909 - ETA: 1s - loss: 0.2527 - acc: 0.910 - ETA: 1s - loss: 0.2519 - acc: 0.910 - ETA: 1s - loss: 0.2521 - acc: 0.910 - ETA: 1s - loss: 0.2517 - acc: 0.910 - ETA: 1s - loss: 0.2510 - acc: 0.910 - ETA: 1s - loss: 0.2495 - acc: 0.911 - ETA: 1s - loss: 0.2490 - acc: 0.911 - ETA: 1s - loss: 0.2486 - acc: 0.911 - ETA: 1s - loss: 0.2481 - acc: 0.911 - ETA: 1s - loss: 0.2488 - acc: 0.911 - ETA: 1s - loss: 0.2484 - acc: 0.911 - ETA: 1s - loss: 0.2485 - acc: 0.911 - ETA: 1s - loss: 0.2486 - acc: 0.911 - ETA: 0s - loss: 0.2488 - acc: 0.911 - ETA: 0s - loss: 0.2489 - acc: 0.911 - ETA: 0s - loss: 0.2491 - acc: 0.911 - ETA: 0s - loss: 0.2492 - acc: 0.910 - ETA: 0s - loss: 0.2496 - acc: 0.910 - ETA: 0s - loss: 0.2495 - acc: 0.910 - ETA: 0s - loss: 0.2493 - acc: 0.910 - ETA: 0s - loss: 0.2498 - acc: 0.910 - ETA: 0s - loss: 0.2497 - acc: 0.910 - ETA: 0s - loss: 0.2496 - acc: 0.910 - ETA: 0s - loss: 0.2496 - acc: 0.910 - ETA: 0s - loss: 0.2498 - acc: 0.910 - ETA: 0s - loss: 0.2498 - acc: 0.910 - ETA: 0s - loss: 0.2498 - acc: 0.910 - ETA: 0s - loss: 0.2485 - acc: 0.911 - ETA: 0s - loss: 0.2485 - acc: 0.911 - 3s 91us/step - loss: 0.2487 - acc: 0.9113\n",
      "Epoch 4/20\n",
      "33902/33902 [==============================] - ETA: 2s - loss: 0.2598 - acc: 0.898 - ETA: 2s - loss: 0.2444 - acc: 0.912 - ETA: 4s - loss: 0.2424 - acc: 0.912 - ETA: 3s - loss: 0.2298 - acc: 0.919 - ETA: 3s - loss: 0.2312 - acc: 0.919 - ETA: 3s - loss: 0.2340 - acc: 0.918 - ETA: 3s - loss: 0.2418 - acc: 0.913 - ETA: 3s - loss: 0.2398 - acc: 0.914 - ETA: 2s - loss: 0.2371 - acc: 0.914 - ETA: 2s - loss: 0.2363 - acc: 0.914 - ETA: 2s - loss: 0.2358 - acc: 0.913 - ETA: 2s - loss: 0.2381 - acc: 0.913 - ETA: 2s - loss: 0.2375 - acc: 0.914 - ETA: 2s - loss: 0.2399 - acc: 0.914 - ETA: 2s - loss: 0.2378 - acc: 0.914 - ETA: 2s - loss: 0.2368 - acc: 0.914 - ETA: 2s - loss: 0.2374 - acc: 0.914 - ETA: 2s - loss: 0.2374 - acc: 0.914 - ETA: 1s - loss: 0.2375 - acc: 0.913 - ETA: 1s - loss: 0.2384 - acc: 0.913 - ETA: 1s - loss: 0.2393 - acc: 0.913 - ETA: 1s - loss: 0.2404 - acc: 0.913 - ETA: 1s - loss: 0.2399 - acc: 0.913 - ETA: 1s - loss: 0.2426 - acc: 0.912 - ETA: 1s - loss: 0.2419 - acc: 0.912 - ETA: 1s - loss: 0.2426 - acc: 0.912 - ETA: 1s - loss: 0.2424 - acc: 0.913 - ETA: 1s - loss: 0.2431 - acc: 0.913 - ETA: 1s - loss: 0.2430 - acc: 0.912 - ETA: 1s - loss: 0.2444 - acc: 0.912 - ETA: 1s - loss: 0.2439 - acc: 0.912 - ETA: 1s - loss: 0.2433 - acc: 0.912 - ETA: 0s - loss: 0.2433 - acc: 0.912 - ETA: 0s - loss: 0.2442 - acc: 0.912 - ETA: 0s - loss: 0.2447 - acc: 0.912 - ETA: 0s - loss: 0.2442 - acc: 0.912 - ETA: 0s - loss: 0.2440 - acc: 0.912 - ETA: 0s - loss: 0.2444 - acc: 0.911 - ETA: 0s - loss: 0.2443 - acc: 0.911 - ETA: 0s - loss: 0.2440 - acc: 0.912 - ETA: 0s - loss: 0.2438 - acc: 0.912 - ETA: 0s - loss: 0.2436 - acc: 0.912 - ETA: 0s - loss: 0.2435 - acc: 0.912 - ETA: 0s - loss: 0.2436 - acc: 0.912 - ETA: 0s - loss: 0.2439 - acc: 0.911 - ETA: 0s - loss: 0.2442 - acc: 0.911 - 3s 93us/step - loss: 0.2441 - acc: 0.9116\n",
      "Epoch 5/20\n",
      "33902/33902 [==============================] - ETA: 3s - loss: 0.1856 - acc: 0.925 - ETA: 4s - loss: 0.2382 - acc: 0.918 - ETA: 3s - loss: 0.2315 - acc: 0.916 - ETA: 3s - loss: 0.2268 - acc: 0.920 - ETA: 2s - loss: 0.2332 - acc: 0.916 - ETA: 2s - loss: 0.2384 - acc: 0.914 - ETA: 2s - loss: 0.2395 - acc: 0.914 - ETA: 2s - loss: 0.2453 - acc: 0.911 - ETA: 2s - loss: 0.2427 - acc: 0.913 - ETA: 2s - loss: 0.2434 - acc: 0.911 - ETA: 2s - loss: 0.2436 - acc: 0.911 - ETA: 2s - loss: 0.2433 - acc: 0.911 - ETA: 2s - loss: 0.2431 - acc: 0.911 - ETA: 2s - loss: 0.2425 - acc: 0.912 - ETA: 2s - loss: 0.2410 - acc: 0.912 - ETA: 2s - loss: 0.2408 - acc: 0.912 - ETA: 2s - loss: 0.2403 - acc: 0.911 - ETA: 1s - loss: 0.2403 - acc: 0.911 - ETA: 1s - loss: 0.2420 - acc: 0.911 - ETA: 1s - loss: 0.2410 - acc: 0.911 - ETA: 1s - loss: 0.2391 - acc: 0.912 - ETA: 1s - loss: 0.2400 - acc: 0.911 - ETA: 1s - loss: 0.2400 - acc: 0.911 - ETA: 1s - loss: 0.2394 - acc: 0.912 - ETA: 1s - loss: 0.2399 - acc: 0.911 - ETA: 1s - loss: 0.2399 - acc: 0.911 - ETA: 1s - loss: 0.2397 - acc: 0.912 - ETA: 1s - loss: 0.2406 - acc: 0.911 - ETA: 1s - loss: 0.2421 - acc: 0.911 - ETA: 1s - loss: 0.2425 - acc: 0.911 - ETA: 1s - loss: 0.2426 - acc: 0.911 - ETA: 1s - loss: 0.2422 - acc: 0.912 - ETA: 0s - loss: 0.2424 - acc: 0.912 - ETA: 0s - loss: 0.2433 - acc: 0.911 - ETA: 0s - loss: 0.2435 - acc: 0.911 - ETA: 0s - loss: 0.2432 - acc: 0.911 - ETA: 0s - loss: 0.2433 - acc: 0.911 - ETA: 0s - loss: 0.2432 - acc: 0.911 - ETA: 0s - loss: 0.2437 - acc: 0.911 - ETA: 0s - loss: 0.2436 - acc: 0.911 - ETA: 0s - loss: 0.2434 - acc: 0.911 - ETA: 0s - loss: 0.2432 - acc: 0.911 - ETA: 0s - loss: 0.2434 - acc: 0.911 - ETA: 0s - loss: 0.2433 - acc: 0.911 - ETA: 0s - loss: 0.2427 - acc: 0.911 - ETA: 0s - loss: 0.2433 - acc: 0.911 - ETA: 0s - loss: 0.2426 - acc: 0.911 - ETA: 0s - loss: 0.2423 - acc: 0.911 - 3s 91us/step - loss: 0.2428 - acc: 0.9116\n",
      "Epoch 6/20\n",
      "33902/33902 [==============================] - ETA: 3s - loss: 0.2474 - acc: 0.906 - ETA: 2s - loss: 0.2413 - acc: 0.916 - ETA: 3s - loss: 0.2546 - acc: 0.910 - ETA: 3s - loss: 0.2418 - acc: 0.914 - ETA: 2s - loss: 0.2307 - acc: 0.917 - ETA: 2s - loss: 0.2392 - acc: 0.913 - ETA: 2s - loss: 0.2363 - acc: 0.913 - ETA: 2s - loss: 0.2372 - acc: 0.911 - ETA: 2s - loss: 0.2392 - acc: 0.909 - ETA: 2s - loss: 0.2404 - acc: 0.910 - ETA: 2s - loss: 0.2398 - acc: 0.910 - ETA: 2s - loss: 0.2403 - acc: 0.909 - ETA: 2s - loss: 0.2436 - acc: 0.908 - ETA: 2s - loss: 0.2449 - acc: 0.907 - ETA: 1s - loss: 0.2433 - acc: 0.908 - ETA: 1s - loss: 0.2409 - acc: 0.910 - ETA: 1s - loss: 0.2419 - acc: 0.909 - ETA: 1s - loss: 0.2411 - acc: 0.909 - ETA: 1s - loss: 0.2403 - acc: 0.910 - ETA: 1s - loss: 0.2416 - acc: 0.909 - ETA: 1s - loss: 0.2397 - acc: 0.910 - ETA: 1s - loss: 0.2390 - acc: 0.910 - ETA: 1s - loss: 0.2397 - acc: 0.910 - ETA: 1s - loss: 0.2389 - acc: 0.911 - ETA: 1s - loss: 0.2376 - acc: 0.911 - ETA: 1s - loss: 0.2381 - acc: 0.911 - ETA: 1s - loss: 0.2383 - acc: 0.911 - ETA: 1s - loss: 0.2388 - acc: 0.910 - ETA: 1s - loss: 0.2392 - acc: 0.910 - ETA: 1s - loss: 0.2397 - acc: 0.910 - ETA: 0s - loss: 0.2393 - acc: 0.911 - ETA: 0s - loss: 0.2398 - acc: 0.910 - ETA: 0s - loss: 0.2409 - acc: 0.910 - ETA: 0s - loss: 0.2408 - acc: 0.910 - ETA: 0s - loss: 0.2405 - acc: 0.910 - ETA: 0s - loss: 0.2404 - acc: 0.910 - ETA: 0s - loss: 0.2399 - acc: 0.910 - ETA: 0s - loss: 0.2400 - acc: 0.910 - ETA: 0s - loss: 0.2394 - acc: 0.910 - ETA: 0s - loss: 0.2390 - acc: 0.911 - ETA: 0s - loss: 0.2395 - acc: 0.911 - ETA: 0s - loss: 0.2392 - acc: 0.911 - ETA: 0s - loss: 0.2394 - acc: 0.911 - ETA: 0s - loss: 0.2380 - acc: 0.911 - ETA: 0s - loss: 0.2384 - acc: 0.911 - 3s 81us/step - loss: 0.2383 - acc: 0.9116\n",
      "Epoch 7/20\n",
      "33902/33902 [==============================] - ETA: 2s - loss: 0.2594 - acc: 0.894 - ETA: 3s - loss: 0.2201 - acc: 0.912 - ETA: 3s - loss: 0.2252 - acc: 0.910 - ETA: 2s - loss: 0.2347 - acc: 0.910 - ETA: 3s - loss: 0.2272 - acc: 0.913 - ETA: 3s - loss: 0.2281 - acc: 0.914 - ETA: 3s - loss: 0.2258 - acc: 0.915 - ETA: 3s - loss: 0.2287 - acc: 0.915 - ETA: 3s - loss: 0.2301 - acc: 0.917 - ETA: 3s - loss: 0.2274 - acc: 0.917 - ETA: 3s - loss: 0.2294 - acc: 0.916 - ETA: 3s - loss: 0.2273 - acc: 0.916 - ETA: 3s - loss: 0.2278 - acc: 0.916 - ETA: 3s - loss: 0.2295 - acc: 0.916 - ETA: 2s - loss: 0.2301 - acc: 0.915 - ETA: 2s - loss: 0.2329 - acc: 0.914 - ETA: 2s - loss: 0.2312 - acc: 0.914 - ETA: 2s - loss: 0.2318 - acc: 0.913 - ETA: 2s - loss: 0.2332 - acc: 0.912 - ETA: 2s - loss: 0.2318 - acc: 0.913 - ETA: 2s - loss: 0.2311 - acc: 0.913 - ETA: 2s - loss: 0.2327 - acc: 0.913 - ETA: 2s - loss: 0.2344 - acc: 0.912 - ETA: 1s - loss: 0.2353 - acc: 0.911 - ETA: 1s - loss: 0.2353 - acc: 0.912 - ETA: 1s - loss: 0.2367 - acc: 0.911 - ETA: 1s - loss: 0.2362 - acc: 0.911 - ETA: 1s - loss: 0.2365 - acc: 0.911 - ETA: 1s - loss: 0.2371 - acc: 0.911 - ETA: 1s - loss: 0.2357 - acc: 0.912 - ETA: 1s - loss: 0.2355 - acc: 0.911 - ETA: 1s - loss: 0.2359 - acc: 0.911 - ETA: 1s - loss: 0.2366 - acc: 0.911 - ETA: 1s - loss: 0.2369 - acc: 0.911 - ETA: 0s - loss: 0.2361 - acc: 0.911 - ETA: 0s - loss: 0.2367 - acc: 0.911 - ETA: 0s - loss: 0.2361 - acc: 0.911 - ETA: 0s - loss: 0.2349 - acc: 0.911 - ETA: 0s - loss: 0.2349 - acc: 0.911 - ETA: 0s - loss: 0.2348 - acc: 0.911 - ETA: 0s - loss: 0.2352 - acc: 0.911 - ETA: 0s - loss: 0.2356 - acc: 0.911 - ETA: 0s - loss: 0.2356 - acc: 0.911 - ETA: 0s - loss: 0.2353 - acc: 0.911 - ETA: 0s - loss: 0.2356 - acc: 0.911 - ETA: 0s - loss: 0.2356 - acc: 0.911 - ETA: 0s - loss: 0.2350 - acc: 0.911 - 3s 89us/step - loss: 0.2347 - acc: 0.9116\n",
      "Epoch 8/20\n",
      "33902/33902 [==============================] - ETA: 2s - loss: 0.2946 - acc: 0.878 - ETA: 2s - loss: 0.2589 - acc: 0.901 - ETA: 2s - loss: 0.2424 - acc: 0.907 - ETA: 2s - loss: 0.2364 - acc: 0.910 - ETA: 2s - loss: 0.2351 - acc: 0.911 - ETA: 2s - loss: 0.2392 - acc: 0.910 - ETA: 2s - loss: 0.2386 - acc: 0.910 - ETA: 2s - loss: 0.2367 - acc: 0.911 - ETA: 2s - loss: 0.2370 - acc: 0.910 - ETA: 2s - loss: 0.2350 - acc: 0.910 - ETA: 1s - loss: 0.2358 - acc: 0.909 - ETA: 1s - loss: 0.2343 - acc: 0.909 - ETA: 1s - loss: 0.2340 - acc: 0.910 - ETA: 1s - loss: 0.2360 - acc: 0.909 - ETA: 1s - loss: 0.2350 - acc: 0.910 - ETA: 1s - loss: 0.2339 - acc: 0.911 - ETA: 1s - loss: 0.2361 - acc: 0.910 - ETA: 1s - loss: 0.2372 - acc: 0.909 - ETA: 1s - loss: 0.2362 - acc: 0.910 - ETA: 1s - loss: 0.2353 - acc: 0.910 - ETA: 1s - loss: 0.2346 - acc: 0.911 - ETA: 1s - loss: 0.2345 - acc: 0.910 - ETA: 1s - loss: 0.2348 - acc: 0.910 - ETA: 1s - loss: 0.2350 - acc: 0.910 - ETA: 1s - loss: 0.2341 - acc: 0.910 - ETA: 1s - loss: 0.2335 - acc: 0.911 - ETA: 1s - loss: 0.2331 - acc: 0.911 - ETA: 1s - loss: 0.2328 - acc: 0.911 - ETA: 1s - loss: 0.2336 - acc: 0.911 - ETA: 1s - loss: 0.2327 - acc: 0.911 - ETA: 0s - loss: 0.2328 - acc: 0.911 - ETA: 0s - loss: 0.2322 - acc: 0.912 - ETA: 0s - loss: 0.2322 - acc: 0.912 - ETA: 0s - loss: 0.2333 - acc: 0.912 - ETA: 0s - loss: 0.2335 - acc: 0.911 - ETA: 0s - loss: 0.2341 - acc: 0.911 - ETA: 0s - loss: 0.2341 - acc: 0.911 - ETA: 0s - loss: 0.2342 - acc: 0.911 - ETA: 0s - loss: 0.2343 - acc: 0.911 - ETA: 0s - loss: 0.2344 - acc: 0.911 - ETA: 0s - loss: 0.2338 - acc: 0.911 - ETA: 0s - loss: 0.2331 - acc: 0.911 - ETA: 0s - loss: 0.2334 - acc: 0.911 - ETA: 0s - loss: 0.2340 - acc: 0.911 - ETA: 0s - loss: 0.2333 - acc: 0.912 - ETA: 0s - loss: 0.2329 - acc: 0.912 - ETA: 0s - loss: 0.2329 - acc: 0.911 - 3s 89us/step - loss: 0.2330 - acc: 0.9116\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33902/33902 [==============================] - ETA: 2s - loss: 0.1991 - acc: 0.929 - ETA: 2s - loss: 0.2247 - acc: 0.917 - ETA: 2s - loss: 0.2241 - acc: 0.918 - ETA: 2s - loss: 0.2328 - acc: 0.914 - ETA: 2s - loss: 0.2343 - acc: 0.913 - ETA: 2s - loss: 0.2304 - acc: 0.914 - ETA: 2s - loss: 0.2321 - acc: 0.913 - ETA: 2s - loss: 0.2333 - acc: 0.912 - ETA: 2s - loss: 0.2362 - acc: 0.909 - ETA: 2s - loss: 0.2372 - acc: 0.909 - ETA: 2s - loss: 0.2366 - acc: 0.908 - ETA: 2s - loss: 0.2365 - acc: 0.908 - ETA: 2s - loss: 0.2368 - acc: 0.907 - ETA: 2s - loss: 0.2375 - acc: 0.908 - ETA: 2s - loss: 0.2370 - acc: 0.908 - ETA: 2s - loss: 0.2376 - acc: 0.907 - ETA: 2s - loss: 0.2353 - acc: 0.907 - ETA: 1s - loss: 0.2353 - acc: 0.907 - ETA: 1s - loss: 0.2346 - acc: 0.908 - ETA: 1s - loss: 0.2338 - acc: 0.908 - ETA: 1s - loss: 0.2325 - acc: 0.909 - ETA: 1s - loss: 0.2323 - acc: 0.909 - ETA: 1s - loss: 0.2324 - acc: 0.909 - ETA: 1s - loss: 0.2328 - acc: 0.909 - ETA: 1s - loss: 0.2320 - acc: 0.909 - ETA: 1s - loss: 0.2313 - acc: 0.909 - ETA: 1s - loss: 0.2313 - acc: 0.909 - ETA: 1s - loss: 0.2317 - acc: 0.909 - ETA: 1s - loss: 0.2316 - acc: 0.909 - ETA: 1s - loss: 0.2309 - acc: 0.910 - ETA: 1s - loss: 0.2302 - acc: 0.910 - ETA: 1s - loss: 0.2293 - acc: 0.910 - ETA: 1s - loss: 0.2295 - acc: 0.910 - ETA: 1s - loss: 0.2294 - acc: 0.910 - ETA: 0s - loss: 0.2295 - acc: 0.910 - ETA: 0s - loss: 0.2293 - acc: 0.910 - ETA: 0s - loss: 0.2290 - acc: 0.911 - ETA: 0s - loss: 0.2286 - acc: 0.911 - ETA: 0s - loss: 0.2283 - acc: 0.911 - ETA: 0s - loss: 0.2286 - acc: 0.911 - ETA: 0s - loss: 0.2282 - acc: 0.911 - ETA: 0s - loss: 0.2274 - acc: 0.912 - ETA: 0s - loss: 0.2280 - acc: 0.912 - ETA: 0s - loss: 0.2285 - acc: 0.912 - ETA: 0s - loss: 0.2287 - acc: 0.912 - ETA: 0s - loss: 0.2296 - acc: 0.911 - ETA: 0s - loss: 0.2296 - acc: 0.911 - ETA: 0s - loss: 0.2299 - acc: 0.911 - ETA: 0s - loss: 0.2294 - acc: 0.911 - ETA: 0s - loss: 0.2292 - acc: 0.911 - 3s 91us/step - loss: 0.2294 - acc: 0.9115\n",
      "Epoch 10/20\n",
      "33902/33902 [==============================] - ETA: 2s - loss: 0.2341 - acc: 0.902 - ETA: 2s - loss: 0.2488 - acc: 0.905 - ETA: 2s - loss: 0.2339 - acc: 0.910 - ETA: 2s - loss: 0.2330 - acc: 0.913 - ETA: 2s - loss: 0.2286 - acc: 0.914 - ETA: 2s - loss: 0.2214 - acc: 0.916 - ETA: 2s - loss: 0.2169 - acc: 0.917 - ETA: 2s - loss: 0.2215 - acc: 0.915 - ETA: 2s - loss: 0.2226 - acc: 0.915 - ETA: 2s - loss: 0.2233 - acc: 0.914 - ETA: 2s - loss: 0.2290 - acc: 0.912 - ETA: 2s - loss: 0.2300 - acc: 0.911 - ETA: 2s - loss: 0.2292 - acc: 0.911 - ETA: 2s - loss: 0.2318 - acc: 0.910 - ETA: 2s - loss: 0.2307 - acc: 0.910 - ETA: 2s - loss: 0.2313 - acc: 0.910 - ETA: 2s - loss: 0.2311 - acc: 0.910 - ETA: 1s - loss: 0.2302 - acc: 0.910 - ETA: 1s - loss: 0.2307 - acc: 0.910 - ETA: 1s - loss: 0.2302 - acc: 0.910 - ETA: 1s - loss: 0.2297 - acc: 0.910 - ETA: 1s - loss: 0.2302 - acc: 0.910 - ETA: 1s - loss: 0.2311 - acc: 0.910 - ETA: 1s - loss: 0.2301 - acc: 0.910 - ETA: 1s - loss: 0.2310 - acc: 0.910 - ETA: 1s - loss: 0.2312 - acc: 0.909 - ETA: 1s - loss: 0.2306 - acc: 0.910 - ETA: 1s - loss: 0.2306 - acc: 0.910 - ETA: 1s - loss: 0.2293 - acc: 0.910 - ETA: 1s - loss: 0.2294 - acc: 0.910 - ETA: 0s - loss: 0.2286 - acc: 0.910 - ETA: 0s - loss: 0.2278 - acc: 0.911 - ETA: 0s - loss: 0.2279 - acc: 0.911 - ETA: 0s - loss: 0.2271 - acc: 0.912 - ETA: 0s - loss: 0.2273 - acc: 0.912 - ETA: 0s - loss: 0.2276 - acc: 0.911 - ETA: 0s - loss: 0.2279 - acc: 0.911 - ETA: 0s - loss: 0.2277 - acc: 0.911 - ETA: 0s - loss: 0.2273 - acc: 0.911 - ETA: 0s - loss: 0.2270 - acc: 0.911 - ETA: 0s - loss: 0.2265 - acc: 0.912 - ETA: 0s - loss: 0.2265 - acc: 0.912 - ETA: 0s - loss: 0.2273 - acc: 0.912 - ETA: 0s - loss: 0.2270 - acc: 0.912 - ETA: 0s - loss: 0.2268 - acc: 0.912 - ETA: 0s - loss: 0.2275 - acc: 0.911 - 3s 78us/step - loss: 0.2277 - acc: 0.9118\n",
      "Epoch 11/20\n",
      "33902/33902 [==============================] - ETA: 3s - loss: 0.1800 - acc: 0.925 - ETA: 2s - loss: 0.2048 - acc: 0.918 - ETA: 2s - loss: 0.2088 - acc: 0.913 - ETA: 2s - loss: 0.2110 - acc: 0.915 - ETA: 2s - loss: 0.2118 - acc: 0.916 - ETA: 2s - loss: 0.2110 - acc: 0.914 - ETA: 2s - loss: 0.2146 - acc: 0.913 - ETA: 2s - loss: 0.2168 - acc: 0.912 - ETA: 2s - loss: 0.2190 - acc: 0.911 - ETA: 2s - loss: 0.2230 - acc: 0.909 - ETA: 2s - loss: 0.2211 - acc: 0.910 - ETA: 1s - loss: 0.2217 - acc: 0.910 - ETA: 1s - loss: 0.2236 - acc: 0.909 - ETA: 1s - loss: 0.2231 - acc: 0.911 - ETA: 1s - loss: 0.2249 - acc: 0.910 - ETA: 1s - loss: 0.2245 - acc: 0.911 - ETA: 1s - loss: 0.2242 - acc: 0.911 - ETA: 1s - loss: 0.2240 - acc: 0.911 - ETA: 1s - loss: 0.2249 - acc: 0.910 - ETA: 1s - loss: 0.2235 - acc: 0.911 - ETA: 1s - loss: 0.2222 - acc: 0.912 - ETA: 1s - loss: 0.2240 - acc: 0.911 - ETA: 1s - loss: 0.2229 - acc: 0.912 - ETA: 1s - loss: 0.2232 - acc: 0.913 - ETA: 1s - loss: 0.2231 - acc: 0.913 - ETA: 1s - loss: 0.2242 - acc: 0.912 - ETA: 1s - loss: 0.2244 - acc: 0.912 - ETA: 0s - loss: 0.2249 - acc: 0.912 - ETA: 0s - loss: 0.2239 - acc: 0.912 - ETA: 0s - loss: 0.2232 - acc: 0.913 - ETA: 0s - loss: 0.2237 - acc: 0.913 - ETA: 0s - loss: 0.2235 - acc: 0.913 - ETA: 0s - loss: 0.2229 - acc: 0.913 - ETA: 0s - loss: 0.2233 - acc: 0.913 - ETA: 0s - loss: 0.2239 - acc: 0.913 - ETA: 0s - loss: 0.2250 - acc: 0.912 - ETA: 0s - loss: 0.2245 - acc: 0.912 - ETA: 0s - loss: 0.2250 - acc: 0.912 - ETA: 0s - loss: 0.2244 - acc: 0.912 - ETA: 0s - loss: 0.2250 - acc: 0.912 - ETA: 0s - loss: 0.2261 - acc: 0.911 - ETA: 0s - loss: 0.2262 - acc: 0.911 - ETA: 0s - loss: 0.2261 - acc: 0.911 - ETA: 0s - loss: 0.2258 - acc: 0.912 - 3s 77us/step - loss: 0.2264 - acc: 0.9120\n",
      "Epoch 12/20\n",
      "33902/33902 [==============================] - ETA: 2s - loss: 0.2234 - acc: 0.921 - ETA: 2s - loss: 0.2091 - acc: 0.918 - ETA: 2s - loss: 0.2325 - acc: 0.905 - ETA: 2s - loss: 0.2363 - acc: 0.903 - ETA: 2s - loss: 0.2342 - acc: 0.902 - ETA: 2s - loss: 0.2397 - acc: 0.903 - ETA: 2s - loss: 0.2329 - acc: 0.907 - ETA: 2s - loss: 0.2335 - acc: 0.907 - ETA: 2s - loss: 0.2295 - acc: 0.909 - ETA: 1s - loss: 0.2290 - acc: 0.910 - ETA: 1s - loss: 0.2294 - acc: 0.909 - ETA: 1s - loss: 0.2292 - acc: 0.910 - ETA: 1s - loss: 0.2281 - acc: 0.912 - ETA: 1s - loss: 0.2290 - acc: 0.910 - ETA: 1s - loss: 0.2270 - acc: 0.911 - ETA: 1s - loss: 0.2259 - acc: 0.912 - ETA: 1s - loss: 0.2278 - acc: 0.911 - ETA: 1s - loss: 0.2266 - acc: 0.911 - ETA: 1s - loss: 0.2277 - acc: 0.911 - ETA: 1s - loss: 0.2277 - acc: 0.910 - ETA: 1s - loss: 0.2282 - acc: 0.910 - ETA: 1s - loss: 0.2270 - acc: 0.911 - ETA: 1s - loss: 0.2255 - acc: 0.912 - ETA: 1s - loss: 0.2242 - acc: 0.912 - ETA: 1s - loss: 0.2229 - acc: 0.913 - ETA: 1s - loss: 0.2229 - acc: 0.913 - ETA: 0s - loss: 0.2225 - acc: 0.913 - ETA: 0s - loss: 0.2232 - acc: 0.913 - ETA: 0s - loss: 0.2247 - acc: 0.912 - ETA: 0s - loss: 0.2242 - acc: 0.913 - ETA: 0s - loss: 0.2239 - acc: 0.913 - ETA: 0s - loss: 0.2232 - acc: 0.913 - ETA: 0s - loss: 0.2229 - acc: 0.913 - ETA: 0s - loss: 0.2230 - acc: 0.913 - ETA: 0s - loss: 0.2222 - acc: 0.914 - ETA: 0s - loss: 0.2225 - acc: 0.913 - ETA: 0s - loss: 0.2223 - acc: 0.913 - ETA: 0s - loss: 0.2221 - acc: 0.913 - ETA: 0s - loss: 0.2225 - acc: 0.913 - ETA: 0s - loss: 0.2223 - acc: 0.913 - ETA: 0s - loss: 0.2222 - acc: 0.913 - ETA: 0s - loss: 0.2230 - acc: 0.913 - ETA: 0s - loss: 0.2237 - acc: 0.913 - ETA: 0s - loss: 0.2247 - acc: 0.912 - 3s 77us/step - loss: 0.2246 - acc: 0.9126\n",
      "Epoch 13/20\n",
      "33902/33902 [==============================] - ETA: 2s - loss: 0.2346 - acc: 0.890 - ETA: 2s - loss: 0.2220 - acc: 0.908 - ETA: 2s - loss: 0.2263 - acc: 0.910 - ETA: 2s - loss: 0.2260 - acc: 0.910 - ETA: 2s - loss: 0.2283 - acc: 0.911 - ETA: 2s - loss: 0.2259 - acc: 0.912 - ETA: 2s - loss: 0.2303 - acc: 0.909 - ETA: 2s - loss: 0.2285 - acc: 0.910 - ETA: 2s - loss: 0.2277 - acc: 0.911 - ETA: 1s - loss: 0.2281 - acc: 0.911 - ETA: 1s - loss: 0.2277 - acc: 0.911 - ETA: 1s - loss: 0.2248 - acc: 0.912 - ETA: 1s - loss: 0.2228 - acc: 0.912 - ETA: 1s - loss: 0.2218 - acc: 0.913 - ETA: 1s - loss: 0.2238 - acc: 0.912 - ETA: 1s - loss: 0.2237 - acc: 0.912 - ETA: 1s - loss: 0.2226 - acc: 0.912 - ETA: 1s - loss: 0.2236 - acc: 0.912 - ETA: 1s - loss: 0.2243 - acc: 0.912 - ETA: 1s - loss: 0.2239 - acc: 0.912 - ETA: 1s - loss: 0.2239 - acc: 0.912 - ETA: 1s - loss: 0.2228 - acc: 0.913 - ETA: 1s - loss: 0.2232 - acc: 0.913 - ETA: 1s - loss: 0.2253 - acc: 0.912 - ETA: 1s - loss: 0.2256 - acc: 0.912 - ETA: 1s - loss: 0.2264 - acc: 0.912 - ETA: 0s - loss: 0.2261 - acc: 0.912 - ETA: 0s - loss: 0.2257 - acc: 0.912 - ETA: 0s - loss: 0.2250 - acc: 0.912 - ETA: 0s - loss: 0.2242 - acc: 0.913 - ETA: 0s - loss: 0.2250 - acc: 0.912 - ETA: 0s - loss: 0.2250 - acc: 0.912 - ETA: 0s - loss: 0.2235 - acc: 0.913 - ETA: 0s - loss: 0.2239 - acc: 0.913 - ETA: 0s - loss: 0.2241 - acc: 0.912 - ETA: 0s - loss: 0.2244 - acc: 0.912 - ETA: 0s - loss: 0.2245 - acc: 0.912 - ETA: 0s - loss: 0.2239 - acc: 0.913 - ETA: 0s - loss: 0.2244 - acc: 0.912 - ETA: 0s - loss: 0.2239 - acc: 0.912 - ETA: 0s - loss: 0.2239 - acc: 0.912 - ETA: 0s - loss: 0.2238 - acc: 0.913 - ETA: 0s - loss: 0.2233 - acc: 0.913 - 2s 73us/step - loss: 0.2234 - acc: 0.9131\n",
      "Epoch 14/20\n",
      "33902/33902 [==============================] - ETA: 3s - loss: 0.2146 - acc: 0.914 - ETA: 2s - loss: 0.2116 - acc: 0.918 - ETA: 2s - loss: 0.2240 - acc: 0.912 - ETA: 2s - loss: 0.2189 - acc: 0.914 - ETA: 2s - loss: 0.2268 - acc: 0.910 - ETA: 2s - loss: 0.2237 - acc: 0.910 - ETA: 2s - loss: 0.2275 - acc: 0.909 - ETA: 2s - loss: 0.2223 - acc: 0.912 - ETA: 2s - loss: 0.2232 - acc: 0.912 - ETA: 1s - loss: 0.2218 - acc: 0.913 - ETA: 1s - loss: 0.2201 - acc: 0.914 - ETA: 1s - loss: 0.2213 - acc: 0.914 - ETA: 1s - loss: 0.2219 - acc: 0.913 - ETA: 1s - loss: 0.2219 - acc: 0.913 - ETA: 1s - loss: 0.2211 - acc: 0.913 - ETA: 1s - loss: 0.2210 - acc: 0.913 - ETA: 1s - loss: 0.2201 - acc: 0.914 - ETA: 1s - loss: 0.2197 - acc: 0.914 - ETA: 1s - loss: 0.2194 - acc: 0.914 - ETA: 1s - loss: 0.2206 - acc: 0.914 - ETA: 1s - loss: 0.2193 - acc: 0.914 - ETA: 1s - loss: 0.2176 - acc: 0.915 - ETA: 1s - loss: 0.2185 - acc: 0.915 - ETA: 1s - loss: 0.2196 - acc: 0.914 - ETA: 1s - loss: 0.2199 - acc: 0.914 - ETA: 0s - loss: 0.2193 - acc: 0.915 - ETA: 0s - loss: 0.2189 - acc: 0.915 - ETA: 0s - loss: 0.2195 - acc: 0.915 - ETA: 0s - loss: 0.2199 - acc: 0.915 - ETA: 0s - loss: 0.2195 - acc: 0.915 - ETA: 0s - loss: 0.2204 - acc: 0.915 - ETA: 0s - loss: 0.2204 - acc: 0.914 - ETA: 0s - loss: 0.2197 - acc: 0.915 - ETA: 0s - loss: 0.2202 - acc: 0.915 - ETA: 0s - loss: 0.2207 - acc: 0.915 - ETA: 0s - loss: 0.2202 - acc: 0.915 - ETA: 0s - loss: 0.2202 - acc: 0.915 - ETA: 0s - loss: 0.2206 - acc: 0.915 - ETA: 0s - loss: 0.2212 - acc: 0.915 - ETA: 0s - loss: 0.2213 - acc: 0.915 - ETA: 0s - loss: 0.2218 - acc: 0.915 - ETA: 0s - loss: 0.2219 - acc: 0.915 - ETA: 0s - loss: 0.2215 - acc: 0.915 - 2s 73us/step - loss: 0.2219 - acc: 0.9152\n",
      "Epoch 15/20\n",
      "33902/33902 [==============================] - ETA: 2s - loss: 0.1822 - acc: 0.918 - ETA: 2s - loss: 0.2011 - acc: 0.925 - ETA: 2s - loss: 0.2071 - acc: 0.922 - ETA: 2s - loss: 0.2107 - acc: 0.919 - ETA: 2s - loss: 0.2107 - acc: 0.918 - ETA: 2s - loss: 0.2157 - acc: 0.916 - ETA: 1s - loss: 0.2195 - acc: 0.916 - ETA: 1s - loss: 0.2245 - acc: 0.914 - ETA: 1s - loss: 0.2211 - acc: 0.915 - ETA: 1s - loss: 0.2206 - acc: 0.915 - ETA: 1s - loss: 0.2203 - acc: 0.915 - ETA: 1s - loss: 0.2211 - acc: 0.915 - ETA: 1s - loss: 0.2201 - acc: 0.915 - ETA: 1s - loss: 0.2182 - acc: 0.916 - ETA: 1s - loss: 0.2196 - acc: 0.915 - ETA: 1s - loss: 0.2190 - acc: 0.916 - ETA: 1s - loss: 0.2195 - acc: 0.916 - ETA: 1s - loss: 0.2197 - acc: 0.916 - ETA: 1s - loss: 0.2202 - acc: 0.916 - ETA: 1s - loss: 0.2184 - acc: 0.916 - ETA: 1s - loss: 0.2185 - acc: 0.917 - ETA: 1s - loss: 0.2195 - acc: 0.917 - ETA: 1s - loss: 0.2193 - acc: 0.916 - ETA: 1s - loss: 0.2199 - acc: 0.916 - ETA: 0s - loss: 0.2199 - acc: 0.916 - ETA: 0s - loss: 0.2206 - acc: 0.916 - ETA: 0s - loss: 0.2211 - acc: 0.915 - ETA: 0s - loss: 0.2214 - acc: 0.915 - ETA: 0s - loss: 0.2211 - acc: 0.915 - ETA: 0s - loss: 0.2206 - acc: 0.915 - ETA: 0s - loss: 0.2201 - acc: 0.915 - ETA: 0s - loss: 0.2198 - acc: 0.915 - ETA: 0s - loss: 0.2196 - acc: 0.916 - ETA: 0s - loss: 0.2193 - acc: 0.915 - ETA: 0s - loss: 0.2191 - acc: 0.915 - ETA: 0s - loss: 0.2184 - acc: 0.916 - ETA: 0s - loss: 0.2188 - acc: 0.915 - ETA: 0s - loss: 0.2188 - acc: 0.916 - ETA: 0s - loss: 0.2184 - acc: 0.916 - ETA: 0s - loss: 0.2180 - acc: 0.916 - ETA: 0s - loss: 0.2190 - acc: 0.916 - 2s 71us/step - loss: 0.2188 - acc: 0.9161\n",
      "Epoch 16/20\n",
      "33902/33902 [==============================] - ETA: 2s - loss: 0.2483 - acc: 0.882 - ETA: 2s - loss: 0.2039 - acc: 0.917 - ETA: 2s - loss: 0.2142 - acc: 0.912 - ETA: 2s - loss: 0.2132 - acc: 0.915 - ETA: 2s - loss: 0.2083 - acc: 0.917 - ETA: 2s - loss: 0.2023 - acc: 0.919 - ETA: 2s - loss: 0.2064 - acc: 0.919 - ETA: 2s - loss: 0.2118 - acc: 0.916 - ETA: 2s - loss: 0.2146 - acc: 0.915 - ETA: 2s - loss: 0.2152 - acc: 0.916 - ETA: 2s - loss: 0.2174 - acc: 0.916 - ETA: 1s - loss: 0.2158 - acc: 0.917 - ETA: 1s - loss: 0.2177 - acc: 0.916 - ETA: 1s - loss: 0.2194 - acc: 0.915 - ETA: 1s - loss: 0.2199 - acc: 0.915 - ETA: 1s - loss: 0.2201 - acc: 0.916 - ETA: 1s - loss: 0.2195 - acc: 0.916 - ETA: 1s - loss: 0.2207 - acc: 0.915 - ETA: 1s - loss: 0.2205 - acc: 0.915 - ETA: 1s - loss: 0.2205 - acc: 0.916 - ETA: 1s - loss: 0.2188 - acc: 0.916 - ETA: 1s - loss: 0.2196 - acc: 0.916 - ETA: 1s - loss: 0.2189 - acc: 0.917 - ETA: 1s - loss: 0.2181 - acc: 0.917 - ETA: 1s - loss: 0.2199 - acc: 0.916 - ETA: 1s - loss: 0.2202 - acc: 0.916 - ETA: 0s - loss: 0.2207 - acc: 0.916 - ETA: 0s - loss: 0.2203 - acc: 0.916 - ETA: 0s - loss: 0.2198 - acc: 0.916 - ETA: 0s - loss: 0.2197 - acc: 0.916 - ETA: 0s - loss: 0.2200 - acc: 0.916 - ETA: 0s - loss: 0.2200 - acc: 0.916 - ETA: 0s - loss: 0.2201 - acc: 0.916 - ETA: 0s - loss: 0.2196 - acc: 0.916 - ETA: 0s - loss: 0.2200 - acc: 0.916 - ETA: 0s - loss: 0.2198 - acc: 0.916 - ETA: 0s - loss: 0.2195 - acc: 0.916 - ETA: 0s - loss: 0.2188 - acc: 0.916 - ETA: 0s - loss: 0.2188 - acc: 0.916 - ETA: 0s - loss: 0.2192 - acc: 0.916 - ETA: 0s - loss: 0.2189 - acc: 0.916 - ETA: 0s - loss: 0.2183 - acc: 0.916 - 2s 73us/step - loss: 0.2180 - acc: 0.9169\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33902/33902 [==============================] - ETA: 2s - loss: 0.1696 - acc: 0.929 - ETA: 2s - loss: 0.2551 - acc: 0.902 - ETA: 2s - loss: 0.2497 - acc: 0.905 - ETA: 2s - loss: 0.2398 - acc: 0.910 - ETA: 2s - loss: 0.2329 - acc: 0.912 - ETA: 2s - loss: 0.2282 - acc: 0.913 - ETA: 1s - loss: 0.2266 - acc: 0.914 - ETA: 1s - loss: 0.2253 - acc: 0.915 - ETA: 1s - loss: 0.2241 - acc: 0.915 - ETA: 1s - loss: 0.2283 - acc: 0.913 - ETA: 1s - loss: 0.2291 - acc: 0.913 - ETA: 1s - loss: 0.2257 - acc: 0.915 - ETA: 1s - loss: 0.2252 - acc: 0.916 - ETA: 1s - loss: 0.2253 - acc: 0.915 - ETA: 1s - loss: 0.2217 - acc: 0.917 - ETA: 1s - loss: 0.2221 - acc: 0.916 - ETA: 1s - loss: 0.2223 - acc: 0.916 - ETA: 1s - loss: 0.2209 - acc: 0.916 - ETA: 1s - loss: 0.2198 - acc: 0.917 - ETA: 1s - loss: 0.2180 - acc: 0.917 - ETA: 1s - loss: 0.2183 - acc: 0.917 - ETA: 1s - loss: 0.2195 - acc: 0.916 - ETA: 1s - loss: 0.2189 - acc: 0.916 - ETA: 1s - loss: 0.2183 - acc: 0.917 - ETA: 0s - loss: 0.2189 - acc: 0.916 - ETA: 0s - loss: 0.2180 - acc: 0.917 - ETA: 0s - loss: 0.2175 - acc: 0.917 - ETA: 0s - loss: 0.2175 - acc: 0.917 - ETA: 0s - loss: 0.2180 - acc: 0.917 - ETA: 0s - loss: 0.2179 - acc: 0.917 - ETA: 0s - loss: 0.2183 - acc: 0.916 - ETA: 0s - loss: 0.2179 - acc: 0.916 - ETA: 0s - loss: 0.2179 - acc: 0.916 - ETA: 0s - loss: 0.2169 - acc: 0.917 - ETA: 0s - loss: 0.2170 - acc: 0.917 - ETA: 0s - loss: 0.2174 - acc: 0.917 - ETA: 0s - loss: 0.2178 - acc: 0.917 - ETA: 0s - loss: 0.2183 - acc: 0.917 - ETA: 0s - loss: 0.2178 - acc: 0.917 - ETA: 0s - loss: 0.2170 - acc: 0.917 - ETA: 0s - loss: 0.2168 - acc: 0.917 - 2s 71us/step - loss: 0.2173 - acc: 0.9176\n",
      "Epoch 18/20\n",
      "33902/33902 [==============================] - ETA: 3s - loss: 0.2065 - acc: 0.918 - ETA: 2s - loss: 0.2366 - acc: 0.909 - ETA: 2s - loss: 0.2274 - acc: 0.909 - ETA: 2s - loss: 0.2272 - acc: 0.909 - ETA: 2s - loss: 0.2271 - acc: 0.910 - ETA: 2s - loss: 0.2252 - acc: 0.911 - ETA: 1s - loss: 0.2203 - acc: 0.913 - ETA: 1s - loss: 0.2235 - acc: 0.913 - ETA: 1s - loss: 0.2258 - acc: 0.913 - ETA: 1s - loss: 0.2235 - acc: 0.914 - ETA: 1s - loss: 0.2221 - acc: 0.914 - ETA: 1s - loss: 0.2224 - acc: 0.914 - ETA: 1s - loss: 0.2248 - acc: 0.912 - ETA: 1s - loss: 0.2255 - acc: 0.912 - ETA: 1s - loss: 0.2245 - acc: 0.912 - ETA: 1s - loss: 0.2239 - acc: 0.913 - ETA: 1s - loss: 0.2222 - acc: 0.914 - ETA: 1s - loss: 0.2227 - acc: 0.914 - ETA: 1s - loss: 0.2214 - acc: 0.914 - ETA: 1s - loss: 0.2219 - acc: 0.914 - ETA: 1s - loss: 0.2209 - acc: 0.914 - ETA: 1s - loss: 0.2206 - acc: 0.915 - ETA: 1s - loss: 0.2193 - acc: 0.915 - ETA: 1s - loss: 0.2189 - acc: 0.916 - ETA: 0s - loss: 0.2189 - acc: 0.916 - ETA: 0s - loss: 0.2200 - acc: 0.915 - ETA: 0s - loss: 0.2198 - acc: 0.915 - ETA: 0s - loss: 0.2194 - acc: 0.916 - ETA: 0s - loss: 0.2186 - acc: 0.916 - ETA: 0s - loss: 0.2175 - acc: 0.917 - ETA: 0s - loss: 0.2169 - acc: 0.917 - ETA: 0s - loss: 0.2168 - acc: 0.917 - ETA: 0s - loss: 0.2161 - acc: 0.917 - ETA: 0s - loss: 0.2158 - acc: 0.917 - ETA: 0s - loss: 0.2159 - acc: 0.917 - ETA: 0s - loss: 0.2165 - acc: 0.917 - ETA: 0s - loss: 0.2164 - acc: 0.917 - ETA: 0s - loss: 0.2168 - acc: 0.917 - ETA: 0s - loss: 0.2166 - acc: 0.917 - ETA: 0s - loss: 0.2154 - acc: 0.917 - 2s 70us/step - loss: 0.2162 - acc: 0.9175\n",
      "Epoch 19/20\n",
      "33902/33902 [==============================] - ETA: 2s - loss: 0.2532 - acc: 0.894 - ETA: 2s - loss: 0.2331 - acc: 0.910 - ETA: 2s - loss: 0.2308 - acc: 0.908 - ETA: 2s - loss: 0.2215 - acc: 0.910 - ETA: 2s - loss: 0.2191 - acc: 0.913 - ETA: 2s - loss: 0.2193 - acc: 0.913 - ETA: 2s - loss: 0.2165 - acc: 0.915 - ETA: 2s - loss: 0.2172 - acc: 0.916 - ETA: 1s - loss: 0.2185 - acc: 0.915 - ETA: 1s - loss: 0.2167 - acc: 0.915 - ETA: 1s - loss: 0.2174 - acc: 0.916 - ETA: 1s - loss: 0.2168 - acc: 0.916 - ETA: 1s - loss: 0.2178 - acc: 0.915 - ETA: 1s - loss: 0.2179 - acc: 0.915 - ETA: 1s - loss: 0.2170 - acc: 0.915 - ETA: 1s - loss: 0.2151 - acc: 0.916 - ETA: 1s - loss: 0.2142 - acc: 0.916 - ETA: 1s - loss: 0.2139 - acc: 0.917 - ETA: 1s - loss: 0.2135 - acc: 0.917 - ETA: 1s - loss: 0.2150 - acc: 0.916 - ETA: 1s - loss: 0.2163 - acc: 0.916 - ETA: 1s - loss: 0.2150 - acc: 0.916 - ETA: 1s - loss: 0.2153 - acc: 0.916 - ETA: 1s - loss: 0.2143 - acc: 0.916 - ETA: 1s - loss: 0.2146 - acc: 0.916 - ETA: 1s - loss: 0.2159 - acc: 0.916 - ETA: 0s - loss: 0.2159 - acc: 0.915 - ETA: 0s - loss: 0.2167 - acc: 0.915 - ETA: 0s - loss: 0.2155 - acc: 0.915 - ETA: 0s - loss: 0.2149 - acc: 0.916 - ETA: 0s - loss: 0.2143 - acc: 0.916 - ETA: 0s - loss: 0.2145 - acc: 0.916 - ETA: 0s - loss: 0.2150 - acc: 0.916 - ETA: 0s - loss: 0.2148 - acc: 0.916 - ETA: 0s - loss: 0.2137 - acc: 0.917 - ETA: 0s - loss: 0.2133 - acc: 0.917 - ETA: 0s - loss: 0.2134 - acc: 0.917 - ETA: 0s - loss: 0.2136 - acc: 0.917 - ETA: 0s - loss: 0.2133 - acc: 0.917 - ETA: 0s - loss: 0.2130 - acc: 0.917 - ETA: 0s - loss: 0.2129 - acc: 0.917 - ETA: 0s - loss: 0.2131 - acc: 0.917 - ETA: 0s - loss: 0.2139 - acc: 0.917 - 2s 73us/step - loss: 0.2143 - acc: 0.9173\n",
      "Epoch 20/20\n",
      "33902/33902 [==============================] - ETA: 2s - loss: 0.2274 - acc: 0.921 - ETA: 2s - loss: 0.2332 - acc: 0.910 - ETA: 2s - loss: 0.2231 - acc: 0.911 - ETA: 2s - loss: 0.2226 - acc: 0.911 - ETA: 2s - loss: 0.2226 - acc: 0.913 - ETA: 2s - loss: 0.2244 - acc: 0.912 - ETA: 2s - loss: 0.2221 - acc: 0.914 - ETA: 1s - loss: 0.2223 - acc: 0.913 - ETA: 1s - loss: 0.2258 - acc: 0.910 - ETA: 1s - loss: 0.2270 - acc: 0.911 - ETA: 1s - loss: 0.2234 - acc: 0.913 - ETA: 1s - loss: 0.2207 - acc: 0.914 - ETA: 1s - loss: 0.2191 - acc: 0.915 - ETA: 1s - loss: 0.2181 - acc: 0.915 - ETA: 1s - loss: 0.2189 - acc: 0.915 - ETA: 1s - loss: 0.2198 - acc: 0.915 - ETA: 1s - loss: 0.2193 - acc: 0.915 - ETA: 1s - loss: 0.2185 - acc: 0.916 - ETA: 1s - loss: 0.2201 - acc: 0.916 - ETA: 1s - loss: 0.2180 - acc: 0.916 - ETA: 1s - loss: 0.2184 - acc: 0.917 - ETA: 1s - loss: 0.2182 - acc: 0.917 - ETA: 1s - loss: 0.2187 - acc: 0.917 - ETA: 0s - loss: 0.2188 - acc: 0.917 - ETA: 0s - loss: 0.2197 - acc: 0.917 - ETA: 0s - loss: 0.2192 - acc: 0.917 - ETA: 0s - loss: 0.2192 - acc: 0.916 - ETA: 0s - loss: 0.2192 - acc: 0.916 - ETA: 0s - loss: 0.2184 - acc: 0.917 - ETA: 0s - loss: 0.2181 - acc: 0.917 - ETA: 0s - loss: 0.2184 - acc: 0.917 - ETA: 0s - loss: 0.2176 - acc: 0.917 - ETA: 0s - loss: 0.2168 - acc: 0.917 - ETA: 0s - loss: 0.2164 - acc: 0.917 - ETA: 0s - loss: 0.2164 - acc: 0.917 - ETA: 0s - loss: 0.2162 - acc: 0.917 - ETA: 0s - loss: 0.2157 - acc: 0.917 - ETA: 0s - loss: 0.2161 - acc: 0.917 - ETA: 0s - loss: 0.2155 - acc: 0.917 - 2s 69us/step - loss: 0.2159 - acc: 0.9175\n",
      "8476/8476 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "[0.20319069388932134, 0.9193015572539811]\n"
     ]
    }
   ],
   "source": [
    "# KERAS\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=256)\n",
    "score = model.evaluate(X_test, y_test, batch_size=256)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 12603), (1.0, 15225)]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "smote_enn = SMOTEENN(random_state=0)\n",
    "X_resampled_smote_enn, y_resampled_smote_enn = smote_enn.fit_resample(features_list_array, labels_list_array)\n",
    "print(sorted(Counter(y_resampled_smote_enn).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled_smote_enn, y_resampled_smote_enn, random_state=35, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.98      0.98      2587\n",
      "         1.0       0.98      0.99      0.98      2979\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      5566\n",
      "   macro avg       0.98      0.98      0.98      5566\n",
      "weighted avg       0.98      0.98      0.98      5566\n",
      "\n",
      "[[2538   49]\n",
      " [  44 2935]]\n",
      "Accuracy is  98.32914121451671\n",
      "Time on model's work: 2.02 s\n",
      "====================================================================================================\n",
      "GradientBoostingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.83      0.89      2587\n",
      "         1.0       0.87      0.96      0.91      2979\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      5566\n",
      "   macro avg       0.91      0.90      0.90      5566\n",
      "weighted avg       0.91      0.90      0.90      5566\n",
      "\n",
      "[[2149  438]\n",
      " [ 105 2874]]\n",
      "Accuracy is  90.24434063959755\n",
      "Time on model's work: 140.479 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.98      0.98      2587\n",
      "         1.0       0.98      0.99      0.99      2979\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      5566\n",
      "   macro avg       0.99      0.98      0.99      5566\n",
      "weighted avg       0.99      0.99      0.99      5566\n",
      "\n",
      "[[2524   63]\n",
      " [  17 2962]]\n",
      "Accuracy is  98.56270212001438\n",
      "Time on model's work: 4.549 s\n",
      "====================================================================================================\n",
      "AdaBoostClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.82      0.87      2587\n",
      "         1.0       0.86      0.95      0.90      2979\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      5566\n",
      "   macro avg       0.90      0.88      0.89      5566\n",
      "weighted avg       0.89      0.89      0.89      5566\n",
      "\n",
      "[[2109  478]\n",
      " [ 142 2837]]\n",
      "Accuracy is  88.8609414301114\n",
      "Time on model's work: 35.168 s\n",
      "====================================================================================================\n",
      "BaggingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.97      0.97      2587\n",
      "         1.0       0.97      0.98      0.98      2979\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      5566\n",
      "   macro avg       0.97      0.97      0.97      5566\n",
      "weighted avg       0.97      0.97      0.97      5566\n",
      "\n",
      "[[2504   83]\n",
      " [  65 2914]]\n",
      "Accuracy is  97.34099892202659\n",
      "Time on model's work: 32.323 s\n",
      "====================================================================================================\n",
      "DecisionTreeClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.97      0.97      2587\n",
      "         1.0       0.97      0.98      0.97      2979\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      5566\n",
      "   macro avg       0.97      0.97      0.97      5566\n",
      "weighted avg       0.97      0.97      0.97      5566\n",
      "\n",
      "[[2502   85]\n",
      " [  70 2909]]\n",
      "Accuracy is  97.21523535752785\n",
      "Time on model's work: 13.027 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.97      0.98      2587\n",
      "         1.0       0.97      0.99      0.98      2979\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      5566\n",
      "   macro avg       0.98      0.98      0.98      5566\n",
      "weighted avg       0.98      0.98      0.98      5566\n",
      "\n",
      "[[2497   90]\n",
      " [  23 2956]]\n",
      "Accuracy is  97.9698167445203\n",
      "Time on model's work: 465.867 s\n",
      "====================================================================================================\n",
      "XGBClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.82      0.88      2587\n",
      "         1.0       0.86      0.97      0.91      2979\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      5566\n",
      "   macro avg       0.91      0.89      0.90      5566\n",
      "weighted avg       0.91      0.90      0.90      5566\n",
      "\n",
      "[[2126  461]\n",
      " [ 101 2878]]\n",
      "Accuracy is  89.90298239310097\n",
      "Time on model's work: 117.317 s\n",
      "====================================================================================================\n",
      "0:\tlearn: 0.6583400\ttotal: 287ms\tremaining: 4m 46s\n",
      "1:\tlearn: 0.6265091\ttotal: 515ms\tremaining: 4m 16s\n",
      "2:\tlearn: 0.5994792\ttotal: 728ms\tremaining: 4m 1s\n",
      "3:\tlearn: 0.5750190\ttotal: 940ms\tremaining: 3m 54s\n",
      "4:\tlearn: 0.5528353\ttotal: 1.28s\tremaining: 4m 14s\n",
      "5:\tlearn: 0.5334718\ttotal: 1.5s\tremaining: 4m 8s\n",
      "6:\tlearn: 0.5159543\ttotal: 1.69s\tremaining: 3m 59s\n",
      "7:\tlearn: 0.4998349\ttotal: 1.89s\tremaining: 3m 54s\n",
      "8:\tlearn: 0.4831772\ttotal: 2.1s\tremaining: 3m 51s\n",
      "9:\tlearn: 0.4707023\ttotal: 2.31s\tremaining: 3m 48s\n",
      "10:\tlearn: 0.4583399\ttotal: 2.52s\tremaining: 3m 46s\n",
      "11:\tlearn: 0.4466549\ttotal: 2.74s\tremaining: 3m 45s\n",
      "12:\tlearn: 0.4359328\ttotal: 2.95s\tremaining: 3m 44s\n",
      "13:\tlearn: 0.4275560\ttotal: 3.16s\tremaining: 3m 42s\n",
      "14:\tlearn: 0.4186047\ttotal: 3.37s\tremaining: 3m 41s\n",
      "15:\tlearn: 0.4097344\ttotal: 3.59s\tremaining: 3m 41s\n",
      "16:\tlearn: 0.4033646\ttotal: 3.8s\tremaining: 3m 39s\n",
      "17:\tlearn: 0.3941933\ttotal: 4.04s\tremaining: 3m 40s\n",
      "18:\tlearn: 0.3892389\ttotal: 4.24s\tremaining: 3m 38s\n",
      "19:\tlearn: 0.3829484\ttotal: 4.45s\tremaining: 3m 37s\n",
      "20:\tlearn: 0.3784542\ttotal: 4.65s\tremaining: 3m 36s\n",
      "21:\tlearn: 0.3735900\ttotal: 4.86s\tremaining: 3m 36s\n",
      "22:\tlearn: 0.3693936\ttotal: 5.08s\tremaining: 3m 35s\n",
      "23:\tlearn: 0.3657980\ttotal: 5.3s\tremaining: 3m 35s\n",
      "24:\tlearn: 0.3622943\ttotal: 5.51s\tremaining: 3m 34s\n",
      "25:\tlearn: 0.3580092\ttotal: 5.74s\tremaining: 3m 35s\n",
      "26:\tlearn: 0.3516590\ttotal: 5.98s\tremaining: 3m 35s\n",
      "27:\tlearn: 0.3473376\ttotal: 6.21s\tremaining: 3m 35s\n",
      "28:\tlearn: 0.3450064\ttotal: 6.4s\tremaining: 3m 34s\n",
      "29:\tlearn: 0.3395315\ttotal: 6.63s\tremaining: 3m 34s\n",
      "30:\tlearn: 0.3356303\ttotal: 6.84s\tremaining: 3m 33s\n",
      "31:\tlearn: 0.3330472\ttotal: 7.04s\tremaining: 3m 33s\n",
      "32:\tlearn: 0.3308416\ttotal: 7.27s\tremaining: 3m 32s\n",
      "33:\tlearn: 0.3276051\ttotal: 7.48s\tremaining: 3m 32s\n",
      "34:\tlearn: 0.3240994\ttotal: 7.7s\tremaining: 3m 32s\n",
      "35:\tlearn: 0.3201851\ttotal: 7.93s\tremaining: 3m 32s\n",
      "36:\tlearn: 0.3169439\ttotal: 8.16s\tremaining: 3m 32s\n",
      "37:\tlearn: 0.3157398\ttotal: 8.36s\tremaining: 3m 31s\n",
      "38:\tlearn: 0.3143446\ttotal: 8.58s\tremaining: 3m 31s\n",
      "39:\tlearn: 0.3129235\ttotal: 8.78s\tremaining: 3m 30s\n",
      "40:\tlearn: 0.3092579\ttotal: 9.03s\tremaining: 3m 31s\n",
      "41:\tlearn: 0.3073815\ttotal: 9.25s\tremaining: 3m 30s\n",
      "42:\tlearn: 0.3047678\ttotal: 9.45s\tremaining: 3m 30s\n",
      "43:\tlearn: 0.3038544\ttotal: 9.64s\tremaining: 3m 29s\n",
      "44:\tlearn: 0.3020940\ttotal: 9.86s\tremaining: 3m 29s\n",
      "45:\tlearn: 0.2995993\ttotal: 10.1s\tremaining: 3m 28s\n",
      "46:\tlearn: 0.2971987\ttotal: 10.3s\tremaining: 3m 29s\n",
      "47:\tlearn: 0.2950034\ttotal: 10.6s\tremaining: 3m 29s\n",
      "48:\tlearn: 0.2920788\ttotal: 10.8s\tremaining: 3m 29s\n",
      "49:\tlearn: 0.2892004\ttotal: 11s\tremaining: 3m 28s\n",
      "50:\tlearn: 0.2865356\ttotal: 11.2s\tremaining: 3m 28s\n",
      "51:\tlearn: 0.2842283\ttotal: 11.4s\tremaining: 3m 28s\n",
      "52:\tlearn: 0.2828364\ttotal: 11.6s\tremaining: 3m 27s\n",
      "53:\tlearn: 0.2821297\ttotal: 11.8s\tremaining: 3m 27s\n",
      "54:\tlearn: 0.2802160\ttotal: 12.1s\tremaining: 3m 27s\n",
      "55:\tlearn: 0.2791949\ttotal: 12.2s\tremaining: 3m 26s\n",
      "56:\tlearn: 0.2773139\ttotal: 12.5s\tremaining: 3m 26s\n",
      "57:\tlearn: 0.2767222\ttotal: 12.7s\tremaining: 3m 25s\n",
      "58:\tlearn: 0.2761344\ttotal: 12.8s\tremaining: 3m 24s\n",
      "59:\tlearn: 0.2751657\ttotal: 13s\tremaining: 3m 24s\n",
      "60:\tlearn: 0.2735012\ttotal: 13.3s\tremaining: 3m 24s\n",
      "61:\tlearn: 0.2730860\ttotal: 13.5s\tremaining: 3m 23s\n",
      "62:\tlearn: 0.2725774\ttotal: 13.6s\tremaining: 3m 22s\n",
      "63:\tlearn: 0.2721400\ttotal: 13.8s\tremaining: 3m 22s\n",
      "64:\tlearn: 0.2717195\ttotal: 14s\tremaining: 3m 21s\n",
      "65:\tlearn: 0.2697618\ttotal: 14.2s\tremaining: 3m 21s\n",
      "66:\tlearn: 0.2684811\ttotal: 14.4s\tremaining: 3m 21s\n",
      "67:\tlearn: 0.2681117\ttotal: 14.6s\tremaining: 3m 20s\n",
      "68:\tlearn: 0.2665278\ttotal: 14.8s\tremaining: 3m 20s\n",
      "69:\tlearn: 0.2652554\ttotal: 15s\tremaining: 3m 19s\n",
      "70:\tlearn: 0.2628852\ttotal: 15.2s\tremaining: 3m 19s\n",
      "71:\tlearn: 0.2625212\ttotal: 15.4s\tremaining: 3m 18s\n",
      "72:\tlearn: 0.2598574\ttotal: 15.6s\tremaining: 3m 18s\n",
      "73:\tlearn: 0.2595843\ttotal: 15.8s\tremaining: 3m 17s\n",
      "74:\tlearn: 0.2593124\ttotal: 16s\tremaining: 3m 17s\n",
      "75:\tlearn: 0.2589616\ttotal: 16.2s\tremaining: 3m 16s\n",
      "76:\tlearn: 0.2565676\ttotal: 16.4s\tremaining: 3m 16s\n",
      "77:\tlearn: 0.2547279\ttotal: 16.6s\tremaining: 3m 15s\n",
      "78:\tlearn: 0.2526373\ttotal: 16.8s\tremaining: 3m 15s\n",
      "79:\tlearn: 0.2511223\ttotal: 17s\tremaining: 3m 15s\n",
      "80:\tlearn: 0.2497876\ttotal: 17.2s\tremaining: 3m 15s\n",
      "81:\tlearn: 0.2494143\ttotal: 17.4s\tremaining: 3m 14s\n",
      "82:\tlearn: 0.2490897\ttotal: 17.6s\tremaining: 3m 13s\n",
      "83:\tlearn: 0.2475896\ttotal: 17.8s\tremaining: 3m 14s\n",
      "84:\tlearn: 0.2473474\ttotal: 18s\tremaining: 3m 13s\n",
      "85:\tlearn: 0.2470371\ttotal: 18.1s\tremaining: 3m 12s\n",
      "86:\tlearn: 0.2468141\ttotal: 18.3s\tremaining: 3m 11s\n",
      "87:\tlearn: 0.2458451\ttotal: 18.6s\tremaining: 3m 12s\n",
      "88:\tlearn: 0.2455674\ttotal: 18.7s\tremaining: 3m 11s\n",
      "89:\tlearn: 0.2443200\ttotal: 18.9s\tremaining: 3m 11s\n",
      "90:\tlearn: 0.2440651\ttotal: 19.1s\tremaining: 3m 10s\n",
      "91:\tlearn: 0.2438166\ttotal: 19.3s\tremaining: 3m 10s\n",
      "92:\tlearn: 0.2422716\ttotal: 19.5s\tremaining: 3m 10s\n",
      "93:\tlearn: 0.2410367\ttotal: 19.7s\tremaining: 3m 9s\n",
      "94:\tlearn: 0.2398350\ttotal: 19.9s\tremaining: 3m 9s\n",
      "95:\tlearn: 0.2386155\ttotal: 20.1s\tremaining: 3m 9s\n",
      "96:\tlearn: 0.2381855\ttotal: 20.3s\tremaining: 3m 9s\n",
      "97:\tlearn: 0.2379675\ttotal: 20.5s\tremaining: 3m 8s\n",
      "98:\tlearn: 0.2371742\ttotal: 20.7s\tremaining: 3m 8s\n",
      "99:\tlearn: 0.2368312\ttotal: 20.9s\tremaining: 3m 7s\n",
      "100:\tlearn: 0.2366192\ttotal: 21.1s\tremaining: 3m 7s\n",
      "101:\tlearn: 0.2353537\ttotal: 21.3s\tremaining: 3m 7s\n",
      "102:\tlearn: 0.2341876\ttotal: 21.5s\tremaining: 3m 7s\n",
      "103:\tlearn: 0.2332476\ttotal: 21.7s\tremaining: 3m 7s\n",
      "104:\tlearn: 0.2323115\ttotal: 22s\tremaining: 3m 7s\n",
      "105:\tlearn: 0.2320007\ttotal: 22.1s\tremaining: 3m 6s\n",
      "106:\tlearn: 0.2314830\ttotal: 22.3s\tremaining: 3m 6s\n",
      "107:\tlearn: 0.2312866\ttotal: 22.5s\tremaining: 3m 5s\n",
      "108:\tlearn: 0.2303202\ttotal: 22.7s\tremaining: 3m 5s\n",
      "109:\tlearn: 0.2293790\ttotal: 22.9s\tremaining: 3m 4s\n",
      "110:\tlearn: 0.2286301\ttotal: 23s\tremaining: 3m 4s\n",
      "111:\tlearn: 0.2283094\ttotal: 23.2s\tremaining: 3m 4s\n",
      "112:\tlearn: 0.2275779\ttotal: 23.4s\tremaining: 3m 3s\n",
      "113:\tlearn: 0.2271521\ttotal: 23.6s\tremaining: 3m 3s\n",
      "114:\tlearn: 0.2259948\ttotal: 23.8s\tremaining: 3m 3s\n",
      "115:\tlearn: 0.2258372\ttotal: 24s\tremaining: 3m 2s\n",
      "116:\tlearn: 0.2254432\ttotal: 24.2s\tremaining: 3m 2s\n",
      "117:\tlearn: 0.2252007\ttotal: 24.4s\tremaining: 3m 2s\n",
      "118:\tlearn: 0.2238288\ttotal: 24.6s\tremaining: 3m 2s\n",
      "119:\tlearn: 0.2229745\ttotal: 24.8s\tremaining: 3m 1s\n",
      "120:\tlearn: 0.2217337\ttotal: 25s\tremaining: 3m 1s\n",
      "121:\tlearn: 0.2213711\ttotal: 25.2s\tremaining: 3m 1s\n",
      "122:\tlearn: 0.2207456\ttotal: 25.4s\tremaining: 3m\n",
      "123:\tlearn: 0.2200018\ttotal: 25.6s\tremaining: 3m\n",
      "124:\tlearn: 0.2192535\ttotal: 25.8s\tremaining: 3m\n",
      "125:\tlearn: 0.2186260\ttotal: 26s\tremaining: 3m\n",
      "126:\tlearn: 0.2176604\ttotal: 26.2s\tremaining: 2m 59s\n",
      "127:\tlearn: 0.2166194\ttotal: 26.4s\tremaining: 2m 59s\n",
      "128:\tlearn: 0.2164130\ttotal: 26.5s\tremaining: 2m 59s\n",
      "129:\tlearn: 0.2162029\ttotal: 26.7s\tremaining: 2m 58s\n",
      "130:\tlearn: 0.2158887\ttotal: 26.9s\tremaining: 2m 58s\n",
      "131:\tlearn: 0.2154523\ttotal: 27.1s\tremaining: 2m 58s\n",
      "132:\tlearn: 0.2152992\ttotal: 27.3s\tremaining: 2m 57s\n",
      "133:\tlearn: 0.2150926\ttotal: 27.5s\tremaining: 2m 57s\n",
      "134:\tlearn: 0.2147180\ttotal: 27.7s\tremaining: 2m 57s\n",
      "135:\tlearn: 0.2131852\ttotal: 27.9s\tremaining: 2m 57s\n",
      "136:\tlearn: 0.2130292\ttotal: 28.1s\tremaining: 2m 56s\n",
      "137:\tlearn: 0.2128663\ttotal: 28.3s\tremaining: 2m 56s\n",
      "138:\tlearn: 0.2119109\ttotal: 28.5s\tremaining: 2m 56s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139:\tlearn: 0.2116318\ttotal: 28.6s\tremaining: 2m 55s\n",
      "140:\tlearn: 0.2113064\ttotal: 28.8s\tremaining: 2m 55s\n",
      "141:\tlearn: 0.2111223\ttotal: 29s\tremaining: 2m 55s\n",
      "142:\tlearn: 0.2109607\ttotal: 29.2s\tremaining: 2m 54s\n",
      "143:\tlearn: 0.2099574\ttotal: 29.3s\tremaining: 2m 54s\n",
      "144:\tlearn: 0.2097866\ttotal: 29.5s\tremaining: 2m 54s\n",
      "145:\tlearn: 0.2096651\ttotal: 29.7s\tremaining: 2m 53s\n",
      "146:\tlearn: 0.2095745\ttotal: 29.9s\tremaining: 2m 53s\n",
      "147:\tlearn: 0.2086767\ttotal: 30.1s\tremaining: 2m 53s\n",
      "148:\tlearn: 0.2084267\ttotal: 30.2s\tremaining: 2m 52s\n",
      "149:\tlearn: 0.2080643\ttotal: 30.4s\tremaining: 2m 52s\n",
      "150:\tlearn: 0.2079191\ttotal: 30.6s\tremaining: 2m 52s\n",
      "151:\tlearn: 0.2071393\ttotal: 30.8s\tremaining: 2m 51s\n",
      "152:\tlearn: 0.2062555\ttotal: 31s\tremaining: 2m 51s\n",
      "153:\tlearn: 0.2053842\ttotal: 31.3s\tremaining: 2m 51s\n",
      "154:\tlearn: 0.2052411\ttotal: 31.4s\tremaining: 2m 51s\n",
      "155:\tlearn: 0.2040973\ttotal: 31.6s\tremaining: 2m 51s\n",
      "156:\tlearn: 0.2039477\ttotal: 31.8s\tremaining: 2m 50s\n",
      "157:\tlearn: 0.2035948\ttotal: 32s\tremaining: 2m 50s\n",
      "158:\tlearn: 0.2034545\ttotal: 32.2s\tremaining: 2m 50s\n",
      "159:\tlearn: 0.2032654\ttotal: 32.4s\tremaining: 2m 49s\n",
      "160:\tlearn: 0.2031052\ttotal: 32.5s\tremaining: 2m 49s\n",
      "161:\tlearn: 0.2028923\ttotal: 32.7s\tremaining: 2m 49s\n",
      "162:\tlearn: 0.2024587\ttotal: 32.9s\tremaining: 2m 49s\n",
      "163:\tlearn: 0.2023096\ttotal: 33.1s\tremaining: 2m 48s\n",
      "164:\tlearn: 0.2021739\ttotal: 33.2s\tremaining: 2m 48s\n",
      "165:\tlearn: 0.2017963\ttotal: 33.4s\tremaining: 2m 47s\n",
      "166:\tlearn: 0.2016471\ttotal: 33.6s\tremaining: 2m 47s\n",
      "167:\tlearn: 0.2015373\ttotal: 33.8s\tremaining: 2m 47s\n",
      "168:\tlearn: 0.2013673\ttotal: 33.9s\tremaining: 2m 46s\n",
      "169:\tlearn: 0.2004673\ttotal: 34.2s\tremaining: 2m 46s\n",
      "170:\tlearn: 0.1999659\ttotal: 34.4s\tremaining: 2m 46s\n",
      "171:\tlearn: 0.1997671\ttotal: 34.5s\tremaining: 2m 46s\n",
      "172:\tlearn: 0.1989739\ttotal: 34.7s\tremaining: 2m 45s\n",
      "173:\tlearn: 0.1988550\ttotal: 34.9s\tremaining: 2m 45s\n",
      "174:\tlearn: 0.1987273\ttotal: 35.1s\tremaining: 2m 45s\n",
      "175:\tlearn: 0.1983412\ttotal: 35.3s\tremaining: 2m 45s\n",
      "176:\tlearn: 0.1982084\ttotal: 35.4s\tremaining: 2m 44s\n",
      "177:\tlearn: 0.1977226\ttotal: 35.7s\tremaining: 2m 44s\n",
      "178:\tlearn: 0.1975771\ttotal: 35.8s\tremaining: 2m 44s\n",
      "179:\tlearn: 0.1972125\ttotal: 36s\tremaining: 2m 44s\n",
      "180:\tlearn: 0.1970505\ttotal: 36.2s\tremaining: 2m 43s\n",
      "181:\tlearn: 0.1956354\ttotal: 36.4s\tremaining: 2m 43s\n",
      "182:\tlearn: 0.1955056\ttotal: 36.6s\tremaining: 2m 43s\n",
      "183:\tlearn: 0.1953925\ttotal: 36.7s\tremaining: 2m 42s\n",
      "184:\tlearn: 0.1949725\ttotal: 36.9s\tremaining: 2m 42s\n",
      "185:\tlearn: 0.1948600\ttotal: 37.1s\tremaining: 2m 42s\n",
      "186:\tlearn: 0.1946525\ttotal: 37.3s\tremaining: 2m 42s\n",
      "187:\tlearn: 0.1932733\ttotal: 37.5s\tremaining: 2m 42s\n",
      "188:\tlearn: 0.1931393\ttotal: 37.7s\tremaining: 2m 41s\n",
      "189:\tlearn: 0.1928010\ttotal: 37.9s\tremaining: 2m 41s\n",
      "190:\tlearn: 0.1925884\ttotal: 38s\tremaining: 2m 41s\n",
      "191:\tlearn: 0.1916643\ttotal: 38.3s\tremaining: 2m 40s\n",
      "192:\tlearn: 0.1913389\ttotal: 38.5s\tremaining: 2m 40s\n",
      "193:\tlearn: 0.1912004\ttotal: 38.6s\tremaining: 2m 40s\n",
      "194:\tlearn: 0.1911250\ttotal: 38.8s\tremaining: 2m 40s\n",
      "195:\tlearn: 0.1909243\ttotal: 39s\tremaining: 2m 39s\n",
      "196:\tlearn: 0.1907976\ttotal: 39.2s\tremaining: 2m 39s\n",
      "197:\tlearn: 0.1907104\ttotal: 39.4s\tremaining: 2m 39s\n",
      "198:\tlearn: 0.1900386\ttotal: 39.6s\tremaining: 2m 39s\n",
      "199:\tlearn: 0.1898948\ttotal: 39.8s\tremaining: 2m 39s\n",
      "200:\tlearn: 0.1890382\ttotal: 40s\tremaining: 2m 39s\n",
      "201:\tlearn: 0.1889384\ttotal: 40.2s\tremaining: 2m 38s\n",
      "202:\tlearn: 0.1887357\ttotal: 40.3s\tremaining: 2m 38s\n",
      "203:\tlearn: 0.1878029\ttotal: 40.6s\tremaining: 2m 38s\n",
      "204:\tlearn: 0.1875789\ttotal: 40.8s\tremaining: 2m 38s\n",
      "205:\tlearn: 0.1874312\ttotal: 40.9s\tremaining: 2m 37s\n",
      "206:\tlearn: 0.1868454\ttotal: 41.1s\tremaining: 2m 37s\n",
      "207:\tlearn: 0.1865431\ttotal: 41.3s\tremaining: 2m 37s\n",
      "208:\tlearn: 0.1863040\ttotal: 41.6s\tremaining: 2m 37s\n",
      "209:\tlearn: 0.1858640\ttotal: 41.8s\tremaining: 2m 37s\n",
      "210:\tlearn: 0.1854688\ttotal: 42s\tremaining: 2m 36s\n",
      "211:\tlearn: 0.1843435\ttotal: 42.2s\tremaining: 2m 36s\n",
      "212:\tlearn: 0.1841183\ttotal: 42.4s\tremaining: 2m 36s\n",
      "213:\tlearn: 0.1840257\ttotal: 42.5s\tremaining: 2m 36s\n",
      "214:\tlearn: 0.1838694\ttotal: 42.7s\tremaining: 2m 35s\n",
      "215:\tlearn: 0.1835465\ttotal: 42.9s\tremaining: 2m 35s\n",
      "216:\tlearn: 0.1834713\ttotal: 43.1s\tremaining: 2m 35s\n",
      "217:\tlearn: 0.1831179\ttotal: 43.3s\tremaining: 2m 35s\n",
      "218:\tlearn: 0.1830314\ttotal: 43.4s\tremaining: 2m 34s\n",
      "219:\tlearn: 0.1829536\ttotal: 43.6s\tremaining: 2m 34s\n",
      "220:\tlearn: 0.1828312\ttotal: 43.8s\tremaining: 2m 34s\n",
      "221:\tlearn: 0.1827228\ttotal: 44s\tremaining: 2m 34s\n",
      "222:\tlearn: 0.1815731\ttotal: 44.2s\tremaining: 2m 33s\n",
      "223:\tlearn: 0.1813013\ttotal: 44.4s\tremaining: 2m 33s\n",
      "224:\tlearn: 0.1812130\ttotal: 44.6s\tremaining: 2m 33s\n",
      "225:\tlearn: 0.1811168\ttotal: 44.7s\tremaining: 2m 33s\n",
      "226:\tlearn: 0.1807314\ttotal: 44.9s\tremaining: 2m 32s\n",
      "227:\tlearn: 0.1801183\ttotal: 45.1s\tremaining: 2m 32s\n",
      "228:\tlearn: 0.1795917\ttotal: 45.3s\tremaining: 2m 32s\n",
      "229:\tlearn: 0.1794858\ttotal: 45.5s\tremaining: 2m 32s\n",
      "230:\tlearn: 0.1786594\ttotal: 45.7s\tremaining: 2m 32s\n",
      "231:\tlearn: 0.1777179\ttotal: 45.9s\tremaining: 2m 32s\n",
      "232:\tlearn: 0.1770607\ttotal: 46.1s\tremaining: 2m 31s\n",
      "233:\tlearn: 0.1765671\ttotal: 46.4s\tremaining: 2m 31s\n",
      "234:\tlearn: 0.1764230\ttotal: 46.5s\tremaining: 2m 31s\n",
      "235:\tlearn: 0.1762860\ttotal: 46.7s\tremaining: 2m 31s\n",
      "236:\tlearn: 0.1761668\ttotal: 46.9s\tremaining: 2m 30s\n",
      "237:\tlearn: 0.1756995\ttotal: 47.1s\tremaining: 2m 30s\n",
      "238:\tlearn: 0.1755720\ttotal: 47.2s\tremaining: 2m 30s\n",
      "239:\tlearn: 0.1752899\ttotal: 47.4s\tremaining: 2m 30s\n",
      "240:\tlearn: 0.1751126\ttotal: 47.6s\tremaining: 2m 29s\n",
      "241:\tlearn: 0.1749661\ttotal: 47.8s\tremaining: 2m 29s\n",
      "242:\tlearn: 0.1744460\ttotal: 48s\tremaining: 2m 29s\n",
      "243:\tlearn: 0.1736870\ttotal: 48.2s\tremaining: 2m 29s\n",
      "244:\tlearn: 0.1735853\ttotal: 48.4s\tremaining: 2m 29s\n",
      "245:\tlearn: 0.1734842\ttotal: 48.6s\tremaining: 2m 29s\n",
      "246:\tlearn: 0.1734030\ttotal: 48.8s\tremaining: 2m 28s\n",
      "247:\tlearn: 0.1732278\ttotal: 49s\tremaining: 2m 28s\n",
      "248:\tlearn: 0.1731338\ttotal: 49.2s\tremaining: 2m 28s\n",
      "249:\tlearn: 0.1730135\ttotal: 49.4s\tremaining: 2m 28s\n",
      "250:\tlearn: 0.1729384\ttotal: 49.5s\tremaining: 2m 27s\n",
      "251:\tlearn: 0.1728433\ttotal: 49.7s\tremaining: 2m 27s\n",
      "252:\tlearn: 0.1722316\ttotal: 49.9s\tremaining: 2m 27s\n",
      "253:\tlearn: 0.1721489\ttotal: 50.1s\tremaining: 2m 27s\n",
      "254:\tlearn: 0.1720613\ttotal: 50.3s\tremaining: 2m 26s\n",
      "255:\tlearn: 0.1709824\ttotal: 50.5s\tremaining: 2m 26s\n",
      "256:\tlearn: 0.1702638\ttotal: 50.7s\tremaining: 2m 26s\n",
      "257:\tlearn: 0.1701934\ttotal: 50.9s\tremaining: 2m 26s\n",
      "258:\tlearn: 0.1696502\ttotal: 51.1s\tremaining: 2m 26s\n",
      "259:\tlearn: 0.1693094\ttotal: 51.3s\tremaining: 2m 25s\n",
      "260:\tlearn: 0.1689235\ttotal: 51.5s\tremaining: 2m 25s\n",
      "261:\tlearn: 0.1682446\ttotal: 51.7s\tremaining: 2m 25s\n",
      "262:\tlearn: 0.1680702\ttotal: 51.9s\tremaining: 2m 25s\n",
      "263:\tlearn: 0.1679311\ttotal: 52.1s\tremaining: 2m 25s\n",
      "264:\tlearn: 0.1674452\ttotal: 52.3s\tremaining: 2m 25s\n",
      "265:\tlearn: 0.1673529\ttotal: 52.5s\tremaining: 2m 24s\n",
      "266:\tlearn: 0.1672311\ttotal: 52.6s\tremaining: 2m 24s\n",
      "267:\tlearn: 0.1671365\ttotal: 52.8s\tremaining: 2m 24s\n",
      "268:\tlearn: 0.1670646\ttotal: 53s\tremaining: 2m 24s\n",
      "269:\tlearn: 0.1660387\ttotal: 53.2s\tremaining: 2m 23s\n",
      "270:\tlearn: 0.1656898\ttotal: 53.4s\tremaining: 2m 23s\n",
      "271:\tlearn: 0.1654554\ttotal: 53.6s\tremaining: 2m 23s\n",
      "272:\tlearn: 0.1645202\ttotal: 53.9s\tremaining: 2m 23s\n",
      "273:\tlearn: 0.1642262\ttotal: 54.1s\tremaining: 2m 23s\n",
      "274:\tlearn: 0.1641050\ttotal: 54.3s\tremaining: 2m 23s\n",
      "275:\tlearn: 0.1634661\ttotal: 54.5s\tremaining: 2m 22s\n",
      "276:\tlearn: 0.1632756\ttotal: 54.7s\tremaining: 2m 22s\n",
      "277:\tlearn: 0.1627462\ttotal: 55s\tremaining: 2m 22s\n",
      "278:\tlearn: 0.1617246\ttotal: 55.3s\tremaining: 2m 22s\n",
      "279:\tlearn: 0.1614921\ttotal: 55.5s\tremaining: 2m 22s\n",
      "280:\tlearn: 0.1614156\ttotal: 55.7s\tremaining: 2m 22s\n",
      "281:\tlearn: 0.1606755\ttotal: 55.9s\tremaining: 2m 22s\n",
      "282:\tlearn: 0.1600439\ttotal: 56.1s\tremaining: 2m 22s\n",
      "283:\tlearn: 0.1593870\ttotal: 56.4s\tremaining: 2m 22s\n",
      "284:\tlearn: 0.1592985\ttotal: 56.6s\tremaining: 2m 22s\n",
      "285:\tlearn: 0.1591244\ttotal: 56.9s\tremaining: 2m 22s\n",
      "286:\tlearn: 0.1589947\ttotal: 57.2s\tremaining: 2m 21s\n",
      "287:\tlearn: 0.1584329\ttotal: 57.4s\tremaining: 2m 21s\n",
      "288:\tlearn: 0.1582654\ttotal: 57.7s\tremaining: 2m 22s\n",
      "289:\tlearn: 0.1580135\ttotal: 58.1s\tremaining: 2m 22s\n",
      "290:\tlearn: 0.1578330\ttotal: 58.3s\tremaining: 2m 21s\n",
      "291:\tlearn: 0.1577329\ttotal: 58.5s\tremaining: 2m 21s\n",
      "292:\tlearn: 0.1576596\ttotal: 58.8s\tremaining: 2m 21s\n",
      "293:\tlearn: 0.1576037\ttotal: 59s\tremaining: 2m 21s\n",
      "294:\tlearn: 0.1567636\ttotal: 59.3s\tremaining: 2m 21s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295:\tlearn: 0.1560774\ttotal: 59.5s\tremaining: 2m 21s\n",
      "296:\tlearn: 0.1559852\ttotal: 59.8s\tremaining: 2m 21s\n",
      "297:\tlearn: 0.1558816\ttotal: 60s\tremaining: 2m 21s\n",
      "298:\tlearn: 0.1555807\ttotal: 1m\tremaining: 2m 21s\n",
      "299:\tlearn: 0.1555136\ttotal: 1m\tremaining: 2m 20s\n",
      "300:\tlearn: 0.1554384\ttotal: 1m\tremaining: 2m 20s\n",
      "301:\tlearn: 0.1553610\ttotal: 1m\tremaining: 2m 20s\n",
      "302:\tlearn: 0.1542484\ttotal: 1m\tremaining: 2m 20s\n",
      "303:\tlearn: 0.1541097\ttotal: 1m 1s\tremaining: 2m 19s\n",
      "304:\tlearn: 0.1538491\ttotal: 1m 1s\tremaining: 2m 19s\n",
      "305:\tlearn: 0.1534202\ttotal: 1m 1s\tremaining: 2m 19s\n",
      "306:\tlearn: 0.1528383\ttotal: 1m 1s\tremaining: 2m 19s\n",
      "307:\tlearn: 0.1525760\ttotal: 1m 1s\tremaining: 2m 19s\n",
      "308:\tlearn: 0.1525120\ttotal: 1m 2s\tremaining: 2m 18s\n",
      "309:\tlearn: 0.1521892\ttotal: 1m 2s\tremaining: 2m 18s\n",
      "310:\tlearn: 0.1519246\ttotal: 1m 2s\tremaining: 2m 18s\n",
      "311:\tlearn: 0.1513583\ttotal: 1m 2s\tremaining: 2m 18s\n",
      "312:\tlearn: 0.1508249\ttotal: 1m 2s\tremaining: 2m 18s\n",
      "313:\tlearn: 0.1504005\ttotal: 1m 3s\tremaining: 2m 18s\n",
      "314:\tlearn: 0.1499136\ttotal: 1m 3s\tremaining: 2m 17s\n",
      "315:\tlearn: 0.1497164\ttotal: 1m 3s\tremaining: 2m 17s\n",
      "316:\tlearn: 0.1496342\ttotal: 1m 3s\tremaining: 2m 17s\n",
      "317:\tlearn: 0.1490155\ttotal: 1m 3s\tremaining: 2m 17s\n",
      "318:\tlearn: 0.1489126\ttotal: 1m 4s\tremaining: 2m 17s\n",
      "319:\tlearn: 0.1483135\ttotal: 1m 4s\tremaining: 2m 16s\n",
      "320:\tlearn: 0.1482261\ttotal: 1m 4s\tremaining: 2m 16s\n",
      "321:\tlearn: 0.1479774\ttotal: 1m 4s\tremaining: 2m 16s\n",
      "322:\tlearn: 0.1474501\ttotal: 1m 5s\tremaining: 2m 16s\n",
      "323:\tlearn: 0.1473905\ttotal: 1m 5s\tremaining: 2m 16s\n",
      "324:\tlearn: 0.1468021\ttotal: 1m 5s\tremaining: 2m 16s\n",
      "325:\tlearn: 0.1460796\ttotal: 1m 5s\tremaining: 2m 15s\n",
      "326:\tlearn: 0.1457633\ttotal: 1m 5s\tremaining: 2m 15s\n",
      "327:\tlearn: 0.1456401\ttotal: 1m 6s\tremaining: 2m 15s\n",
      "328:\tlearn: 0.1454741\ttotal: 1m 6s\tremaining: 2m 15s\n",
      "329:\tlearn: 0.1454202\ttotal: 1m 6s\tremaining: 2m 14s\n",
      "330:\tlearn: 0.1450704\ttotal: 1m 6s\tremaining: 2m 14s\n",
      "331:\tlearn: 0.1445886\ttotal: 1m 6s\tremaining: 2m 14s\n",
      "332:\tlearn: 0.1442064\ttotal: 1m 7s\tremaining: 2m 14s\n",
      "333:\tlearn: 0.1437452\ttotal: 1m 7s\tremaining: 2m 14s\n",
      "334:\tlearn: 0.1428617\ttotal: 1m 7s\tremaining: 2m 13s\n",
      "335:\tlearn: 0.1427815\ttotal: 1m 7s\tremaining: 2m 13s\n",
      "336:\tlearn: 0.1426740\ttotal: 1m 7s\tremaining: 2m 13s\n",
      "337:\tlearn: 0.1426090\ttotal: 1m 8s\tremaining: 2m 13s\n",
      "338:\tlearn: 0.1425244\ttotal: 1m 8s\tremaining: 2m 12s\n",
      "339:\tlearn: 0.1424617\ttotal: 1m 8s\tremaining: 2m 12s\n",
      "340:\tlearn: 0.1416694\ttotal: 1m 8s\tremaining: 2m 12s\n",
      "341:\tlearn: 0.1412472\ttotal: 1m 8s\tremaining: 2m 12s\n",
      "342:\tlearn: 0.1411897\ttotal: 1m 8s\tremaining: 2m 12s\n",
      "343:\tlearn: 0.1411263\ttotal: 1m 9s\tremaining: 2m 11s\n",
      "344:\tlearn: 0.1410685\ttotal: 1m 9s\tremaining: 2m 11s\n",
      "345:\tlearn: 0.1410164\ttotal: 1m 9s\tremaining: 2m 11s\n",
      "346:\tlearn: 0.1406783\ttotal: 1m 9s\tremaining: 2m 11s\n",
      "347:\tlearn: 0.1401378\ttotal: 1m 9s\tremaining: 2m 10s\n",
      "348:\tlearn: 0.1398312\ttotal: 1m 10s\tremaining: 2m 10s\n",
      "349:\tlearn: 0.1391483\ttotal: 1m 10s\tremaining: 2m 10s\n",
      "350:\tlearn: 0.1388342\ttotal: 1m 10s\tremaining: 2m 10s\n",
      "351:\tlearn: 0.1384113\ttotal: 1m 10s\tremaining: 2m 10s\n",
      "352:\tlearn: 0.1382877\ttotal: 1m 10s\tremaining: 2m 9s\n",
      "353:\tlearn: 0.1375452\ttotal: 1m 11s\tremaining: 2m 9s\n",
      "354:\tlearn: 0.1374171\ttotal: 1m 11s\tremaining: 2m 9s\n",
      "355:\tlearn: 0.1373373\ttotal: 1m 11s\tremaining: 2m 9s\n",
      "356:\tlearn: 0.1372665\ttotal: 1m 11s\tremaining: 2m 9s\n",
      "357:\tlearn: 0.1371929\ttotal: 1m 11s\tremaining: 2m 8s\n",
      "358:\tlearn: 0.1370643\ttotal: 1m 11s\tremaining: 2m 8s\n",
      "359:\tlearn: 0.1370098\ttotal: 1m 12s\tremaining: 2m 8s\n",
      "360:\tlearn: 0.1369626\ttotal: 1m 12s\tremaining: 2m 8s\n",
      "361:\tlearn: 0.1369148\ttotal: 1m 12s\tremaining: 2m 7s\n",
      "362:\tlearn: 0.1368618\ttotal: 1m 12s\tremaining: 2m 7s\n",
      "363:\tlearn: 0.1368000\ttotal: 1m 12s\tremaining: 2m 7s\n",
      "364:\tlearn: 0.1367409\ttotal: 1m 13s\tremaining: 2m 7s\n",
      "365:\tlearn: 0.1366838\ttotal: 1m 13s\tremaining: 2m 6s\n",
      "366:\tlearn: 0.1363947\ttotal: 1m 13s\tremaining: 2m 6s\n",
      "367:\tlearn: 0.1360609\ttotal: 1m 13s\tremaining: 2m 6s\n",
      "368:\tlearn: 0.1359707\ttotal: 1m 13s\tremaining: 2m 6s\n",
      "369:\tlearn: 0.1354933\ttotal: 1m 14s\tremaining: 2m 6s\n",
      "370:\tlearn: 0.1354519\ttotal: 1m 14s\tremaining: 2m 5s\n",
      "371:\tlearn: 0.1352179\ttotal: 1m 14s\tremaining: 2m 5s\n",
      "372:\tlearn: 0.1350460\ttotal: 1m 14s\tremaining: 2m 5s\n",
      "373:\tlearn: 0.1348741\ttotal: 1m 14s\tremaining: 2m 5s\n",
      "374:\tlearn: 0.1345801\ttotal: 1m 15s\tremaining: 2m 5s\n",
      "375:\tlearn: 0.1345306\ttotal: 1m 15s\tremaining: 2m 4s\n",
      "376:\tlearn: 0.1344395\ttotal: 1m 15s\tremaining: 2m 4s\n",
      "377:\tlearn: 0.1343662\ttotal: 1m 15s\tremaining: 2m 4s\n",
      "378:\tlearn: 0.1342591\ttotal: 1m 15s\tremaining: 2m 4s\n",
      "379:\tlearn: 0.1340545\ttotal: 1m 15s\tremaining: 2m 3s\n",
      "380:\tlearn: 0.1338034\ttotal: 1m 16s\tremaining: 2m 3s\n",
      "381:\tlearn: 0.1333829\ttotal: 1m 16s\tremaining: 2m 3s\n",
      "382:\tlearn: 0.1333409\ttotal: 1m 16s\tremaining: 2m 3s\n",
      "383:\tlearn: 0.1333029\ttotal: 1m 16s\tremaining: 2m 2s\n",
      "384:\tlearn: 0.1327890\ttotal: 1m 16s\tremaining: 2m 2s\n",
      "385:\tlearn: 0.1327556\ttotal: 1m 17s\tremaining: 2m 2s\n",
      "386:\tlearn: 0.1327124\ttotal: 1m 17s\tremaining: 2m 2s\n",
      "387:\tlearn: 0.1321861\ttotal: 1m 17s\tremaining: 2m 2s\n",
      "388:\tlearn: 0.1321356\ttotal: 1m 17s\tremaining: 2m 1s\n",
      "389:\tlearn: 0.1320708\ttotal: 1m 17s\tremaining: 2m 1s\n",
      "390:\tlearn: 0.1320179\ttotal: 1m 17s\tremaining: 2m 1s\n",
      "391:\tlearn: 0.1319722\ttotal: 1m 18s\tremaining: 2m 1s\n",
      "392:\tlearn: 0.1319171\ttotal: 1m 18s\tremaining: 2m 1s\n",
      "393:\tlearn: 0.1318730\ttotal: 1m 18s\tremaining: 2m\n",
      "394:\tlearn: 0.1317286\ttotal: 1m 18s\tremaining: 2m\n",
      "395:\tlearn: 0.1317030\ttotal: 1m 18s\tremaining: 2m\n",
      "396:\tlearn: 0.1315841\ttotal: 1m 19s\tremaining: 2m\n",
      "397:\tlearn: 0.1315400\ttotal: 1m 19s\tremaining: 1m 59s\n",
      "398:\tlearn: 0.1312591\ttotal: 1m 19s\tremaining: 1m 59s\n",
      "399:\tlearn: 0.1312123\ttotal: 1m 19s\tremaining: 1m 59s\n",
      "400:\tlearn: 0.1308602\ttotal: 1m 19s\tremaining: 1m 59s\n",
      "401:\tlearn: 0.1308251\ttotal: 1m 20s\tremaining: 1m 59s\n",
      "402:\tlearn: 0.1307881\ttotal: 1m 20s\tremaining: 1m 58s\n",
      "403:\tlearn: 0.1307007\ttotal: 1m 20s\tremaining: 1m 58s\n",
      "404:\tlearn: 0.1306586\ttotal: 1m 20s\tremaining: 1m 58s\n",
      "405:\tlearn: 0.1305671\ttotal: 1m 20s\tremaining: 1m 58s\n",
      "406:\tlearn: 0.1305129\ttotal: 1m 20s\tremaining: 1m 57s\n",
      "407:\tlearn: 0.1304827\ttotal: 1m 21s\tremaining: 1m 57s\n",
      "408:\tlearn: 0.1297870\ttotal: 1m 21s\tremaining: 1m 57s\n",
      "409:\tlearn: 0.1297529\ttotal: 1m 21s\tremaining: 1m 57s\n",
      "410:\tlearn: 0.1296795\ttotal: 1m 21s\tremaining: 1m 57s\n",
      "411:\tlearn: 0.1296108\ttotal: 1m 21s\tremaining: 1m 56s\n",
      "412:\tlearn: 0.1295741\ttotal: 1m 22s\tremaining: 1m 56s\n",
      "413:\tlearn: 0.1289952\ttotal: 1m 22s\tremaining: 1m 56s\n",
      "414:\tlearn: 0.1289622\ttotal: 1m 22s\tremaining: 1m 56s\n",
      "415:\tlearn: 0.1288087\ttotal: 1m 22s\tremaining: 1m 56s\n",
      "416:\tlearn: 0.1287658\ttotal: 1m 22s\tremaining: 1m 55s\n",
      "417:\tlearn: 0.1285761\ttotal: 1m 23s\tremaining: 1m 55s\n",
      "418:\tlearn: 0.1280870\ttotal: 1m 23s\tremaining: 1m 55s\n",
      "419:\tlearn: 0.1277431\ttotal: 1m 23s\tremaining: 1m 55s\n",
      "420:\tlearn: 0.1276124\ttotal: 1m 23s\tremaining: 1m 55s\n",
      "421:\tlearn: 0.1275792\ttotal: 1m 23s\tremaining: 1m 54s\n",
      "422:\tlearn: 0.1275338\ttotal: 1m 23s\tremaining: 1m 54s\n",
      "423:\tlearn: 0.1270698\ttotal: 1m 24s\tremaining: 1m 54s\n",
      "424:\tlearn: 0.1270336\ttotal: 1m 24s\tremaining: 1m 54s\n",
      "425:\tlearn: 0.1269856\ttotal: 1m 24s\tremaining: 1m 53s\n",
      "426:\tlearn: 0.1269580\ttotal: 1m 24s\tremaining: 1m 53s\n",
      "427:\tlearn: 0.1269121\ttotal: 1m 24s\tremaining: 1m 53s\n",
      "428:\tlearn: 0.1266524\ttotal: 1m 24s\tremaining: 1m 53s\n",
      "429:\tlearn: 0.1265701\ttotal: 1m 25s\tremaining: 1m 52s\n",
      "430:\tlearn: 0.1263247\ttotal: 1m 25s\tremaining: 1m 52s\n",
      "431:\tlearn: 0.1262826\ttotal: 1m 25s\tremaining: 1m 52s\n",
      "432:\tlearn: 0.1261688\ttotal: 1m 25s\tremaining: 1m 52s\n",
      "433:\tlearn: 0.1261339\ttotal: 1m 25s\tremaining: 1m 51s\n",
      "434:\tlearn: 0.1260989\ttotal: 1m 26s\tremaining: 1m 51s\n",
      "435:\tlearn: 0.1259285\ttotal: 1m 26s\tremaining: 1m 51s\n",
      "436:\tlearn: 0.1257862\ttotal: 1m 26s\tremaining: 1m 51s\n",
      "437:\tlearn: 0.1256832\ttotal: 1m 26s\tremaining: 1m 51s\n",
      "438:\tlearn: 0.1256312\ttotal: 1m 26s\tremaining: 1m 50s\n",
      "439:\tlearn: 0.1256026\ttotal: 1m 26s\tremaining: 1m 50s\n",
      "440:\tlearn: 0.1255657\ttotal: 1m 27s\tremaining: 1m 50s\n",
      "441:\tlearn: 0.1255333\ttotal: 1m 27s\tremaining: 1m 50s\n",
      "442:\tlearn: 0.1255103\ttotal: 1m 27s\tremaining: 1m 49s\n",
      "443:\tlearn: 0.1251706\ttotal: 1m 27s\tremaining: 1m 49s\n",
      "444:\tlearn: 0.1251418\ttotal: 1m 27s\tremaining: 1m 49s\n",
      "445:\tlearn: 0.1251123\ttotal: 1m 27s\tremaining: 1m 49s\n",
      "446:\tlearn: 0.1250722\ttotal: 1m 28s\tremaining: 1m 49s\n",
      "447:\tlearn: 0.1249741\ttotal: 1m 28s\tremaining: 1m 48s\n",
      "448:\tlearn: 0.1249448\ttotal: 1m 28s\tremaining: 1m 48s\n",
      "449:\tlearn: 0.1249109\ttotal: 1m 28s\tremaining: 1m 48s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450:\tlearn: 0.1248746\ttotal: 1m 28s\tremaining: 1m 48s\n",
      "451:\tlearn: 0.1248214\ttotal: 1m 28s\tremaining: 1m 47s\n",
      "452:\tlearn: 0.1245711\ttotal: 1m 29s\tremaining: 1m 47s\n",
      "453:\tlearn: 0.1245462\ttotal: 1m 29s\tremaining: 1m 47s\n",
      "454:\tlearn: 0.1245041\ttotal: 1m 29s\tremaining: 1m 47s\n",
      "455:\tlearn: 0.1244658\ttotal: 1m 29s\tremaining: 1m 46s\n",
      "456:\tlearn: 0.1244066\ttotal: 1m 29s\tremaining: 1m 46s\n",
      "457:\tlearn: 0.1243811\ttotal: 1m 29s\tremaining: 1m 46s\n",
      "458:\tlearn: 0.1243509\ttotal: 1m 30s\tremaining: 1m 46s\n",
      "459:\tlearn: 0.1243078\ttotal: 1m 30s\tremaining: 1m 46s\n",
      "460:\tlearn: 0.1242714\ttotal: 1m 30s\tremaining: 1m 45s\n",
      "461:\tlearn: 0.1241344\ttotal: 1m 30s\tremaining: 1m 45s\n",
      "462:\tlearn: 0.1235750\ttotal: 1m 30s\tremaining: 1m 45s\n",
      "463:\tlearn: 0.1235479\ttotal: 1m 31s\tremaining: 1m 45s\n",
      "464:\tlearn: 0.1234153\ttotal: 1m 31s\tremaining: 1m 44s\n",
      "465:\tlearn: 0.1230330\ttotal: 1m 31s\tremaining: 1m 44s\n",
      "466:\tlearn: 0.1230085\ttotal: 1m 31s\tremaining: 1m 44s\n",
      "467:\tlearn: 0.1226429\ttotal: 1m 31s\tremaining: 1m 44s\n",
      "468:\tlearn: 0.1226079\ttotal: 1m 31s\tremaining: 1m 44s\n",
      "469:\tlearn: 0.1225341\ttotal: 1m 32s\tremaining: 1m 43s\n",
      "470:\tlearn: 0.1225006\ttotal: 1m 32s\tremaining: 1m 43s\n",
      "471:\tlearn: 0.1224706\ttotal: 1m 32s\tremaining: 1m 43s\n",
      "472:\tlearn: 0.1223408\ttotal: 1m 32s\tremaining: 1m 43s\n",
      "473:\tlearn: 0.1223099\ttotal: 1m 32s\tremaining: 1m 42s\n",
      "474:\tlearn: 0.1218821\ttotal: 1m 32s\tremaining: 1m 42s\n",
      "475:\tlearn: 0.1218511\ttotal: 1m 33s\tremaining: 1m 42s\n",
      "476:\tlearn: 0.1217619\ttotal: 1m 33s\tremaining: 1m 42s\n",
      "477:\tlearn: 0.1214982\ttotal: 1m 33s\tremaining: 1m 42s\n",
      "478:\tlearn: 0.1214466\ttotal: 1m 33s\tremaining: 1m 41s\n",
      "479:\tlearn: 0.1214255\ttotal: 1m 33s\tremaining: 1m 41s\n",
      "480:\tlearn: 0.1213889\ttotal: 1m 33s\tremaining: 1m 41s\n",
      "481:\tlearn: 0.1209805\ttotal: 1m 34s\tremaining: 1m 41s\n",
      "482:\tlearn: 0.1205227\ttotal: 1m 34s\tremaining: 1m 40s\n",
      "483:\tlearn: 0.1204854\ttotal: 1m 34s\tremaining: 1m 40s\n",
      "484:\tlearn: 0.1202742\ttotal: 1m 34s\tremaining: 1m 40s\n",
      "485:\tlearn: 0.1202351\ttotal: 1m 34s\tremaining: 1m 40s\n",
      "486:\tlearn: 0.1201482\ttotal: 1m 35s\tremaining: 1m 40s\n",
      "487:\tlearn: 0.1201142\ttotal: 1m 35s\tremaining: 1m 39s\n",
      "488:\tlearn: 0.1200573\ttotal: 1m 35s\tremaining: 1m 39s\n",
      "489:\tlearn: 0.1200229\ttotal: 1m 35s\tremaining: 1m 39s\n",
      "490:\tlearn: 0.1198968\ttotal: 1m 35s\tremaining: 1m 39s\n",
      "491:\tlearn: 0.1195434\ttotal: 1m 35s\tremaining: 1m 39s\n",
      "492:\tlearn: 0.1193237\ttotal: 1m 36s\tremaining: 1m 38s\n",
      "493:\tlearn: 0.1192967\ttotal: 1m 36s\tremaining: 1m 38s\n",
      "494:\tlearn: 0.1192710\ttotal: 1m 36s\tremaining: 1m 38s\n",
      "495:\tlearn: 0.1192472\ttotal: 1m 36s\tremaining: 1m 38s\n",
      "496:\tlearn: 0.1190618\ttotal: 1m 36s\tremaining: 1m 37s\n",
      "497:\tlearn: 0.1190351\ttotal: 1m 36s\tremaining: 1m 37s\n",
      "498:\tlearn: 0.1190051\ttotal: 1m 37s\tremaining: 1m 37s\n",
      "499:\tlearn: 0.1189631\ttotal: 1m 37s\tremaining: 1m 37s\n",
      "500:\tlearn: 0.1187366\ttotal: 1m 37s\tremaining: 1m 37s\n",
      "501:\tlearn: 0.1183143\ttotal: 1m 37s\tremaining: 1m 36s\n",
      "502:\tlearn: 0.1182394\ttotal: 1m 37s\tremaining: 1m 36s\n",
      "503:\tlearn: 0.1182177\ttotal: 1m 37s\tremaining: 1m 36s\n",
      "504:\tlearn: 0.1180421\ttotal: 1m 38s\tremaining: 1m 36s\n",
      "505:\tlearn: 0.1180022\ttotal: 1m 38s\tremaining: 1m 36s\n",
      "506:\tlearn: 0.1179711\ttotal: 1m 38s\tremaining: 1m 35s\n",
      "507:\tlearn: 0.1179360\ttotal: 1m 38s\tremaining: 1m 35s\n",
      "508:\tlearn: 0.1179032\ttotal: 1m 38s\tremaining: 1m 35s\n",
      "509:\tlearn: 0.1178569\ttotal: 1m 39s\tremaining: 1m 35s\n",
      "510:\tlearn: 0.1178371\ttotal: 1m 39s\tremaining: 1m 34s\n",
      "511:\tlearn: 0.1177972\ttotal: 1m 39s\tremaining: 1m 34s\n",
      "512:\tlearn: 0.1175271\ttotal: 1m 39s\tremaining: 1m 34s\n",
      "513:\tlearn: 0.1175056\ttotal: 1m 39s\tremaining: 1m 34s\n",
      "514:\tlearn: 0.1173103\ttotal: 1m 39s\tremaining: 1m 34s\n",
      "515:\tlearn: 0.1172862\ttotal: 1m 40s\tremaining: 1m 33s\n",
      "516:\tlearn: 0.1172503\ttotal: 1m 40s\tremaining: 1m 33s\n",
      "517:\tlearn: 0.1172279\ttotal: 1m 40s\tremaining: 1m 33s\n",
      "518:\tlearn: 0.1171991\ttotal: 1m 40s\tremaining: 1m 33s\n",
      "519:\tlearn: 0.1171752\ttotal: 1m 40s\tremaining: 1m 32s\n",
      "520:\tlearn: 0.1171282\ttotal: 1m 40s\tremaining: 1m 32s\n",
      "521:\tlearn: 0.1171033\ttotal: 1m 41s\tremaining: 1m 32s\n",
      "522:\tlearn: 0.1170206\ttotal: 1m 41s\tremaining: 1m 32s\n",
      "523:\tlearn: 0.1168829\ttotal: 1m 41s\tremaining: 1m 32s\n",
      "524:\tlearn: 0.1162208\ttotal: 1m 41s\tremaining: 1m 31s\n",
      "525:\tlearn: 0.1161157\ttotal: 1m 41s\tremaining: 1m 31s\n",
      "526:\tlearn: 0.1159540\ttotal: 1m 42s\tremaining: 1m 31s\n",
      "527:\tlearn: 0.1156520\ttotal: 1m 42s\tremaining: 1m 31s\n",
      "528:\tlearn: 0.1155757\ttotal: 1m 42s\tremaining: 1m 31s\n",
      "529:\tlearn: 0.1155016\ttotal: 1m 42s\tremaining: 1m 30s\n",
      "530:\tlearn: 0.1154717\ttotal: 1m 42s\tremaining: 1m 30s\n",
      "531:\tlearn: 0.1154367\ttotal: 1m 42s\tremaining: 1m 30s\n",
      "532:\tlearn: 0.1153992\ttotal: 1m 43s\tremaining: 1m 30s\n",
      "533:\tlearn: 0.1152414\ttotal: 1m 43s\tremaining: 1m 30s\n",
      "534:\tlearn: 0.1146132\ttotal: 1m 43s\tremaining: 1m 29s\n",
      "535:\tlearn: 0.1143830\ttotal: 1m 43s\tremaining: 1m 29s\n",
      "536:\tlearn: 0.1143203\ttotal: 1m 43s\tremaining: 1m 29s\n",
      "537:\tlearn: 0.1142629\ttotal: 1m 43s\tremaining: 1m 29s\n",
      "538:\tlearn: 0.1137475\ttotal: 1m 44s\tremaining: 1m 29s\n",
      "539:\tlearn: 0.1131267\ttotal: 1m 44s\tremaining: 1m 28s\n",
      "540:\tlearn: 0.1130924\ttotal: 1m 44s\tremaining: 1m 28s\n",
      "541:\tlearn: 0.1130620\ttotal: 1m 44s\tremaining: 1m 28s\n",
      "542:\tlearn: 0.1129925\ttotal: 1m 44s\tremaining: 1m 28s\n",
      "543:\tlearn: 0.1128919\ttotal: 1m 45s\tremaining: 1m 28s\n",
      "544:\tlearn: 0.1126836\ttotal: 1m 45s\tremaining: 1m 27s\n",
      "545:\tlearn: 0.1123434\ttotal: 1m 45s\tremaining: 1m 27s\n",
      "546:\tlearn: 0.1123047\ttotal: 1m 45s\tremaining: 1m 27s\n",
      "547:\tlearn: 0.1122812\ttotal: 1m 45s\tremaining: 1m 27s\n",
      "548:\tlearn: 0.1122540\ttotal: 1m 46s\tremaining: 1m 27s\n",
      "549:\tlearn: 0.1122023\ttotal: 1m 46s\tremaining: 1m 26s\n",
      "550:\tlearn: 0.1121788\ttotal: 1m 46s\tremaining: 1m 26s\n",
      "551:\tlearn: 0.1121506\ttotal: 1m 46s\tremaining: 1m 26s\n",
      "552:\tlearn: 0.1121177\ttotal: 1m 46s\tremaining: 1m 26s\n",
      "553:\tlearn: 0.1117567\ttotal: 1m 46s\tremaining: 1m 26s\n",
      "554:\tlearn: 0.1116892\ttotal: 1m 47s\tremaining: 1m 25s\n",
      "555:\tlearn: 0.1115802\ttotal: 1m 47s\tremaining: 1m 25s\n",
      "556:\tlearn: 0.1115568\ttotal: 1m 47s\tremaining: 1m 25s\n",
      "557:\tlearn: 0.1112636\ttotal: 1m 47s\tremaining: 1m 25s\n",
      "558:\tlearn: 0.1110738\ttotal: 1m 47s\tremaining: 1m 25s\n",
      "559:\tlearn: 0.1106958\ttotal: 1m 48s\tremaining: 1m 24s\n",
      "560:\tlearn: 0.1106715\ttotal: 1m 48s\tremaining: 1m 24s\n",
      "561:\tlearn: 0.1106498\ttotal: 1m 48s\tremaining: 1m 24s\n",
      "562:\tlearn: 0.1105764\ttotal: 1m 48s\tremaining: 1m 24s\n",
      "563:\tlearn: 0.1101829\ttotal: 1m 48s\tremaining: 1m 24s\n",
      "564:\tlearn: 0.1100637\ttotal: 1m 48s\tremaining: 1m 23s\n",
      "565:\tlearn: 0.1100433\ttotal: 1m 49s\tremaining: 1m 23s\n",
      "566:\tlearn: 0.1100211\ttotal: 1m 49s\tremaining: 1m 23s\n",
      "567:\tlearn: 0.1098790\ttotal: 1m 49s\tremaining: 1m 23s\n",
      "568:\tlearn: 0.1098479\ttotal: 1m 49s\tremaining: 1m 23s\n",
      "569:\tlearn: 0.1094140\ttotal: 1m 49s\tremaining: 1m 22s\n",
      "570:\tlearn: 0.1093952\ttotal: 1m 50s\tremaining: 1m 22s\n",
      "571:\tlearn: 0.1091076\ttotal: 1m 50s\tremaining: 1m 22s\n",
      "572:\tlearn: 0.1086252\ttotal: 1m 50s\tremaining: 1m 22s\n",
      "573:\tlearn: 0.1085617\ttotal: 1m 50s\tremaining: 1m 22s\n",
      "574:\tlearn: 0.1085429\ttotal: 1m 50s\tremaining: 1m 21s\n",
      "575:\tlearn: 0.1084786\ttotal: 1m 50s\tremaining: 1m 21s\n",
      "576:\tlearn: 0.1084290\ttotal: 1m 51s\tremaining: 1m 21s\n",
      "577:\tlearn: 0.1081896\ttotal: 1m 51s\tremaining: 1m 21s\n",
      "578:\tlearn: 0.1081579\ttotal: 1m 51s\tremaining: 1m 21s\n",
      "579:\tlearn: 0.1079618\ttotal: 1m 51s\tremaining: 1m 20s\n",
      "580:\tlearn: 0.1079403\ttotal: 1m 51s\tremaining: 1m 20s\n",
      "581:\tlearn: 0.1079045\ttotal: 1m 52s\tremaining: 1m 20s\n",
      "582:\tlearn: 0.1078713\ttotal: 1m 52s\tremaining: 1m 20s\n",
      "583:\tlearn: 0.1078039\ttotal: 1m 52s\tremaining: 1m 20s\n",
      "584:\tlearn: 0.1077371\ttotal: 1m 52s\tremaining: 1m 19s\n",
      "585:\tlearn: 0.1077095\ttotal: 1m 52s\tremaining: 1m 19s\n",
      "586:\tlearn: 0.1076743\ttotal: 1m 53s\tremaining: 1m 19s\n",
      "587:\tlearn: 0.1075680\ttotal: 1m 53s\tremaining: 1m 19s\n",
      "588:\tlearn: 0.1073950\ttotal: 1m 53s\tremaining: 1m 19s\n",
      "589:\tlearn: 0.1073827\ttotal: 1m 53s\tremaining: 1m 18s\n",
      "590:\tlearn: 0.1073639\ttotal: 1m 53s\tremaining: 1m 18s\n",
      "591:\tlearn: 0.1073362\ttotal: 1m 53s\tremaining: 1m 18s\n",
      "592:\tlearn: 0.1073180\ttotal: 1m 54s\tremaining: 1m 18s\n",
      "593:\tlearn: 0.1072066\ttotal: 1m 54s\tremaining: 1m 18s\n",
      "594:\tlearn: 0.1071765\ttotal: 1m 54s\tremaining: 1m 17s\n",
      "595:\tlearn: 0.1071476\ttotal: 1m 54s\tremaining: 1m 17s\n",
      "596:\tlearn: 0.1071140\ttotal: 1m 54s\tremaining: 1m 17s\n",
      "597:\tlearn: 0.1070417\ttotal: 1m 54s\tremaining: 1m 17s\n",
      "598:\tlearn: 0.1070177\ttotal: 1m 55s\tremaining: 1m 17s\n",
      "599:\tlearn: 0.1067514\ttotal: 1m 55s\tremaining: 1m 16s\n",
      "600:\tlearn: 0.1066870\ttotal: 1m 55s\tremaining: 1m 16s\n",
      "601:\tlearn: 0.1066605\ttotal: 1m 55s\tremaining: 1m 16s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602:\tlearn: 0.1066334\ttotal: 1m 55s\tremaining: 1m 16s\n",
      "603:\tlearn: 0.1066169\ttotal: 1m 56s\tremaining: 1m 16s\n",
      "604:\tlearn: 0.1064784\ttotal: 1m 56s\tremaining: 1m 15s\n",
      "605:\tlearn: 0.1064579\ttotal: 1m 56s\tremaining: 1m 15s\n",
      "606:\tlearn: 0.1064443\ttotal: 1m 56s\tremaining: 1m 15s\n",
      "607:\tlearn: 0.1063975\ttotal: 1m 56s\tremaining: 1m 15s\n",
      "608:\tlearn: 0.1063743\ttotal: 1m 56s\tremaining: 1m 15s\n",
      "609:\tlearn: 0.1063537\ttotal: 1m 57s\tremaining: 1m 14s\n",
      "610:\tlearn: 0.1063089\ttotal: 1m 57s\tremaining: 1m 14s\n",
      "611:\tlearn: 0.1062952\ttotal: 1m 57s\tremaining: 1m 14s\n",
      "612:\tlearn: 0.1062772\ttotal: 1m 57s\tremaining: 1m 14s\n",
      "613:\tlearn: 0.1062515\ttotal: 1m 57s\tremaining: 1m 14s\n",
      "614:\tlearn: 0.1062263\ttotal: 1m 57s\tremaining: 1m 13s\n",
      "615:\tlearn: 0.1061943\ttotal: 1m 58s\tremaining: 1m 13s\n",
      "616:\tlearn: 0.1058710\ttotal: 1m 58s\tremaining: 1m 13s\n",
      "617:\tlearn: 0.1058479\ttotal: 1m 58s\tremaining: 1m 13s\n",
      "618:\tlearn: 0.1055850\ttotal: 1m 58s\tremaining: 1m 13s\n",
      "619:\tlearn: 0.1055711\ttotal: 1m 58s\tremaining: 1m 12s\n",
      "620:\tlearn: 0.1055531\ttotal: 1m 59s\tremaining: 1m 12s\n",
      "621:\tlearn: 0.1055300\ttotal: 1m 59s\tremaining: 1m 12s\n",
      "622:\tlearn: 0.1054957\ttotal: 1m 59s\tremaining: 1m 12s\n",
      "623:\tlearn: 0.1054717\ttotal: 1m 59s\tremaining: 1m 12s\n",
      "624:\tlearn: 0.1054283\ttotal: 1m 59s\tremaining: 1m 11s\n",
      "625:\tlearn: 0.1052961\ttotal: 1m 59s\tremaining: 1m 11s\n",
      "626:\tlearn: 0.1052558\ttotal: 2m\tremaining: 1m 11s\n",
      "627:\tlearn: 0.1052123\ttotal: 2m\tremaining: 1m 11s\n",
      "628:\tlearn: 0.1049929\ttotal: 2m\tremaining: 1m 11s\n",
      "629:\tlearn: 0.1049589\ttotal: 2m\tremaining: 1m 10s\n",
      "630:\tlearn: 0.1049383\ttotal: 2m\tremaining: 1m 10s\n",
      "631:\tlearn: 0.1049238\ttotal: 2m 1s\tremaining: 1m 10s\n",
      "632:\tlearn: 0.1049071\ttotal: 2m 1s\tremaining: 1m 10s\n",
      "633:\tlearn: 0.1048939\ttotal: 2m 1s\tremaining: 1m 10s\n",
      "634:\tlearn: 0.1048700\ttotal: 2m 1s\tremaining: 1m 9s\n",
      "635:\tlearn: 0.1045743\ttotal: 2m 1s\tremaining: 1m 9s\n",
      "636:\tlearn: 0.1045484\ttotal: 2m 1s\tremaining: 1m 9s\n",
      "637:\tlearn: 0.1044452\ttotal: 2m 2s\tremaining: 1m 9s\n",
      "638:\tlearn: 0.1044224\ttotal: 2m 2s\tremaining: 1m 9s\n",
      "639:\tlearn: 0.1043848\ttotal: 2m 2s\tremaining: 1m 8s\n",
      "640:\tlearn: 0.1043117\ttotal: 2m 2s\tremaining: 1m 8s\n",
      "641:\tlearn: 0.1042907\ttotal: 2m 2s\tremaining: 1m 8s\n",
      "642:\tlearn: 0.1042789\ttotal: 2m 3s\tremaining: 1m 8s\n",
      "643:\tlearn: 0.1042427\ttotal: 2m 3s\tremaining: 1m 8s\n",
      "644:\tlearn: 0.1042052\ttotal: 2m 3s\tremaining: 1m 7s\n",
      "645:\tlearn: 0.1041937\ttotal: 2m 3s\tremaining: 1m 7s\n",
      "646:\tlearn: 0.1041789\ttotal: 2m 4s\tremaining: 1m 7s\n",
      "647:\tlearn: 0.1041102\ttotal: 2m 4s\tremaining: 1m 7s\n",
      "648:\tlearn: 0.1040986\ttotal: 2m 4s\tremaining: 1m 7s\n",
      "649:\tlearn: 0.1040825\ttotal: 2m 4s\tremaining: 1m 7s\n",
      "650:\tlearn: 0.1040102\ttotal: 2m 4s\tremaining: 1m 6s\n",
      "651:\tlearn: 0.1040013\ttotal: 2m 5s\tremaining: 1m 6s\n",
      "652:\tlearn: 0.1039843\ttotal: 2m 5s\tremaining: 1m 6s\n",
      "653:\tlearn: 0.1039694\ttotal: 2m 5s\tremaining: 1m 6s\n",
      "654:\tlearn: 0.1036020\ttotal: 2m 5s\tremaining: 1m 6s\n",
      "655:\tlearn: 0.1035351\ttotal: 2m 5s\tremaining: 1m 6s\n",
      "656:\tlearn: 0.1035249\ttotal: 2m 6s\tremaining: 1m 5s\n",
      "657:\tlearn: 0.1033834\ttotal: 2m 6s\tremaining: 1m 5s\n",
      "658:\tlearn: 0.1033492\ttotal: 2m 6s\tremaining: 1m 5s\n",
      "659:\tlearn: 0.1032780\ttotal: 2m 6s\tremaining: 1m 5s\n",
      "660:\tlearn: 0.1032010\ttotal: 2m 6s\tremaining: 1m 5s\n",
      "661:\tlearn: 0.1031809\ttotal: 2m 7s\tremaining: 1m 4s\n",
      "662:\tlearn: 0.1031686\ttotal: 2m 7s\tremaining: 1m 4s\n",
      "663:\tlearn: 0.1031504\ttotal: 2m 7s\tremaining: 1m 4s\n",
      "664:\tlearn: 0.1030673\ttotal: 2m 7s\tremaining: 1m 4s\n",
      "665:\tlearn: 0.1029698\ttotal: 2m 7s\tremaining: 1m 4s\n",
      "666:\tlearn: 0.1027989\ttotal: 2m 8s\tremaining: 1m 3s\n",
      "667:\tlearn: 0.1025859\ttotal: 2m 8s\tremaining: 1m 3s\n",
      "668:\tlearn: 0.1025043\ttotal: 2m 8s\tremaining: 1m 3s\n",
      "669:\tlearn: 0.1024778\ttotal: 2m 8s\tremaining: 1m 3s\n",
      "670:\tlearn: 0.1024542\ttotal: 2m 8s\tremaining: 1m 3s\n",
      "671:\tlearn: 0.1021718\ttotal: 2m 9s\tremaining: 1m 2s\n",
      "672:\tlearn: 0.1020287\ttotal: 2m 9s\tremaining: 1m 2s\n",
      "673:\tlearn: 0.1020119\ttotal: 2m 9s\tremaining: 1m 2s\n",
      "674:\tlearn: 0.1019942\ttotal: 2m 9s\tremaining: 1m 2s\n",
      "675:\tlearn: 0.1019143\ttotal: 2m 9s\tremaining: 1m 2s\n",
      "676:\tlearn: 0.1018081\ttotal: 2m 9s\tremaining: 1m 2s\n",
      "677:\tlearn: 0.1017712\ttotal: 2m 10s\tremaining: 1m 1s\n",
      "678:\tlearn: 0.1017546\ttotal: 2m 10s\tremaining: 1m 1s\n",
      "679:\tlearn: 0.1013358\ttotal: 2m 10s\tremaining: 1m 1s\n",
      "680:\tlearn: 0.1013187\ttotal: 2m 10s\tremaining: 1m 1s\n",
      "681:\tlearn: 0.1011190\ttotal: 2m 10s\tremaining: 1m 1s\n",
      "682:\tlearn: 0.1009480\ttotal: 2m 11s\tremaining: 1m\n",
      "683:\tlearn: 0.1009076\ttotal: 2m 11s\tremaining: 1m\n",
      "684:\tlearn: 0.1008246\ttotal: 2m 11s\tremaining: 1m\n",
      "685:\tlearn: 0.1008130\ttotal: 2m 11s\tremaining: 1m\n",
      "686:\tlearn: 0.1007939\ttotal: 2m 11s\tremaining: 1m\n",
      "687:\tlearn: 0.1007798\ttotal: 2m 12s\tremaining: 59.9s\n",
      "688:\tlearn: 0.1007527\ttotal: 2m 12s\tremaining: 59.7s\n",
      "689:\tlearn: 0.1006028\ttotal: 2m 12s\tremaining: 59.5s\n",
      "690:\tlearn: 0.1003735\ttotal: 2m 12s\tremaining: 59.3s\n",
      "691:\tlearn: 0.1001897\ttotal: 2m 12s\tremaining: 59.1s\n",
      "692:\tlearn: 0.1000911\ttotal: 2m 12s\tremaining: 58.9s\n",
      "693:\tlearn: 0.1000524\ttotal: 2m 13s\tremaining: 58.7s\n",
      "694:\tlearn: 0.0998914\ttotal: 2m 13s\tremaining: 58.5s\n",
      "695:\tlearn: 0.0997400\ttotal: 2m 13s\tremaining: 58.4s\n",
      "696:\tlearn: 0.0996388\ttotal: 2m 13s\tremaining: 58.2s\n",
      "697:\tlearn: 0.0996205\ttotal: 2m 13s\tremaining: 58s\n",
      "698:\tlearn: 0.0996066\ttotal: 2m 14s\tremaining: 57.8s\n",
      "699:\tlearn: 0.0995913\ttotal: 2m 14s\tremaining: 57.6s\n",
      "700:\tlearn: 0.0995767\ttotal: 2m 14s\tremaining: 57.4s\n",
      "701:\tlearn: 0.0994944\ttotal: 2m 14s\tremaining: 57.2s\n",
      "702:\tlearn: 0.0994788\ttotal: 2m 14s\tremaining: 57s\n",
      "703:\tlearn: 0.0994268\ttotal: 2m 15s\tremaining: 56.8s\n",
      "704:\tlearn: 0.0994116\ttotal: 2m 15s\tremaining: 56.6s\n",
      "705:\tlearn: 0.0992955\ttotal: 2m 15s\tremaining: 56.5s\n",
      "706:\tlearn: 0.0992713\ttotal: 2m 15s\tremaining: 56.3s\n",
      "707:\tlearn: 0.0992585\ttotal: 2m 16s\tremaining: 56.2s\n",
      "708:\tlearn: 0.0992309\ttotal: 2m 16s\tremaining: 56s\n",
      "709:\tlearn: 0.0991780\ttotal: 2m 16s\tremaining: 55.8s\n",
      "710:\tlearn: 0.0991319\ttotal: 2m 16s\tremaining: 55.6s\n",
      "711:\tlearn: 0.0991166\ttotal: 2m 16s\tremaining: 55.4s\n",
      "712:\tlearn: 0.0987837\ttotal: 2m 17s\tremaining: 55.2s\n",
      "713:\tlearn: 0.0987730\ttotal: 2m 17s\tremaining: 55s\n",
      "714:\tlearn: 0.0987564\ttotal: 2m 17s\tremaining: 54.9s\n",
      "715:\tlearn: 0.0987450\ttotal: 2m 17s\tremaining: 54.7s\n",
      "716:\tlearn: 0.0987307\ttotal: 2m 18s\tremaining: 54.5s\n",
      "717:\tlearn: 0.0987193\ttotal: 2m 18s\tremaining: 54.3s\n",
      "718:\tlearn: 0.0986880\ttotal: 2m 18s\tremaining: 54.1s\n",
      "719:\tlearn: 0.0986570\ttotal: 2m 18s\tremaining: 54s\n",
      "720:\tlearn: 0.0985691\ttotal: 2m 18s\tremaining: 53.8s\n",
      "721:\tlearn: 0.0985505\ttotal: 2m 19s\tremaining: 53.6s\n",
      "722:\tlearn: 0.0985016\ttotal: 2m 19s\tremaining: 53.4s\n",
      "723:\tlearn: 0.0984845\ttotal: 2m 19s\tremaining: 53.2s\n",
      "724:\tlearn: 0.0984755\ttotal: 2m 19s\tremaining: 53.1s\n",
      "725:\tlearn: 0.0983961\ttotal: 2m 20s\tremaining: 52.9s\n",
      "726:\tlearn: 0.0983846\ttotal: 2m 20s\tremaining: 52.7s\n",
      "727:\tlearn: 0.0983498\ttotal: 2m 20s\tremaining: 52.5s\n",
      "728:\tlearn: 0.0983351\ttotal: 2m 20s\tremaining: 52.3s\n",
      "729:\tlearn: 0.0983119\ttotal: 2m 20s\tremaining: 52.1s\n",
      "730:\tlearn: 0.0983015\ttotal: 2m 21s\tremaining: 51.9s\n",
      "731:\tlearn: 0.0982872\ttotal: 2m 21s\tremaining: 51.8s\n",
      "732:\tlearn: 0.0982723\ttotal: 2m 21s\tremaining: 51.6s\n",
      "733:\tlearn: 0.0982489\ttotal: 2m 21s\tremaining: 51.4s\n",
      "734:\tlearn: 0.0982392\ttotal: 2m 22s\tremaining: 51.2s\n",
      "735:\tlearn: 0.0982263\ttotal: 2m 22s\tremaining: 51s\n",
      "736:\tlearn: 0.0982018\ttotal: 2m 22s\tremaining: 50.8s\n",
      "737:\tlearn: 0.0981738\ttotal: 2m 22s\tremaining: 50.6s\n",
      "738:\tlearn: 0.0981535\ttotal: 2m 22s\tremaining: 50.4s\n",
      "739:\tlearn: 0.0981446\ttotal: 2m 22s\tremaining: 50.2s\n",
      "740:\tlearn: 0.0981319\ttotal: 2m 23s\tremaining: 50s\n",
      "741:\tlearn: 0.0981223\ttotal: 2m 23s\tremaining: 49.8s\n",
      "742:\tlearn: 0.0978782\ttotal: 2m 23s\tremaining: 49.6s\n",
      "743:\tlearn: 0.0978575\ttotal: 2m 23s\tremaining: 49.4s\n",
      "744:\tlearn: 0.0975405\ttotal: 2m 23s\tremaining: 49.3s\n",
      "745:\tlearn: 0.0975215\ttotal: 2m 24s\tremaining: 49.1s\n",
      "746:\tlearn: 0.0971719\ttotal: 2m 24s\tremaining: 48.9s\n",
      "747:\tlearn: 0.0971575\ttotal: 2m 24s\tremaining: 48.7s\n",
      "748:\tlearn: 0.0970529\ttotal: 2m 24s\tremaining: 48.5s\n",
      "749:\tlearn: 0.0969947\ttotal: 2m 24s\tremaining: 48.3s\n",
      "750:\tlearn: 0.0966622\ttotal: 2m 25s\tremaining: 48.1s\n",
      "751:\tlearn: 0.0966506\ttotal: 2m 25s\tremaining: 47.9s\n",
      "752:\tlearn: 0.0966306\ttotal: 2m 25s\tremaining: 47.7s\n",
      "753:\tlearn: 0.0966206\ttotal: 2m 25s\tremaining: 47.5s\n",
      "754:\tlearn: 0.0965189\ttotal: 2m 25s\tremaining: 47.3s\n",
      "755:\tlearn: 0.0965096\ttotal: 2m 25s\tremaining: 47.1s\n",
      "756:\tlearn: 0.0964414\ttotal: 2m 26s\tremaining: 46.9s\n",
      "757:\tlearn: 0.0964241\ttotal: 2m 26s\tremaining: 46.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "758:\tlearn: 0.0963984\ttotal: 2m 26s\tremaining: 46.5s\n",
      "759:\tlearn: 0.0963788\ttotal: 2m 26s\tremaining: 46.3s\n",
      "760:\tlearn: 0.0963650\ttotal: 2m 26s\tremaining: 46.1s\n",
      "761:\tlearn: 0.0963494\ttotal: 2m 26s\tremaining: 45.9s\n",
      "762:\tlearn: 0.0960717\ttotal: 2m 27s\tremaining: 45.7s\n",
      "763:\tlearn: 0.0960646\ttotal: 2m 27s\tremaining: 45.5s\n",
      "764:\tlearn: 0.0959945\ttotal: 2m 27s\tremaining: 45.3s\n",
      "765:\tlearn: 0.0959824\ttotal: 2m 27s\tremaining: 45.1s\n",
      "766:\tlearn: 0.0959720\ttotal: 2m 27s\tremaining: 44.9s\n",
      "767:\tlearn: 0.0959599\ttotal: 2m 27s\tremaining: 44.7s\n",
      "768:\tlearn: 0.0959480\ttotal: 2m 28s\tremaining: 44.5s\n",
      "769:\tlearn: 0.0959336\ttotal: 2m 28s\tremaining: 44.3s\n",
      "770:\tlearn: 0.0958712\ttotal: 2m 28s\tremaining: 44.1s\n",
      "771:\tlearn: 0.0957467\ttotal: 2m 28s\tremaining: 43.9s\n",
      "772:\tlearn: 0.0957284\ttotal: 2m 28s\tremaining: 43.7s\n",
      "773:\tlearn: 0.0954188\ttotal: 2m 29s\tremaining: 43.5s\n",
      "774:\tlearn: 0.0953214\ttotal: 2m 29s\tremaining: 43.3s\n",
      "775:\tlearn: 0.0951268\ttotal: 2m 29s\tremaining: 43.2s\n",
      "776:\tlearn: 0.0951172\ttotal: 2m 29s\tremaining: 43s\n",
      "777:\tlearn: 0.0951081\ttotal: 2m 29s\tremaining: 42.8s\n",
      "778:\tlearn: 0.0950982\ttotal: 2m 29s\tremaining: 42.6s\n",
      "779:\tlearn: 0.0950411\ttotal: 2m 30s\tremaining: 42.4s\n",
      "780:\tlearn: 0.0950259\ttotal: 2m 30s\tremaining: 42.2s\n",
      "781:\tlearn: 0.0950149\ttotal: 2m 30s\tremaining: 42s\n",
      "782:\tlearn: 0.0946793\ttotal: 2m 30s\tremaining: 41.8s\n",
      "783:\tlearn: 0.0945165\ttotal: 2m 30s\tremaining: 41.6s\n",
      "784:\tlearn: 0.0944979\ttotal: 2m 31s\tremaining: 41.4s\n",
      "785:\tlearn: 0.0944601\ttotal: 2m 31s\tremaining: 41.2s\n",
      "786:\tlearn: 0.0944416\ttotal: 2m 31s\tremaining: 41s\n",
      "787:\tlearn: 0.0944030\ttotal: 2m 31s\tremaining: 40.8s\n",
      "788:\tlearn: 0.0943953\ttotal: 2m 31s\tremaining: 40.6s\n",
      "789:\tlearn: 0.0943718\ttotal: 2m 31s\tremaining: 40.4s\n",
      "790:\tlearn: 0.0943326\ttotal: 2m 32s\tremaining: 40.2s\n",
      "791:\tlearn: 0.0942745\ttotal: 2m 32s\tremaining: 40s\n",
      "792:\tlearn: 0.0942652\ttotal: 2m 32s\tremaining: 39.8s\n",
      "793:\tlearn: 0.0942526\ttotal: 2m 32s\tremaining: 39.6s\n",
      "794:\tlearn: 0.0942069\ttotal: 2m 32s\tremaining: 39.4s\n",
      "795:\tlearn: 0.0941873\ttotal: 2m 32s\tremaining: 39.2s\n",
      "796:\tlearn: 0.0941326\ttotal: 2m 33s\tremaining: 39s\n",
      "797:\tlearn: 0.0941162\ttotal: 2m 33s\tremaining: 38.8s\n",
      "798:\tlearn: 0.0941005\ttotal: 2m 33s\tremaining: 38.6s\n",
      "799:\tlearn: 0.0940749\ttotal: 2m 33s\tremaining: 38.4s\n",
      "800:\tlearn: 0.0940631\ttotal: 2m 33s\tremaining: 38.2s\n",
      "801:\tlearn: 0.0940537\ttotal: 2m 33s\tremaining: 38s\n",
      "802:\tlearn: 0.0940325\ttotal: 2m 34s\tremaining: 37.8s\n",
      "803:\tlearn: 0.0938283\ttotal: 2m 34s\tremaining: 37.6s\n",
      "804:\tlearn: 0.0938198\ttotal: 2m 34s\tremaining: 37.4s\n",
      "805:\tlearn: 0.0937926\ttotal: 2m 34s\tremaining: 37.2s\n",
      "806:\tlearn: 0.0937596\ttotal: 2m 34s\tremaining: 37s\n",
      "807:\tlearn: 0.0937312\ttotal: 2m 35s\tremaining: 36.8s\n",
      "808:\tlearn: 0.0935009\ttotal: 2m 35s\tremaining: 36.7s\n",
      "809:\tlearn: 0.0934633\ttotal: 2m 35s\tremaining: 36.5s\n",
      "810:\tlearn: 0.0934501\ttotal: 2m 35s\tremaining: 36.3s\n",
      "811:\tlearn: 0.0934401\ttotal: 2m 35s\tremaining: 36.1s\n",
      "812:\tlearn: 0.0934194\ttotal: 2m 36s\tremaining: 35.9s\n",
      "813:\tlearn: 0.0934095\ttotal: 2m 36s\tremaining: 35.7s\n",
      "814:\tlearn: 0.0933985\ttotal: 2m 36s\tremaining: 35.6s\n",
      "815:\tlearn: 0.0933722\ttotal: 2m 36s\tremaining: 35.4s\n",
      "816:\tlearn: 0.0933233\ttotal: 2m 37s\tremaining: 35.2s\n",
      "817:\tlearn: 0.0932367\ttotal: 2m 37s\tremaining: 35s\n",
      "818:\tlearn: 0.0930657\ttotal: 2m 37s\tremaining: 34.8s\n",
      "819:\tlearn: 0.0928442\ttotal: 2m 37s\tremaining: 34.6s\n",
      "820:\tlearn: 0.0928283\ttotal: 2m 37s\tremaining: 34.4s\n",
      "821:\tlearn: 0.0926401\ttotal: 2m 38s\tremaining: 34.2s\n",
      "822:\tlearn: 0.0926067\ttotal: 2m 38s\tremaining: 34s\n",
      "823:\tlearn: 0.0923739\ttotal: 2m 38s\tremaining: 33.9s\n",
      "824:\tlearn: 0.0923379\ttotal: 2m 38s\tremaining: 33.7s\n",
      "825:\tlearn: 0.0922943\ttotal: 2m 38s\tremaining: 33.5s\n",
      "826:\tlearn: 0.0922175\ttotal: 2m 39s\tremaining: 33.3s\n",
      "827:\tlearn: 0.0922032\ttotal: 2m 39s\tremaining: 33.1s\n",
      "828:\tlearn: 0.0919928\ttotal: 2m 39s\tremaining: 32.9s\n",
      "829:\tlearn: 0.0919813\ttotal: 2m 39s\tremaining: 32.7s\n",
      "830:\tlearn: 0.0919617\ttotal: 2m 39s\tremaining: 32.5s\n",
      "831:\tlearn: 0.0919550\ttotal: 2m 40s\tremaining: 32.3s\n",
      "832:\tlearn: 0.0919317\ttotal: 2m 40s\tremaining: 32.1s\n",
      "833:\tlearn: 0.0919135\ttotal: 2m 40s\tremaining: 32s\n",
      "834:\tlearn: 0.0918568\ttotal: 2m 40s\tremaining: 31.8s\n",
      "835:\tlearn: 0.0918388\ttotal: 2m 41s\tremaining: 31.6s\n",
      "836:\tlearn: 0.0918291\ttotal: 2m 41s\tremaining: 31.4s\n",
      "837:\tlearn: 0.0918162\ttotal: 2m 41s\tremaining: 31.2s\n",
      "838:\tlearn: 0.0918066\ttotal: 2m 41s\tremaining: 31s\n",
      "839:\tlearn: 0.0917421\ttotal: 2m 41s\tremaining: 30.8s\n",
      "840:\tlearn: 0.0917341\ttotal: 2m 41s\tremaining: 30.6s\n",
      "841:\tlearn: 0.0917048\ttotal: 2m 42s\tremaining: 30.4s\n",
      "842:\tlearn: 0.0915089\ttotal: 2m 42s\tremaining: 30.2s\n",
      "843:\tlearn: 0.0915020\ttotal: 2m 42s\tremaining: 30s\n",
      "844:\tlearn: 0.0914756\ttotal: 2m 42s\tremaining: 29.8s\n",
      "845:\tlearn: 0.0914565\ttotal: 2m 42s\tremaining: 29.7s\n",
      "846:\tlearn: 0.0914389\ttotal: 2m 43s\tremaining: 29.5s\n",
      "847:\tlearn: 0.0914129\ttotal: 2m 43s\tremaining: 29.3s\n",
      "848:\tlearn: 0.0914043\ttotal: 2m 43s\tremaining: 29.1s\n",
      "849:\tlearn: 0.0913690\ttotal: 2m 43s\tremaining: 28.9s\n",
      "850:\tlearn: 0.0912981\ttotal: 2m 43s\tremaining: 28.7s\n",
      "851:\tlearn: 0.0912851\ttotal: 2m 43s\tremaining: 28.5s\n",
      "852:\tlearn: 0.0912762\ttotal: 2m 44s\tremaining: 28.3s\n",
      "853:\tlearn: 0.0912660\ttotal: 2m 44s\tremaining: 28.1s\n",
      "854:\tlearn: 0.0912480\ttotal: 2m 44s\tremaining: 27.9s\n",
      "855:\tlearn: 0.0912230\ttotal: 2m 44s\tremaining: 27.7s\n",
      "856:\tlearn: 0.0912026\ttotal: 2m 44s\tremaining: 27.5s\n",
      "857:\tlearn: 0.0911905\ttotal: 2m 45s\tremaining: 27.3s\n",
      "858:\tlearn: 0.0909581\ttotal: 2m 45s\tremaining: 27.1s\n",
      "859:\tlearn: 0.0909239\ttotal: 2m 45s\tremaining: 26.9s\n",
      "860:\tlearn: 0.0909035\ttotal: 2m 45s\tremaining: 26.8s\n",
      "861:\tlearn: 0.0908916\ttotal: 2m 45s\tremaining: 26.6s\n",
      "862:\tlearn: 0.0908347\ttotal: 2m 46s\tremaining: 26.4s\n",
      "863:\tlearn: 0.0907866\ttotal: 2m 46s\tremaining: 26.2s\n",
      "864:\tlearn: 0.0907730\ttotal: 2m 46s\tremaining: 26s\n",
      "865:\tlearn: 0.0907662\ttotal: 2m 46s\tremaining: 25.8s\n",
      "866:\tlearn: 0.0907578\ttotal: 2m 46s\tremaining: 25.6s\n",
      "867:\tlearn: 0.0906497\ttotal: 2m 47s\tremaining: 25.4s\n",
      "868:\tlearn: 0.0906368\ttotal: 2m 47s\tremaining: 25.2s\n",
      "869:\tlearn: 0.0906274\ttotal: 2m 47s\tremaining: 25.1s\n",
      "870:\tlearn: 0.0906173\ttotal: 2m 47s\tremaining: 24.9s\n",
      "871:\tlearn: 0.0906053\ttotal: 2m 48s\tremaining: 24.7s\n",
      "872:\tlearn: 0.0905940\ttotal: 2m 48s\tremaining: 24.5s\n",
      "873:\tlearn: 0.0905865\ttotal: 2m 48s\tremaining: 24.3s\n",
      "874:\tlearn: 0.0905755\ttotal: 2m 48s\tremaining: 24.1s\n",
      "875:\tlearn: 0.0905457\ttotal: 2m 48s\tremaining: 23.9s\n",
      "876:\tlearn: 0.0905391\ttotal: 2m 49s\tremaining: 23.7s\n",
      "877:\tlearn: 0.0905113\ttotal: 2m 49s\tremaining: 23.5s\n",
      "878:\tlearn: 0.0904978\ttotal: 2m 49s\tremaining: 23.3s\n",
      "879:\tlearn: 0.0904871\ttotal: 2m 49s\tremaining: 23.1s\n",
      "880:\tlearn: 0.0904757\ttotal: 2m 49s\tremaining: 22.9s\n",
      "881:\tlearn: 0.0904268\ttotal: 2m 49s\tremaining: 22.7s\n",
      "882:\tlearn: 0.0904036\ttotal: 2m 50s\tremaining: 22.5s\n",
      "883:\tlearn: 0.0903915\ttotal: 2m 50s\tremaining: 22.3s\n",
      "884:\tlearn: 0.0903866\ttotal: 2m 50s\tremaining: 22.1s\n",
      "885:\tlearn: 0.0903701\ttotal: 2m 50s\tremaining: 22s\n",
      "886:\tlearn: 0.0903614\ttotal: 2m 50s\tremaining: 21.8s\n",
      "887:\tlearn: 0.0903508\ttotal: 2m 50s\tremaining: 21.6s\n",
      "888:\tlearn: 0.0903401\ttotal: 2m 51s\tremaining: 21.4s\n",
      "889:\tlearn: 0.0903338\ttotal: 2m 51s\tremaining: 21.2s\n",
      "890:\tlearn: 0.0902924\ttotal: 2m 51s\tremaining: 21s\n",
      "891:\tlearn: 0.0902849\ttotal: 2m 51s\tremaining: 20.8s\n",
      "892:\tlearn: 0.0902719\ttotal: 2m 51s\tremaining: 20.6s\n",
      "893:\tlearn: 0.0902248\ttotal: 2m 51s\tremaining: 20.4s\n",
      "894:\tlearn: 0.0902141\ttotal: 2m 52s\tremaining: 20.2s\n",
      "895:\tlearn: 0.0902056\ttotal: 2m 52s\tremaining: 20s\n",
      "896:\tlearn: 0.0901990\ttotal: 2m 52s\tremaining: 19.8s\n",
      "897:\tlearn: 0.0901550\ttotal: 2m 52s\tremaining: 19.6s\n",
      "898:\tlearn: 0.0901376\ttotal: 2m 53s\tremaining: 19.4s\n",
      "899:\tlearn: 0.0901302\ttotal: 2m 53s\tremaining: 19.3s\n",
      "900:\tlearn: 0.0901243\ttotal: 2m 53s\tremaining: 19.1s\n",
      "901:\tlearn: 0.0900287\ttotal: 2m 53s\tremaining: 18.9s\n",
      "902:\tlearn: 0.0900106\ttotal: 2m 54s\tremaining: 18.7s\n",
      "903:\tlearn: 0.0900027\ttotal: 2m 54s\tremaining: 18.5s\n",
      "904:\tlearn: 0.0899748\ttotal: 2m 54s\tremaining: 18.3s\n",
      "905:\tlearn: 0.0899508\ttotal: 2m 54s\tremaining: 18.1s\n",
      "906:\tlearn: 0.0899250\ttotal: 2m 54s\tremaining: 17.9s\n",
      "907:\tlearn: 0.0897075\ttotal: 2m 55s\tremaining: 17.7s\n",
      "908:\tlearn: 0.0896469\ttotal: 2m 55s\tremaining: 17.6s\n",
      "909:\tlearn: 0.0896339\ttotal: 2m 55s\tremaining: 17.4s\n",
      "910:\tlearn: 0.0895313\ttotal: 2m 55s\tremaining: 17.2s\n",
      "911:\tlearn: 0.0895103\ttotal: 2m 55s\tremaining: 17s\n",
      "912:\tlearn: 0.0894986\ttotal: 2m 56s\tremaining: 16.8s\n",
      "913:\tlearn: 0.0894651\ttotal: 2m 56s\tremaining: 16.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "914:\tlearn: 0.0893955\ttotal: 2m 56s\tremaining: 16.4s\n",
      "915:\tlearn: 0.0893610\ttotal: 2m 56s\tremaining: 16.2s\n",
      "916:\tlearn: 0.0893508\ttotal: 2m 56s\tremaining: 16s\n",
      "917:\tlearn: 0.0893421\ttotal: 2m 57s\tremaining: 15.8s\n",
      "918:\tlearn: 0.0892201\ttotal: 2m 57s\tremaining: 15.6s\n",
      "919:\tlearn: 0.0891807\ttotal: 2m 57s\tremaining: 15.4s\n",
      "920:\tlearn: 0.0891573\ttotal: 2m 57s\tremaining: 15.2s\n",
      "921:\tlearn: 0.0891464\ttotal: 2m 57s\tremaining: 15s\n",
      "922:\tlearn: 0.0891228\ttotal: 2m 57s\tremaining: 14.8s\n",
      "923:\tlearn: 0.0890563\ttotal: 2m 58s\tremaining: 14.7s\n",
      "924:\tlearn: 0.0889708\ttotal: 2m 58s\tremaining: 14.5s\n",
      "925:\tlearn: 0.0889651\ttotal: 2m 58s\tremaining: 14.3s\n",
      "926:\tlearn: 0.0889507\ttotal: 2m 58s\tremaining: 14.1s\n",
      "927:\tlearn: 0.0889378\ttotal: 2m 58s\tremaining: 13.9s\n",
      "928:\tlearn: 0.0889330\ttotal: 2m 59s\tremaining: 13.7s\n",
      "929:\tlearn: 0.0889219\ttotal: 2m 59s\tremaining: 13.5s\n",
      "930:\tlearn: 0.0889113\ttotal: 2m 59s\tremaining: 13.3s\n",
      "931:\tlearn: 0.0888994\ttotal: 2m 59s\tremaining: 13.1s\n",
      "932:\tlearn: 0.0888901\ttotal: 2m 59s\tremaining: 12.9s\n",
      "933:\tlearn: 0.0888808\ttotal: 2m 59s\tremaining: 12.7s\n",
      "934:\tlearn: 0.0888769\ttotal: 3m\tremaining: 12.5s\n",
      "935:\tlearn: 0.0888660\ttotal: 3m\tremaining: 12.3s\n",
      "936:\tlearn: 0.0888587\ttotal: 3m\tremaining: 12.1s\n",
      "937:\tlearn: 0.0887476\ttotal: 3m\tremaining: 11.9s\n",
      "938:\tlearn: 0.0886375\ttotal: 3m\tremaining: 11.7s\n",
      "939:\tlearn: 0.0885434\ttotal: 3m\tremaining: 11.5s\n",
      "940:\tlearn: 0.0884325\ttotal: 3m 1s\tremaining: 11.4s\n",
      "941:\tlearn: 0.0884138\ttotal: 3m 1s\tremaining: 11.2s\n",
      "942:\tlearn: 0.0884017\ttotal: 3m 1s\tremaining: 11s\n",
      "943:\tlearn: 0.0883753\ttotal: 3m 1s\tremaining: 10.8s\n",
      "944:\tlearn: 0.0883625\ttotal: 3m 1s\tremaining: 10.6s\n",
      "945:\tlearn: 0.0883178\ttotal: 3m 2s\tremaining: 10.4s\n",
      "946:\tlearn: 0.0883121\ttotal: 3m 2s\tremaining: 10.2s\n",
      "947:\tlearn: 0.0882124\ttotal: 3m 2s\tremaining: 10s\n",
      "948:\tlearn: 0.0879756\ttotal: 3m 2s\tremaining: 9.81s\n",
      "949:\tlearn: 0.0879648\ttotal: 3m 2s\tremaining: 9.62s\n",
      "950:\tlearn: 0.0879557\ttotal: 3m 3s\tremaining: 9.43s\n",
      "951:\tlearn: 0.0879429\ttotal: 3m 3s\tremaining: 9.24s\n",
      "952:\tlearn: 0.0879304\ttotal: 3m 3s\tremaining: 9.05s\n",
      "953:\tlearn: 0.0878797\ttotal: 3m 3s\tremaining: 8.85s\n",
      "954:\tlearn: 0.0878545\ttotal: 3m 3s\tremaining: 8.66s\n",
      "955:\tlearn: 0.0878263\ttotal: 3m 3s\tremaining: 8.47s\n",
      "956:\tlearn: 0.0878121\ttotal: 3m 4s\tremaining: 8.27s\n",
      "957:\tlearn: 0.0877968\ttotal: 3m 4s\tremaining: 8.08s\n",
      "958:\tlearn: 0.0877837\ttotal: 3m 4s\tremaining: 7.89s\n",
      "959:\tlearn: 0.0877581\ttotal: 3m 4s\tremaining: 7.69s\n",
      "960:\tlearn: 0.0877109\ttotal: 3m 4s\tremaining: 7.5s\n",
      "961:\tlearn: 0.0877016\ttotal: 3m 5s\tremaining: 7.31s\n",
      "962:\tlearn: 0.0876870\ttotal: 3m 5s\tremaining: 7.12s\n",
      "963:\tlearn: 0.0876820\ttotal: 3m 5s\tremaining: 6.92s\n",
      "964:\tlearn: 0.0875001\ttotal: 3m 5s\tremaining: 6.73s\n",
      "965:\tlearn: 0.0872748\ttotal: 3m 5s\tremaining: 6.54s\n",
      "966:\tlearn: 0.0872382\ttotal: 3m 6s\tremaining: 6.35s\n",
      "967:\tlearn: 0.0872286\ttotal: 3m 6s\tremaining: 6.16s\n",
      "968:\tlearn: 0.0872150\ttotal: 3m 6s\tremaining: 5.96s\n",
      "969:\tlearn: 0.0872063\ttotal: 3m 6s\tremaining: 5.77s\n",
      "970:\tlearn: 0.0871474\ttotal: 3m 6s\tremaining: 5.58s\n",
      "971:\tlearn: 0.0871375\ttotal: 3m 6s\tremaining: 5.38s\n",
      "972:\tlearn: 0.0871074\ttotal: 3m 7s\tremaining: 5.19s\n",
      "973:\tlearn: 0.0870989\ttotal: 3m 7s\tremaining: 5s\n",
      "974:\tlearn: 0.0870893\ttotal: 3m 7s\tremaining: 4.81s\n",
      "975:\tlearn: 0.0870783\ttotal: 3m 7s\tremaining: 4.61s\n",
      "976:\tlearn: 0.0870498\ttotal: 3m 7s\tremaining: 4.42s\n",
      "977:\tlearn: 0.0870445\ttotal: 3m 7s\tremaining: 4.23s\n",
      "978:\tlearn: 0.0870287\ttotal: 3m 8s\tremaining: 4.04s\n",
      "979:\tlearn: 0.0870192\ttotal: 3m 8s\tremaining: 3.84s\n",
      "980:\tlearn: 0.0869052\ttotal: 3m 8s\tremaining: 3.65s\n",
      "981:\tlearn: 0.0868964\ttotal: 3m 8s\tremaining: 3.46s\n",
      "982:\tlearn: 0.0868888\ttotal: 3m 8s\tremaining: 3.27s\n",
      "983:\tlearn: 0.0868279\ttotal: 3m 9s\tremaining: 3.07s\n",
      "984:\tlearn: 0.0868176\ttotal: 3m 9s\tremaining: 2.88s\n",
      "985:\tlearn: 0.0868126\ttotal: 3m 9s\tremaining: 2.69s\n",
      "986:\tlearn: 0.0867924\ttotal: 3m 9s\tremaining: 2.5s\n",
      "987:\tlearn: 0.0867818\ttotal: 3m 9s\tremaining: 2.3s\n",
      "988:\tlearn: 0.0867690\ttotal: 3m 9s\tremaining: 2.11s\n",
      "989:\tlearn: 0.0866644\ttotal: 3m 10s\tremaining: 1.92s\n",
      "990:\tlearn: 0.0866568\ttotal: 3m 10s\tremaining: 1.73s\n",
      "991:\tlearn: 0.0866289\ttotal: 3m 10s\tremaining: 1.54s\n",
      "992:\tlearn: 0.0866089\ttotal: 3m 10s\tremaining: 1.34s\n",
      "993:\tlearn: 0.0866029\ttotal: 3m 10s\tremaining: 1.15s\n",
      "994:\tlearn: 0.0865431\ttotal: 3m 11s\tremaining: 960ms\n",
      "995:\tlearn: 0.0864065\ttotal: 3m 11s\tremaining: 768ms\n",
      "996:\tlearn: 0.0863996\ttotal: 3m 11s\tremaining: 576ms\n",
      "997:\tlearn: 0.0863875\ttotal: 3m 11s\tremaining: 384ms\n",
      "998:\tlearn: 0.0863286\ttotal: 3m 11s\tremaining: 192ms\n",
      "999:\tlearn: 0.0863156\ttotal: 3m 11s\tremaining: 0us\n",
      "CatBoostClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.95      0.96      2587\n",
      "         1.0       0.96      0.98      0.97      2979\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      5566\n",
      "   macro avg       0.96      0.96      0.96      5566\n",
      "weighted avg       0.96      0.96      0.96      5566\n",
      "\n",
      "[[2451  136]\n",
      " [  66 2913]]\n",
      "Accuracy is  96.37082285303629\n",
      "Time on model's work: 197.711 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.81      0.87      2587\n",
      "         1.0       0.85      0.96      0.90      2979\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      5566\n",
      "   macro avg       0.90      0.88      0.89      5566\n",
      "weighted avg       0.90      0.89      0.89      5566\n",
      "\n",
      "[[2103  484]\n",
      " [ 130 2849]]\n",
      "Accuracy is  88.96873877111031\n",
      "Time on model's work: 0.633 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.83      0.87      2587\n",
      "         1.0       0.86      0.93      0.90      2979\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      5566\n",
      "   macro avg       0.89      0.88      0.88      5566\n",
      "weighted avg       0.89      0.88      0.88      5566\n",
      "\n",
      "[[2143  444]\n",
      " [ 197 2782]]\n",
      "Accuracy is  88.48365073661516\n",
      "Time on model's work: 0.382 s\n",
      "====================================================================================================\n",
      "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n",
      "TFFMClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.82      0.88      2587\n",
      "         1.0       0.86      0.96      0.91      2979\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      5566\n",
      "   macro avg       0.90      0.89      0.89      5566\n",
      "weighted avg       0.90      0.90      0.89      5566\n",
      "\n",
      "[[2126  461]\n",
      " [ 123 2856]]\n",
      "Accuracy is  89.50772547610492\n",
      "Time on model's work: 52.556 s\n",
      "====================================================================================================\n",
      "TOTAL TIME SPENT:  1062.091 s\n"
     ]
    }
   ],
   "source": [
    "clfs = [\n",
    "        ['RandomForestClassifier', RandomForestClassifier()],\n",
    "        ['GradientBoostingClassifier', GradientBoostingClassifier()],\n",
    "        ['ExtraTreesClassifier', ExtraTreesClassifier()],\n",
    "        ['AdaBoostClassifier', AdaBoostClassifier()],\n",
    "        ['BaggingClassifier', BaggingClassifier()],\n",
    "        ['DecisionTreeClassifier', DecisionTreeClassifier()],\n",
    "        ['MLPClassifier', MLPClassifier()],\n",
    "        ['XGBClassifier', XGBClassifier()],\n",
    "        ['CatBoostClassifier', CatBoostClassifier()],\n",
    "        ['LogisticRegression', LogisticRegression()],\n",
    "        ['SGDClassifier', SGDClassifier()],\n",
    "        ['TFFMClassifier', TFFMClassifier()]\n",
    "       ]\n",
    "t = time()\n",
    "for name, clf in clfs:\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    prediction = clf.predict(X_test)\n",
    "    print(name)\n",
    "    print(classification_report(y_test, prediction))\n",
    "    print(confusion_matrix(y_test, prediction))\n",
    "    print('Accuracy is ', accuracy_score(y_test, prediction)*100)\n",
    "    print (\"Time on model's work:\", round(time()-t0, 3), \"s\")\n",
    "    print('='*100)\n",
    "print (\"TOTAL TIME SPENT: \", round(time()-t, 3), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:09<00:00,  5.15epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8979518505210204\n",
      "[[2140  447]\n",
      " [ 121 2858]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.83      0.88      2587\n",
      "         1.0       0.86      0.96      0.91      2979\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      5566\n",
      "   macro avg       0.91      0.89      0.90      5566\n",
      "weighted avg       0.90      0.90      0.90      5566\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:09<00:00,  5.18epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8981315127560187\n",
      "[[2088  499]\n",
      " [  68 2911]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.81      0.88      2587\n",
      "         1.0       0.85      0.98      0.91      2979\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      5566\n",
      "   macro avg       0.91      0.89      0.90      5566\n",
      "weighted avg       0.91      0.90      0.90      5566\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:09<00:00,  5.16epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8889687387711103\n",
      "[[2020  567]\n",
      " [  51 2928]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.78      0.87      2587\n",
      "         1.0       0.84      0.98      0.90      2979\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      5566\n",
      "   macro avg       0.91      0.88      0.89      5566\n",
      "weighted avg       0.90      0.89      0.89      5566\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:09<00:00,  5.16epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8842975206611571\n",
      "[[1978  609]\n",
      " [  35 2944]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.76      0.86      2587\n",
      "         1.0       0.83      0.99      0.90      2979\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      5566\n",
      "   macro avg       0.91      0.88      0.88      5566\n",
      "weighted avg       0.90      0.88      0.88      5566\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TFFM sparse - works worse with sparse\n",
    "# only CSR format supported\n",
    "X_train_sparse = sp.csr_matrix(X_train)\n",
    "X_test_sparse = sp.csr_matrix(X_test)\n",
    "# weight - optional / AdamOptimizer\n",
    "pos_class_weight = list(map(float,range(1, 5)))\n",
    "for weight in pos_class_weight:\n",
    "    model = TFFMClassifier(\n",
    "        order=2,\n",
    "        pos_class_weight=weight,\n",
    "        rank=10, \n",
    "        optimizer=tf.train.AdamOptimizer(learning_rate=0.001), \n",
    "        n_epochs=50, \n",
    "        batch_size=1024,\n",
    "        init_std=0.001,\n",
    "        reg=0.01,\n",
    "        input_type='sparse',\n",
    "        #log_dir='./tmp/logs',\n",
    "        #verbose=1,\n",
    "        seed=42\n",
    "    )\n",
    "    model.fit(X_train_sparse, y_train, show_progress=True)\n",
    "    predictions = model.predict(X_test_sparse)\n",
    "    print('accuracy: {}'.format(accuracy_score(y_test, predictions)))\n",
    "    # this will close tf.Session and free resources\n",
    "    print(confusion_matrix(y_test,predictions)) \n",
    "    print(classification_report(y_test, predictions))\n",
    "    model.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:10<00:00,  4.73epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8512396694214877\n",
      "[[2063  524]\n",
      " [ 304 2675]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.80      0.83      2587\n",
      "         1.0       0.84      0.90      0.87      2979\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      5566\n",
      "   macro avg       0.85      0.85      0.85      5566\n",
      "weighted avg       0.85      0.85      0.85      5566\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:10<00:00,  5.01epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8681279195113187\n",
      "[[2043  544]\n",
      " [ 190 2789]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.79      0.85      2587\n",
      "         1.0       0.84      0.94      0.88      2979\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      5566\n",
      "   macro avg       0.88      0.86      0.87      5566\n",
      "weighted avg       0.87      0.87      0.87      5566\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:10<00:00,  5.02epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8607617678763924\n",
      "[[1964  623]\n",
      " [ 152 2827]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.76      0.84      2587\n",
      "         1.0       0.82      0.95      0.88      2979\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      5566\n",
      "   macro avg       0.87      0.85      0.86      5566\n",
      "weighted avg       0.87      0.86      0.86      5566\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:10<00:00,  4.99epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8559108875314408\n",
      "[[1894  693]\n",
      " [ 109 2870]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.73      0.83      2587\n",
      "         1.0       0.81      0.96      0.88      2979\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      5566\n",
      "   macro avg       0.88      0.85      0.85      5566\n",
      "weighted avg       0.87      0.86      0.85      5566\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# weight - optional / FtrlOptimizer\n",
    "pos_class_weight = list(map(float,range(1, 5)))\n",
    "for weight in pos_class_weight:\n",
    "    model = TFFMClassifier(\n",
    "        order=2,\n",
    "        pos_class_weight=weight,\n",
    "        rank=10, \n",
    "        optimizer=tf.train.FtrlOptimizer(0.01, l2_regularization_strength=0.001, l2_shrinkage_regularization_strength=0.001), \n",
    "        n_epochs=50, \n",
    "        batch_size=1024,\n",
    "        init_std=0.001,\n",
    "        reg=0.01,\n",
    "        input_type='sparse',\n",
    "        #log_dir='./tmp/logs',\n",
    "        #verbose=1,\n",
    "        seed=42\n",
    "    )\n",
    "    model.fit(X_train_sparse, y_train, show_progress=True)\n",
    "    predictions = model.predict(X_test_sparse)\n",
    "    print('accuracy: {}'.format(accuracy_score(y_test, predictions)))\n",
    "    # this will close tf.Session and free resources\n",
    "    print(confusion_matrix(y_test,predictions)) \n",
    "    print(classification_report(y_test, predictions))\n",
    "    model.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "22262/22262 [==============================] - ETA: 22s - loss: 0.8499 - acc: 0.45 - ETA: 4s - loss: 0.7651 - acc: 0.5176 - ETA: 2s - loss: 0.7428 - acc: 0.535 - ETA: 2s - loss: 0.7325 - acc: 0.541 - ETA: 2s - loss: 0.7277 - acc: 0.544 - ETA: 1s - loss: 0.7165 - acc: 0.556 - ETA: 1s - loss: 0.7106 - acc: 0.560 - ETA: 1s - loss: 0.7063 - acc: 0.564 - ETA: 1s - loss: 0.7018 - acc: 0.568 - ETA: 1s - loss: 0.6943 - acc: 0.578 - ETA: 1s - loss: 0.6818 - acc: 0.591 - ETA: 1s - loss: 0.6740 - acc: 0.600 - ETA: 1s - loss: 0.6671 - acc: 0.609 - ETA: 0s - loss: 0.6612 - acc: 0.616 - ETA: 0s - loss: 0.6504 - acc: 0.629 - ETA: 0s - loss: 0.6422 - acc: 0.639 - ETA: 0s - loss: 0.6326 - acc: 0.650 - ETA: 0s - loss: 0.6261 - acc: 0.657 - ETA: 0s - loss: 0.6174 - acc: 0.666 - ETA: 0s - loss: 0.6095 - acc: 0.674 - ETA: 0s - loss: 0.6011 - acc: 0.683 - ETA: 0s - loss: 0.5960 - acc: 0.687 - ETA: 0s - loss: 0.5902 - acc: 0.694 - 2s 70us/step - loss: 0.5851 - acc: 0.6992\n",
      "Epoch 2/20\n",
      "22262/22262 [==============================] - ETA: 1s - loss: 0.3602 - acc: 0.875 - ETA: 1s - loss: 0.4358 - acc: 0.835 - ETA: 1s - loss: 0.4570 - acc: 0.827 - ETA: 1s - loss: 0.4644 - acc: 0.820 - ETA: 1s - loss: 0.4568 - acc: 0.826 - ETA: 1s - loss: 0.4543 - acc: 0.828 - ETA: 0s - loss: 0.4515 - acc: 0.829 - ETA: 0s - loss: 0.4497 - acc: 0.829 - ETA: 0s - loss: 0.4503 - acc: 0.830 - ETA: 0s - loss: 0.4545 - acc: 0.828 - ETA: 0s - loss: 0.4520 - acc: 0.829 - ETA: 0s - loss: 0.4533 - acc: 0.829 - ETA: 0s - loss: 0.4501 - acc: 0.831 - ETA: 0s - loss: 0.4475 - acc: 0.833 - ETA: 0s - loss: 0.4467 - acc: 0.833 - ETA: 0s - loss: 0.4453 - acc: 0.833 - ETA: 0s - loss: 0.4453 - acc: 0.834 - ETA: 0s - loss: 0.4420 - acc: 0.835 - ETA: 0s - loss: 0.4406 - acc: 0.836 - ETA: 0s - loss: 0.4421 - acc: 0.835 - ETA: 0s - loss: 0.4416 - acc: 0.836 - ETA: 0s - loss: 0.4417 - acc: 0.836 - 1s 57us/step - loss: 0.4412 - acc: 0.8364\n",
      "Epoch 3/20\n",
      "22262/22262 [==============================] - ETA: 1s - loss: 0.4310 - acc: 0.839 - ETA: 1s - loss: 0.4410 - acc: 0.843 - ETA: 1s - loss: 0.4419 - acc: 0.836 - ETA: 1s - loss: 0.4346 - acc: 0.838 - ETA: 1s - loss: 0.4313 - acc: 0.840 - ETA: 1s - loss: 0.4334 - acc: 0.840 - ETA: 1s - loss: 0.4262 - acc: 0.844 - ETA: 0s - loss: 0.4221 - acc: 0.845 - ETA: 0s - loss: 0.4175 - acc: 0.848 - ETA: 0s - loss: 0.4168 - acc: 0.848 - ETA: 0s - loss: 0.4150 - acc: 0.849 - ETA: 0s - loss: 0.4131 - acc: 0.849 - ETA: 0s - loss: 0.4152 - acc: 0.847 - ETA: 0s - loss: 0.4141 - acc: 0.848 - ETA: 0s - loss: 0.4153 - acc: 0.847 - ETA: 0s - loss: 0.4144 - acc: 0.847 - ETA: 0s - loss: 0.4151 - acc: 0.847 - ETA: 0s - loss: 0.4146 - acc: 0.847 - ETA: 0s - loss: 0.4140 - acc: 0.847 - ETA: 0s - loss: 0.4148 - acc: 0.846 - ETA: 0s - loss: 0.4124 - acc: 0.847 - ETA: 0s - loss: 0.4119 - acc: 0.847 - 1s 57us/step - loss: 0.4104 - acc: 0.8480\n",
      "Epoch 4/20\n",
      "22262/22262 [==============================] - ETA: 1s - loss: 0.4309 - acc: 0.843 - ETA: 1s - loss: 0.4094 - acc: 0.846 - ETA: 1s - loss: 0.4143 - acc: 0.844 - ETA: 1s - loss: 0.4114 - acc: 0.845 - ETA: 1s - loss: 0.4105 - acc: 0.846 - ETA: 1s - loss: 0.4082 - acc: 0.848 - ETA: 0s - loss: 0.4042 - acc: 0.851 - ETA: 0s - loss: 0.4038 - acc: 0.851 - ETA: 0s - loss: 0.4005 - acc: 0.852 - ETA: 0s - loss: 0.4050 - acc: 0.850 - ETA: 0s - loss: 0.4019 - acc: 0.851 - ETA: 0s - loss: 0.3997 - acc: 0.852 - ETA: 0s - loss: 0.3986 - acc: 0.852 - ETA: 0s - loss: 0.3984 - acc: 0.852 - ETA: 0s - loss: 0.3979 - acc: 0.852 - ETA: 0s - loss: 0.3974 - acc: 0.853 - ETA: 0s - loss: 0.3962 - acc: 0.853 - ETA: 0s - loss: 0.3954 - acc: 0.854 - ETA: 0s - loss: 0.3956 - acc: 0.854 - ETA: 0s - loss: 0.3947 - acc: 0.854 - ETA: 0s - loss: 0.3936 - acc: 0.854 - ETA: 0s - loss: 0.3929 - acc: 0.854 - 1s 56us/step - loss: 0.3918 - acc: 0.8551\n",
      "Epoch 5/20\n",
      "22262/22262 [==============================] - ETA: 1s - loss: 0.2929 - acc: 0.894 - ETA: 1s - loss: 0.3407 - acc: 0.883 - ETA: 1s - loss: 0.3633 - acc: 0.869 - ETA: 1s - loss: 0.3727 - acc: 0.866 - ETA: 0s - loss: 0.3702 - acc: 0.865 - ETA: 0s - loss: 0.3712 - acc: 0.865 - ETA: 0s - loss: 0.3677 - acc: 0.867 - ETA: 0s - loss: 0.3743 - acc: 0.864 - ETA: 0s - loss: 0.3777 - acc: 0.862 - ETA: 0s - loss: 0.3776 - acc: 0.861 - ETA: 0s - loss: 0.3779 - acc: 0.861 - ETA: 0s - loss: 0.3778 - acc: 0.860 - ETA: 0s - loss: 0.3783 - acc: 0.860 - ETA: 0s - loss: 0.3806 - acc: 0.859 - ETA: 0s - loss: 0.3790 - acc: 0.860 - ETA: 0s - loss: 0.3815 - acc: 0.859 - ETA: 0s - loss: 0.3788 - acc: 0.861 - ETA: 0s - loss: 0.3772 - acc: 0.861 - ETA: 0s - loss: 0.3763 - acc: 0.862 - ETA: 0s - loss: 0.3759 - acc: 0.862 - ETA: 0s - loss: 0.3785 - acc: 0.861 - ETA: 0s - loss: 0.3778 - acc: 0.861 - ETA: 0s - loss: 0.3775 - acc: 0.861 - 1s 57us/step - loss: 0.3771 - acc: 0.8618\n",
      "Epoch 6/20\n",
      "22262/22262 [==============================] - ETA: 1s - loss: 0.3624 - acc: 0.875 - ETA: 1s - loss: 0.3667 - acc: 0.863 - ETA: 1s - loss: 0.3612 - acc: 0.868 - ETA: 0s - loss: 0.3585 - acc: 0.868 - ETA: 0s - loss: 0.3573 - acc: 0.869 - ETA: 0s - loss: 0.3559 - acc: 0.870 - ETA: 0s - loss: 0.3560 - acc: 0.871 - ETA: 0s - loss: 0.3590 - acc: 0.869 - ETA: 0s - loss: 0.3586 - acc: 0.869 - ETA: 0s - loss: 0.3573 - acc: 0.869 - ETA: 0s - loss: 0.3561 - acc: 0.871 - ETA: 0s - loss: 0.3541 - acc: 0.872 - ETA: 0s - loss: 0.3515 - acc: 0.873 - ETA: 0s - loss: 0.3522 - acc: 0.873 - ETA: 0s - loss: 0.3554 - acc: 0.872 - ETA: 0s - loss: 0.3564 - acc: 0.871 - ETA: 0s - loss: 0.3587 - acc: 0.870 - ETA: 0s - loss: 0.3589 - acc: 0.870 - ETA: 0s - loss: 0.3598 - acc: 0.869 - ETA: 0s - loss: 0.3605 - acc: 0.868 - ETA: 0s - loss: 0.3626 - acc: 0.868 - ETA: 0s - loss: 0.3619 - acc: 0.868 - 1s 56us/step - loss: 0.3625 - acc: 0.8684\n",
      "Epoch 7/20\n",
      "22262/22262 [==============================] - ETA: 1s - loss: 0.3397 - acc: 0.875 - ETA: 1s - loss: 0.3618 - acc: 0.868 - ETA: 1s - loss: 0.3472 - acc: 0.871 - ETA: 1s - loss: 0.3483 - acc: 0.871 - ETA: 1s - loss: 0.3584 - acc: 0.865 - ETA: 1s - loss: 0.3565 - acc: 0.867 - ETA: 1s - loss: 0.3551 - acc: 0.868 - ETA: 0s - loss: 0.3566 - acc: 0.866 - ETA: 0s - loss: 0.3557 - acc: 0.869 - ETA: 0s - loss: 0.3582 - acc: 0.867 - ETA: 0s - loss: 0.3572 - acc: 0.867 - ETA: 0s - loss: 0.3543 - acc: 0.869 - ETA: 0s - loss: 0.3518 - acc: 0.870 - ETA: 0s - loss: 0.3527 - acc: 0.870 - ETA: 0s - loss: 0.3525 - acc: 0.869 - ETA: 0s - loss: 0.3494 - acc: 0.871 - ETA: 0s - loss: 0.3497 - acc: 0.871 - ETA: 0s - loss: 0.3495 - acc: 0.871 - ETA: 0s - loss: 0.3485 - acc: 0.872 - ETA: 0s - loss: 0.3482 - acc: 0.872 - ETA: 0s - loss: 0.3500 - acc: 0.871 - ETA: 0s - loss: 0.3489 - acc: 0.872 - ETA: 0s - loss: 0.3492 - acc: 0.872 - ETA: 0s - loss: 0.3500 - acc: 0.872 - 1s 61us/step - loss: 0.3505 - acc: 0.8721\n",
      "Epoch 8/20\n",
      "22262/22262 [==============================] - ETA: 1s - loss: 0.3490 - acc: 0.863 - ETA: 1s - loss: 0.3549 - acc: 0.866 - ETA: 1s - loss: 0.3556 - acc: 0.866 - ETA: 1s - loss: 0.3459 - acc: 0.874 - ETA: 0s - loss: 0.3435 - acc: 0.877 - ETA: 0s - loss: 0.3472 - acc: 0.877 - ETA: 0s - loss: 0.3515 - acc: 0.874 - ETA: 0s - loss: 0.3482 - acc: 0.875 - ETA: 0s - loss: 0.3472 - acc: 0.875 - ETA: 0s - loss: 0.3457 - acc: 0.876 - ETA: 0s - loss: 0.3471 - acc: 0.875 - ETA: 0s - loss: 0.3440 - acc: 0.876 - ETA: 0s - loss: 0.3461 - acc: 0.875 - ETA: 0s - loss: 0.3441 - acc: 0.877 - ETA: 0s - loss: 0.3450 - acc: 0.876 - ETA: 0s - loss: 0.3445 - acc: 0.876 - ETA: 0s - loss: 0.3440 - acc: 0.876 - ETA: 0s - loss: 0.3444 - acc: 0.875 - ETA: 0s - loss: 0.3443 - acc: 0.875 - ETA: 0s - loss: 0.3445 - acc: 0.874 - ETA: 0s - loss: 0.3429 - acc: 0.875 - ETA: 0s - loss: 0.3427 - acc: 0.875 - 1s 55us/step - loss: 0.3421 - acc: 0.8758\n",
      "Epoch 9/20\n",
      "22262/22262 [==============================] - ETA: 1s - loss: 0.3474 - acc: 0.878 - ETA: 1s - loss: 0.3356 - acc: 0.882 - ETA: 0s - loss: 0.3396 - acc: 0.879 - ETA: 0s - loss: 0.3312 - acc: 0.881 - ETA: 0s - loss: 0.3346 - acc: 0.877 - ETA: 0s - loss: 0.3396 - acc: 0.875 - ETA: 0s - loss: 0.3415 - acc: 0.874 - ETA: 0s - loss: 0.3374 - acc: 0.876 - ETA: 0s - loss: 0.3386 - acc: 0.875 - ETA: 0s - loss: 0.3386 - acc: 0.875 - ETA: 0s - loss: 0.3363 - acc: 0.876 - ETA: 0s - loss: 0.3341 - acc: 0.877 - ETA: 0s - loss: 0.3316 - acc: 0.878 - ETA: 0s - loss: 0.3337 - acc: 0.878 - ETA: 0s - loss: 0.3344 - acc: 0.878 - ETA: 0s - loss: 0.3351 - acc: 0.877 - ETA: 0s - loss: 0.3350 - acc: 0.877 - ETA: 0s - loss: 0.3361 - acc: 0.877 - ETA: 0s - loss: 0.3352 - acc: 0.877 - ETA: 0s - loss: 0.3357 - acc: 0.877 - ETA: 0s - loss: 0.3350 - acc: 0.877 - ETA: 0s - loss: 0.3353 - acc: 0.877 - 1s 57us/step - loss: 0.3358 - acc: 0.8778\n",
      "Epoch 10/20\n",
      "22262/22262 [==============================] - ETA: 1s - loss: 0.3954 - acc: 0.851 - ETA: 1s - loss: 0.3663 - acc: 0.869 - ETA: 1s - loss: 0.3500 - acc: 0.876 - ETA: 1s - loss: 0.3419 - acc: 0.879 - ETA: 1s - loss: 0.3457 - acc: 0.877 - ETA: 1s - loss: 0.3413 - acc: 0.877 - ETA: 1s - loss: 0.3390 - acc: 0.877 - ETA: 1s - loss: 0.3336 - acc: 0.878 - ETA: 1s - loss: 0.3339 - acc: 0.879 - ETA: 0s - loss: 0.3335 - acc: 0.879 - ETA: 0s - loss: 0.3318 - acc: 0.880 - ETA: 0s - loss: 0.3360 - acc: 0.878 - ETA: 0s - loss: 0.3374 - acc: 0.877 - ETA: 0s - loss: 0.3371 - acc: 0.877 - ETA: 0s - loss: 0.3325 - acc: 0.878 - ETA: 0s - loss: 0.3336 - acc: 0.878 - ETA: 0s - loss: 0.3352 - acc: 0.876 - ETA: 0s - loss: 0.3330 - acc: 0.877 - ETA: 0s - loss: 0.3337 - acc: 0.877 - ETA: 0s - loss: 0.3320 - acc: 0.878 - ETA: 0s - loss: 0.3322 - acc: 0.878 - ETA: 0s - loss: 0.3313 - acc: 0.879 - ETA: 0s - loss: 0.3303 - acc: 0.879 - 1s 59us/step - loss: 0.3292 - acc: 0.8797\n",
      "Epoch 11/20\n",
      "22262/22262 [==============================] - ETA: 1s - loss: 0.4071 - acc: 0.839 - ETA: 1s - loss: 0.3384 - acc: 0.875 - ETA: 1s - loss: 0.3329 - acc: 0.874 - ETA: 1s - loss: 0.3322 - acc: 0.876 - ETA: 1s - loss: 0.3220 - acc: 0.881 - ETA: 1s - loss: 0.3265 - acc: 0.879 - ETA: 0s - loss: 0.3291 - acc: 0.877 - ETA: 0s - loss: 0.3288 - acc: 0.877 - ETA: 0s - loss: 0.3312 - acc: 0.876 - ETA: 0s - loss: 0.3290 - acc: 0.876 - ETA: 0s - loss: 0.3280 - acc: 0.876 - ETA: 0s - loss: 0.3238 - acc: 0.879 - ETA: 0s - loss: 0.3244 - acc: 0.879 - ETA: 0s - loss: 0.3287 - acc: 0.876 - ETA: 0s - loss: 0.3290 - acc: 0.876 - ETA: 0s - loss: 0.3286 - acc: 0.877 - ETA: 0s - loss: 0.3286 - acc: 0.877 - ETA: 0s - loss: 0.3267 - acc: 0.878 - ETA: 0s - loss: 0.3254 - acc: 0.879 - ETA: 0s - loss: 0.3260 - acc: 0.878 - ETA: 0s - loss: 0.3246 - acc: 0.879 - ETA: 0s - loss: 0.3242 - acc: 0.879 - 1s 58us/step - loss: 0.3232 - acc: 0.8802\n",
      "Epoch 12/20\n",
      "22262/22262 [==============================] - ETA: 1s - loss: 0.2888 - acc: 0.890 - ETA: 1s - loss: 0.2941 - acc: 0.893 - ETA: 1s - loss: 0.3100 - acc: 0.888 - ETA: 0s - loss: 0.3116 - acc: 0.887 - ETA: 0s - loss: 0.3171 - acc: 0.885 - ETA: 0s - loss: 0.3179 - acc: 0.883 - ETA: 0s - loss: 0.3141 - acc: 0.885 - ETA: 0s - loss: 0.3166 - acc: 0.886 - ETA: 0s - loss: 0.3189 - acc: 0.884 - ETA: 0s - loss: 0.3158 - acc: 0.885 - ETA: 0s - loss: 0.3191 - acc: 0.883 - ETA: 0s - loss: 0.3164 - acc: 0.884 - ETA: 0s - loss: 0.3163 - acc: 0.883 - ETA: 0s - loss: 0.3167 - acc: 0.883 - ETA: 0s - loss: 0.3167 - acc: 0.883 - ETA: 0s - loss: 0.3165 - acc: 0.883 - ETA: 0s - loss: 0.3177 - acc: 0.882 - ETA: 0s - loss: 0.3185 - acc: 0.882 - ETA: 0s - loss: 0.3182 - acc: 0.882 - ETA: 0s - loss: 0.3178 - acc: 0.882 - ETA: 0s - loss: 0.3173 - acc: 0.882 - ETA: 0s - loss: 0.3168 - acc: 0.882 - 1s 57us/step - loss: 0.3158 - acc: 0.8830\n",
      "Epoch 13/20\n",
      "22262/22262 [==============================] - ETA: 1s - loss: 0.3586 - acc: 0.859 - ETA: 1s - loss: 0.3272 - acc: 0.881 - ETA: 1s - loss: 0.3134 - acc: 0.885 - ETA: 1s - loss: 0.3114 - acc: 0.884 - ETA: 0s - loss: 0.3102 - acc: 0.883 - ETA: 0s - loss: 0.3154 - acc: 0.881 - ETA: 0s - loss: 0.3170 - acc: 0.881 - ETA: 0s - loss: 0.3130 - acc: 0.881 - ETA: 0s - loss: 0.3132 - acc: 0.882 - ETA: 0s - loss: 0.3124 - acc: 0.882 - ETA: 0s - loss: 0.3152 - acc: 0.881 - ETA: 0s - loss: 0.3141 - acc: 0.882 - ETA: 0s - loss: 0.3138 - acc: 0.881 - ETA: 0s - loss: 0.3118 - acc: 0.882 - ETA: 0s - loss: 0.3107 - acc: 0.882 - ETA: 0s - loss: 0.3110 - acc: 0.882 - ETA: 0s - loss: 0.3124 - acc: 0.882 - ETA: 0s - loss: 0.3124 - acc: 0.882 - ETA: 0s - loss: 0.3112 - acc: 0.882 - ETA: 0s - loss: 0.3101 - acc: 0.882 - ETA: 0s - loss: 0.3108 - acc: 0.882 - ETA: 0s - loss: 0.3118 - acc: 0.882 - ETA: 0s - loss: 0.3126 - acc: 0.881 - 1s 60us/step - loss: 0.3126 - acc: 0.8820\n",
      "Epoch 14/20\n",
      "22262/22262 [==============================] - ETA: 3s - loss: 0.3653 - acc: 0.855 - ETA: 1s - loss: 0.3174 - acc: 0.880 - ETA: 1s - loss: 0.3129 - acc: 0.884 - ETA: 1s - loss: 0.3151 - acc: 0.883 - ETA: 1s - loss: 0.3068 - acc: 0.887 - ETA: 1s - loss: 0.3063 - acc: 0.887 - ETA: 1s - loss: 0.3100 - acc: 0.885 - ETA: 0s - loss: 0.3125 - acc: 0.883 - ETA: 0s - loss: 0.3135 - acc: 0.882 - ETA: 0s - loss: 0.3126 - acc: 0.882 - ETA: 0s - loss: 0.3127 - acc: 0.882 - ETA: 0s - loss: 0.3115 - acc: 0.882 - ETA: 0s - loss: 0.3147 - acc: 0.881 - ETA: 0s - loss: 0.3138 - acc: 0.882 - ETA: 0s - loss: 0.3137 - acc: 0.882 - ETA: 0s - loss: 0.3157 - acc: 0.881 - ETA: 0s - loss: 0.3142 - acc: 0.881 - ETA: 0s - loss: 0.3137 - acc: 0.881 - ETA: 0s - loss: 0.3129 - acc: 0.882 - ETA: 0s - loss: 0.3140 - acc: 0.881 - ETA: 0s - loss: 0.3119 - acc: 0.882 - ETA: 0s - loss: 0.3125 - acc: 0.882 - ETA: 0s - loss: 0.3102 - acc: 0.883 - ETA: 0s - loss: 0.3100 - acc: 0.884 - 1s 62us/step - loss: 0.3101 - acc: 0.8841\n",
      "Epoch 15/20\n",
      "22262/22262 [==============================] - ETA: 2s - loss: 0.3555 - acc: 0.878 - ETA: 1s - loss: 0.3308 - acc: 0.878 - ETA: 1s - loss: 0.3249 - acc: 0.876 - ETA: 1s - loss: 0.3147 - acc: 0.883 - ETA: 1s - loss: 0.3129 - acc: 0.883 - ETA: 1s - loss: 0.3104 - acc: 0.883 - ETA: 1s - loss: 0.3136 - acc: 0.882 - ETA: 1s - loss: 0.3081 - acc: 0.884 - ETA: 0s - loss: 0.3092 - acc: 0.884 - ETA: 0s - loss: 0.3065 - acc: 0.885 - ETA: 0s - loss: 0.3072 - acc: 0.885 - ETA: 0s - loss: 0.3045 - acc: 0.886 - ETA: 0s - loss: 0.3044 - acc: 0.886 - ETA: 0s - loss: 0.3047 - acc: 0.885 - ETA: 0s - loss: 0.3036 - acc: 0.885 - ETA: 0s - loss: 0.3039 - acc: 0.885 - ETA: 0s - loss: 0.3036 - acc: 0.886 - ETA: 0s - loss: 0.3038 - acc: 0.886 - ETA: 0s - loss: 0.3025 - acc: 0.886 - ETA: 0s - loss: 0.3046 - acc: 0.885 - ETA: 0s - loss: 0.3059 - acc: 0.885 - ETA: 0s - loss: 0.3055 - acc: 0.885 - ETA: 0s - loss: 0.3064 - acc: 0.885 - ETA: 0s - loss: 0.3061 - acc: 0.885 - 1s 63us/step - loss: 0.3072 - acc: 0.8845\n",
      "Epoch 16/20\n",
      "22262/22262 [==============================] - ETA: 1s - loss: 0.3257 - acc: 0.894 - ETA: 1s - loss: 0.3283 - acc: 0.872 - ETA: 1s - loss: 0.3216 - acc: 0.872 - ETA: 1s - loss: 0.3175 - acc: 0.875 - ETA: 1s - loss: 0.3162 - acc: 0.875 - ETA: 1s - loss: 0.3174 - acc: 0.875 - ETA: 1s - loss: 0.3131 - acc: 0.878 - ETA: 1s - loss: 0.3080 - acc: 0.880 - ETA: 0s - loss: 0.3129 - acc: 0.880 - ETA: 0s - loss: 0.3149 - acc: 0.879 - ETA: 0s - loss: 0.3152 - acc: 0.880 - ETA: 0s - loss: 0.3131 - acc: 0.880 - ETA: 0s - loss: 0.3111 - acc: 0.881 - ETA: 0s - loss: 0.3097 - acc: 0.883 - ETA: 0s - loss: 0.3075 - acc: 0.884 - ETA: 0s - loss: 0.3049 - acc: 0.884 - ETA: 0s - loss: 0.3051 - acc: 0.884 - ETA: 0s - loss: 0.3049 - acc: 0.884 - ETA: 0s - loss: 0.3042 - acc: 0.885 - ETA: 0s - loss: 0.3040 - acc: 0.885 - ETA: 0s - loss: 0.3037 - acc: 0.885 - ETA: 0s - loss: 0.3024 - acc: 0.886 - ETA: 0s - loss: 0.3028 - acc: 0.885 - ETA: 0s - loss: 0.3036 - acc: 0.885 - 1s 62us/step - loss: 0.3039 - acc: 0.8858\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22262/22262 [==============================] - ETA: 1s - loss: 0.2732 - acc: 0.886 - ETA: 1s - loss: 0.3155 - acc: 0.881 - ETA: 1s - loss: 0.3229 - acc: 0.880 - ETA: 1s - loss: 0.3264 - acc: 0.875 - ETA: 1s - loss: 0.3140 - acc: 0.877 - ETA: 1s - loss: 0.3159 - acc: 0.876 - ETA: 1s - loss: 0.3166 - acc: 0.875 - ETA: 0s - loss: 0.3171 - acc: 0.875 - ETA: 0s - loss: 0.3140 - acc: 0.876 - ETA: 0s - loss: 0.3112 - acc: 0.879 - ETA: 0s - loss: 0.3116 - acc: 0.879 - ETA: 0s - loss: 0.3071 - acc: 0.881 - ETA: 0s - loss: 0.3076 - acc: 0.881 - ETA: 0s - loss: 0.3069 - acc: 0.882 - ETA: 0s - loss: 0.3054 - acc: 0.883 - ETA: 0s - loss: 0.3059 - acc: 0.882 - ETA: 0s - loss: 0.3061 - acc: 0.882 - ETA: 0s - loss: 0.3069 - acc: 0.882 - ETA: 0s - loss: 0.3064 - acc: 0.882 - ETA: 0s - loss: 0.3047 - acc: 0.882 - ETA: 0s - loss: 0.3033 - acc: 0.883 - ETA: 0s - loss: 0.3037 - acc: 0.883 - ETA: 0s - loss: 0.3034 - acc: 0.883 - ETA: 0s - loss: 0.3016 - acc: 0.884 - 1s 62us/step - loss: 0.3013 - acc: 0.8846\n",
      "Epoch 18/20\n",
      "22262/22262 [==============================] - ETA: 1s - loss: 0.3443 - acc: 0.843 - ETA: 1s - loss: 0.2806 - acc: 0.892 - ETA: 1s - loss: 0.2944 - acc: 0.890 - ETA: 1s - loss: 0.2940 - acc: 0.890 - ETA: 1s - loss: 0.3022 - acc: 0.886 - ETA: 1s - loss: 0.2967 - acc: 0.886 - ETA: 1s - loss: 0.3007 - acc: 0.885 - ETA: 1s - loss: 0.2985 - acc: 0.887 - ETA: 1s - loss: 0.2985 - acc: 0.886 - ETA: 0s - loss: 0.2987 - acc: 0.886 - ETA: 0s - loss: 0.2977 - acc: 0.886 - ETA: 0s - loss: 0.3010 - acc: 0.884 - ETA: 0s - loss: 0.3009 - acc: 0.883 - ETA: 0s - loss: 0.3002 - acc: 0.884 - ETA: 0s - loss: 0.2962 - acc: 0.886 - ETA: 0s - loss: 0.2967 - acc: 0.885 - ETA: 0s - loss: 0.2980 - acc: 0.885 - ETA: 0s - loss: 0.2970 - acc: 0.886 - ETA: 0s - loss: 0.2965 - acc: 0.886 - ETA: 0s - loss: 0.2970 - acc: 0.885 - ETA: 0s - loss: 0.2969 - acc: 0.885 - ETA: 0s - loss: 0.2965 - acc: 0.885 - ETA: 0s - loss: 0.2968 - acc: 0.885 - ETA: 0s - loss: 0.2958 - acc: 0.886 - ETA: 0s - loss: 0.2957 - acc: 0.886 - ETA: 0s - loss: 0.2962 - acc: 0.885 - 1s 67us/step - loss: 0.2965 - acc: 0.8858\n",
      "Epoch 19/20\n",
      "22262/22262 [==============================] - ETA: 1s - loss: 0.3429 - acc: 0.871 - ETA: 1s - loss: 0.3164 - acc: 0.884 - ETA: 1s - loss: 0.3111 - acc: 0.883 - ETA: 1s - loss: 0.3057 - acc: 0.887 - ETA: 1s - loss: 0.3059 - acc: 0.883 - ETA: 0s - loss: 0.3103 - acc: 0.882 - ETA: 0s - loss: 0.3068 - acc: 0.884 - ETA: 0s - loss: 0.3071 - acc: 0.884 - ETA: 0s - loss: 0.3036 - acc: 0.884 - ETA: 0s - loss: 0.3030 - acc: 0.883 - ETA: 0s - loss: 0.3034 - acc: 0.883 - ETA: 0s - loss: 0.3039 - acc: 0.882 - ETA: 0s - loss: 0.3022 - acc: 0.883 - ETA: 0s - loss: 0.3016 - acc: 0.883 - ETA: 0s - loss: 0.2998 - acc: 0.884 - ETA: 0s - loss: 0.2973 - acc: 0.885 - ETA: 0s - loss: 0.2971 - acc: 0.885 - ETA: 0s - loss: 0.2970 - acc: 0.885 - ETA: 0s - loss: 0.2976 - acc: 0.885 - ETA: 0s - loss: 0.2978 - acc: 0.885 - ETA: 0s - loss: 0.2958 - acc: 0.885 - ETA: 0s - loss: 0.2960 - acc: 0.886 - ETA: 0s - loss: 0.2968 - acc: 0.885 - ETA: 0s - loss: 0.2964 - acc: 0.886 - 1s 63us/step - loss: 0.2974 - acc: 0.8854\n",
      "Epoch 20/20\n",
      "22262/22262 [==============================] - ETA: 1s - loss: 0.3089 - acc: 0.875 - ETA: 1s - loss: 0.2997 - acc: 0.878 - ETA: 1s - loss: 0.3117 - acc: 0.873 - ETA: 1s - loss: 0.3196 - acc: 0.869 - ETA: 1s - loss: 0.3204 - acc: 0.871 - ETA: 1s - loss: 0.3146 - acc: 0.875 - ETA: 1s - loss: 0.3128 - acc: 0.876 - ETA: 1s - loss: 0.3133 - acc: 0.876 - ETA: 0s - loss: 0.3124 - acc: 0.875 - ETA: 0s - loss: 0.3120 - acc: 0.876 - ETA: 0s - loss: 0.3090 - acc: 0.878 - ETA: 0s - loss: 0.3069 - acc: 0.880 - ETA: 0s - loss: 0.3090 - acc: 0.879 - ETA: 0s - loss: 0.3046 - acc: 0.881 - ETA: 0s - loss: 0.3057 - acc: 0.881 - ETA: 0s - loss: 0.3062 - acc: 0.882 - ETA: 0s - loss: 0.3042 - acc: 0.883 - ETA: 0s - loss: 0.3048 - acc: 0.884 - ETA: 0s - loss: 0.3024 - acc: 0.885 - ETA: 0s - loss: 0.3012 - acc: 0.885 - ETA: 0s - loss: 0.3014 - acc: 0.885 - ETA: 0s - loss: 0.2993 - acc: 0.886 - ETA: 0s - loss: 0.2983 - acc: 0.886 - ETA: 0s - loss: 0.2975 - acc: 0.886 - ETA: 0s - loss: 0.2976 - acc: 0.886 - 1s 64us/step - loss: 0.2967 - acc: 0.8871\n",
      "5566/5566 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "[0.2920728946306103, 0.8812432620450827]\n"
     ]
    }
   ],
   "source": [
    "# KERAS\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=256)\n",
    "score = model.evaluate(X_test, y_test, batch_size=256)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 18502), (1.0, 18502)]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "smote_tomek = SMOTETomek(random_state=0)\n",
    "X_resampled_smote_tomek, y_resampled_smote_tomek = smote_tomek.fit_resample(features_list_array, labels_list_array)\n",
    "print(sorted(Counter(y_resampled_smote_tomek).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled_smote_tomek, y_resampled_smote_tomek, random_state=35, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.95      0.94      3781\n",
      "         1.0       0.95      0.92      0.94      3620\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      7401\n",
      "   macro avg       0.94      0.94      0.94      7401\n",
      "weighted avg       0.94      0.94      0.94      7401\n",
      "\n",
      "[[3609  172]\n",
      " [ 282 3338]]\n",
      "Accuracy is  93.86569382515876\n",
      "Time on model's work: 3.074 s\n",
      "====================================================================================================\n",
      "GradientBoostingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.78      0.84      3781\n",
      "         1.0       0.80      0.92      0.86      3620\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      7401\n",
      "   macro avg       0.86      0.85      0.85      7401\n",
      "weighted avg       0.86      0.85      0.85      7401\n",
      "\n",
      "[[2960  821]\n",
      " [ 286 3334]]\n",
      "Accuracy is  85.04256181597081\n",
      "Time on model's work: 214.916 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.95      0.94      3781\n",
      "         1.0       0.95      0.93      0.94      3620\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      7401\n",
      "   macro avg       0.94      0.94      0.94      7401\n",
      "weighted avg       0.94      0.94      0.94      7401\n",
      "\n",
      "[[3595  186]\n",
      " [ 244 3376]]\n",
      "Accuracy is  94.18997432779355\n",
      "Time on model's work: 6.802 s\n",
      "====================================================================================================\n",
      "AdaBoostClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.73      0.80      3781\n",
      "         1.0       0.76      0.89      0.82      3620\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      7401\n",
      "   macro avg       0.82      0.81      0.81      7401\n",
      "weighted avg       0.82      0.81      0.81      7401\n",
      "\n",
      "[[2753 1028]\n",
      " [ 384 3236]]\n",
      "Accuracy is  80.92149709498716\n",
      "Time on model's work: 47.91 s\n",
      "====================================================================================================\n",
      "BaggingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.95      0.94      3781\n",
      "         1.0       0.94      0.92      0.93      3620\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      7401\n",
      "   macro avg       0.93      0.93      0.93      7401\n",
      "weighted avg       0.93      0.93      0.93      7401\n",
      "\n",
      "[[3583  198]\n",
      " [ 296 3324]]\n",
      "Accuracy is  93.32522632076746\n",
      "Time on model's work: 27.254 s\n",
      "====================================================================================================\n",
      "DecisionTreeClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.94      0.93      3781\n",
      "         1.0       0.93      0.91      0.92      3620\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      7401\n",
      "   macro avg       0.93      0.93      0.93      7401\n",
      "weighted avg       0.93      0.93      0.93      7401\n",
      "\n",
      "[[3546  235]\n",
      " [ 312 3308]]\n",
      "Accuracy is  92.609106877449\n",
      "Time on model's work: 9.242 s\n",
      "====================================================================================================\n",
      "MLPClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.90      0.93      3781\n",
      "         1.0       0.90      0.97      0.93      3620\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      7401\n",
      "   macro avg       0.93      0.93      0.93      7401\n",
      "weighted avg       0.93      0.93      0.93      7401\n",
      "\n",
      "[[3408  373]\n",
      " [ 125 3495]]\n",
      "Accuracy is  93.27117957032833\n",
      "Time on model's work: 429.025 s\n",
      "====================================================================================================\n",
      "XGBClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.75      0.82      3781\n",
      "         1.0       0.78      0.93      0.85      3620\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      7401\n",
      "   macro avg       0.85      0.84      0.83      7401\n",
      "weighted avg       0.85      0.84      0.83      7401\n",
      "\n",
      "[[2823  958]\n",
      " [ 257 3363]]\n",
      "Accuracy is  83.5832995541143\n",
      "Time on model's work: 155.216 s\n",
      "====================================================================================================\n",
      "0:\tlearn: 0.6709775\ttotal: 337ms\tremaining: 5m 36s\n",
      "1:\tlearn: 0.6517017\ttotal: 655ms\tremaining: 5m 26s\n",
      "2:\tlearn: 0.6345647\ttotal: 964ms\tremaining: 5m 20s\n",
      "3:\tlearn: 0.6198443\ttotal: 1.28s\tremaining: 5m 18s\n",
      "4:\tlearn: 0.6035702\ttotal: 1.61s\tremaining: 5m 20s\n",
      "5:\tlearn: 0.5914615\ttotal: 1.96s\tremaining: 5m 24s\n",
      "6:\tlearn: 0.5804016\ttotal: 2.34s\tremaining: 5m 31s\n",
      "7:\tlearn: 0.5708499\ttotal: 2.71s\tremaining: 5m 36s\n",
      "8:\tlearn: 0.5618688\ttotal: 3.03s\tremaining: 5m 33s\n",
      "9:\tlearn: 0.5510487\ttotal: 3.35s\tremaining: 5m 31s\n",
      "10:\tlearn: 0.5434918\ttotal: 3.67s\tremaining: 5m 29s\n",
      "11:\tlearn: 0.5375275\ttotal: 3.97s\tremaining: 5m 26s\n",
      "12:\tlearn: 0.5256602\ttotal: 4.34s\tremaining: 5m 29s\n",
      "13:\tlearn: 0.5207748\ttotal: 4.67s\tremaining: 5m 29s\n",
      "14:\tlearn: 0.5117765\ttotal: 5.09s\tremaining: 5m 34s\n",
      "15:\tlearn: 0.5034357\ttotal: 5.5s\tremaining: 5m 38s\n",
      "16:\tlearn: 0.4944214\ttotal: 5.95s\tremaining: 5m 44s\n",
      "17:\tlearn: 0.4904201\ttotal: 6.28s\tremaining: 5m 42s\n",
      "18:\tlearn: 0.4845946\ttotal: 6.7s\tremaining: 5m 46s\n",
      "19:\tlearn: 0.4751168\ttotal: 7.08s\tremaining: 5m 46s\n",
      "20:\tlearn: 0.4714052\ttotal: 7.4s\tremaining: 5m 44s\n",
      "21:\tlearn: 0.4642802\ttotal: 7.77s\tremaining: 5m 45s\n",
      "22:\tlearn: 0.4599142\ttotal: 8.18s\tremaining: 5m 47s\n",
      "23:\tlearn: 0.4571170\ttotal: 8.51s\tremaining: 5m 46s\n",
      "24:\tlearn: 0.4515086\ttotal: 8.92s\tremaining: 5m 47s\n",
      "25:\tlearn: 0.4478952\ttotal: 9.28s\tremaining: 5m 47s\n",
      "26:\tlearn: 0.4392305\ttotal: 9.71s\tremaining: 5m 49s\n",
      "27:\tlearn: 0.4328405\ttotal: 10.1s\tremaining: 5m 50s\n",
      "28:\tlearn: 0.4271826\ttotal: 10.5s\tremaining: 5m 50s\n",
      "29:\tlearn: 0.4239374\ttotal: 10.8s\tremaining: 5m 49s\n",
      "30:\tlearn: 0.4200935\ttotal: 11.2s\tremaining: 5m 48s\n",
      "31:\tlearn: 0.4170273\ttotal: 11.6s\tremaining: 5m 49s\n",
      "32:\tlearn: 0.4129131\ttotal: 11.9s\tremaining: 5m 49s\n",
      "33:\tlearn: 0.4087257\ttotal: 12.3s\tremaining: 5m 50s\n",
      "34:\tlearn: 0.4053486\ttotal: 12.7s\tremaining: 5m 50s\n",
      "35:\tlearn: 0.4031907\ttotal: 13s\tremaining: 5m 49s\n",
      "36:\tlearn: 0.3996518\ttotal: 13.4s\tremaining: 5m 48s\n",
      "37:\tlearn: 0.3962631\ttotal: 13.8s\tremaining: 5m 48s\n",
      "38:\tlearn: 0.3924589\ttotal: 14.2s\tremaining: 5m 48s\n",
      "39:\tlearn: 0.3896518\ttotal: 14.5s\tremaining: 5m 47s\n",
      "40:\tlearn: 0.3875997\ttotal: 14.9s\tremaining: 5m 49s\n",
      "41:\tlearn: 0.3839132\ttotal: 15.3s\tremaining: 5m 49s\n",
      "42:\tlearn: 0.3820182\ttotal: 15.6s\tremaining: 5m 47s\n",
      "43:\tlearn: 0.3796665\ttotal: 16s\tremaining: 5m 47s\n",
      "44:\tlearn: 0.3777502\ttotal: 16.3s\tremaining: 5m 46s\n",
      "45:\tlearn: 0.3739043\ttotal: 16.7s\tremaining: 5m 46s\n",
      "46:\tlearn: 0.3712988\ttotal: 17.1s\tremaining: 5m 45s\n",
      "47:\tlearn: 0.3687789\ttotal: 17.4s\tremaining: 5m 44s\n",
      "48:\tlearn: 0.3667262\ttotal: 17.7s\tremaining: 5m 43s\n",
      "49:\tlearn: 0.3634822\ttotal: 18.1s\tremaining: 5m 43s\n",
      "50:\tlearn: 0.3607795\ttotal: 18.5s\tremaining: 5m 44s\n",
      "51:\tlearn: 0.3577546\ttotal: 18.9s\tremaining: 5m 43s\n",
      "52:\tlearn: 0.3562806\ttotal: 19.2s\tremaining: 5m 42s\n",
      "53:\tlearn: 0.3537322\ttotal: 19.6s\tremaining: 5m 42s\n",
      "54:\tlearn: 0.3524527\ttotal: 19.9s\tremaining: 5m 41s\n",
      "55:\tlearn: 0.3499915\ttotal: 20.2s\tremaining: 5m 40s\n",
      "56:\tlearn: 0.3488666\ttotal: 20.5s\tremaining: 5m 39s\n",
      "57:\tlearn: 0.3476224\ttotal: 20.9s\tremaining: 5m 39s\n",
      "58:\tlearn: 0.3457870\ttotal: 21.2s\tremaining: 5m 38s\n",
      "59:\tlearn: 0.3443724\ttotal: 21.5s\tremaining: 5m 37s\n",
      "60:\tlearn: 0.3425940\ttotal: 21.8s\tremaining: 5m 35s\n",
      "61:\tlearn: 0.3414635\ttotal: 22.1s\tremaining: 5m 34s\n",
      "62:\tlearn: 0.3387566\ttotal: 22.5s\tremaining: 5m 34s\n",
      "63:\tlearn: 0.3378668\ttotal: 22.8s\tremaining: 5m 33s\n",
      "64:\tlearn: 0.3364238\ttotal: 23.1s\tremaining: 5m 32s\n",
      "65:\tlearn: 0.3346305\ttotal: 23.4s\tremaining: 5m 31s\n",
      "66:\tlearn: 0.3343382\ttotal: 23.7s\tremaining: 5m 29s\n",
      "67:\tlearn: 0.3332689\ttotal: 23.9s\tremaining: 5m 28s\n",
      "68:\tlearn: 0.3313726\ttotal: 24.3s\tremaining: 5m 27s\n",
      "69:\tlearn: 0.3303140\ttotal: 24.6s\tremaining: 5m 26s\n",
      "70:\tlearn: 0.3289358\ttotal: 24.9s\tremaining: 5m 26s\n",
      "71:\tlearn: 0.3277526\ttotal: 25.2s\tremaining: 5m 24s\n",
      "72:\tlearn: 0.3265960\ttotal: 25.5s\tremaining: 5m 23s\n",
      "73:\tlearn: 0.3257008\ttotal: 25.8s\tremaining: 5m 22s\n",
      "74:\tlearn: 0.3243888\ttotal: 26.1s\tremaining: 5m 21s\n",
      "75:\tlearn: 0.3233950\ttotal: 26.4s\tremaining: 5m 20s\n",
      "76:\tlearn: 0.3231837\ttotal: 26.6s\tremaining: 5m 19s\n",
      "77:\tlearn: 0.3222009\ttotal: 26.9s\tremaining: 5m 18s\n",
      "78:\tlearn: 0.3205081\ttotal: 27.2s\tremaining: 5m 17s\n",
      "79:\tlearn: 0.3193444\ttotal: 27.5s\tremaining: 5m 16s\n",
      "80:\tlearn: 0.3182223\ttotal: 27.8s\tremaining: 5m 15s\n",
      "81:\tlearn: 0.3172606\ttotal: 28.1s\tremaining: 5m 14s\n",
      "82:\tlearn: 0.3162474\ttotal: 28.4s\tremaining: 5m 14s\n",
      "83:\tlearn: 0.3158023\ttotal: 28.7s\tremaining: 5m 13s\n",
      "84:\tlearn: 0.3133580\ttotal: 29.1s\tremaining: 5m 13s\n",
      "85:\tlearn: 0.3126152\ttotal: 29.4s\tremaining: 5m 12s\n",
      "86:\tlearn: 0.3114495\ttotal: 29.7s\tremaining: 5m 11s\n",
      "87:\tlearn: 0.3107097\ttotal: 30s\tremaining: 5m 11s\n",
      "88:\tlearn: 0.3104813\ttotal: 30.3s\tremaining: 5m 9s\n",
      "89:\tlearn: 0.3083270\ttotal: 30.7s\tremaining: 5m 9s\n",
      "90:\tlearn: 0.3074490\ttotal: 31s\tremaining: 5m 9s\n",
      "91:\tlearn: 0.3064862\ttotal: 31.3s\tremaining: 5m 8s\n",
      "92:\tlearn: 0.3062586\ttotal: 31.5s\tremaining: 5m 7s\n",
      "93:\tlearn: 0.3054431\ttotal: 31.8s\tremaining: 5m 6s\n",
      "94:\tlearn: 0.3047182\ttotal: 32.2s\tremaining: 5m 6s\n",
      "95:\tlearn: 0.3042829\ttotal: 32.4s\tremaining: 5m 5s\n",
      "96:\tlearn: 0.3023581\ttotal: 32.7s\tremaining: 5m 4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97:\tlearn: 0.3020851\ttotal: 33.1s\tremaining: 5m 4s\n",
      "98:\tlearn: 0.3014498\ttotal: 33.4s\tremaining: 5m 3s\n",
      "99:\tlearn: 0.3005859\ttotal: 33.7s\tremaining: 5m 3s\n",
      "100:\tlearn: 0.2992419\ttotal: 34s\tremaining: 5m 2s\n",
      "101:\tlearn: 0.2989841\ttotal: 34.3s\tremaining: 5m 1s\n",
      "102:\tlearn: 0.2988311\ttotal: 34.5s\tremaining: 5m\n",
      "103:\tlearn: 0.2983429\ttotal: 34.8s\tremaining: 4m 59s\n",
      "104:\tlearn: 0.2981288\ttotal: 35.1s\tremaining: 4m 58s\n",
      "105:\tlearn: 0.2976077\ttotal: 35.4s\tremaining: 4m 58s\n",
      "106:\tlearn: 0.2970635\ttotal: 35.7s\tremaining: 4m 57s\n",
      "107:\tlearn: 0.2963908\ttotal: 35.9s\tremaining: 4m 56s\n",
      "108:\tlearn: 0.2948922\ttotal: 36.3s\tremaining: 4m 56s\n",
      "109:\tlearn: 0.2944404\ttotal: 36.6s\tremaining: 4m 55s\n",
      "110:\tlearn: 0.2941960\ttotal: 36.8s\tremaining: 4m 55s\n",
      "111:\tlearn: 0.2934196\ttotal: 37.1s\tremaining: 4m 54s\n",
      "112:\tlearn: 0.2928920\ttotal: 37.5s\tremaining: 4m 53s\n",
      "113:\tlearn: 0.2921070\ttotal: 37.7s\tremaining: 4m 53s\n",
      "114:\tlearn: 0.2916729\ttotal: 38s\tremaining: 4m 52s\n",
      "115:\tlearn: 0.2914540\ttotal: 38.3s\tremaining: 4m 52s\n",
      "116:\tlearn: 0.2912357\ttotal: 38.6s\tremaining: 4m 51s\n",
      "117:\tlearn: 0.2904068\ttotal: 38.9s\tremaining: 4m 50s\n",
      "118:\tlearn: 0.2890474\ttotal: 39.3s\tremaining: 4m 50s\n",
      "119:\tlearn: 0.2888558\ttotal: 39.6s\tremaining: 4m 50s\n",
      "120:\tlearn: 0.2887044\ttotal: 39.9s\tremaining: 4m 49s\n",
      "121:\tlearn: 0.2881123\ttotal: 40.1s\tremaining: 4m 48s\n",
      "122:\tlearn: 0.2877026\ttotal: 40.4s\tremaining: 4m 47s\n",
      "123:\tlearn: 0.2871931\ttotal: 40.7s\tremaining: 4m 47s\n",
      "124:\tlearn: 0.2870160\ttotal: 40.9s\tremaining: 4m 46s\n",
      "125:\tlearn: 0.2857253\ttotal: 41.3s\tremaining: 4m 46s\n",
      "126:\tlearn: 0.2848986\ttotal: 41.7s\tremaining: 4m 46s\n",
      "127:\tlearn: 0.2840846\ttotal: 41.9s\tremaining: 4m 45s\n",
      "128:\tlearn: 0.2837475\ttotal: 42.2s\tremaining: 4m 44s\n",
      "129:\tlearn: 0.2834683\ttotal: 42.4s\tremaining: 4m 44s\n",
      "130:\tlearn: 0.2829706\ttotal: 42.7s\tremaining: 4m 43s\n",
      "131:\tlearn: 0.2827949\ttotal: 43s\tremaining: 4m 42s\n",
      "132:\tlearn: 0.2827067\ttotal: 43.2s\tremaining: 4m 41s\n",
      "133:\tlearn: 0.2817073\ttotal: 43.5s\tremaining: 4m 41s\n",
      "134:\tlearn: 0.2815283\ttotal: 43.8s\tremaining: 4m 40s\n",
      "135:\tlearn: 0.2811492\ttotal: 44s\tremaining: 4m 39s\n",
      "136:\tlearn: 0.2804964\ttotal: 44.3s\tremaining: 4m 39s\n",
      "137:\tlearn: 0.2797156\ttotal: 44.6s\tremaining: 4m 38s\n",
      "138:\tlearn: 0.2789486\ttotal: 44.9s\tremaining: 4m 38s\n",
      "139:\tlearn: 0.2781995\ttotal: 45.3s\tremaining: 4m 38s\n",
      "140:\tlearn: 0.2778189\ttotal: 45.5s\tremaining: 4m 37s\n",
      "141:\tlearn: 0.2773107\ttotal: 45.8s\tremaining: 4m 36s\n",
      "142:\tlearn: 0.2769446\ttotal: 46.1s\tremaining: 4m 36s\n",
      "143:\tlearn: 0.2768077\ttotal: 46.4s\tremaining: 4m 35s\n",
      "144:\tlearn: 0.2749474\ttotal: 46.6s\tremaining: 4m 35s\n",
      "145:\tlearn: 0.2741328\ttotal: 47s\tremaining: 4m 34s\n",
      "146:\tlearn: 0.2727736\ttotal: 47.4s\tremaining: 4m 34s\n",
      "147:\tlearn: 0.2720462\ttotal: 47.6s\tremaining: 4m 34s\n",
      "148:\tlearn: 0.2719267\ttotal: 48s\tremaining: 4m 34s\n",
      "149:\tlearn: 0.2716208\ttotal: 48.3s\tremaining: 4m 33s\n",
      "150:\tlearn: 0.2703519\ttotal: 48.7s\tremaining: 4m 34s\n",
      "151:\tlearn: 0.2695075\ttotal: 49.1s\tremaining: 4m 34s\n",
      "152:\tlearn: 0.2693085\ttotal: 49.4s\tremaining: 4m 33s\n",
      "153:\tlearn: 0.2691835\ttotal: 49.7s\tremaining: 4m 33s\n",
      "154:\tlearn: 0.2690325\ttotal: 50s\tremaining: 4m 32s\n",
      "155:\tlearn: 0.2683258\ttotal: 50.3s\tremaining: 4m 32s\n",
      "156:\tlearn: 0.2679267\ttotal: 50.7s\tremaining: 4m 32s\n",
      "157:\tlearn: 0.2677572\ttotal: 51s\tremaining: 4m 31s\n",
      "158:\tlearn: 0.2670771\ttotal: 51.4s\tremaining: 4m 31s\n",
      "159:\tlearn: 0.2664579\ttotal: 51.7s\tremaining: 4m 31s\n",
      "160:\tlearn: 0.2663436\ttotal: 52s\tremaining: 4m 31s\n",
      "161:\tlearn: 0.2659411\ttotal: 52.4s\tremaining: 4m 30s\n",
      "162:\tlearn: 0.2657985\ttotal: 52.7s\tremaining: 4m 30s\n",
      "163:\tlearn: 0.2656529\ttotal: 53s\tremaining: 4m 30s\n",
      "164:\tlearn: 0.2651924\ttotal: 53.3s\tremaining: 4m 29s\n",
      "165:\tlearn: 0.2645262\ttotal: 53.7s\tremaining: 4m 29s\n",
      "166:\tlearn: 0.2637477\ttotal: 54s\tremaining: 4m 29s\n",
      "167:\tlearn: 0.2628926\ttotal: 54.4s\tremaining: 4m 29s\n",
      "168:\tlearn: 0.2628308\ttotal: 54.6s\tremaining: 4m 28s\n",
      "169:\tlearn: 0.2623027\ttotal: 55s\tremaining: 4m 28s\n",
      "170:\tlearn: 0.2616908\ttotal: 55.3s\tremaining: 4m 28s\n",
      "171:\tlearn: 0.2615963\ttotal: 55.7s\tremaining: 4m 28s\n",
      "172:\tlearn: 0.2612795\ttotal: 56.1s\tremaining: 4m 28s\n",
      "173:\tlearn: 0.2610686\ttotal: 56.5s\tremaining: 4m 28s\n",
      "174:\tlearn: 0.2609811\ttotal: 56.8s\tremaining: 4m 27s\n",
      "175:\tlearn: 0.2608511\ttotal: 57.1s\tremaining: 4m 27s\n",
      "176:\tlearn: 0.2607819\ttotal: 57.3s\tremaining: 4m 26s\n",
      "177:\tlearn: 0.2602885\ttotal: 57.7s\tremaining: 4m 26s\n",
      "178:\tlearn: 0.2601655\ttotal: 58s\tremaining: 4m 25s\n",
      "179:\tlearn: 0.2600962\ttotal: 58.2s\tremaining: 4m 25s\n",
      "180:\tlearn: 0.2597393\ttotal: 58.5s\tremaining: 4m 24s\n",
      "181:\tlearn: 0.2585363\ttotal: 58.8s\tremaining: 4m 24s\n",
      "182:\tlearn: 0.2578637\ttotal: 59.2s\tremaining: 4m 24s\n",
      "183:\tlearn: 0.2578143\ttotal: 59.4s\tremaining: 4m 23s\n",
      "184:\tlearn: 0.2573667\ttotal: 59.7s\tremaining: 4m 23s\n",
      "185:\tlearn: 0.2569670\ttotal: 1m\tremaining: 4m 22s\n",
      "186:\tlearn: 0.2564251\ttotal: 1m\tremaining: 4m 22s\n",
      "187:\tlearn: 0.2561429\ttotal: 1m\tremaining: 4m 21s\n",
      "188:\tlearn: 0.2560633\ttotal: 1m\tremaining: 4m 20s\n",
      "189:\tlearn: 0.2559818\ttotal: 1m 1s\tremaining: 4m 20s\n",
      "190:\tlearn: 0.2556881\ttotal: 1m 1s\tremaining: 4m 19s\n",
      "191:\tlearn: 0.2550376\ttotal: 1m 1s\tremaining: 4m 19s\n",
      "192:\tlearn: 0.2542364\ttotal: 1m 2s\tremaining: 4m 19s\n",
      "193:\tlearn: 0.2537779\ttotal: 1m 2s\tremaining: 4m 18s\n",
      "194:\tlearn: 0.2532243\ttotal: 1m 2s\tremaining: 4m 18s\n",
      "195:\tlearn: 0.2531126\ttotal: 1m 2s\tremaining: 4m 17s\n",
      "196:\tlearn: 0.2530086\ttotal: 1m 3s\tremaining: 4m 17s\n",
      "197:\tlearn: 0.2526346\ttotal: 1m 3s\tremaining: 4m 17s\n",
      "198:\tlearn: 0.2525678\ttotal: 1m 3s\tremaining: 4m 16s\n",
      "199:\tlearn: 0.2521290\ttotal: 1m 4s\tremaining: 4m 16s\n",
      "200:\tlearn: 0.2520362\ttotal: 1m 4s\tremaining: 4m 15s\n",
      "201:\tlearn: 0.2516472\ttotal: 1m 4s\tremaining: 4m 14s\n",
      "202:\tlearn: 0.2514283\ttotal: 1m 4s\tremaining: 4m 14s\n",
      "203:\tlearn: 0.2508304\ttotal: 1m 5s\tremaining: 4m 14s\n",
      "204:\tlearn: 0.2505666\ttotal: 1m 5s\tremaining: 4m 13s\n",
      "205:\tlearn: 0.2501594\ttotal: 1m 5s\tremaining: 4m 13s\n",
      "206:\tlearn: 0.2498662\ttotal: 1m 6s\tremaining: 4m 12s\n",
      "207:\tlearn: 0.2496174\ttotal: 1m 6s\tremaining: 4m 12s\n",
      "208:\tlearn: 0.2490232\ttotal: 1m 6s\tremaining: 4m 12s\n",
      "209:\tlearn: 0.2485959\ttotal: 1m 6s\tremaining: 4m 11s\n",
      "210:\tlearn: 0.2482664\ttotal: 1m 7s\tremaining: 4m 11s\n",
      "211:\tlearn: 0.2478433\ttotal: 1m 7s\tremaining: 4m 10s\n",
      "212:\tlearn: 0.2471437\ttotal: 1m 7s\tremaining: 4m 10s\n",
      "213:\tlearn: 0.2468102\ttotal: 1m 8s\tremaining: 4m 9s\n",
      "214:\tlearn: 0.2455001\ttotal: 1m 8s\tremaining: 4m 9s\n",
      "215:\tlearn: 0.2453131\ttotal: 1m 8s\tremaining: 4m 8s\n",
      "216:\tlearn: 0.2452079\ttotal: 1m 8s\tremaining: 4m 8s\n",
      "217:\tlearn: 0.2446984\ttotal: 1m 9s\tremaining: 4m 8s\n",
      "218:\tlearn: 0.2445474\ttotal: 1m 9s\tremaining: 4m 7s\n",
      "219:\tlearn: 0.2442383\ttotal: 1m 9s\tremaining: 4m 7s\n",
      "220:\tlearn: 0.2437053\ttotal: 1m 10s\tremaining: 4m 6s\n",
      "221:\tlearn: 0.2435448\ttotal: 1m 10s\tremaining: 4m 6s\n",
      "222:\tlearn: 0.2431696\ttotal: 1m 10s\tremaining: 4m 5s\n",
      "223:\tlearn: 0.2430922\ttotal: 1m 10s\tremaining: 4m 5s\n",
      "224:\tlearn: 0.2428073\ttotal: 1m 11s\tremaining: 4m 4s\n",
      "225:\tlearn: 0.2424758\ttotal: 1m 11s\tremaining: 4m 4s\n",
      "226:\tlearn: 0.2422086\ttotal: 1m 11s\tremaining: 4m 4s\n",
      "227:\tlearn: 0.2413153\ttotal: 1m 12s\tremaining: 4m 3s\n",
      "228:\tlearn: 0.2407767\ttotal: 1m 12s\tremaining: 4m 3s\n",
      "229:\tlearn: 0.2406676\ttotal: 1m 12s\tremaining: 4m 3s\n",
      "230:\tlearn: 0.2401071\ttotal: 1m 12s\tremaining: 4m 2s\n",
      "231:\tlearn: 0.2392956\ttotal: 1m 13s\tremaining: 4m 2s\n",
      "232:\tlearn: 0.2391702\ttotal: 1m 13s\tremaining: 4m 2s\n",
      "233:\tlearn: 0.2390145\ttotal: 1m 13s\tremaining: 4m 1s\n",
      "234:\tlearn: 0.2386858\ttotal: 1m 14s\tremaining: 4m 1s\n",
      "235:\tlearn: 0.2382123\ttotal: 1m 14s\tremaining: 4m 1s\n",
      "236:\tlearn: 0.2380804\ttotal: 1m 14s\tremaining: 4m\n",
      "237:\tlearn: 0.2377149\ttotal: 1m 14s\tremaining: 4m\n",
      "238:\tlearn: 0.2371901\ttotal: 1m 15s\tremaining: 3m 59s\n",
      "239:\tlearn: 0.2364891\ttotal: 1m 15s\tremaining: 3m 59s\n",
      "240:\tlearn: 0.2361198\ttotal: 1m 15s\tremaining: 3m 59s\n",
      "241:\tlearn: 0.2358830\ttotal: 1m 16s\tremaining: 3m 58s\n",
      "242:\tlearn: 0.2351669\ttotal: 1m 16s\tremaining: 3m 58s\n",
      "243:\tlearn: 0.2349709\ttotal: 1m 16s\tremaining: 3m 57s\n",
      "244:\tlearn: 0.2345382\ttotal: 1m 17s\tremaining: 3m 57s\n",
      "245:\tlearn: 0.2342567\ttotal: 1m 17s\tremaining: 3m 57s\n",
      "246:\tlearn: 0.2341567\ttotal: 1m 17s\tremaining: 3m 56s\n",
      "247:\tlearn: 0.2332955\ttotal: 1m 17s\tremaining: 3m 56s\n",
      "248:\tlearn: 0.2327397\ttotal: 1m 18s\tremaining: 3m 56s\n",
      "249:\tlearn: 0.2326558\ttotal: 1m 18s\tremaining: 3m 55s\n",
      "250:\tlearn: 0.2318606\ttotal: 1m 18s\tremaining: 3m 55s\n",
      "251:\tlearn: 0.2316597\ttotal: 1m 19s\tremaining: 3m 54s\n",
      "252:\tlearn: 0.2315762\ttotal: 1m 19s\tremaining: 3m 54s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253:\tlearn: 0.2304796\ttotal: 1m 19s\tremaining: 3m 54s\n",
      "254:\tlearn: 0.2299709\ttotal: 1m 20s\tremaining: 3m 53s\n",
      "255:\tlearn: 0.2296216\ttotal: 1m 20s\tremaining: 3m 53s\n",
      "256:\tlearn: 0.2292042\ttotal: 1m 20s\tremaining: 3m 53s\n",
      "257:\tlearn: 0.2286464\ttotal: 1m 21s\tremaining: 3m 52s\n",
      "258:\tlearn: 0.2280058\ttotal: 1m 21s\tremaining: 3m 52s\n",
      "259:\tlearn: 0.2276208\ttotal: 1m 21s\tremaining: 3m 52s\n",
      "260:\tlearn: 0.2275388\ttotal: 1m 21s\tremaining: 3m 51s\n",
      "261:\tlearn: 0.2269536\ttotal: 1m 22s\tremaining: 3m 51s\n",
      "262:\tlearn: 0.2265725\ttotal: 1m 22s\tremaining: 3m 51s\n",
      "263:\tlearn: 0.2264630\ttotal: 1m 22s\tremaining: 3m 50s\n",
      "264:\tlearn: 0.2262428\ttotal: 1m 23s\tremaining: 3m 50s\n",
      "265:\tlearn: 0.2253850\ttotal: 1m 23s\tremaining: 3m 50s\n",
      "266:\tlearn: 0.2247655\ttotal: 1m 23s\tremaining: 3m 49s\n",
      "267:\tlearn: 0.2244315\ttotal: 1m 24s\tremaining: 3m 49s\n",
      "268:\tlearn: 0.2241323\ttotal: 1m 24s\tremaining: 3m 49s\n",
      "269:\tlearn: 0.2240538\ttotal: 1m 24s\tremaining: 3m 48s\n",
      "270:\tlearn: 0.2230588\ttotal: 1m 24s\tremaining: 3m 48s\n",
      "271:\tlearn: 0.2229371\ttotal: 1m 25s\tremaining: 3m 47s\n",
      "272:\tlearn: 0.2224056\ttotal: 1m 25s\tremaining: 3m 47s\n",
      "273:\tlearn: 0.2219093\ttotal: 1m 25s\tremaining: 3m 47s\n",
      "274:\tlearn: 0.2215685\ttotal: 1m 26s\tremaining: 3m 46s\n",
      "275:\tlearn: 0.2211077\ttotal: 1m 26s\tremaining: 3m 46s\n",
      "276:\tlearn: 0.2203106\ttotal: 1m 26s\tremaining: 3m 46s\n",
      "277:\tlearn: 0.2195506\ttotal: 1m 27s\tremaining: 3m 45s\n",
      "278:\tlearn: 0.2191051\ttotal: 1m 27s\tremaining: 3m 45s\n",
      "279:\tlearn: 0.2187980\ttotal: 1m 27s\tremaining: 3m 45s\n",
      "280:\tlearn: 0.2187142\ttotal: 1m 27s\tremaining: 3m 44s\n",
      "281:\tlearn: 0.2185409\ttotal: 1m 28s\tremaining: 3m 44s\n",
      "282:\tlearn: 0.2181221\ttotal: 1m 28s\tremaining: 3m 44s\n",
      "283:\tlearn: 0.2179955\ttotal: 1m 28s\tremaining: 3m 43s\n",
      "284:\tlearn: 0.2177529\ttotal: 1m 29s\tremaining: 3m 43s\n",
      "285:\tlearn: 0.2174628\ttotal: 1m 29s\tremaining: 3m 43s\n",
      "286:\tlearn: 0.2168956\ttotal: 1m 29s\tremaining: 3m 42s\n",
      "287:\tlearn: 0.2165001\ttotal: 1m 29s\tremaining: 3m 42s\n",
      "288:\tlearn: 0.2160719\ttotal: 1m 30s\tremaining: 3m 42s\n",
      "289:\tlearn: 0.2156564\ttotal: 1m 30s\tremaining: 3m 41s\n",
      "290:\tlearn: 0.2149787\ttotal: 1m 30s\tremaining: 3m 41s\n",
      "291:\tlearn: 0.2147260\ttotal: 1m 31s\tremaining: 3m 41s\n",
      "292:\tlearn: 0.2146473\ttotal: 1m 31s\tremaining: 3m 40s\n",
      "293:\tlearn: 0.2141689\ttotal: 1m 31s\tremaining: 3m 40s\n",
      "294:\tlearn: 0.2138642\ttotal: 1m 32s\tremaining: 3m 40s\n",
      "295:\tlearn: 0.2136350\ttotal: 1m 32s\tremaining: 3m 39s\n",
      "296:\tlearn: 0.2135715\ttotal: 1m 32s\tremaining: 3m 39s\n",
      "297:\tlearn: 0.2134949\ttotal: 1m 32s\tremaining: 3m 38s\n",
      "298:\tlearn: 0.2132664\ttotal: 1m 33s\tremaining: 3m 38s\n",
      "299:\tlearn: 0.2131851\ttotal: 1m 33s\tremaining: 3m 38s\n",
      "300:\tlearn: 0.2130652\ttotal: 1m 33s\tremaining: 3m 37s\n",
      "301:\tlearn: 0.2129841\ttotal: 1m 33s\tremaining: 3m 37s\n",
      "302:\tlearn: 0.2129194\ttotal: 1m 34s\tremaining: 3m 36s\n",
      "303:\tlearn: 0.2122516\ttotal: 1m 34s\tremaining: 3m 36s\n",
      "304:\tlearn: 0.2119910\ttotal: 1m 34s\tremaining: 3m 36s\n",
      "305:\tlearn: 0.2118525\ttotal: 1m 35s\tremaining: 3m 35s\n",
      "306:\tlearn: 0.2114122\ttotal: 1m 35s\tremaining: 3m 35s\n",
      "307:\tlearn: 0.2109022\ttotal: 1m 35s\tremaining: 3m 35s\n",
      "308:\tlearn: 0.2103880\ttotal: 1m 36s\tremaining: 3m 34s\n",
      "309:\tlearn: 0.2100627\ttotal: 1m 36s\tremaining: 3m 34s\n",
      "310:\tlearn: 0.2097217\ttotal: 1m 36s\tremaining: 3m 34s\n",
      "311:\tlearn: 0.2095336\ttotal: 1m 36s\tremaining: 3m 33s\n",
      "312:\tlearn: 0.2094063\ttotal: 1m 37s\tremaining: 3m 33s\n",
      "313:\tlearn: 0.2093661\ttotal: 1m 37s\tremaining: 3m 32s\n",
      "314:\tlearn: 0.2090807\ttotal: 1m 37s\tremaining: 3m 32s\n",
      "315:\tlearn: 0.2088516\ttotal: 1m 38s\tremaining: 3m 32s\n",
      "316:\tlearn: 0.2086053\ttotal: 1m 38s\tremaining: 3m 31s\n",
      "317:\tlearn: 0.2084725\ttotal: 1m 38s\tremaining: 3m 31s\n",
      "318:\tlearn: 0.2081795\ttotal: 1m 38s\tremaining: 3m 30s\n",
      "319:\tlearn: 0.2081380\ttotal: 1m 39s\tremaining: 3m 30s\n",
      "320:\tlearn: 0.2074498\ttotal: 1m 39s\tremaining: 3m 30s\n",
      "321:\tlearn: 0.2073430\ttotal: 1m 39s\tremaining: 3m 29s\n",
      "322:\tlearn: 0.2072249\ttotal: 1m 39s\tremaining: 3m 29s\n",
      "323:\tlearn: 0.2070234\ttotal: 1m 40s\tremaining: 3m 29s\n",
      "324:\tlearn: 0.2067947\ttotal: 1m 40s\tremaining: 3m 28s\n",
      "325:\tlearn: 0.2067303\ttotal: 1m 40s\tremaining: 3m 28s\n",
      "326:\tlearn: 0.2062219\ttotal: 1m 41s\tremaining: 3m 28s\n",
      "327:\tlearn: 0.2060147\ttotal: 1m 41s\tremaining: 3m 27s\n",
      "328:\tlearn: 0.2052606\ttotal: 1m 41s\tremaining: 3m 27s\n",
      "329:\tlearn: 0.2051947\ttotal: 1m 41s\tremaining: 3m 27s\n",
      "330:\tlearn: 0.2051404\ttotal: 1m 42s\tremaining: 3m 26s\n",
      "331:\tlearn: 0.2050883\ttotal: 1m 42s\tremaining: 3m 26s\n",
      "332:\tlearn: 0.2050274\ttotal: 1m 42s\tremaining: 3m 25s\n",
      "333:\tlearn: 0.2046006\ttotal: 1m 43s\tremaining: 3m 25s\n",
      "334:\tlearn: 0.2045329\ttotal: 1m 43s\tremaining: 3m 25s\n",
      "335:\tlearn: 0.2041007\ttotal: 1m 43s\tremaining: 3m 25s\n",
      "336:\tlearn: 0.2036758\ttotal: 1m 44s\tremaining: 3m 24s\n",
      "337:\tlearn: 0.2036439\ttotal: 1m 44s\tremaining: 3m 24s\n",
      "338:\tlearn: 0.2029070\ttotal: 1m 44s\tremaining: 3m 24s\n",
      "339:\tlearn: 0.2025712\ttotal: 1m 44s\tremaining: 3m 23s\n",
      "340:\tlearn: 0.2023876\ttotal: 1m 45s\tremaining: 3m 23s\n",
      "341:\tlearn: 0.2019012\ttotal: 1m 45s\tremaining: 3m 23s\n",
      "342:\tlearn: 0.2017520\ttotal: 1m 45s\tremaining: 3m 22s\n",
      "343:\tlearn: 0.2016178\ttotal: 1m 46s\tremaining: 3m 22s\n",
      "344:\tlearn: 0.2015583\ttotal: 1m 46s\tremaining: 3m 21s\n",
      "345:\tlearn: 0.2011937\ttotal: 1m 46s\tremaining: 3m 21s\n",
      "346:\tlearn: 0.2011448\ttotal: 1m 46s\tremaining: 3m 21s\n",
      "347:\tlearn: 0.2007872\ttotal: 1m 47s\tremaining: 3m 20s\n",
      "348:\tlearn: 0.2004368\ttotal: 1m 47s\tremaining: 3m 20s\n",
      "349:\tlearn: 0.2000824\ttotal: 1m 47s\tremaining: 3m 20s\n",
      "350:\tlearn: 0.1996507\ttotal: 1m 48s\tremaining: 3m 19s\n",
      "351:\tlearn: 0.1994152\ttotal: 1m 48s\tremaining: 3m 19s\n",
      "352:\tlearn: 0.1993492\ttotal: 1m 48s\tremaining: 3m 19s\n",
      "353:\tlearn: 0.1990506\ttotal: 1m 48s\tremaining: 3m 18s\n",
      "354:\tlearn: 0.1989723\ttotal: 1m 49s\tremaining: 3m 18s\n",
      "355:\tlearn: 0.1989202\ttotal: 1m 49s\tremaining: 3m 18s\n",
      "356:\tlearn: 0.1986290\ttotal: 1m 49s\tremaining: 3m 17s\n",
      "357:\tlearn: 0.1983378\ttotal: 1m 50s\tremaining: 3m 17s\n",
      "358:\tlearn: 0.1982995\ttotal: 1m 50s\tremaining: 3m 17s\n",
      "359:\tlearn: 0.1979455\ttotal: 1m 50s\tremaining: 3m 16s\n",
      "360:\tlearn: 0.1978691\ttotal: 1m 50s\tremaining: 3m 16s\n",
      "361:\tlearn: 0.1978397\ttotal: 1m 51s\tremaining: 3m 16s\n",
      "362:\tlearn: 0.1977960\ttotal: 1m 51s\tremaining: 3m 15s\n",
      "363:\tlearn: 0.1977313\ttotal: 1m 51s\tremaining: 3m 15s\n",
      "364:\tlearn: 0.1975999\ttotal: 1m 52s\tremaining: 3m 15s\n",
      "365:\tlearn: 0.1973726\ttotal: 1m 52s\tremaining: 3m 14s\n",
      "366:\tlearn: 0.1973100\ttotal: 1m 52s\tremaining: 3m 14s\n",
      "367:\tlearn: 0.1972678\ttotal: 1m 53s\tremaining: 3m 14s\n",
      "368:\tlearn: 0.1972251\ttotal: 1m 53s\tremaining: 3m 13s\n",
      "369:\tlearn: 0.1971184\ttotal: 1m 53s\tremaining: 3m 13s\n",
      "370:\tlearn: 0.1970884\ttotal: 1m 53s\tremaining: 3m 12s\n",
      "371:\tlearn: 0.1970348\ttotal: 1m 54s\tremaining: 3m 12s\n",
      "372:\tlearn: 0.1969890\ttotal: 1m 54s\tremaining: 3m 12s\n",
      "373:\tlearn: 0.1969305\ttotal: 1m 54s\tremaining: 3m 11s\n",
      "374:\tlearn: 0.1963888\ttotal: 1m 54s\tremaining: 3m 11s\n",
      "375:\tlearn: 0.1961079\ttotal: 1m 55s\tremaining: 3m 11s\n",
      "376:\tlearn: 0.1955573\ttotal: 1m 55s\tremaining: 3m 11s\n",
      "377:\tlearn: 0.1953792\ttotal: 1m 55s\tremaining: 3m 10s\n",
      "378:\tlearn: 0.1950007\ttotal: 1m 56s\tremaining: 3m 10s\n",
      "379:\tlearn: 0.1948167\ttotal: 1m 56s\tremaining: 3m 10s\n",
      "380:\tlearn: 0.1947578\ttotal: 1m 56s\tremaining: 3m 9s\n",
      "381:\tlearn: 0.1945021\ttotal: 1m 57s\tremaining: 3m 9s\n",
      "382:\tlearn: 0.1944438\ttotal: 1m 57s\tremaining: 3m 9s\n",
      "383:\tlearn: 0.1943094\ttotal: 1m 57s\tremaining: 3m 8s\n",
      "384:\tlearn: 0.1942158\ttotal: 1m 57s\tremaining: 3m 8s\n",
      "385:\tlearn: 0.1937956\ttotal: 1m 58s\tremaining: 3m 8s\n",
      "386:\tlearn: 0.1933588\ttotal: 1m 58s\tremaining: 3m 7s\n",
      "387:\tlearn: 0.1930168\ttotal: 1m 58s\tremaining: 3m 7s\n",
      "388:\tlearn: 0.1928596\ttotal: 1m 59s\tremaining: 3m 7s\n",
      "389:\tlearn: 0.1927930\ttotal: 1m 59s\tremaining: 3m 6s\n",
      "390:\tlearn: 0.1925036\ttotal: 1m 59s\tremaining: 3m 6s\n",
      "391:\tlearn: 0.1924604\ttotal: 2m\tremaining: 3m 6s\n",
      "392:\tlearn: 0.1924307\ttotal: 2m\tremaining: 3m 5s\n",
      "393:\tlearn: 0.1923286\ttotal: 2m\tremaining: 3m 5s\n",
      "394:\tlearn: 0.1921326\ttotal: 2m\tremaining: 3m 5s\n",
      "395:\tlearn: 0.1920903\ttotal: 2m 1s\tremaining: 3m 4s\n",
      "396:\tlearn: 0.1916861\ttotal: 2m 1s\tremaining: 3m 4s\n",
      "397:\tlearn: 0.1914974\ttotal: 2m 1s\tremaining: 3m 4s\n",
      "398:\tlearn: 0.1913907\ttotal: 2m 2s\tremaining: 3m 3s\n",
      "399:\tlearn: 0.1913425\ttotal: 2m 2s\tremaining: 3m 3s\n",
      "400:\tlearn: 0.1912993\ttotal: 2m 2s\tremaining: 3m 3s\n",
      "401:\tlearn: 0.1912088\ttotal: 2m 2s\tremaining: 3m 2s\n",
      "402:\tlearn: 0.1911368\ttotal: 2m 3s\tremaining: 3m 2s\n",
      "403:\tlearn: 0.1910861\ttotal: 2m 3s\tremaining: 3m 2s\n",
      "404:\tlearn: 0.1908320\ttotal: 2m 3s\tremaining: 3m 1s\n",
      "405:\tlearn: 0.1907866\ttotal: 2m 4s\tremaining: 3m 1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406:\tlearn: 0.1906009\ttotal: 2m 4s\tremaining: 3m 1s\n",
      "407:\tlearn: 0.1905221\ttotal: 2m 4s\tremaining: 3m\n",
      "408:\tlearn: 0.1904706\ttotal: 2m 4s\tremaining: 3m\n",
      "409:\tlearn: 0.1904235\ttotal: 2m 5s\tremaining: 3m\n",
      "410:\tlearn: 0.1903907\ttotal: 2m 5s\tremaining: 2m 59s\n",
      "411:\tlearn: 0.1901512\ttotal: 2m 5s\tremaining: 2m 59s\n",
      "412:\tlearn: 0.1900960\ttotal: 2m 5s\tremaining: 2m 58s\n",
      "413:\tlearn: 0.1900694\ttotal: 2m 6s\tremaining: 2m 58s\n",
      "414:\tlearn: 0.1900191\ttotal: 2m 6s\tremaining: 2m 58s\n",
      "415:\tlearn: 0.1899946\ttotal: 2m 6s\tremaining: 2m 57s\n",
      "416:\tlearn: 0.1899498\ttotal: 2m 6s\tremaining: 2m 57s\n",
      "417:\tlearn: 0.1898860\ttotal: 2m 7s\tremaining: 2m 57s\n",
      "418:\tlearn: 0.1898479\ttotal: 2m 7s\tremaining: 2m 56s\n",
      "419:\tlearn: 0.1896859\ttotal: 2m 7s\tremaining: 2m 56s\n",
      "420:\tlearn: 0.1894315\ttotal: 2m 7s\tremaining: 2m 55s\n",
      "421:\tlearn: 0.1894047\ttotal: 2m 8s\tremaining: 2m 55s\n",
      "422:\tlearn: 0.1893838\ttotal: 2m 8s\tremaining: 2m 55s\n",
      "423:\tlearn: 0.1893136\ttotal: 2m 8s\tremaining: 2m 54s\n",
      "424:\tlearn: 0.1892546\ttotal: 2m 8s\tremaining: 2m 54s\n",
      "425:\tlearn: 0.1891006\ttotal: 2m 9s\tremaining: 2m 54s\n",
      "426:\tlearn: 0.1888181\ttotal: 2m 9s\tremaining: 2m 53s\n",
      "427:\tlearn: 0.1885537\ttotal: 2m 9s\tremaining: 2m 53s\n",
      "428:\tlearn: 0.1882713\ttotal: 2m 10s\tremaining: 2m 53s\n",
      "429:\tlearn: 0.1881398\ttotal: 2m 10s\tremaining: 2m 52s\n",
      "430:\tlearn: 0.1876544\ttotal: 2m 10s\tremaining: 2m 52s\n",
      "431:\tlearn: 0.1874845\ttotal: 2m 11s\tremaining: 2m 52s\n",
      "432:\tlearn: 0.1874416\ttotal: 2m 11s\tremaining: 2m 51s\n",
      "433:\tlearn: 0.1873897\ttotal: 2m 11s\tremaining: 2m 51s\n",
      "434:\tlearn: 0.1873427\ttotal: 2m 11s\tremaining: 2m 51s\n",
      "435:\tlearn: 0.1873180\ttotal: 2m 12s\tremaining: 2m 50s\n",
      "436:\tlearn: 0.1872647\ttotal: 2m 12s\tremaining: 2m 50s\n",
      "437:\tlearn: 0.1872418\ttotal: 2m 12s\tremaining: 2m 50s\n",
      "438:\tlearn: 0.1872040\ttotal: 2m 12s\tremaining: 2m 49s\n",
      "439:\tlearn: 0.1871599\ttotal: 2m 13s\tremaining: 2m 49s\n",
      "440:\tlearn: 0.1870851\ttotal: 2m 13s\tremaining: 2m 48s\n",
      "441:\tlearn: 0.1870426\ttotal: 2m 13s\tremaining: 2m 48s\n",
      "442:\tlearn: 0.1870188\ttotal: 2m 13s\tremaining: 2m 48s\n",
      "443:\tlearn: 0.1868916\ttotal: 2m 14s\tremaining: 2m 47s\n",
      "444:\tlearn: 0.1865401\ttotal: 2m 14s\tremaining: 2m 47s\n",
      "445:\tlearn: 0.1865268\ttotal: 2m 14s\tremaining: 2m 47s\n",
      "446:\tlearn: 0.1862698\ttotal: 2m 15s\tremaining: 2m 47s\n",
      "447:\tlearn: 0.1862369\ttotal: 2m 15s\tremaining: 2m 46s\n",
      "448:\tlearn: 0.1862028\ttotal: 2m 15s\tremaining: 2m 46s\n",
      "449:\tlearn: 0.1861684\ttotal: 2m 15s\tremaining: 2m 46s\n",
      "450:\tlearn: 0.1861420\ttotal: 2m 16s\tremaining: 2m 45s\n",
      "451:\tlearn: 0.1860954\ttotal: 2m 16s\tremaining: 2m 45s\n",
      "452:\tlearn: 0.1860531\ttotal: 2m 16s\tremaining: 2m 44s\n",
      "453:\tlearn: 0.1858973\ttotal: 2m 16s\tremaining: 2m 44s\n",
      "454:\tlearn: 0.1857450\ttotal: 2m 17s\tremaining: 2m 44s\n",
      "455:\tlearn: 0.1857233\ttotal: 2m 17s\tremaining: 2m 43s\n",
      "456:\tlearn: 0.1857002\ttotal: 2m 17s\tremaining: 2m 43s\n",
      "457:\tlearn: 0.1856623\ttotal: 2m 17s\tremaining: 2m 43s\n",
      "458:\tlearn: 0.1856408\ttotal: 2m 18s\tremaining: 2m 42s\n",
      "459:\tlearn: 0.1856085\ttotal: 2m 18s\tremaining: 2m 42s\n",
      "460:\tlearn: 0.1855879\ttotal: 2m 18s\tremaining: 2m 42s\n",
      "461:\tlearn: 0.1855781\ttotal: 2m 18s\tremaining: 2m 41s\n",
      "462:\tlearn: 0.1854192\ttotal: 2m 19s\tremaining: 2m 41s\n",
      "463:\tlearn: 0.1853355\ttotal: 2m 19s\tremaining: 2m 41s\n",
      "464:\tlearn: 0.1852927\ttotal: 2m 19s\tremaining: 2m 40s\n",
      "465:\tlearn: 0.1851942\ttotal: 2m 20s\tremaining: 2m 40s\n",
      "466:\tlearn: 0.1851704\ttotal: 2m 20s\tremaining: 2m 40s\n",
      "467:\tlearn: 0.1850662\ttotal: 2m 20s\tremaining: 2m 39s\n",
      "468:\tlearn: 0.1850485\ttotal: 2m 20s\tremaining: 2m 39s\n",
      "469:\tlearn: 0.1850316\ttotal: 2m 21s\tremaining: 2m 39s\n",
      "470:\tlearn: 0.1849829\ttotal: 2m 21s\tremaining: 2m 38s\n",
      "471:\tlearn: 0.1848939\ttotal: 2m 21s\tremaining: 2m 38s\n",
      "472:\tlearn: 0.1847596\ttotal: 2m 21s\tremaining: 2m 38s\n",
      "473:\tlearn: 0.1847108\ttotal: 2m 22s\tremaining: 2m 37s\n",
      "474:\tlearn: 0.1844947\ttotal: 2m 22s\tremaining: 2m 37s\n",
      "475:\tlearn: 0.1844792\ttotal: 2m 22s\tremaining: 2m 37s\n",
      "476:\tlearn: 0.1841177\ttotal: 2m 23s\tremaining: 2m 36s\n",
      "477:\tlearn: 0.1840158\ttotal: 2m 23s\tremaining: 2m 36s\n",
      "478:\tlearn: 0.1838469\ttotal: 2m 23s\tremaining: 2m 36s\n",
      "479:\tlearn: 0.1836345\ttotal: 2m 24s\tremaining: 2m 36s\n",
      "480:\tlearn: 0.1835822\ttotal: 2m 24s\tremaining: 2m 35s\n",
      "481:\tlearn: 0.1834784\ttotal: 2m 24s\tremaining: 2m 35s\n",
      "482:\tlearn: 0.1834583\ttotal: 2m 24s\tremaining: 2m 34s\n",
      "483:\tlearn: 0.1833713\ttotal: 2m 25s\tremaining: 2m 34s\n",
      "484:\tlearn: 0.1833185\ttotal: 2m 25s\tremaining: 2m 34s\n",
      "485:\tlearn: 0.1832493\ttotal: 2m 25s\tremaining: 2m 34s\n",
      "486:\tlearn: 0.1832155\ttotal: 2m 25s\tremaining: 2m 33s\n",
      "487:\tlearn: 0.1831987\ttotal: 2m 26s\tremaining: 2m 33s\n",
      "488:\tlearn: 0.1831852\ttotal: 2m 26s\tremaining: 2m 32s\n",
      "489:\tlearn: 0.1830070\ttotal: 2m 26s\tremaining: 2m 32s\n",
      "490:\tlearn: 0.1829303\ttotal: 2m 26s\tremaining: 2m 32s\n",
      "491:\tlearn: 0.1829148\ttotal: 2m 27s\tremaining: 2m 31s\n",
      "492:\tlearn: 0.1828188\ttotal: 2m 27s\tremaining: 2m 31s\n",
      "493:\tlearn: 0.1827913\ttotal: 2m 27s\tremaining: 2m 31s\n",
      "494:\tlearn: 0.1826782\ttotal: 2m 28s\tremaining: 2m 31s\n",
      "495:\tlearn: 0.1826553\ttotal: 2m 28s\tremaining: 2m 30s\n",
      "496:\tlearn: 0.1826191\ttotal: 2m 28s\tremaining: 2m 30s\n",
      "497:\tlearn: 0.1825630\ttotal: 2m 28s\tremaining: 2m 30s\n",
      "498:\tlearn: 0.1825317\ttotal: 2m 29s\tremaining: 2m 29s\n",
      "499:\tlearn: 0.1825100\ttotal: 2m 29s\tremaining: 2m 29s\n",
      "500:\tlearn: 0.1825004\ttotal: 2m 29s\tremaining: 2m 29s\n",
      "501:\tlearn: 0.1822725\ttotal: 2m 30s\tremaining: 2m 28s\n",
      "502:\tlearn: 0.1822511\ttotal: 2m 30s\tremaining: 2m 28s\n",
      "503:\tlearn: 0.1817945\ttotal: 2m 30s\tremaining: 2m 28s\n",
      "504:\tlearn: 0.1817770\ttotal: 2m 30s\tremaining: 2m 27s\n",
      "505:\tlearn: 0.1817554\ttotal: 2m 31s\tremaining: 2m 27s\n",
      "506:\tlearn: 0.1817270\ttotal: 2m 31s\tremaining: 2m 27s\n",
      "507:\tlearn: 0.1817030\ttotal: 2m 31s\tremaining: 2m 26s\n",
      "508:\tlearn: 0.1816885\ttotal: 2m 31s\tremaining: 2m 26s\n",
      "509:\tlearn: 0.1816470\ttotal: 2m 32s\tremaining: 2m 26s\n",
      "510:\tlearn: 0.1816080\ttotal: 2m 32s\tremaining: 2m 25s\n",
      "511:\tlearn: 0.1814110\ttotal: 2m 32s\tremaining: 2m 25s\n",
      "512:\tlearn: 0.1813917\ttotal: 2m 32s\tremaining: 2m 25s\n",
      "513:\tlearn: 0.1813803\ttotal: 2m 33s\tremaining: 2m 24s\n",
      "514:\tlearn: 0.1813588\ttotal: 2m 33s\tremaining: 2m 24s\n",
      "515:\tlearn: 0.1813400\ttotal: 2m 33s\tremaining: 2m 24s\n",
      "516:\tlearn: 0.1813109\ttotal: 2m 33s\tremaining: 2m 23s\n",
      "517:\tlearn: 0.1810045\ttotal: 2m 34s\tremaining: 2m 23s\n",
      "518:\tlearn: 0.1809207\ttotal: 2m 34s\tremaining: 2m 23s\n",
      "519:\tlearn: 0.1809020\ttotal: 2m 34s\tremaining: 2m 22s\n",
      "520:\tlearn: 0.1808797\ttotal: 2m 35s\tremaining: 2m 22s\n",
      "521:\tlearn: 0.1808566\ttotal: 2m 35s\tremaining: 2m 22s\n",
      "522:\tlearn: 0.1807683\ttotal: 2m 35s\tremaining: 2m 21s\n",
      "523:\tlearn: 0.1807539\ttotal: 2m 35s\tremaining: 2m 21s\n",
      "524:\tlearn: 0.1805452\ttotal: 2m 36s\tremaining: 2m 21s\n",
      "525:\tlearn: 0.1803084\ttotal: 2m 36s\tremaining: 2m 21s\n",
      "526:\tlearn: 0.1802682\ttotal: 2m 36s\tremaining: 2m 20s\n",
      "527:\tlearn: 0.1802264\ttotal: 2m 37s\tremaining: 2m 20s\n",
      "528:\tlearn: 0.1801753\ttotal: 2m 37s\tremaining: 2m 20s\n",
      "529:\tlearn: 0.1801433\ttotal: 2m 37s\tremaining: 2m 19s\n",
      "530:\tlearn: 0.1800960\ttotal: 2m 37s\tremaining: 2m 19s\n",
      "531:\tlearn: 0.1797830\ttotal: 2m 38s\tremaining: 2m 19s\n",
      "532:\tlearn: 0.1797522\ttotal: 2m 38s\tremaining: 2m 18s\n",
      "533:\tlearn: 0.1795987\ttotal: 2m 38s\tremaining: 2m 18s\n",
      "534:\tlearn: 0.1795639\ttotal: 2m 38s\tremaining: 2m 18s\n",
      "535:\tlearn: 0.1792407\ttotal: 2m 39s\tremaining: 2m 17s\n",
      "536:\tlearn: 0.1792194\ttotal: 2m 39s\tremaining: 2m 17s\n",
      "537:\tlearn: 0.1789122\ttotal: 2m 39s\tremaining: 2m 17s\n",
      "538:\tlearn: 0.1787549\ttotal: 2m 40s\tremaining: 2m 16s\n",
      "539:\tlearn: 0.1787345\ttotal: 2m 40s\tremaining: 2m 16s\n",
      "540:\tlearn: 0.1786949\ttotal: 2m 40s\tremaining: 2m 16s\n",
      "541:\tlearn: 0.1786813\ttotal: 2m 40s\tremaining: 2m 15s\n",
      "542:\tlearn: 0.1786050\ttotal: 2m 41s\tremaining: 2m 15s\n",
      "543:\tlearn: 0.1785271\ttotal: 2m 41s\tremaining: 2m 15s\n",
      "544:\tlearn: 0.1785052\ttotal: 2m 41s\tremaining: 2m 14s\n",
      "545:\tlearn: 0.1784675\ttotal: 2m 41s\tremaining: 2m 14s\n",
      "546:\tlearn: 0.1784605\ttotal: 2m 42s\tremaining: 2m 14s\n",
      "547:\tlearn: 0.1784265\ttotal: 2m 42s\tremaining: 2m 13s\n",
      "548:\tlearn: 0.1784153\ttotal: 2m 42s\tremaining: 2m 13s\n",
      "549:\tlearn: 0.1781835\ttotal: 2m 42s\tremaining: 2m 13s\n",
      "550:\tlearn: 0.1780909\ttotal: 2m 43s\tremaining: 2m 12s\n",
      "551:\tlearn: 0.1780769\ttotal: 2m 43s\tremaining: 2m 12s\n",
      "552:\tlearn: 0.1780522\ttotal: 2m 43s\tremaining: 2m 12s\n",
      "553:\tlearn: 0.1779476\ttotal: 2m 44s\tremaining: 2m 12s\n",
      "554:\tlearn: 0.1779387\ttotal: 2m 44s\tremaining: 2m 11s\n",
      "555:\tlearn: 0.1778839\ttotal: 2m 44s\tremaining: 2m 11s\n",
      "556:\tlearn: 0.1778681\ttotal: 2m 44s\tremaining: 2m 11s\n",
      "557:\tlearn: 0.1778495\ttotal: 2m 45s\tremaining: 2m 10s\n",
      "558:\tlearn: 0.1777327\ttotal: 2m 45s\tremaining: 2m 10s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "559:\tlearn: 0.1775201\ttotal: 2m 45s\tremaining: 2m 10s\n",
      "560:\tlearn: 0.1774864\ttotal: 2m 45s\tremaining: 2m 9s\n",
      "561:\tlearn: 0.1774565\ttotal: 2m 46s\tremaining: 2m 9s\n",
      "562:\tlearn: 0.1774261\ttotal: 2m 46s\tremaining: 2m 9s\n",
      "563:\tlearn: 0.1773424\ttotal: 2m 46s\tremaining: 2m 8s\n",
      "564:\tlearn: 0.1770985\ttotal: 2m 46s\tremaining: 2m 8s\n",
      "565:\tlearn: 0.1770137\ttotal: 2m 47s\tremaining: 2m 8s\n",
      "566:\tlearn: 0.1769145\ttotal: 2m 47s\tremaining: 2m 7s\n",
      "567:\tlearn: 0.1768945\ttotal: 2m 47s\tremaining: 2m 7s\n",
      "568:\tlearn: 0.1768741\ttotal: 2m 47s\tremaining: 2m 7s\n",
      "569:\tlearn: 0.1768409\ttotal: 2m 48s\tremaining: 2m 6s\n",
      "570:\tlearn: 0.1768199\ttotal: 2m 48s\tremaining: 2m 6s\n",
      "571:\tlearn: 0.1767891\ttotal: 2m 48s\tremaining: 2m 6s\n",
      "572:\tlearn: 0.1767766\ttotal: 2m 48s\tremaining: 2m 5s\n",
      "573:\tlearn: 0.1767512\ttotal: 2m 49s\tremaining: 2m 5s\n",
      "574:\tlearn: 0.1765594\ttotal: 2m 49s\tremaining: 2m 5s\n",
      "575:\tlearn: 0.1763091\ttotal: 2m 49s\tremaining: 2m 5s\n",
      "576:\tlearn: 0.1762964\ttotal: 2m 50s\tremaining: 2m 4s\n",
      "577:\tlearn: 0.1760920\ttotal: 2m 50s\tremaining: 2m 4s\n",
      "578:\tlearn: 0.1758212\ttotal: 2m 50s\tremaining: 2m 4s\n",
      "579:\tlearn: 0.1757088\ttotal: 2m 50s\tremaining: 2m 3s\n",
      "580:\tlearn: 0.1756411\ttotal: 2m 51s\tremaining: 2m 3s\n",
      "581:\tlearn: 0.1756226\ttotal: 2m 51s\tremaining: 2m 3s\n",
      "582:\tlearn: 0.1755967\ttotal: 2m 51s\tremaining: 2m 2s\n",
      "583:\tlearn: 0.1755459\ttotal: 2m 52s\tremaining: 2m 2s\n",
      "584:\tlearn: 0.1755317\ttotal: 2m 52s\tremaining: 2m 2s\n",
      "585:\tlearn: 0.1755033\ttotal: 2m 52s\tremaining: 2m 1s\n",
      "586:\tlearn: 0.1754711\ttotal: 2m 52s\tremaining: 2m 1s\n",
      "587:\tlearn: 0.1753818\ttotal: 2m 53s\tremaining: 2m 1s\n",
      "588:\tlearn: 0.1752929\ttotal: 2m 53s\tremaining: 2m 1s\n",
      "589:\tlearn: 0.1752655\ttotal: 2m 53s\tremaining: 2m\n",
      "590:\tlearn: 0.1749910\ttotal: 2m 54s\tremaining: 2m\n",
      "591:\tlearn: 0.1747578\ttotal: 2m 54s\tremaining: 2m\n",
      "592:\tlearn: 0.1747314\ttotal: 2m 54s\tremaining: 1m 59s\n",
      "593:\tlearn: 0.1747119\ttotal: 2m 54s\tremaining: 1m 59s\n",
      "594:\tlearn: 0.1745447\ttotal: 2m 55s\tremaining: 1m 59s\n",
      "595:\tlearn: 0.1745041\ttotal: 2m 55s\tremaining: 1m 58s\n",
      "596:\tlearn: 0.1744902\ttotal: 2m 55s\tremaining: 1m 58s\n",
      "597:\tlearn: 0.1744588\ttotal: 2m 55s\tremaining: 1m 58s\n",
      "598:\tlearn: 0.1744434\ttotal: 2m 56s\tremaining: 1m 57s\n",
      "599:\tlearn: 0.1744122\ttotal: 2m 56s\tremaining: 1m 57s\n",
      "600:\tlearn: 0.1741521\ttotal: 2m 56s\tremaining: 1m 57s\n",
      "601:\tlearn: 0.1741271\ttotal: 2m 57s\tremaining: 1m 57s\n",
      "602:\tlearn: 0.1741100\ttotal: 2m 57s\tremaining: 1m 56s\n",
      "603:\tlearn: 0.1740830\ttotal: 2m 57s\tremaining: 1m 56s\n",
      "604:\tlearn: 0.1740480\ttotal: 2m 57s\tremaining: 1m 56s\n",
      "605:\tlearn: 0.1739095\ttotal: 2m 58s\tremaining: 1m 55s\n",
      "606:\tlearn: 0.1738866\ttotal: 2m 58s\tremaining: 1m 55s\n",
      "607:\tlearn: 0.1738300\ttotal: 2m 58s\tremaining: 1m 55s\n",
      "608:\tlearn: 0.1738181\ttotal: 2m 58s\tremaining: 1m 54s\n",
      "609:\tlearn: 0.1736230\ttotal: 2m 59s\tremaining: 1m 54s\n",
      "610:\tlearn: 0.1735681\ttotal: 2m 59s\tremaining: 1m 54s\n",
      "611:\tlearn: 0.1734947\ttotal: 2m 59s\tremaining: 1m 53s\n",
      "612:\tlearn: 0.1733949\ttotal: 3m\tremaining: 1m 53s\n",
      "613:\tlearn: 0.1733669\ttotal: 3m\tremaining: 1m 53s\n",
      "614:\tlearn: 0.1733582\ttotal: 3m\tremaining: 1m 53s\n",
      "615:\tlearn: 0.1733463\ttotal: 3m\tremaining: 1m 52s\n",
      "616:\tlearn: 0.1731649\ttotal: 3m 1s\tremaining: 1m 52s\n",
      "617:\tlearn: 0.1731418\ttotal: 3m 1s\tremaining: 1m 52s\n",
      "618:\tlearn: 0.1731270\ttotal: 3m 1s\tremaining: 1m 51s\n",
      "619:\tlearn: 0.1730189\ttotal: 3m 1s\tremaining: 1m 51s\n",
      "620:\tlearn: 0.1729980\ttotal: 3m 2s\tremaining: 1m 51s\n",
      "621:\tlearn: 0.1729258\ttotal: 3m 2s\tremaining: 1m 50s\n",
      "622:\tlearn: 0.1728652\ttotal: 3m 2s\tremaining: 1m 50s\n",
      "623:\tlearn: 0.1728525\ttotal: 3m 3s\tremaining: 1m 50s\n",
      "624:\tlearn: 0.1727110\ttotal: 3m 3s\tremaining: 1m 50s\n",
      "625:\tlearn: 0.1727023\ttotal: 3m 3s\tremaining: 1m 49s\n",
      "626:\tlearn: 0.1726950\ttotal: 3m 3s\tremaining: 1m 49s\n",
      "627:\tlearn: 0.1726285\ttotal: 3m 4s\tremaining: 1m 49s\n",
      "628:\tlearn: 0.1725942\ttotal: 3m 4s\tremaining: 1m 48s\n",
      "629:\tlearn: 0.1725851\ttotal: 3m 4s\tremaining: 1m 48s\n",
      "630:\tlearn: 0.1725706\ttotal: 3m 4s\tremaining: 1m 48s\n",
      "631:\tlearn: 0.1725444\ttotal: 3m 5s\tremaining: 1m 47s\n",
      "632:\tlearn: 0.1724040\ttotal: 3m 5s\tremaining: 1m 47s\n",
      "633:\tlearn: 0.1721632\ttotal: 3m 5s\tremaining: 1m 47s\n",
      "634:\tlearn: 0.1721467\ttotal: 3m 6s\tremaining: 1m 46s\n",
      "635:\tlearn: 0.1721333\ttotal: 3m 6s\tremaining: 1m 46s\n",
      "636:\tlearn: 0.1720916\ttotal: 3m 6s\tremaining: 1m 46s\n",
      "637:\tlearn: 0.1719223\ttotal: 3m 6s\tremaining: 1m 46s\n",
      "638:\tlearn: 0.1719119\ttotal: 3m 7s\tremaining: 1m 45s\n",
      "639:\tlearn: 0.1718469\ttotal: 3m 7s\tremaining: 1m 45s\n",
      "640:\tlearn: 0.1717573\ttotal: 3m 7s\tremaining: 1m 45s\n",
      "641:\tlearn: 0.1716513\ttotal: 3m 7s\tremaining: 1m 44s\n",
      "642:\tlearn: 0.1716342\ttotal: 3m 8s\tremaining: 1m 44s\n",
      "643:\tlearn: 0.1715561\ttotal: 3m 8s\tremaining: 1m 44s\n",
      "644:\tlearn: 0.1715396\ttotal: 3m 8s\tremaining: 1m 43s\n",
      "645:\tlearn: 0.1714869\ttotal: 3m 9s\tremaining: 1m 43s\n",
      "646:\tlearn: 0.1714770\ttotal: 3m 9s\tremaining: 1m 43s\n",
      "647:\tlearn: 0.1714083\ttotal: 3m 9s\tremaining: 1m 42s\n",
      "648:\tlearn: 0.1713570\ttotal: 3m 9s\tremaining: 1m 42s\n",
      "649:\tlearn: 0.1713480\ttotal: 3m 10s\tremaining: 1m 42s\n",
      "650:\tlearn: 0.1713372\ttotal: 3m 10s\tremaining: 1m 42s\n",
      "651:\tlearn: 0.1712317\ttotal: 3m 10s\tremaining: 1m 41s\n",
      "652:\tlearn: 0.1712243\ttotal: 3m 10s\tremaining: 1m 41s\n",
      "653:\tlearn: 0.1712153\ttotal: 3m 11s\tremaining: 1m 41s\n",
      "654:\tlearn: 0.1711347\ttotal: 3m 11s\tremaining: 1m 40s\n",
      "655:\tlearn: 0.1711122\ttotal: 3m 11s\tremaining: 1m 40s\n",
      "656:\tlearn: 0.1710730\ttotal: 3m 11s\tremaining: 1m 40s\n",
      "657:\tlearn: 0.1710382\ttotal: 3m 12s\tremaining: 1m 39s\n",
      "658:\tlearn: 0.1709589\ttotal: 3m 12s\tremaining: 1m 39s\n",
      "659:\tlearn: 0.1709215\ttotal: 3m 12s\tremaining: 1m 39s\n",
      "660:\tlearn: 0.1709131\ttotal: 3m 13s\tremaining: 1m 38s\n",
      "661:\tlearn: 0.1708683\ttotal: 3m 13s\tremaining: 1m 38s\n",
      "662:\tlearn: 0.1707929\ttotal: 3m 13s\tremaining: 1m 38s\n",
      "663:\tlearn: 0.1707851\ttotal: 3m 13s\tremaining: 1m 38s\n",
      "664:\tlearn: 0.1706617\ttotal: 3m 14s\tremaining: 1m 37s\n",
      "665:\tlearn: 0.1705612\ttotal: 3m 14s\tremaining: 1m 37s\n",
      "666:\tlearn: 0.1705543\ttotal: 3m 14s\tremaining: 1m 37s\n",
      "667:\tlearn: 0.1705328\ttotal: 3m 14s\tremaining: 1m 36s\n",
      "668:\tlearn: 0.1705221\ttotal: 3m 15s\tremaining: 1m 36s\n",
      "669:\tlearn: 0.1704880\ttotal: 3m 15s\tremaining: 1m 36s\n",
      "670:\tlearn: 0.1704660\ttotal: 3m 15s\tremaining: 1m 35s\n",
      "671:\tlearn: 0.1704574\ttotal: 3m 15s\tremaining: 1m 35s\n",
      "672:\tlearn: 0.1703485\ttotal: 3m 16s\tremaining: 1m 35s\n",
      "673:\tlearn: 0.1703379\ttotal: 3m 16s\tremaining: 1m 35s\n",
      "674:\tlearn: 0.1702684\ttotal: 3m 16s\tremaining: 1m 34s\n",
      "675:\tlearn: 0.1702392\ttotal: 3m 17s\tremaining: 1m 34s\n",
      "676:\tlearn: 0.1702326\ttotal: 3m 17s\tremaining: 1m 34s\n",
      "677:\tlearn: 0.1700573\ttotal: 3m 17s\tremaining: 1m 33s\n",
      "678:\tlearn: 0.1700483\ttotal: 3m 17s\tremaining: 1m 33s\n",
      "679:\tlearn: 0.1700390\ttotal: 3m 18s\tremaining: 1m 33s\n",
      "680:\tlearn: 0.1700249\ttotal: 3m 18s\tremaining: 1m 32s\n",
      "681:\tlearn: 0.1698981\ttotal: 3m 18s\tremaining: 1m 32s\n",
      "682:\tlearn: 0.1698660\ttotal: 3m 18s\tremaining: 1m 32s\n",
      "683:\tlearn: 0.1698586\ttotal: 3m 19s\tremaining: 1m 32s\n",
      "684:\tlearn: 0.1698512\ttotal: 3m 19s\tremaining: 1m 31s\n",
      "685:\tlearn: 0.1697703\ttotal: 3m 19s\tremaining: 1m 31s\n",
      "686:\tlearn: 0.1697579\ttotal: 3m 20s\tremaining: 1m 31s\n",
      "687:\tlearn: 0.1696861\ttotal: 3m 20s\tremaining: 1m 30s\n",
      "688:\tlearn: 0.1696792\ttotal: 3m 20s\tremaining: 1m 30s\n",
      "689:\tlearn: 0.1696702\ttotal: 3m 20s\tremaining: 1m 30s\n",
      "690:\tlearn: 0.1695638\ttotal: 3m 21s\tremaining: 1m 29s\n",
      "691:\tlearn: 0.1695496\ttotal: 3m 21s\tremaining: 1m 29s\n",
      "692:\tlearn: 0.1695314\ttotal: 3m 21s\tremaining: 1m 29s\n",
      "693:\tlearn: 0.1694414\ttotal: 3m 21s\tremaining: 1m 29s\n",
      "694:\tlearn: 0.1694324\ttotal: 3m 22s\tremaining: 1m 28s\n",
      "695:\tlearn: 0.1694252\ttotal: 3m 22s\tremaining: 1m 28s\n",
      "696:\tlearn: 0.1694180\ttotal: 3m 22s\tremaining: 1m 28s\n",
      "697:\tlearn: 0.1692039\ttotal: 3m 22s\tremaining: 1m 27s\n",
      "698:\tlearn: 0.1690044\ttotal: 3m 23s\tremaining: 1m 27s\n",
      "699:\tlearn: 0.1689254\ttotal: 3m 23s\tremaining: 1m 27s\n",
      "700:\tlearn: 0.1689124\ttotal: 3m 23s\tremaining: 1m 26s\n",
      "701:\tlearn: 0.1688878\ttotal: 3m 24s\tremaining: 1m 26s\n",
      "702:\tlearn: 0.1688782\ttotal: 3m 24s\tremaining: 1m 26s\n",
      "703:\tlearn: 0.1688044\ttotal: 3m 24s\tremaining: 1m 26s\n",
      "704:\tlearn: 0.1687865\ttotal: 3m 24s\tremaining: 1m 25s\n",
      "705:\tlearn: 0.1687788\ttotal: 3m 25s\tremaining: 1m 25s\n",
      "706:\tlearn: 0.1687592\ttotal: 3m 25s\tremaining: 1m 25s\n",
      "707:\tlearn: 0.1687475\ttotal: 3m 25s\tremaining: 1m 24s\n",
      "708:\tlearn: 0.1687065\ttotal: 3m 25s\tremaining: 1m 24s\n",
      "709:\tlearn: 0.1686999\ttotal: 3m 26s\tremaining: 1m 24s\n",
      "710:\tlearn: 0.1686952\ttotal: 3m 26s\tremaining: 1m 23s\n",
      "711:\tlearn: 0.1686717\ttotal: 3m 26s\tremaining: 1m 23s\n",
      "712:\tlearn: 0.1686591\ttotal: 3m 26s\tremaining: 1m 23s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "713:\tlearn: 0.1686387\ttotal: 3m 27s\tremaining: 1m 23s\n",
      "714:\tlearn: 0.1686219\ttotal: 3m 27s\tremaining: 1m 22s\n",
      "715:\tlearn: 0.1686153\ttotal: 3m 27s\tremaining: 1m 22s\n",
      "716:\tlearn: 0.1684440\ttotal: 3m 28s\tremaining: 1m 22s\n",
      "717:\tlearn: 0.1684384\ttotal: 3m 28s\tremaining: 1m 21s\n",
      "718:\tlearn: 0.1682042\ttotal: 3m 28s\tremaining: 1m 21s\n",
      "719:\tlearn: 0.1681984\ttotal: 3m 28s\tremaining: 1m 21s\n",
      "720:\tlearn: 0.1681913\ttotal: 3m 29s\tremaining: 1m 20s\n",
      "721:\tlearn: 0.1681733\ttotal: 3m 29s\tremaining: 1m 20s\n",
      "722:\tlearn: 0.1681426\ttotal: 3m 29s\tremaining: 1m 20s\n",
      "723:\tlearn: 0.1681003\ttotal: 3m 29s\tremaining: 1m 20s\n",
      "724:\tlearn: 0.1680769\ttotal: 3m 30s\tremaining: 1m 19s\n",
      "725:\tlearn: 0.1680682\ttotal: 3m 30s\tremaining: 1m 19s\n",
      "726:\tlearn: 0.1680626\ttotal: 3m 30s\tremaining: 1m 19s\n",
      "727:\tlearn: 0.1680541\ttotal: 3m 30s\tremaining: 1m 18s\n",
      "728:\tlearn: 0.1680363\ttotal: 3m 31s\tremaining: 1m 18s\n",
      "729:\tlearn: 0.1679363\ttotal: 3m 31s\tremaining: 1m 18s\n",
      "730:\tlearn: 0.1679115\ttotal: 3m 31s\tremaining: 1m 17s\n",
      "731:\tlearn: 0.1678854\ttotal: 3m 31s\tremaining: 1m 17s\n",
      "732:\tlearn: 0.1678521\ttotal: 3m 32s\tremaining: 1m 17s\n",
      "733:\tlearn: 0.1678255\ttotal: 3m 32s\tremaining: 1m 17s\n",
      "734:\tlearn: 0.1678036\ttotal: 3m 32s\tremaining: 1m 16s\n",
      "735:\tlearn: 0.1677933\ttotal: 3m 33s\tremaining: 1m 16s\n",
      "736:\tlearn: 0.1677836\ttotal: 3m 33s\tremaining: 1m 16s\n",
      "737:\tlearn: 0.1677736\ttotal: 3m 33s\tremaining: 1m 15s\n",
      "738:\tlearn: 0.1677316\ttotal: 3m 33s\tremaining: 1m 15s\n",
      "739:\tlearn: 0.1677221\ttotal: 3m 34s\tremaining: 1m 15s\n",
      "740:\tlearn: 0.1677158\ttotal: 3m 34s\tremaining: 1m 14s\n",
      "741:\tlearn: 0.1677067\ttotal: 3m 34s\tremaining: 1m 14s\n",
      "742:\tlearn: 0.1676961\ttotal: 3m 34s\tremaining: 1m 14s\n",
      "743:\tlearn: 0.1675653\ttotal: 3m 35s\tremaining: 1m 14s\n",
      "744:\tlearn: 0.1675559\ttotal: 3m 35s\tremaining: 1m 13s\n",
      "745:\tlearn: 0.1675458\ttotal: 3m 35s\tremaining: 1m 13s\n",
      "746:\tlearn: 0.1674578\ttotal: 3m 35s\tremaining: 1m 13s\n",
      "747:\tlearn: 0.1674339\ttotal: 3m 36s\tremaining: 1m 12s\n",
      "748:\tlearn: 0.1672836\ttotal: 3m 36s\tremaining: 1m 12s\n",
      "749:\tlearn: 0.1672751\ttotal: 3m 36s\tremaining: 1m 12s\n",
      "750:\tlearn: 0.1672710\ttotal: 3m 36s\tremaining: 1m 11s\n",
      "751:\tlearn: 0.1672663\ttotal: 3m 37s\tremaining: 1m 11s\n",
      "752:\tlearn: 0.1672514\ttotal: 3m 37s\tremaining: 1m 11s\n",
      "753:\tlearn: 0.1672446\ttotal: 3m 37s\tremaining: 1m 11s\n",
      "754:\tlearn: 0.1672396\ttotal: 3m 37s\tremaining: 1m 10s\n",
      "755:\tlearn: 0.1671347\ttotal: 3m 38s\tremaining: 1m 10s\n",
      "756:\tlearn: 0.1669790\ttotal: 3m 38s\tremaining: 1m 10s\n",
      "757:\tlearn: 0.1669602\ttotal: 3m 38s\tremaining: 1m 9s\n",
      "758:\tlearn: 0.1669404\ttotal: 3m 38s\tremaining: 1m 9s\n",
      "759:\tlearn: 0.1668729\ttotal: 3m 39s\tremaining: 1m 9s\n",
      "760:\tlearn: 0.1668644\ttotal: 3m 39s\tremaining: 1m 8s\n",
      "761:\tlearn: 0.1668571\ttotal: 3m 39s\tremaining: 1m 8s\n",
      "762:\tlearn: 0.1668353\ttotal: 3m 40s\tremaining: 1m 8s\n",
      "763:\tlearn: 0.1668303\ttotal: 3m 40s\tremaining: 1m 8s\n",
      "764:\tlearn: 0.1668179\ttotal: 3m 40s\tremaining: 1m 7s\n",
      "765:\tlearn: 0.1667998\ttotal: 3m 40s\tremaining: 1m 7s\n",
      "766:\tlearn: 0.1667926\ttotal: 3m 40s\tremaining: 1m 7s\n",
      "767:\tlearn: 0.1666864\ttotal: 3m 41s\tremaining: 1m 6s\n",
      "768:\tlearn: 0.1666025\ttotal: 3m 41s\tremaining: 1m 6s\n",
      "769:\tlearn: 0.1665646\ttotal: 3m 41s\tremaining: 1m 6s\n",
      "770:\tlearn: 0.1664094\ttotal: 3m 42s\tremaining: 1m 5s\n",
      "771:\tlearn: 0.1663987\ttotal: 3m 42s\tremaining: 1m 5s\n",
      "772:\tlearn: 0.1663357\ttotal: 3m 42s\tremaining: 1m 5s\n",
      "773:\tlearn: 0.1663132\ttotal: 3m 42s\tremaining: 1m 5s\n",
      "774:\tlearn: 0.1663046\ttotal: 3m 43s\tremaining: 1m 4s\n",
      "775:\tlearn: 0.1662958\ttotal: 3m 43s\tremaining: 1m 4s\n",
      "776:\tlearn: 0.1662823\ttotal: 3m 43s\tremaining: 1m 4s\n",
      "777:\tlearn: 0.1662755\ttotal: 3m 43s\tremaining: 1m 3s\n",
      "778:\tlearn: 0.1662165\ttotal: 3m 44s\tremaining: 1m 3s\n",
      "779:\tlearn: 0.1662016\ttotal: 3m 44s\tremaining: 1m 3s\n",
      "780:\tlearn: 0.1661569\ttotal: 3m 44s\tremaining: 1m 3s\n",
      "781:\tlearn: 0.1661351\ttotal: 3m 45s\tremaining: 1m 2s\n",
      "782:\tlearn: 0.1660884\ttotal: 3m 45s\tremaining: 1m 2s\n",
      "783:\tlearn: 0.1660823\ttotal: 3m 45s\tremaining: 1m 2s\n",
      "784:\tlearn: 0.1660592\ttotal: 3m 45s\tremaining: 1m 1s\n",
      "785:\tlearn: 0.1659530\ttotal: 3m 46s\tremaining: 1m 1s\n",
      "786:\tlearn: 0.1658832\ttotal: 3m 46s\tremaining: 1m 1s\n",
      "787:\tlearn: 0.1658762\ttotal: 3m 46s\tremaining: 1m\n",
      "788:\tlearn: 0.1658635\ttotal: 3m 46s\tremaining: 1m\n",
      "789:\tlearn: 0.1658576\ttotal: 3m 47s\tremaining: 1m\n",
      "790:\tlearn: 0.1657499\ttotal: 3m 47s\tremaining: 1m\n",
      "791:\tlearn: 0.1657441\ttotal: 3m 47s\tremaining: 59.8s\n",
      "792:\tlearn: 0.1657309\ttotal: 3m 47s\tremaining: 59.5s\n",
      "793:\tlearn: 0.1656864\ttotal: 3m 48s\tremaining: 59.2s\n",
      "794:\tlearn: 0.1656724\ttotal: 3m 48s\tremaining: 58.9s\n",
      "795:\tlearn: 0.1656617\ttotal: 3m 48s\tremaining: 58.6s\n",
      "796:\tlearn: 0.1656566\ttotal: 3m 48s\tremaining: 58.3s\n",
      "797:\tlearn: 0.1656036\ttotal: 3m 49s\tremaining: 58s\n",
      "798:\tlearn: 0.1655956\ttotal: 3m 49s\tremaining: 57.7s\n",
      "799:\tlearn: 0.1655185\ttotal: 3m 49s\tremaining: 57.4s\n",
      "800:\tlearn: 0.1654464\ttotal: 3m 49s\tremaining: 57.1s\n",
      "801:\tlearn: 0.1653974\ttotal: 3m 50s\tremaining: 56.8s\n",
      "802:\tlearn: 0.1653828\ttotal: 3m 50s\tremaining: 56.5s\n",
      "803:\tlearn: 0.1652764\ttotal: 3m 50s\tremaining: 56.3s\n",
      "804:\tlearn: 0.1652714\ttotal: 3m 51s\tremaining: 56s\n",
      "805:\tlearn: 0.1652571\ttotal: 3m 51s\tremaining: 55.7s\n",
      "806:\tlearn: 0.1652280\ttotal: 3m 51s\tremaining: 55.4s\n",
      "807:\tlearn: 0.1652184\ttotal: 3m 51s\tremaining: 55.1s\n",
      "808:\tlearn: 0.1652149\ttotal: 3m 52s\tremaining: 54.8s\n",
      "809:\tlearn: 0.1651890\ttotal: 3m 52s\tremaining: 54.5s\n",
      "810:\tlearn: 0.1651595\ttotal: 3m 52s\tremaining: 54.2s\n",
      "811:\tlearn: 0.1651522\ttotal: 3m 52s\tremaining: 53.9s\n",
      "812:\tlearn: 0.1650613\ttotal: 3m 53s\tremaining: 53.6s\n",
      "813:\tlearn: 0.1650388\ttotal: 3m 53s\tremaining: 53.3s\n",
      "814:\tlearn: 0.1650143\ttotal: 3m 53s\tremaining: 53.1s\n",
      "815:\tlearn: 0.1649883\ttotal: 3m 53s\tremaining: 52.8s\n",
      "816:\tlearn: 0.1649815\ttotal: 3m 54s\tremaining: 52.5s\n",
      "817:\tlearn: 0.1649752\ttotal: 3m 54s\tremaining: 52.2s\n",
      "818:\tlearn: 0.1649535\ttotal: 3m 54s\tremaining: 51.9s\n",
      "819:\tlearn: 0.1649360\ttotal: 3m 55s\tremaining: 51.6s\n",
      "820:\tlearn: 0.1649307\ttotal: 3m 55s\tremaining: 51.3s\n",
      "821:\tlearn: 0.1647470\ttotal: 3m 55s\tremaining: 51s\n",
      "822:\tlearn: 0.1647294\ttotal: 3m 55s\tremaining: 50.7s\n",
      "823:\tlearn: 0.1647253\ttotal: 3m 56s\tremaining: 50.5s\n",
      "824:\tlearn: 0.1647031\ttotal: 3m 56s\tremaining: 50.2s\n",
      "825:\tlearn: 0.1646957\ttotal: 3m 56s\tremaining: 49.9s\n",
      "826:\tlearn: 0.1646837\ttotal: 3m 56s\tremaining: 49.6s\n",
      "827:\tlearn: 0.1646769\ttotal: 3m 57s\tremaining: 49.3s\n",
      "828:\tlearn: 0.1646616\ttotal: 3m 57s\tremaining: 49s\n",
      "829:\tlearn: 0.1646575\ttotal: 3m 57s\tremaining: 48.7s\n",
      "830:\tlearn: 0.1645935\ttotal: 3m 57s\tremaining: 48.4s\n",
      "831:\tlearn: 0.1644633\ttotal: 3m 58s\tremaining: 48.1s\n",
      "832:\tlearn: 0.1644591\ttotal: 3m 58s\tremaining: 47.8s\n",
      "833:\tlearn: 0.1644241\ttotal: 3m 58s\tremaining: 47.5s\n",
      "834:\tlearn: 0.1644119\ttotal: 3m 59s\tremaining: 47.2s\n",
      "835:\tlearn: 0.1643948\ttotal: 3m 59s\tremaining: 47s\n",
      "836:\tlearn: 0.1643699\ttotal: 3m 59s\tremaining: 46.7s\n",
      "837:\tlearn: 0.1643563\ttotal: 3m 59s\tremaining: 46.4s\n",
      "838:\tlearn: 0.1643502\ttotal: 4m\tremaining: 46.1s\n",
      "839:\tlearn: 0.1641563\ttotal: 4m\tremaining: 45.8s\n",
      "840:\tlearn: 0.1641474\ttotal: 4m\tremaining: 45.5s\n",
      "841:\tlearn: 0.1641409\ttotal: 4m\tremaining: 45.2s\n",
      "842:\tlearn: 0.1640446\ttotal: 4m 1s\tremaining: 45s\n",
      "843:\tlearn: 0.1639898\ttotal: 4m 1s\tremaining: 44.7s\n",
      "844:\tlearn: 0.1639855\ttotal: 4m 1s\tremaining: 44.4s\n",
      "845:\tlearn: 0.1639718\ttotal: 4m 2s\tremaining: 44.1s\n",
      "846:\tlearn: 0.1639633\ttotal: 4m 2s\tremaining: 43.8s\n",
      "847:\tlearn: 0.1639595\ttotal: 4m 2s\tremaining: 43.5s\n",
      "848:\tlearn: 0.1639200\ttotal: 4m 2s\tremaining: 43.2s\n",
      "849:\tlearn: 0.1638893\ttotal: 4m 3s\tremaining: 42.9s\n",
      "850:\tlearn: 0.1638812\ttotal: 4m 3s\tremaining: 42.6s\n",
      "851:\tlearn: 0.1638785\ttotal: 4m 3s\tremaining: 42.3s\n",
      "852:\tlearn: 0.1638746\ttotal: 4m 4s\tremaining: 42s\n",
      "853:\tlearn: 0.1638327\ttotal: 4m 4s\tremaining: 41.8s\n",
      "854:\tlearn: 0.1638257\ttotal: 4m 4s\tremaining: 41.5s\n",
      "855:\tlearn: 0.1638201\ttotal: 4m 4s\tremaining: 41.2s\n",
      "856:\tlearn: 0.1638137\ttotal: 4m 4s\tremaining: 40.9s\n",
      "857:\tlearn: 0.1638055\ttotal: 4m 5s\tremaining: 40.6s\n",
      "858:\tlearn: 0.1637980\ttotal: 4m 5s\tremaining: 40.3s\n",
      "859:\tlearn: 0.1637930\ttotal: 4m 5s\tremaining: 40s\n",
      "860:\tlearn: 0.1637890\ttotal: 4m 6s\tremaining: 39.7s\n",
      "861:\tlearn: 0.1637703\ttotal: 4m 6s\tremaining: 39.4s\n",
      "862:\tlearn: 0.1637622\ttotal: 4m 6s\tremaining: 39.1s\n",
      "863:\tlearn: 0.1637352\ttotal: 4m 6s\tremaining: 38.8s\n",
      "864:\tlearn: 0.1637064\ttotal: 4m 7s\tremaining: 38.6s\n",
      "865:\tlearn: 0.1637017\ttotal: 4m 7s\tremaining: 38.3s\n",
      "866:\tlearn: 0.1636956\ttotal: 4m 7s\tremaining: 38s\n",
      "867:\tlearn: 0.1636889\ttotal: 4m 7s\tremaining: 37.7s\n",
      "868:\tlearn: 0.1636848\ttotal: 4m 8s\tremaining: 37.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "869:\tlearn: 0.1636642\ttotal: 4m 8s\tremaining: 37.1s\n",
      "870:\tlearn: 0.1636578\ttotal: 4m 8s\tremaining: 36.8s\n",
      "871:\tlearn: 0.1636540\ttotal: 4m 8s\tremaining: 36.5s\n",
      "872:\tlearn: 0.1636495\ttotal: 4m 9s\tremaining: 36.2s\n",
      "873:\tlearn: 0.1636438\ttotal: 4m 9s\tremaining: 35.9s\n",
      "874:\tlearn: 0.1636221\ttotal: 4m 9s\tremaining: 35.6s\n",
      "875:\tlearn: 0.1636195\ttotal: 4m 9s\tremaining: 35.4s\n",
      "876:\tlearn: 0.1636097\ttotal: 4m 10s\tremaining: 35.1s\n",
      "877:\tlearn: 0.1636077\ttotal: 4m 10s\tremaining: 34.8s\n",
      "878:\tlearn: 0.1635692\ttotal: 4m 10s\tremaining: 34.5s\n",
      "879:\tlearn: 0.1635662\ttotal: 4m 10s\tremaining: 34.2s\n",
      "880:\tlearn: 0.1634992\ttotal: 4m 11s\tremaining: 33.9s\n",
      "881:\tlearn: 0.1634632\ttotal: 4m 11s\tremaining: 33.6s\n",
      "882:\tlearn: 0.1634389\ttotal: 4m 11s\tremaining: 33.4s\n",
      "883:\tlearn: 0.1634309\ttotal: 4m 11s\tremaining: 33.1s\n",
      "884:\tlearn: 0.1634199\ttotal: 4m 12s\tremaining: 32.8s\n",
      "885:\tlearn: 0.1634072\ttotal: 4m 12s\tremaining: 32.5s\n",
      "886:\tlearn: 0.1634004\ttotal: 4m 12s\tremaining: 32.2s\n",
      "887:\tlearn: 0.1633964\ttotal: 4m 12s\tremaining: 31.9s\n",
      "888:\tlearn: 0.1633921\ttotal: 4m 13s\tremaining: 31.6s\n",
      "889:\tlearn: 0.1633871\ttotal: 4m 13s\tremaining: 31.3s\n",
      "890:\tlearn: 0.1633726\ttotal: 4m 13s\tremaining: 31s\n",
      "891:\tlearn: 0.1633695\ttotal: 4m 13s\tremaining: 30.7s\n",
      "892:\tlearn: 0.1633658\ttotal: 4m 14s\tremaining: 30.5s\n",
      "893:\tlearn: 0.1633000\ttotal: 4m 14s\tremaining: 30.2s\n",
      "894:\tlearn: 0.1632980\ttotal: 4m 14s\tremaining: 29.9s\n",
      "895:\tlearn: 0.1632947\ttotal: 4m 14s\tremaining: 29.6s\n",
      "896:\tlearn: 0.1632840\ttotal: 4m 15s\tremaining: 29.3s\n",
      "897:\tlearn: 0.1632803\ttotal: 4m 15s\tremaining: 29s\n",
      "898:\tlearn: 0.1632653\ttotal: 4m 15s\tremaining: 28.7s\n",
      "899:\tlearn: 0.1632481\ttotal: 4m 16s\tremaining: 28.4s\n",
      "900:\tlearn: 0.1632347\ttotal: 4m 16s\tremaining: 28.2s\n",
      "901:\tlearn: 0.1632283\ttotal: 4m 16s\tremaining: 27.9s\n",
      "902:\tlearn: 0.1632233\ttotal: 4m 16s\tremaining: 27.6s\n",
      "903:\tlearn: 0.1632195\ttotal: 4m 17s\tremaining: 27.3s\n",
      "904:\tlearn: 0.1632134\ttotal: 4m 17s\tremaining: 27s\n",
      "905:\tlearn: 0.1632043\ttotal: 4m 17s\tremaining: 26.7s\n",
      "906:\tlearn: 0.1632013\ttotal: 4m 17s\tremaining: 26.4s\n",
      "907:\tlearn: 0.1631560\ttotal: 4m 18s\tremaining: 26.1s\n",
      "908:\tlearn: 0.1631528\ttotal: 4m 18s\tremaining: 25.9s\n",
      "909:\tlearn: 0.1631286\ttotal: 4m 18s\tremaining: 25.6s\n",
      "910:\tlearn: 0.1631233\ttotal: 4m 18s\tremaining: 25.3s\n",
      "911:\tlearn: 0.1631200\ttotal: 4m 19s\tremaining: 25s\n",
      "912:\tlearn: 0.1631177\ttotal: 4m 19s\tremaining: 24.7s\n",
      "913:\tlearn: 0.1631131\ttotal: 4m 19s\tremaining: 24.4s\n",
      "914:\tlearn: 0.1631073\ttotal: 4m 19s\tremaining: 24.1s\n",
      "915:\tlearn: 0.1630103\ttotal: 4m 20s\tremaining: 23.9s\n",
      "916:\tlearn: 0.1630056\ttotal: 4m 20s\tremaining: 23.6s\n",
      "917:\tlearn: 0.1629662\ttotal: 4m 20s\tremaining: 23.3s\n",
      "918:\tlearn: 0.1629449\ttotal: 4m 20s\tremaining: 23s\n",
      "919:\tlearn: 0.1629426\ttotal: 4m 21s\tremaining: 22.7s\n",
      "920:\tlearn: 0.1629400\ttotal: 4m 21s\tremaining: 22.4s\n",
      "921:\tlearn: 0.1629360\ttotal: 4m 21s\tremaining: 22.1s\n",
      "922:\tlearn: 0.1629303\ttotal: 4m 21s\tremaining: 21.9s\n",
      "923:\tlearn: 0.1629273\ttotal: 4m 22s\tremaining: 21.6s\n",
      "924:\tlearn: 0.1628826\ttotal: 4m 22s\tremaining: 21.3s\n",
      "925:\tlearn: 0.1627915\ttotal: 4m 22s\tremaining: 21s\n",
      "926:\tlearn: 0.1626628\ttotal: 4m 23s\tremaining: 20.7s\n",
      "927:\tlearn: 0.1626403\ttotal: 4m 23s\tremaining: 20.4s\n",
      "928:\tlearn: 0.1626367\ttotal: 4m 23s\tremaining: 20.1s\n",
      "929:\tlearn: 0.1626286\ttotal: 4m 23s\tremaining: 19.9s\n",
      "930:\tlearn: 0.1626236\ttotal: 4m 24s\tremaining: 19.6s\n",
      "931:\tlearn: 0.1626166\ttotal: 4m 24s\tremaining: 19.3s\n",
      "932:\tlearn: 0.1626115\ttotal: 4m 24s\tremaining: 19s\n",
      "933:\tlearn: 0.1626054\ttotal: 4m 24s\tremaining: 18.7s\n",
      "934:\tlearn: 0.1625364\ttotal: 4m 25s\tremaining: 18.4s\n",
      "935:\tlearn: 0.1625212\ttotal: 4m 25s\tremaining: 18.1s\n",
      "936:\tlearn: 0.1625191\ttotal: 4m 25s\tremaining: 17.9s\n",
      "937:\tlearn: 0.1625046\ttotal: 4m 25s\tremaining: 17.6s\n",
      "938:\tlearn: 0.1624886\ttotal: 4m 26s\tremaining: 17.3s\n",
      "939:\tlearn: 0.1624846\ttotal: 4m 26s\tremaining: 17s\n",
      "940:\tlearn: 0.1624803\ttotal: 4m 26s\tremaining: 16.7s\n",
      "941:\tlearn: 0.1624785\ttotal: 4m 26s\tremaining: 16.4s\n",
      "942:\tlearn: 0.1624729\ttotal: 4m 27s\tremaining: 16.1s\n",
      "943:\tlearn: 0.1623726\ttotal: 4m 27s\tremaining: 15.9s\n",
      "944:\tlearn: 0.1623447\ttotal: 4m 27s\tremaining: 15.6s\n",
      "945:\tlearn: 0.1623431\ttotal: 4m 27s\tremaining: 15.3s\n",
      "946:\tlearn: 0.1623413\ttotal: 4m 28s\tremaining: 15s\n",
      "947:\tlearn: 0.1622140\ttotal: 4m 28s\tremaining: 14.7s\n",
      "948:\tlearn: 0.1621990\ttotal: 4m 28s\tremaining: 14.4s\n",
      "949:\tlearn: 0.1621825\ttotal: 4m 29s\tremaining: 14.2s\n",
      "950:\tlearn: 0.1621792\ttotal: 4m 29s\tremaining: 13.9s\n",
      "951:\tlearn: 0.1621624\ttotal: 4m 29s\tremaining: 13.6s\n",
      "952:\tlearn: 0.1621454\ttotal: 4m 29s\tremaining: 13.3s\n",
      "953:\tlearn: 0.1620972\ttotal: 4m 30s\tremaining: 13s\n",
      "954:\tlearn: 0.1620663\ttotal: 4m 30s\tremaining: 12.7s\n",
      "955:\tlearn: 0.1620633\ttotal: 4m 30s\tremaining: 12.5s\n",
      "956:\tlearn: 0.1619910\ttotal: 4m 30s\tremaining: 12.2s\n",
      "957:\tlearn: 0.1619041\ttotal: 4m 31s\tremaining: 11.9s\n",
      "958:\tlearn: 0.1619004\ttotal: 4m 31s\tremaining: 11.6s\n",
      "959:\tlearn: 0.1618852\ttotal: 4m 31s\tremaining: 11.3s\n",
      "960:\tlearn: 0.1618667\ttotal: 4m 31s\tremaining: 11s\n",
      "961:\tlearn: 0.1618635\ttotal: 4m 32s\tremaining: 10.8s\n",
      "962:\tlearn: 0.1618480\ttotal: 4m 32s\tremaining: 10.5s\n",
      "963:\tlearn: 0.1618424\ttotal: 4m 32s\tremaining: 10.2s\n",
      "964:\tlearn: 0.1618409\ttotal: 4m 32s\tremaining: 9.9s\n",
      "965:\tlearn: 0.1617764\ttotal: 4m 33s\tremaining: 9.62s\n",
      "966:\tlearn: 0.1617621\ttotal: 4m 33s\tremaining: 9.33s\n",
      "967:\tlearn: 0.1617328\ttotal: 4m 33s\tremaining: 9.05s\n",
      "968:\tlearn: 0.1617305\ttotal: 4m 34s\tremaining: 8.77s\n",
      "969:\tlearn: 0.1617262\ttotal: 4m 34s\tremaining: 8.48s\n",
      "970:\tlearn: 0.1617147\ttotal: 4m 34s\tremaining: 8.2s\n",
      "971:\tlearn: 0.1616793\ttotal: 4m 34s\tremaining: 7.92s\n",
      "972:\tlearn: 0.1616745\ttotal: 4m 35s\tremaining: 7.63s\n",
      "973:\tlearn: 0.1615385\ttotal: 4m 35s\tremaining: 7.35s\n",
      "974:\tlearn: 0.1614466\ttotal: 4m 35s\tremaining: 7.07s\n",
      "975:\tlearn: 0.1614418\ttotal: 4m 35s\tremaining: 6.78s\n",
      "976:\tlearn: 0.1612723\ttotal: 4m 36s\tremaining: 6.5s\n",
      "977:\tlearn: 0.1611856\ttotal: 4m 36s\tremaining: 6.22s\n",
      "978:\tlearn: 0.1611812\ttotal: 4m 36s\tremaining: 5.94s\n",
      "979:\tlearn: 0.1611771\ttotal: 4m 37s\tremaining: 5.66s\n",
      "980:\tlearn: 0.1611722\ttotal: 4m 37s\tremaining: 5.37s\n",
      "981:\tlearn: 0.1611542\ttotal: 4m 37s\tremaining: 5.09s\n",
      "982:\tlearn: 0.1611512\ttotal: 4m 37s\tremaining: 4.8s\n",
      "983:\tlearn: 0.1610840\ttotal: 4m 38s\tremaining: 4.52s\n",
      "984:\tlearn: 0.1610748\ttotal: 4m 38s\tremaining: 4.24s\n",
      "985:\tlearn: 0.1610032\ttotal: 4m 38s\tremaining: 3.96s\n",
      "986:\tlearn: 0.1609833\ttotal: 4m 38s\tremaining: 3.67s\n",
      "987:\tlearn: 0.1609796\ttotal: 4m 39s\tremaining: 3.39s\n",
      "988:\tlearn: 0.1609738\ttotal: 4m 39s\tremaining: 3.11s\n",
      "989:\tlearn: 0.1609663\ttotal: 4m 39s\tremaining: 2.83s\n",
      "990:\tlearn: 0.1609617\ttotal: 4m 39s\tremaining: 2.54s\n",
      "991:\tlearn: 0.1609544\ttotal: 4m 40s\tremaining: 2.26s\n",
      "992:\tlearn: 0.1609522\ttotal: 4m 40s\tremaining: 1.98s\n",
      "993:\tlearn: 0.1609484\ttotal: 4m 40s\tremaining: 1.69s\n",
      "994:\tlearn: 0.1609455\ttotal: 4m 40s\tremaining: 1.41s\n",
      "995:\tlearn: 0.1609245\ttotal: 4m 41s\tremaining: 1.13s\n",
      "996:\tlearn: 0.1608885\ttotal: 4m 41s\tremaining: 847ms\n",
      "997:\tlearn: 0.1608232\ttotal: 4m 41s\tremaining: 565ms\n",
      "998:\tlearn: 0.1606947\ttotal: 4m 42s\tremaining: 282ms\n",
      "999:\tlearn: 0.1606776\ttotal: 4m 42s\tremaining: 0us\n",
      "CatBoostClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.98      0.94      3781\n",
      "         1.0       0.97      0.89      0.93      3620\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      7401\n",
      "   macro avg       0.94      0.93      0.93      7401\n",
      "weighted avg       0.94      0.93      0.93      7401\n",
      "\n",
      "[[3696   85]\n",
      " [ 409 3211]]\n",
      "Accuracy is  93.32522632076746\n",
      "Time on model's work: 288.965 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.70      0.79      3781\n",
      "         1.0       0.75      0.92      0.82      3620\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      7401\n",
      "   macro avg       0.82      0.81      0.81      7401\n",
      "weighted avg       0.82      0.81      0.81      7401\n",
      "\n",
      "[[2658 1123]\n",
      " [ 298 3322]]\n",
      "Accuracy is  80.79989190649913\n",
      "Time on model's work: 0.92 s\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maksim.parats\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:144: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.62      0.74      3781\n",
      "         1.0       0.70      0.95      0.81      3620\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      7401\n",
      "   macro avg       0.81      0.78      0.77      7401\n",
      "weighted avg       0.82      0.78      0.77      7401\n",
      "\n",
      "[[2337 1444]\n",
      " [ 189 3431]]\n",
      "Accuracy is  77.93541413322525\n",
      "Time on model's work: 0.494 s\n",
      "====================================================================================================\n",
      "TFFMClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.70      0.79      3781\n",
      "         1.0       0.75      0.92      0.83      3620\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      7401\n",
      "   macro avg       0.83      0.81      0.81      7401\n",
      "weighted avg       0.83      0.81      0.81      7401\n",
      "\n",
      "[[2662 1119]\n",
      " [ 274 3346]]\n",
      "Accuracy is  81.17821915957303\n",
      "Time on model's work: 63.489 s\n",
      "====================================================================================================\n",
      "TOTAL TIME SPENT:  1247.359 s\n"
     ]
    }
   ],
   "source": [
    "clfs = [\n",
    "        ['RandomForestClassifier', RandomForestClassifier()],\n",
    "        ['GradientBoostingClassifier', GradientBoostingClassifier()],\n",
    "        ['ExtraTreesClassifier', ExtraTreesClassifier()],\n",
    "        ['AdaBoostClassifier', AdaBoostClassifier()],\n",
    "        ['BaggingClassifier', BaggingClassifier()],\n",
    "        ['DecisionTreeClassifier', DecisionTreeClassifier()],\n",
    "        ['MLPClassifier', MLPClassifier()],\n",
    "        ['XGBClassifier', XGBClassifier()],\n",
    "        ['CatBoostClassifier', CatBoostClassifier()],\n",
    "        ['LogisticRegression', LogisticRegression()],\n",
    "        ['SGDClassifier', SGDClassifier()],\n",
    "        ['TFFMClassifier', TFFMClassifier()]\n",
    "       ]\n",
    "t = time()\n",
    "for name, clf in clfs:\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    prediction = clf.predict(X_test)\n",
    "    print(name)\n",
    "    print(classification_report(y_test, prediction))\n",
    "    print(confusion_matrix(y_test, prediction))\n",
    "    print('Accuracy is ', accuracy_score(y_test, prediction)*100)\n",
    "    print (\"Time on model's work:\", round(time()-t0, 3), \"s\")\n",
    "    print('='*100)\n",
    "print (\"TOTAL TIME SPENT: \", round(time()-t, 3), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:13<00:00,  3.92epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8197540872855019\n",
      "[[2698 1083]\n",
      " [ 251 3369]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.71      0.80      3781\n",
      "         1.0       0.76      0.93      0.83      3620\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      7401\n",
      "   macro avg       0.84      0.82      0.82      7401\n",
      "weighted avg       0.84      0.82      0.82      7401\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:13<00:00,  3.84epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8079989190649912\n",
      "[[2497 1284]\n",
      " [ 137 3483]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.66      0.78      3781\n",
      "         1.0       0.73      0.96      0.83      3620\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      7401\n",
      "   macro avg       0.84      0.81      0.80      7401\n",
      "weighted avg       0.84      0.81      0.80      7401\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:14<00:00,  3.60epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8048912309147412\n",
      "[[2420 1361]\n",
      " [  83 3537]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.64      0.77      3781\n",
      "         1.0       0.72      0.98      0.83      3620\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      7401\n",
      "   macro avg       0.84      0.81      0.80      7401\n",
      "weighted avg       0.85      0.80      0.80      7401\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:13<00:00,  3.83epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7966491014727739\n",
      "[[2347 1434]\n",
      " [  71 3549]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.62      0.76      3781\n",
      "         1.0       0.71      0.98      0.83      3620\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      7401\n",
      "   macro avg       0.84      0.80      0.79      7401\n",
      "weighted avg       0.84      0.80      0.79      7401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TFFM sparse - works worse with sparse\n",
    "# only CSR format supported\n",
    "X_train_sparse = sp.csr_matrix(X_train)\n",
    "X_test_sparse = sp.csr_matrix(X_test)\n",
    "# weight - optional / AdamOptimizer\n",
    "pos_class_weight = list(map(float,range(1, 5)))\n",
    "for weight in pos_class_weight:\n",
    "    model = TFFMClassifier(\n",
    "        order=2,\n",
    "        pos_class_weight=weight,\n",
    "        rank=10, \n",
    "        optimizer=tf.train.AdamOptimizer(learning_rate=0.001), \n",
    "        n_epochs=50, \n",
    "        batch_size=1024,\n",
    "        init_std=0.001,\n",
    "        reg=0.01,\n",
    "        input_type='sparse',\n",
    "        #log_dir='./tmp/logs',\n",
    "        #verbose=1,\n",
    "        seed=42\n",
    "    )\n",
    "    model.fit(X_train_sparse, y_train, show_progress=True)\n",
    "    predictions = model.predict(X_test_sparse)\n",
    "    print('accuracy: {}'.format(accuracy_score(y_test, predictions)))\n",
    "    # this will close tf.Session and free resources\n",
    "    print(confusion_matrix(y_test,predictions)) \n",
    "    print(classification_report(y_test, predictions))\n",
    "    model.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:13<00:00,  3.77epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7419267666531549\n",
      "[[2305 1476]\n",
      " [ 434 3186]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.61      0.71      3781\n",
      "         1.0       0.68      0.88      0.77      3620\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      7401\n",
      "   macro avg       0.76      0.74      0.74      7401\n",
      "weighted avg       0.76      0.74      0.74      7401\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:13<00:00,  3.83epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7592217267936765\n",
      "[[2288 1493]\n",
      " [ 289 3331]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.61      0.72      3781\n",
      "         1.0       0.69      0.92      0.79      3620\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      7401\n",
      "   macro avg       0.79      0.76      0.75      7401\n",
      "weighted avg       0.79      0.76      0.75      7401\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:13<00:00,  3.81epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7588163761653831\n",
      "[[2218 1563]\n",
      " [ 222 3398]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.59      0.71      3781\n",
      "         1.0       0.68      0.94      0.79      3620\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      7401\n",
      "   macro avg       0.80      0.76      0.75      7401\n",
      "weighted avg       0.80      0.76      0.75      7401\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:13<00:00,  3.85epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7471963248209701\n",
      "[[2071 1710]\n",
      " [ 161 3459]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.55      0.69      3781\n",
      "         1.0       0.67      0.96      0.79      3620\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      7401\n",
      "   macro avg       0.80      0.75      0.74      7401\n",
      "weighted avg       0.80      0.75      0.74      7401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# weight - optional / FtrlOptimizer\n",
    "pos_class_weight = list(map(float,range(1, 5)))\n",
    "for weight in pos_class_weight:\n",
    "    model = TFFMClassifier(\n",
    "        order=2,\n",
    "        pos_class_weight=weight,\n",
    "        rank=10, \n",
    "        optimizer=tf.train.FtrlOptimizer(0.01, l2_regularization_strength=0.001, l2_shrinkage_regularization_strength=0.001), \n",
    "        n_epochs=50, \n",
    "        batch_size=1024,\n",
    "        init_std=0.001,\n",
    "        reg=0.01,\n",
    "        input_type='sparse',\n",
    "        #log_dir='./tmp/logs',\n",
    "        #verbose=1,\n",
    "        seed=42\n",
    "    )\n",
    "    model.fit(X_train_sparse, y_train, show_progress=True)\n",
    "    predictions = model.predict(X_test_sparse)\n",
    "    print('accuracy: {}'.format(accuracy_score(y_test, predictions)))\n",
    "    # this will close tf.Session and free resources\n",
    "    print(confusion_matrix(y_test,predictions)) \n",
    "    print(classification_report(y_test, predictions))\n",
    "    model.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "29603/29603 [==============================] - ETA: 31s - loss: 0.7649 - acc: 0.53 - ETA: 6s - loss: 0.7570 - acc: 0.5026 - ETA: 4s - loss: 0.7446 - acc: 0.516 - ETA: 3s - loss: 0.7357 - acc: 0.526 - ETA: 3s - loss: 0.7351 - acc: 0.529 - ETA: 2s - loss: 0.7297 - acc: 0.533 - ETA: 2s - loss: 0.7244 - acc: 0.535 - ETA: 2s - loss: 0.7201 - acc: 0.539 - ETA: 1s - loss: 0.7121 - acc: 0.549 - ETA: 1s - loss: 0.7040 - acc: 0.559 - ETA: 1s - loss: 0.6979 - acc: 0.567 - ETA: 1s - loss: 0.6923 - acc: 0.573 - ETA: 1s - loss: 0.6872 - acc: 0.581 - ETA: 1s - loss: 0.6809 - acc: 0.589 - ETA: 1s - loss: 0.6759 - acc: 0.595 - ETA: 0s - loss: 0.6719 - acc: 0.601 - ETA: 0s - loss: 0.6668 - acc: 0.608 - ETA: 0s - loss: 0.6614 - acc: 0.615 - ETA: 0s - loss: 0.6568 - acc: 0.620 - ETA: 0s - loss: 0.6537 - acc: 0.624 - ETA: 0s - loss: 0.6505 - acc: 0.628 - ETA: 0s - loss: 0.6467 - acc: 0.632 - ETA: 0s - loss: 0.6431 - acc: 0.636 - ETA: 0s - loss: 0.6404 - acc: 0.640 - ETA: 0s - loss: 0.6388 - acc: 0.641 - ETA: 0s - loss: 0.6366 - acc: 0.644 - ETA: 0s - loss: 0.6334 - acc: 0.648 - 2s 61us/step - loss: 0.6322 - acc: 0.6500\n",
      "Epoch 2/20\n",
      "29603/29603 [==============================] - ETA: 1s - loss: 0.5663 - acc: 0.738 - ETA: 1s - loss: 0.5539 - acc: 0.748 - ETA: 1s - loss: 0.5638 - acc: 0.737 - ETA: 1s - loss: 0.5643 - acc: 0.738 - ETA: 1s - loss: 0.5622 - acc: 0.735 - ETA: 1s - loss: 0.5650 - acc: 0.734 - ETA: 1s - loss: 0.5672 - acc: 0.731 - ETA: 1s - loss: 0.5650 - acc: 0.730 - ETA: 0s - loss: 0.5642 - acc: 0.730 - ETA: 0s - loss: 0.5653 - acc: 0.730 - ETA: 0s - loss: 0.5651 - acc: 0.730 - ETA: 0s - loss: 0.5619 - acc: 0.732 - ETA: 0s - loss: 0.5618 - acc: 0.733 - ETA: 0s - loss: 0.5608 - acc: 0.734 - ETA: 0s - loss: 0.5625 - acc: 0.732 - ETA: 0s - loss: 0.5627 - acc: 0.732 - ETA: 0s - loss: 0.5628 - acc: 0.732 - ETA: 0s - loss: 0.5625 - acc: 0.733 - ETA: 0s - loss: 0.5621 - acc: 0.733 - ETA: 0s - loss: 0.5607 - acc: 0.735 - ETA: 0s - loss: 0.5595 - acc: 0.736 - ETA: 0s - loss: 0.5589 - acc: 0.736 - ETA: 0s - loss: 0.5577 - acc: 0.737 - ETA: 0s - loss: 0.5572 - acc: 0.738 - ETA: 0s - loss: 0.5560 - acc: 0.738 - ETA: 0s - loss: 0.5553 - acc: 0.739 - 1s 49us/step - loss: 0.5550 - acc: 0.7391\n",
      "Epoch 3/20\n",
      "29603/29603 [==============================] - ETA: 1s - loss: 0.5772 - acc: 0.710 - ETA: 1s - loss: 0.5540 - acc: 0.745 - ETA: 1s - loss: 0.5497 - acc: 0.740 - ETA: 1s - loss: 0.5554 - acc: 0.733 - ETA: 1s - loss: 0.5513 - acc: 0.738 - ETA: 1s - loss: 0.5483 - acc: 0.739 - ETA: 1s - loss: 0.5475 - acc: 0.740 - ETA: 1s - loss: 0.5484 - acc: 0.739 - ETA: 0s - loss: 0.5465 - acc: 0.743 - ETA: 0s - loss: 0.5443 - acc: 0.745 - ETA: 0s - loss: 0.5425 - acc: 0.746 - ETA: 0s - loss: 0.5397 - acc: 0.748 - ETA: 0s - loss: 0.5394 - acc: 0.748 - ETA: 0s - loss: 0.5412 - acc: 0.748 - ETA: 0s - loss: 0.5391 - acc: 0.749 - ETA: 0s - loss: 0.5393 - acc: 0.748 - ETA: 0s - loss: 0.5403 - acc: 0.747 - ETA: 0s - loss: 0.5413 - acc: 0.746 - ETA: 0s - loss: 0.5400 - acc: 0.747 - ETA: 0s - loss: 0.5400 - acc: 0.747 - ETA: 0s - loss: 0.5378 - acc: 0.749 - ETA: 0s - loss: 0.5383 - acc: 0.749 - ETA: 0s - loss: 0.5386 - acc: 0.748 - ETA: 0s - loss: 0.5386 - acc: 0.748 - ETA: 0s - loss: 0.5384 - acc: 0.748 - ETA: 0s - loss: 0.5382 - acc: 0.749 - 1s 49us/step - loss: 0.5379 - acc: 0.7490\n",
      "Epoch 4/20\n",
      "29603/29603 [==============================] - ETA: 1s - loss: 0.5209 - acc: 0.769 - ETA: 1s - loss: 0.5076 - acc: 0.776 - ETA: 1s - loss: 0.5174 - acc: 0.761 - ETA: 1s - loss: 0.5216 - acc: 0.758 - ETA: 1s - loss: 0.5174 - acc: 0.760 - ETA: 1s - loss: 0.5247 - acc: 0.755 - ETA: 1s - loss: 0.5249 - acc: 0.753 - ETA: 1s - loss: 0.5273 - acc: 0.750 - ETA: 1s - loss: 0.5285 - acc: 0.750 - ETA: 0s - loss: 0.5265 - acc: 0.752 - ETA: 0s - loss: 0.5253 - acc: 0.753 - ETA: 0s - loss: 0.5228 - acc: 0.754 - ETA: 0s - loss: 0.5223 - acc: 0.754 - ETA: 0s - loss: 0.5211 - acc: 0.755 - ETA: 0s - loss: 0.5213 - acc: 0.754 - ETA: 0s - loss: 0.5224 - acc: 0.754 - ETA: 0s - loss: 0.5217 - acc: 0.754 - ETA: 0s - loss: 0.5214 - acc: 0.754 - ETA: 0s - loss: 0.5221 - acc: 0.753 - ETA: 0s - loss: 0.5225 - acc: 0.752 - ETA: 0s - loss: 0.5219 - acc: 0.753 - ETA: 0s - loss: 0.5214 - acc: 0.753 - ETA: 0s - loss: 0.5200 - acc: 0.754 - ETA: 0s - loss: 0.5200 - acc: 0.754 - ETA: 0s - loss: 0.5199 - acc: 0.754 - 1s 49us/step - loss: 0.5195 - acc: 0.7551\n",
      "Epoch 5/20\n",
      "29603/29603 [==============================] - ETA: 1s - loss: 0.5521 - acc: 0.726 - ETA: 1s - loss: 0.5273 - acc: 0.749 - ETA: 1s - loss: 0.5382 - acc: 0.741 - ETA: 1s - loss: 0.5258 - acc: 0.750 - ETA: 1s - loss: 0.5211 - acc: 0.753 - ETA: 1s - loss: 0.5180 - acc: 0.756 - ETA: 1s - loss: 0.5147 - acc: 0.756 - ETA: 1s - loss: 0.5156 - acc: 0.755 - ETA: 0s - loss: 0.5150 - acc: 0.755 - ETA: 0s - loss: 0.5138 - acc: 0.756 - ETA: 0s - loss: 0.5138 - acc: 0.755 - ETA: 0s - loss: 0.5124 - acc: 0.757 - ETA: 0s - loss: 0.5112 - acc: 0.759 - ETA: 0s - loss: 0.5120 - acc: 0.758 - ETA: 0s - loss: 0.5113 - acc: 0.759 - ETA: 0s - loss: 0.5097 - acc: 0.760 - ETA: 0s - loss: 0.5097 - acc: 0.761 - ETA: 0s - loss: 0.5083 - acc: 0.762 - ETA: 0s - loss: 0.5090 - acc: 0.761 - ETA: 0s - loss: 0.5085 - acc: 0.762 - ETA: 0s - loss: 0.5084 - acc: 0.762 - ETA: 0s - loss: 0.5082 - acc: 0.761 - ETA: 0s - loss: 0.5078 - acc: 0.761 - ETA: 0s - loss: 0.5082 - acc: 0.761 - ETA: 0s - loss: 0.5076 - acc: 0.761 - ETA: 0s - loss: 0.5081 - acc: 0.762 - 1s 48us/step - loss: 0.5081 - acc: 0.7621\n",
      "Epoch 6/20\n",
      "29603/29603 [==============================] - ETA: 1s - loss: 0.4869 - acc: 0.789 - ETA: 1s - loss: 0.5201 - acc: 0.754 - ETA: 1s - loss: 0.5081 - acc: 0.764 - ETA: 1s - loss: 0.5011 - acc: 0.768 - ETA: 1s - loss: 0.5009 - acc: 0.766 - ETA: 1s - loss: 0.5048 - acc: 0.766 - ETA: 1s - loss: 0.5020 - acc: 0.768 - ETA: 1s - loss: 0.5010 - acc: 0.768 - ETA: 0s - loss: 0.5028 - acc: 0.767 - ETA: 0s - loss: 0.5060 - acc: 0.764 - ETA: 0s - loss: 0.5048 - acc: 0.765 - ETA: 0s - loss: 0.5041 - acc: 0.766 - ETA: 0s - loss: 0.5050 - acc: 0.766 - ETA: 0s - loss: 0.5046 - acc: 0.764 - ETA: 0s - loss: 0.5017 - acc: 0.766 - ETA: 0s - loss: 0.5022 - acc: 0.766 - ETA: 0s - loss: 0.5014 - acc: 0.767 - ETA: 0s - loss: 0.5010 - acc: 0.767 - ETA: 0s - loss: 0.4988 - acc: 0.768 - ETA: 0s - loss: 0.4982 - acc: 0.768 - ETA: 0s - loss: 0.4975 - acc: 0.769 - ETA: 0s - loss: 0.4965 - acc: 0.769 - ETA: 0s - loss: 0.4961 - acc: 0.770 - ETA: 0s - loss: 0.4965 - acc: 0.769 - ETA: 0s - loss: 0.4969 - acc: 0.769 - 1s 49us/step - loss: 0.4965 - acc: 0.7702\n",
      "Epoch 7/20\n",
      "29603/29603 [==============================] - ETA: 1s - loss: 0.4819 - acc: 0.793 - ETA: 1s - loss: 0.5122 - acc: 0.763 - ETA: 1s - loss: 0.5020 - acc: 0.767 - ETA: 1s - loss: 0.5024 - acc: 0.762 - ETA: 1s - loss: 0.4981 - acc: 0.766 - ETA: 1s - loss: 0.4937 - acc: 0.774 - ETA: 1s - loss: 0.4943 - acc: 0.772 - ETA: 1s - loss: 0.4931 - acc: 0.771 - ETA: 1s - loss: 0.4963 - acc: 0.768 - ETA: 1s - loss: 0.4973 - acc: 0.767 - ETA: 1s - loss: 0.4988 - acc: 0.766 - ETA: 0s - loss: 0.4970 - acc: 0.767 - ETA: 0s - loss: 0.4976 - acc: 0.767 - ETA: 0s - loss: 0.4964 - acc: 0.767 - ETA: 0s - loss: 0.4963 - acc: 0.767 - ETA: 0s - loss: 0.4963 - acc: 0.768 - ETA: 0s - loss: 0.4941 - acc: 0.770 - ETA: 0s - loss: 0.4929 - acc: 0.770 - ETA: 0s - loss: 0.4937 - acc: 0.769 - ETA: 0s - loss: 0.4931 - acc: 0.770 - ETA: 0s - loss: 0.4922 - acc: 0.770 - ETA: 0s - loss: 0.4915 - acc: 0.771 - ETA: 0s - loss: 0.4910 - acc: 0.772 - ETA: 0s - loss: 0.4904 - acc: 0.772 - ETA: 0s - loss: 0.4896 - acc: 0.773 - ETA: 0s - loss: 0.4897 - acc: 0.773 - ETA: 0s - loss: 0.4896 - acc: 0.773 - 2s 52us/step - loss: 0.4897 - acc: 0.7731\n",
      "Epoch 8/20\n",
      "29603/29603 [==============================] - ETA: 1s - loss: 0.5242 - acc: 0.738 - ETA: 1s - loss: 0.4937 - acc: 0.761 - ETA: 1s - loss: 0.4894 - acc: 0.768 - ETA: 1s - loss: 0.4881 - acc: 0.767 - ETA: 1s - loss: 0.4865 - acc: 0.768 - ETA: 1s - loss: 0.4869 - acc: 0.771 - ETA: 1s - loss: 0.4872 - acc: 0.772 - ETA: 1s - loss: 0.4841 - acc: 0.775 - ETA: 1s - loss: 0.4839 - acc: 0.775 - ETA: 1s - loss: 0.4844 - acc: 0.775 - ETA: 0s - loss: 0.4838 - acc: 0.776 - ETA: 0s - loss: 0.4811 - acc: 0.777 - ETA: 0s - loss: 0.4846 - acc: 0.775 - ETA: 0s - loss: 0.4842 - acc: 0.775 - ETA: 0s - loss: 0.4838 - acc: 0.774 - ETA: 0s - loss: 0.4849 - acc: 0.774 - ETA: 0s - loss: 0.4848 - acc: 0.775 - ETA: 0s - loss: 0.4841 - acc: 0.774 - ETA: 0s - loss: 0.4840 - acc: 0.775 - ETA: 0s - loss: 0.4828 - acc: 0.775 - ETA: 0s - loss: 0.4830 - acc: 0.776 - ETA: 0s - loss: 0.4825 - acc: 0.776 - ETA: 0s - loss: 0.4818 - acc: 0.777 - ETA: 0s - loss: 0.4812 - acc: 0.777 - ETA: 0s - loss: 0.4809 - acc: 0.777 - ETA: 0s - loss: 0.4818 - acc: 0.776 - 1s 50us/step - loss: 0.4817 - acc: 0.7769\n",
      "Epoch 9/20\n",
      "29603/29603 [==============================] - ETA: 1s - loss: 0.5156 - acc: 0.781 - ETA: 1s - loss: 0.4750 - acc: 0.787 - ETA: 1s - loss: 0.4746 - acc: 0.785 - ETA: 1s - loss: 0.4744 - acc: 0.785 - ETA: 1s - loss: 0.4780 - acc: 0.780 - ETA: 1s - loss: 0.4773 - acc: 0.782 - ETA: 1s - loss: 0.4795 - acc: 0.780 - ETA: 1s - loss: 0.4796 - acc: 0.780 - ETA: 0s - loss: 0.4824 - acc: 0.776 - ETA: 0s - loss: 0.4799 - acc: 0.777 - ETA: 0s - loss: 0.4820 - acc: 0.776 - ETA: 0s - loss: 0.4803 - acc: 0.778 - ETA: 0s - loss: 0.4769 - acc: 0.779 - ETA: 0s - loss: 0.4789 - acc: 0.777 - ETA: 0s - loss: 0.4775 - acc: 0.779 - ETA: 0s - loss: 0.4772 - acc: 0.779 - ETA: 0s - loss: 0.4768 - acc: 0.779 - ETA: 0s - loss: 0.4756 - acc: 0.780 - ETA: 0s - loss: 0.4758 - acc: 0.780 - ETA: 0s - loss: 0.4761 - acc: 0.779 - ETA: 0s - loss: 0.4757 - acc: 0.780 - ETA: 0s - loss: 0.4750 - acc: 0.780 - ETA: 0s - loss: 0.4757 - acc: 0.779 - ETA: 0s - loss: 0.4756 - acc: 0.779 - ETA: 0s - loss: 0.4758 - acc: 0.779 - ETA: 0s - loss: 0.4748 - acc: 0.779 - ETA: 0s - loss: 0.4749 - acc: 0.779 - 2s 51us/step - loss: 0.4748 - acc: 0.7798\n",
      "Epoch 10/20\n",
      "29603/29603 [==============================] - ETA: 1s - loss: 0.4636 - acc: 0.796 - ETA: 1s - loss: 0.4754 - acc: 0.780 - ETA: 1s - loss: 0.4644 - acc: 0.790 - ETA: 1s - loss: 0.4586 - acc: 0.795 - ETA: 1s - loss: 0.4611 - acc: 0.792 - ETA: 1s - loss: 0.4605 - acc: 0.792 - ETA: 1s - loss: 0.4561 - acc: 0.793 - ETA: 1s - loss: 0.4581 - acc: 0.791 - ETA: 0s - loss: 0.4580 - acc: 0.792 - ETA: 0s - loss: 0.4644 - acc: 0.789 - ETA: 0s - loss: 0.4642 - acc: 0.789 - ETA: 0s - loss: 0.4671 - acc: 0.786 - ETA: 0s - loss: 0.4684 - acc: 0.784 - ETA: 0s - loss: 0.4682 - acc: 0.784 - ETA: 0s - loss: 0.4684 - acc: 0.784 - ETA: 0s - loss: 0.4694 - acc: 0.783 - ETA: 0s - loss: 0.4694 - acc: 0.782 - ETA: 0s - loss: 0.4707 - acc: 0.781 - ETA: 0s - loss: 0.4701 - acc: 0.782 - ETA: 0s - loss: 0.4699 - acc: 0.781 - ETA: 0s - loss: 0.4703 - acc: 0.782 - ETA: 0s - loss: 0.4706 - acc: 0.781 - ETA: 0s - loss: 0.4712 - acc: 0.781 - ETA: 0s - loss: 0.4705 - acc: 0.782 - ETA: 0s - loss: 0.4701 - acc: 0.782 - 1s 48us/step - loss: 0.4705 - acc: 0.7820\n",
      "Epoch 11/20\n",
      "29603/29603 [==============================] - ETA: 1s - loss: 0.4589 - acc: 0.785 - ETA: 1s - loss: 0.4708 - acc: 0.782 - ETA: 1s - loss: 0.4679 - acc: 0.783 - ETA: 1s - loss: 0.4703 - acc: 0.782 - ETA: 1s - loss: 0.4720 - acc: 0.780 - ETA: 1s - loss: 0.4705 - acc: 0.781 - ETA: 1s - loss: 0.4716 - acc: 0.782 - ETA: 0s - loss: 0.4707 - acc: 0.784 - ETA: 0s - loss: 0.4700 - acc: 0.783 - ETA: 0s - loss: 0.4695 - acc: 0.782 - ETA: 0s - loss: 0.4688 - acc: 0.782 - ETA: 0s - loss: 0.4675 - acc: 0.782 - ETA: 0s - loss: 0.4688 - acc: 0.782 - ETA: 0s - loss: 0.4699 - acc: 0.781 - ETA: 0s - loss: 0.4712 - acc: 0.780 - ETA: 0s - loss: 0.4721 - acc: 0.780 - ETA: 0s - loss: 0.4727 - acc: 0.780 - ETA: 0s - loss: 0.4721 - acc: 0.780 - ETA: 0s - loss: 0.4703 - acc: 0.781 - ETA: 0s - loss: 0.4698 - acc: 0.782 - ETA: 0s - loss: 0.4687 - acc: 0.782 - ETA: 0s - loss: 0.4689 - acc: 0.781 - ETA: 0s - loss: 0.4687 - acc: 0.782 - ETA: 0s - loss: 0.4689 - acc: 0.782 - ETA: 0s - loss: 0.4682 - acc: 0.782 - 1s 47us/step - loss: 0.4688 - acc: 0.7825\n",
      "Epoch 12/20\n",
      "29603/29603 [==============================] - ETA: 1s - loss: 0.4570 - acc: 0.781 - ETA: 1s - loss: 0.4690 - acc: 0.778 - ETA: 1s - loss: 0.4584 - acc: 0.788 - ETA: 1s - loss: 0.4587 - acc: 0.784 - ETA: 1s - loss: 0.4616 - acc: 0.781 - ETA: 1s - loss: 0.4594 - acc: 0.784 - ETA: 1s - loss: 0.4573 - acc: 0.785 - ETA: 1s - loss: 0.4597 - acc: 0.785 - ETA: 0s - loss: 0.4621 - acc: 0.783 - ETA: 0s - loss: 0.4646 - acc: 0.781 - ETA: 0s - loss: 0.4655 - acc: 0.780 - ETA: 0s - loss: 0.4643 - acc: 0.782 - ETA: 0s - loss: 0.4661 - acc: 0.780 - ETA: 0s - loss: 0.4667 - acc: 0.781 - ETA: 0s - loss: 0.4653 - acc: 0.782 - ETA: 0s - loss: 0.4644 - acc: 0.782 - ETA: 0s - loss: 0.4658 - acc: 0.782 - ETA: 0s - loss: 0.4667 - acc: 0.782 - ETA: 0s - loss: 0.4664 - acc: 0.783 - ETA: 0s - loss: 0.4664 - acc: 0.782 - ETA: 0s - loss: 0.4658 - acc: 0.783 - ETA: 0s - loss: 0.4655 - acc: 0.783 - ETA: 0s - loss: 0.4658 - acc: 0.782 - ETA: 0s - loss: 0.4662 - acc: 0.782 - ETA: 0s - loss: 0.4647 - acc: 0.783 - ETA: 0s - loss: 0.4647 - acc: 0.783 - 1s 48us/step - loss: 0.4649 - acc: 0.7838\n",
      "Epoch 13/20\n",
      "29603/29603 [==============================] - ETA: 1s - loss: 0.4460 - acc: 0.796 - ETA: 1s - loss: 0.4608 - acc: 0.778 - ETA: 1s - loss: 0.4693 - acc: 0.774 - ETA: 1s - loss: 0.4659 - acc: 0.778 - ETA: 1s - loss: 0.4665 - acc: 0.778 - ETA: 1s - loss: 0.4673 - acc: 0.777 - ETA: 1s - loss: 0.4647 - acc: 0.778 - ETA: 0s - loss: 0.4642 - acc: 0.779 - ETA: 0s - loss: 0.4627 - acc: 0.782 - ETA: 0s - loss: 0.4643 - acc: 0.780 - ETA: 0s - loss: 0.4630 - acc: 0.781 - ETA: 0s - loss: 0.4646 - acc: 0.780 - ETA: 0s - loss: 0.4657 - acc: 0.781 - ETA: 0s - loss: 0.4643 - acc: 0.782 - ETA: 0s - loss: 0.4645 - acc: 0.782 - ETA: 0s - loss: 0.4644 - acc: 0.782 - ETA: 0s - loss: 0.4642 - acc: 0.782 - ETA: 0s - loss: 0.4633 - acc: 0.783 - ETA: 0s - loss: 0.4638 - acc: 0.782 - ETA: 0s - loss: 0.4634 - acc: 0.783 - ETA: 0s - loss: 0.4634 - acc: 0.782 - ETA: 0s - loss: 0.4625 - acc: 0.783 - ETA: 0s - loss: 0.4621 - acc: 0.784 - ETA: 0s - loss: 0.4604 - acc: 0.785 - ETA: 0s - loss: 0.4604 - acc: 0.785 - 1s 47us/step - loss: 0.4600 - acc: 0.7860\n",
      "Epoch 14/20\n",
      "29603/29603 [==============================] - ETA: 1s - loss: 0.4142 - acc: 0.839 - ETA: 1s - loss: 0.4516 - acc: 0.794 - ETA: 1s - loss: 0.4417 - acc: 0.799 - ETA: 1s - loss: 0.4462 - acc: 0.793 - ETA: 1s - loss: 0.4480 - acc: 0.793 - ETA: 1s - loss: 0.4515 - acc: 0.790 - ETA: 1s - loss: 0.4587 - acc: 0.787 - ETA: 0s - loss: 0.4585 - acc: 0.787 - ETA: 0s - loss: 0.4575 - acc: 0.788 - ETA: 0s - loss: 0.4554 - acc: 0.789 - ETA: 0s - loss: 0.4540 - acc: 0.790 - ETA: 0s - loss: 0.4543 - acc: 0.790 - ETA: 0s - loss: 0.4547 - acc: 0.790 - ETA: 0s - loss: 0.4536 - acc: 0.790 - ETA: 0s - loss: 0.4544 - acc: 0.790 - ETA: 0s - loss: 0.4550 - acc: 0.789 - ETA: 0s - loss: 0.4557 - acc: 0.788 - ETA: 0s - loss: 0.4566 - acc: 0.787 - ETA: 0s - loss: 0.4568 - acc: 0.787 - ETA: 0s - loss: 0.4565 - acc: 0.787 - ETA: 0s - loss: 0.4568 - acc: 0.787 - ETA: 0s - loss: 0.4561 - acc: 0.787 - ETA: 0s - loss: 0.4571 - acc: 0.786 - ETA: 0s - loss: 0.4570 - acc: 0.787 - 1s 47us/step - loss: 0.4579 - acc: 0.7861\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29603/29603 [==============================] - ETA: 1s - loss: 0.4074 - acc: 0.812 - ETA: 1s - loss: 0.4527 - acc: 0.785 - ETA: 1s - loss: 0.4578 - acc: 0.784 - ETA: 1s - loss: 0.4599 - acc: 0.778 - ETA: 1s - loss: 0.4522 - acc: 0.784 - ETA: 1s - loss: 0.4480 - acc: 0.788 - ETA: 1s - loss: 0.4445 - acc: 0.790 - ETA: 1s - loss: 0.4441 - acc: 0.791 - ETA: 0s - loss: 0.4476 - acc: 0.789 - ETA: 0s - loss: 0.4489 - acc: 0.788 - ETA: 0s - loss: 0.4516 - acc: 0.786 - ETA: 0s - loss: 0.4529 - acc: 0.785 - ETA: 0s - loss: 0.4535 - acc: 0.785 - ETA: 0s - loss: 0.4544 - acc: 0.785 - ETA: 0s - loss: 0.4553 - acc: 0.783 - ETA: 0s - loss: 0.4548 - acc: 0.784 - ETA: 0s - loss: 0.4543 - acc: 0.785 - ETA: 0s - loss: 0.4543 - acc: 0.785 - ETA: 0s - loss: 0.4538 - acc: 0.786 - ETA: 0s - loss: 0.4541 - acc: 0.786 - ETA: 0s - loss: 0.4547 - acc: 0.786 - ETA: 0s - loss: 0.4553 - acc: 0.786 - ETA: 0s - loss: 0.4542 - acc: 0.787 - ETA: 0s - loss: 0.4547 - acc: 0.786 - ETA: 0s - loss: 0.4537 - acc: 0.787 - 1s 48us/step - loss: 0.4535 - acc: 0.7877\n",
      "Epoch 16/20\n",
      "29603/29603 [==============================] - ETA: 1s - loss: 0.4270 - acc: 0.839 - ETA: 1s - loss: 0.4331 - acc: 0.809 - ETA: 1s - loss: 0.4465 - acc: 0.802 - ETA: 1s - loss: 0.4552 - acc: 0.792 - ETA: 1s - loss: 0.4599 - acc: 0.791 - ETA: 1s - loss: 0.4584 - acc: 0.791 - ETA: 1s - loss: 0.4560 - acc: 0.792 - ETA: 0s - loss: 0.4570 - acc: 0.790 - ETA: 0s - loss: 0.4572 - acc: 0.791 - ETA: 0s - loss: 0.4566 - acc: 0.791 - ETA: 0s - loss: 0.4556 - acc: 0.791 - ETA: 0s - loss: 0.4556 - acc: 0.790 - ETA: 0s - loss: 0.4538 - acc: 0.790 - ETA: 0s - loss: 0.4526 - acc: 0.792 - ETA: 0s - loss: 0.4526 - acc: 0.792 - ETA: 0s - loss: 0.4534 - acc: 0.791 - ETA: 0s - loss: 0.4545 - acc: 0.791 - ETA: 0s - loss: 0.4547 - acc: 0.791 - ETA: 0s - loss: 0.4541 - acc: 0.790 - ETA: 0s - loss: 0.4540 - acc: 0.791 - ETA: 0s - loss: 0.4543 - acc: 0.791 - ETA: 0s - loss: 0.4552 - acc: 0.790 - ETA: 0s - loss: 0.4561 - acc: 0.789 - ETA: 0s - loss: 0.4551 - acc: 0.790 - ETA: 0s - loss: 0.4550 - acc: 0.789 - 1s 47us/step - loss: 0.4546 - acc: 0.7898\n",
      "Epoch 17/20\n",
      "29603/29603 [==============================] - ETA: 1s - loss: 0.3832 - acc: 0.832 - ETA: 1s - loss: 0.4462 - acc: 0.794 - ETA: 1s - loss: 0.4492 - acc: 0.793 - ETA: 1s - loss: 0.4470 - acc: 0.794 - ETA: 1s - loss: 0.4443 - acc: 0.798 - ETA: 1s - loss: 0.4458 - acc: 0.796 - ETA: 1s - loss: 0.4441 - acc: 0.798 - ETA: 1s - loss: 0.4443 - acc: 0.796 - ETA: 0s - loss: 0.4467 - acc: 0.792 - ETA: 0s - loss: 0.4467 - acc: 0.793 - ETA: 0s - loss: 0.4465 - acc: 0.793 - ETA: 0s - loss: 0.4452 - acc: 0.793 - ETA: 0s - loss: 0.4463 - acc: 0.792 - ETA: 0s - loss: 0.4482 - acc: 0.792 - ETA: 0s - loss: 0.4500 - acc: 0.791 - ETA: 0s - loss: 0.4502 - acc: 0.791 - ETA: 0s - loss: 0.4516 - acc: 0.791 - ETA: 0s - loss: 0.4506 - acc: 0.791 - ETA: 0s - loss: 0.4500 - acc: 0.792 - ETA: 0s - loss: 0.4499 - acc: 0.792 - ETA: 0s - loss: 0.4492 - acc: 0.792 - ETA: 0s - loss: 0.4498 - acc: 0.792 - ETA: 0s - loss: 0.4500 - acc: 0.792 - ETA: 0s - loss: 0.4503 - acc: 0.792 - ETA: 0s - loss: 0.4506 - acc: 0.791 - 1s 47us/step - loss: 0.4502 - acc: 0.7921\n",
      "Epoch 18/20\n",
      "29603/29603 [==============================] - ETA: 1s - loss: 0.4907 - acc: 0.746 - ETA: 1s - loss: 0.4627 - acc: 0.778 - ETA: 1s - loss: 0.4683 - acc: 0.772 - ETA: 1s - loss: 0.4543 - acc: 0.783 - ETA: 1s - loss: 0.4514 - acc: 0.786 - ETA: 1s - loss: 0.4503 - acc: 0.787 - ETA: 1s - loss: 0.4522 - acc: 0.786 - ETA: 0s - loss: 0.4529 - acc: 0.785 - ETA: 0s - loss: 0.4525 - acc: 0.785 - ETA: 0s - loss: 0.4526 - acc: 0.786 - ETA: 0s - loss: 0.4525 - acc: 0.787 - ETA: 0s - loss: 0.4507 - acc: 0.788 - ETA: 0s - loss: 0.4489 - acc: 0.790 - ETA: 0s - loss: 0.4479 - acc: 0.790 - ETA: 0s - loss: 0.4472 - acc: 0.791 - ETA: 0s - loss: 0.4458 - acc: 0.792 - ETA: 0s - loss: 0.4471 - acc: 0.792 - ETA: 0s - loss: 0.4461 - acc: 0.792 - ETA: 0s - loss: 0.4455 - acc: 0.792 - ETA: 0s - loss: 0.4444 - acc: 0.793 - ETA: 0s - loss: 0.4440 - acc: 0.793 - ETA: 0s - loss: 0.4441 - acc: 0.793 - ETA: 0s - loss: 0.4442 - acc: 0.793 - ETA: 0s - loss: 0.4450 - acc: 0.793 - 1s 46us/step - loss: 0.4458 - acc: 0.7926\n",
      "Epoch 19/20\n",
      "29603/29603 [==============================] - ETA: 1s - loss: 0.4705 - acc: 0.757 - ETA: 1s - loss: 0.4490 - acc: 0.781 - ETA: 1s - loss: 0.4414 - acc: 0.791 - ETA: 1s - loss: 0.4449 - acc: 0.791 - ETA: 1s - loss: 0.4447 - acc: 0.793 - ETA: 1s - loss: 0.4472 - acc: 0.793 - ETA: 1s - loss: 0.4511 - acc: 0.790 - ETA: 1s - loss: 0.4527 - acc: 0.789 - ETA: 0s - loss: 0.4543 - acc: 0.789 - ETA: 0s - loss: 0.4531 - acc: 0.791 - ETA: 0s - loss: 0.4496 - acc: 0.793 - ETA: 0s - loss: 0.4494 - acc: 0.793 - ETA: 0s - loss: 0.4484 - acc: 0.793 - ETA: 0s - loss: 0.4481 - acc: 0.793 - ETA: 0s - loss: 0.4468 - acc: 0.794 - ETA: 0s - loss: 0.4474 - acc: 0.793 - ETA: 0s - loss: 0.4468 - acc: 0.794 - ETA: 0s - loss: 0.4461 - acc: 0.794 - ETA: 0s - loss: 0.4471 - acc: 0.794 - ETA: 0s - loss: 0.4475 - acc: 0.794 - ETA: 0s - loss: 0.4471 - acc: 0.794 - ETA: 0s - loss: 0.4469 - acc: 0.795 - ETA: 0s - loss: 0.4470 - acc: 0.794 - ETA: 0s - loss: 0.4453 - acc: 0.795 - ETA: 0s - loss: 0.4452 - acc: 0.795 - 1s 47us/step - loss: 0.4456 - acc: 0.7949\n",
      "Epoch 20/20\n",
      "29603/29603 [==============================] - ETA: 1s - loss: 0.4511 - acc: 0.765 - ETA: 1s - loss: 0.4474 - acc: 0.789 - ETA: 1s - loss: 0.4331 - acc: 0.800 - ETA: 1s - loss: 0.4402 - acc: 0.797 - ETA: 1s - loss: 0.4333 - acc: 0.799 - ETA: 1s - loss: 0.4347 - acc: 0.799 - ETA: 1s - loss: 0.4345 - acc: 0.799 - ETA: 0s - loss: 0.4347 - acc: 0.799 - ETA: 0s - loss: 0.4369 - acc: 0.799 - ETA: 0s - loss: 0.4367 - acc: 0.800 - ETA: 0s - loss: 0.4359 - acc: 0.800 - ETA: 0s - loss: 0.4372 - acc: 0.799 - ETA: 0s - loss: 0.4387 - acc: 0.798 - ETA: 0s - loss: 0.4375 - acc: 0.799 - ETA: 0s - loss: 0.4369 - acc: 0.800 - ETA: 0s - loss: 0.4381 - acc: 0.800 - ETA: 0s - loss: 0.4407 - acc: 0.797 - ETA: 0s - loss: 0.4412 - acc: 0.797 - ETA: 0s - loss: 0.4409 - acc: 0.797 - ETA: 0s - loss: 0.4418 - acc: 0.796 - ETA: 0s - loss: 0.4412 - acc: 0.797 - ETA: 0s - loss: 0.4424 - acc: 0.796 - ETA: 0s - loss: 0.4428 - acc: 0.796 - ETA: 0s - loss: 0.4419 - acc: 0.797 - 1s 47us/step - loss: 0.4428 - acc: 0.7965\n",
      "7401/7401 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 35us/step\n",
      "[0.4414152992390214, 0.7905688421208542]\n"
     ]
    }
   ],
   "source": [
    "# KERAS\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=256)\n",
    "score = model.evaluate(X_test, y_test, batch_size=256)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTS BEST SAMPLERS AND MODELS\n",
    "### 1. Sampler 'SMOTEEN'. Models - RandomForest, ExtraTrees, MLP\n",
    "### 2. Sampler 'Nearmiss(version = 1)'. Models - GradientBoosting, XGB, TFFM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
